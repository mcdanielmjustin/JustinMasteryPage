{
  "domain_code": "PMET",
  "domain_name": "Psychometrics & Research Methods",
  "total_questions": 191,
  "questions": [
    {
      "id": "PMET-PC-0001",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Avoidance Learning",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0194",
      "sentences": [
        "Mowrer's (1947) two-factor theory explains avoidance learning through a combination of classical and operant conditioning.",
        "In the first factor, a neutral stimulus becomes associated with an aversive event through classical conditioning, eliciting a conditioned fear response.",
        "In the second factor, the organism learns to perform a behavior that removes the conditioned fear stimulus, and this behavior is maintained through positive reinforcement.",
        "The reduction of fear serves as the reinforcing consequence that strengthens the avoidance response.",
        "One challenge for two-factor theory is explaining why avoidance behaviors are often highly resistant to extinction.",
        "Critics have noted that if the conditioned fear extinguishes over trials, the motivation for the avoidance response should also diminish.",
        "Despite these criticisms, Mowrer's two-factor theory remains one of the most influential accounts of avoidance learning in behavioral psychology."
      ],
      "target_sentence_index": 2,
      "original_sentence": "In the second factor, the organism learns to perform a behavior that removes the conditioned fear stimulus, and this behavior is maintained through negative reinforcement.",
      "error_original": "positive reinforcement",
      "error_correct": "negative reinforcement",
      "explanation": "The second factor in Mowrer's two-factor theory involves negative reinforcement, not positive reinforcement. The avoidance behavior is strengthened because it removes or reduces an aversive stimulus (the conditioned fear), which is the defining feature of negative reinforcement. Positive reinforcement involves the addition of a pleasant stimulus, which is not what occurs in avoidance learning."
    },
    {
      "id": "PMET-PC-0002",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Test-Retest Reliability",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0224",
      "sentences": [
        "Test-retest reliability (also called temporal stability) assesses the consistency of scores across time.",
        "The same test is administered to the same individuals on two occasions, and the correlation between scores indicates internal consistency."
      ],
      "target_sentence_index": 1,
      "original_sentence": "The same test is administered to the same individuals on two occasions, and the correlation between scores indicates stability.",
      "error_original": "internal consistency",
      "error_correct": "stability",
      "explanation": "The correlation obtained from test-retest reliability indicates 'stability' (i.e., temporal stability of scores over time), not 'internal consistency.' Internal consistency is a different type of reliability that assesses the degree to which items within a single test measure the same construct, typically estimated by methods such as Cronbach's alpha or split-half reliability. Test-retest reliability specifically evaluates whether scores remain stable across different time points."
    },
    {
      "id": "PMET-PC-0003",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Meta-Analysis of Validity Coefficients",
      "passage_type": "example",
      "source_passage_id": "PMET-0347",
      "sentences": [
        "Meta-analyses of cognitive ability tests show that their validity for predicting job performance generalizes across jobs and settings, with corrected validity coefficients typically ranging from .50 to .60 for overall job performance.",
        "This suggests that cognitive ability is a valid predictor across many employment contexts."
      ],
      "target_sentence_index": 0,
      "original_sentence": "Meta-analyses of cognitive ability tests show that their validity for predicting job performance generalizes across jobs and settings, with corrected validity coefficients typically ranging from .50 to .60 for overall job performance.",
      "error_original": "typically ranging from .50 to .60",
      "error_correct": "typically ranging from .30 to .50",
      "explanation": "The corrected validity coefficients for cognitive ability tests predicting overall job performance, as reported in validity generalization research (e.g., Schmidt & Hunter, 1998), typically range from approximately .30 to .50 (with values around .50-.55 at the high end for complex jobs). The passage inflated the range to .50 to .60, which overstates the typical findings in the validity generalization literature."
    },
    {
      "id": "PMET-PC-0004",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Test Length",
      "passage_type": "example",
      "source_passage_id": "PMET-0215",
      "sentences": [
        "Spearman-Brown Example: If a 20-item test has reliability of .70, doubling its length (k = 2) would yield an estimated reliability using the Spearman-Brown prophecy formula.",
        "The calculation is: (2 × .70) / [1 + (2 - 1) × .70].",
        "The numerator equals 1.40.",
        "The denominator equals 1.70.",
        "This gives an estimated reliability of .76."
      ],
      "target_sentence_index": 4,
      "original_sentence": "This gives an estimated reliability of .82.",
      "error_original": ".76",
      "error_correct": ".82",
      "explanation": "The correct result of dividing 1.40 by 1.70 is approximately .82, not .76. This is a number/value change error. The Spearman-Brown formula predicts that doubling the length of a test with reliability .70 yields an estimated reliability of .82."
    },
    {
      "id": "PMET-PC-0005",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Four Basic Schedules",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0181",
      "sentences": [
        "Reinforcement is delivered for the first response after a fixed time period has elapsed.",
        "FI schedules produce a characteristic \"scalloped\" pattern - responding is rapid immediately after reinforcement and decelerates as the interval ends.",
        "Checking the mailbox for an expected delivery is an example."
      ],
      "target_sentence_index": 1,
      "original_sentence": "FI schedules produce a characteristic \"scalloped\" pattern - responding is slow immediately after reinforcement and accelerates as the interval ends.",
      "error_original": "responding is rapid immediately after reinforcement and decelerates as the interval ends",
      "error_correct": "responding is slow immediately after reinforcement and accelerates as the interval ends",
      "explanation": "In a Fixed-Interval (FI) schedule, the characteristic 'scalloped' pattern occurs because the organism pauses or responds slowly right after reinforcement is delivered (a post-reinforcement pause), then gradually increases its response rate as the end of the interval approaches. The error reverses this pattern, incorrectly stating that responding is rapid after reinforcement and decelerates toward the end of the interval."
    },
    {
      "id": "PMET-PC-0006",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Test Content",
      "passage_type": "example",
      "source_passage_id": "PMET-0344",
      "sentences": [
        "A licensing exam for clinical psychologists should sample knowledge across relevant domains: assessment, diagnosis, treatment, ethics, and research methods.",
        "If the exam heavily emphasizes one area while neglecting others, it would have poor construct validity for measuring overall competence in clinical psychology."
      ],
      "target_sentence_index": 1,
      "original_sentence": "If the exam heavily emphasizes one area while neglecting others, it would have poor content validity for measuring overall competence in clinical psychology.",
      "error_original": "poor construct validity",
      "error_correct": "poor content validity",
      "explanation": "The passage is specifically discussing content validity, which concerns whether a test adequately samples the full domain of content it is intended to measure. The error substitutes 'construct validity' for 'content validity.' Construct validity refers to whether a test measures the theoretical construct it claims to measure, which is a related but distinct concept. In this example, the issue is that the exam fails to adequately sample all relevant content areas, which is a content validity problem."
    },
    {
      "id": "PMET-PC-0007",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Internal Consistency Reliability",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0230",
      "sentences": [
        "The Kuder-Richardson formulas are special cases of Cronbach's alpha for dichotomously scored items (correct/incorrect).",
        "KR-20 is used when item difficulties are equal, while KR-21 assumes varying item difficulties (Kuder & Richardson, 1937)."
      ],
      "target_sentence_index": 1,
      "original_sentence": "KR-20 is used when item difficulties vary, while KR-21 assumes equal item difficulties (Kuder & Richardson, 1937).",
      "error_original": "KR-20 is used when item difficulties are equal, while KR-21 assumes varying item difficulties",
      "error_correct": "KR-20 is used when item difficulties vary, while KR-21 assumes equal item difficulties",
      "explanation": "The descriptions of KR-20 and KR-21 have been swapped. In reality, KR-20 is the more general formula used when item difficulties vary across items, while KR-21 is a simplified version that assumes all items have equal difficulty levels. KR-21 is computationally easier but less accurate when item difficulties differ."
    },
    {
      "id": "PMET-PC-0008",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Restriction of Range",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0053",
      "sentences": [
        "Restriction of range occurs when the variability in one or both variables is limited compared to the full population.",
        "This artificial restriction inflates (increases) the observed correlation coefficient.",
        "When range is restricted, correlations appear weaker than they would be in the unrestricted population (Sackett & Yang, 2000)."
      ],
      "target_sentence_index": 1,
      "original_sentence": "This artificial restriction attenuates (reduces) the observed correlation coefficient.",
      "error_original": "inflates (increases)",
      "error_correct": "attenuates (reduces)",
      "explanation": "Restriction of range attenuates (reduces) the observed correlation coefficient, making it appear weaker than it truly is. The error reverses this effect by stating that restriction of range 'inflates (increases)' the correlation, which contradicts the well-established psychometric principle that limited variability in scores leads to an underestimation of the true correlation."
    },
    {
      "id": "PMET-PC-0009",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0445",
      "sentences": [
        "Selection ratio refers to the proportion of applicants who are selected.",
        "A higher selection ratio allows an organization to be more selective by choosing only top scorers.",
        "This increased selectivity typically improves the predictive validity of the selection procedure."
      ],
      "target_sentence_index": 1,
      "original_sentence": "A lower selection ratio allows an organization to be more selective by choosing only top scorers.",
      "error_original": "higher selection ratio",
      "error_correct": "lower selection ratio",
      "explanation": "A lower selection ratio (e.g., selecting 5 out of 100 applicants = 0.05) means fewer people are selected, allowing the organization to choose only the top scorers. A higher selection ratio means more applicants are selected, which reduces selectivity. This is a concept reversal error."
    },
    {
      "id": "PMET-PC-0010",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Survey Research",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0272",
      "sentences": [
        "Survey research uses questionnaires or interviews to collect data from a sample, typically to describe characteristics of a population or examine relationships between variables.",
        "Representative sampling is critical for generalizing survey results to the sample of interest."
      ],
      "target_sentence_index": 1,
      "original_sentence": "Representative sampling is critical for generalizing survey results to the population of interest.",
      "error_original": "to the sample of interest",
      "error_correct": "to the population of interest",
      "explanation": "The error substitutes 'sample' for 'population.' The purpose of representative sampling is to allow generalization from the sample to the broader population, not back to the sample itself. The sample is what is directly studied; the population is the larger group to which findings are generalized."
    },
    {
      "id": "PMET-PC-0011",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Cohen's d",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0149",
      "sentences": [
        "Cohen's d measures the standardized difference between two means, expressed in standard deviation units.",
        "It is commonly used with t-tests to quantify effect size.",
        "The formula divides the difference between the two group means by the pooled standard error.",
        "According to Cohen's guidelines, a d of 0.2 is considered small, 0.5 is medium, and 0.8 is large.",
        "Unlike statistical significance, Cohen's d provides information about the practical magnitude of an effect.",
        "A larger Cohen's d indicates greater separation between the two group distributions."
      ],
      "target_sentence_index": 2,
      "original_sentence": "The formula divides the difference between the two group means by the pooled standard deviation.",
      "error_original": "pooled standard error",
      "error_correct": "pooled standard deviation",
      "explanation": "Cohen's d is calculated by dividing the difference between two group means by the pooled standard deviation, not the pooled standard error. The standard error reflects variability of a sampling distribution and is used in computing t-statistics, whereas Cohen's d specifically uses the pooled standard deviation to express the mean difference in standard deviation units."
    },
    {
      "id": "PMET-PC-0012",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Classical Test Theory",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0219",
      "sentences": [
        "Classical Test Theory provides the theoretical foundation for understanding reliability.",
        "The core assumption is that any observed score (X) consists of three components: a true score (T) and error (E)."
      ],
      "target_sentence_index": 1,
      "original_sentence": "The core assumption is that any observed score (X) consists of two components: a true score (T) and error (E).",
      "error_original": "three components",
      "error_correct": "two components",
      "explanation": "In Classical Test Theory, the observed score (X) is composed of exactly two components: the true score (T) and measurement error (E), expressed as X = T + E. The modified sentence incorrectly states 'three components' instead of 'two components.'"
    },
    {
      "id": "PMET-PC-0013",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0453",
      "sentences": [
        "Meehl, P. E., & Rosen, A. (1955).",
        "Antecedent probability and the efficiency of psychometric signs, patterns, or cutting scores.",
        "Psychological Review, 52(3), 194-216."
      ],
      "target_sentence_index": 2,
      "original_sentence": "Psychological Bulletin, 52(3), 194-216.",
      "error_original": "Psychological Review",
      "error_correct": "Psychological Bulletin",
      "explanation": "The seminal Meehl and Rosen (1955) article on antecedent probability and the efficiency of psychometric signs was published in the Psychological Bulletin, not the Psychological Review. These are two distinct APA journals, and misattributing the publication source is a common error."
    },
    {
      "id": "PMET-PC-0014",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Unreliability",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0099",
      "sentences": [
        "Unreliability in either the predictor or criterion inflates the validity coefficient.",
        "The correction for attenuation formula estimates what validity would be if both measures were perfectly reliable."
      ],
      "target_sentence_index": 0,
      "original_sentence": "Unreliability in either the predictor or criterion attenuates the validity coefficient.",
      "error_original": "inflates",
      "error_correct": "attenuates",
      "explanation": "Unreliability in the predictor or criterion attenuates (reduces) the validity coefficient, not inflates it. This is a concept reversal error. The correction for attenuation formula exists precisely because unreliability causes an underestimate of the true relationship between predictor and criterion."
    },
    {
      "id": "PMET-PC-0015",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Response Processes",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0341",
      "sentences": [
        "Response Process Investigation: If a reading comprehension test is designed to measure inference-making, validity evidence should demonstrate that examinees actually make inferences to answer questions.",
        "This evidence should confirm that examinees are not relying on prior knowledge or test-taking strategies that bypass the intended processes.",
        "Methods such as think-aloud protocols and eye-tracking studies are commonly used to gather evidence based on response processes.",
        "This type of evidence is most closely associated with criterion validity in the Standards for Educational and Psychological Testing framework.",
        "Without response process evidence, there is a risk that test scores reflect construct-irrelevant variance rather than the targeted cognitive skill."
      ],
      "target_sentence_index": 3,
      "original_sentence": "This type of evidence is most closely associated with construct validity in the Standards for Educational and Psychological Testing framework.",
      "error_original": "criterion validity",
      "error_correct": "construct validity",
      "explanation": "Evidence based on response processes is a source of construct validity evidence, not criterion validity evidence. According to the Standards for Educational and Psychological Testing, response process evidence helps establish that examinees are engaging in the cognitive processes the test is intended to measure, which is fundamentally a construct validity concern. Criterion validity, by contrast, involves the relationship between test scores and an external criterion measure."
    },
    {
      "id": "PMET-PC-0016",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Major Qualitative Traditions",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0286",
      "sentences": [
        "The case study approach involves in-depth examination of a single case or a small number of cases.",
        "A \"case\" can be an individual, a program, an organization, or an event.",
        "Case studies use multiple data sources (interviews, documents, observations) and are valuable for exploring complex phenomena in real-world contexts.",
        "They provide rich detail but have limited internal validity."
      ],
      "target_sentence_index": 3,
      "original_sentence": "They provide rich detail but have limited generalizability.",
      "error_original": "limited internal validity",
      "error_correct": "limited generalizability",
      "explanation": "The original passage correctly states that case studies have limited generalizability, which is the well-known trade-off of this approach — the rich, in-depth detail comes at the cost of being difficult to generalize to other cases or populations. The error substitutes 'internal validity' for 'generalizability' (which relates to external validity). Case studies are actually often noted for their strength in exploring real-world contexts with depth, but their primary limitation is generalizability, not internal validity."
    },
    {
      "id": "PMET-PC-0017",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Standard Error of Measurement",
      "passage_type": "definition",
      "source_passage_id": "PMET-0207",
      "sentences": [
        "The Standard Error of Measurement (SEM) is calculated using a specific formula.",
        "SEM = SD × √(1 - rxx).",
        "In this formula, SD represents the standard deviation of test scores.",
        "The term rxx refers to the validity coefficient."
      ],
      "target_sentence_index": 3,
      "original_sentence": "The term rxx refers to the reliability coefficient.",
      "error_original": "validity coefficient",
      "error_correct": "reliability coefficient",
      "explanation": "The passage incorrectly refers to rxx as the 'validity coefficient.' In the SEM formula, rxx represents the reliability coefficient of the test, not the validity coefficient. Reliability refers to the consistency of measurement, whereas validity refers to how well a test measures what it is intended to measure. These are distinct psychometric properties."
    },
    {
      "id": "PMET-PC-0018",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0540",
      "sentences": [
        "Grounded theory generates theory from data, as originally described by Glaser and Strauss.",
        "The approach uses three main coding stages: open, axial, and selective coding.",
        "Data collection in grounded theory is guided by theoretical sampling, where subsequent participants are selected based on emerging categories.",
        "The process continues until theoretical saturation is reached, meaning no new themes or categories emerge from additional data.",
        "A key feature of grounded theory is that the researcher begins with a pre-established hypothesis and then collects data to confirm or disconfirm it."
      ],
      "target_sentence_index": 4,
      "original_sentence": "A key feature of grounded theory is that the researcher begins without a pre-established hypothesis and allows theory to emerge from the data.",
      "error_original": "begins with a pre-established hypothesis and then collects data to confirm or disconfirm it",
      "error_correct": "begins without a pre-established hypothesis and allows theory to emerge from the data",
      "explanation": "Grounded theory is an inductive methodology in which theory emerges from the data rather than starting with a pre-established hypothesis. The error reverses this core concept, incorrectly stating that the researcher begins with a hypothesis—a deductive approach that is the opposite of what grounded theory entails."
    },
    {
      "id": "PMET-PC-0019",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Discriminative Stimuli",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0187",
      "sentences": [
        "A discriminative stimulus (S D) is a stimulus in the presence of which a particular response has been reinforced.",
        "Through experience, organisms learn to respond similarly in the presence versus absence of the S D.",
        "The S D does not elicit the response (as in classical conditioning) but rather sets the occasion for the response by signaling reinforcement availability."
      ],
      "target_sentence_index": 1,
      "original_sentence": "Through experience, organisms learn to respond differently in the presence versus absence of the S D.",
      "error_original": "respond similarly",
      "error_correct": "respond differently",
      "explanation": "The concept of a discriminative stimulus is that organisms learn to respond DIFFERENTLY depending on whether the S D is present or absent. The error reverses this core concept by stating they respond 'similarly,' which contradicts the fundamental principle of stimulus discrimination in operant conditioning."
    },
    {
      "id": "PMET-PC-0020",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "One-Sample t-Test",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0137",
      "sentences": [
        "The one-sample t-test is used to compare a sample mean to a known or hypothesized population value.",
        "For example, it can be used to test whether a sample's IQ differs from the population median of 100."
      ],
      "target_sentence_index": 1,
      "original_sentence": "For example, it can be used to test whether a sample's IQ differs from the population mean of 100.",
      "error_original": "population median of 100",
      "error_correct": "population mean of 100",
      "explanation": "The one-sample t-test compares a sample mean to a known or hypothesized population mean, not the population median. The population mean for IQ is 100. A test that compares to a median would be a different nonparametric procedure (e.g., the sign test or Wilcoxon signed-rank test)."
    },
    {
      "id": "PMET-PC-0021",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Relations to Other Variables",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0358",
      "sentences": [
        "Convergent validity is demonstrated when a test correlates strongly with other measures of the same or similar constructs.",
        "High correlations with conceptually unrelated measures support the interpretation that the test measures the intended construct."
      ],
      "target_sentence_index": 1,
      "original_sentence": "High correlations with conceptually related measures support the interpretation that the test measures the intended construct.",
      "error_original": "conceptually unrelated measures",
      "error_correct": "conceptually related measures",
      "explanation": "The passage incorrectly states 'conceptually unrelated measures' when it should say 'conceptually related measures.' Convergent validity is supported when a test correlates highly with measures of related or similar constructs. High correlations with conceptually unrelated measures would actually be evidence against construct validity (and would undermine discriminant validity). The defining feature of convergent validity is strong associations with theoretically related measures."
    },
    {
      "id": "PMET-PC-0022",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Decision Theory and Cutoff Scores",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0108",
      "sentences": [
        "When tests are used to make categorical decisions (select/reject, diagnose/not diagnose), a cutoff score must be established.",
        "The choice of cutoff involves trade-offs between different types of errors.",
        "Raising the cutoff score on a selection test will decrease the number of false positives but increase the number of false negatives.",
        "Lowering the cutoff score will have the opposite effect, increasing false positives while decreasing false negatives.",
        "The base rate, which refers to the proportion of applicants who would succeed on the criterion without any selection, influences the utility of the test.",
        "When the base rate is extremely high or extremely low, the test adds the most incremental validity to the selection process.",
        "The Taylor-Russell tables demonstrate how the selection ratio, base rate, and test validity interact to determine the proportion of correct decisions."
      ],
      "target_sentence_index": 5,
      "original_sentence": "When the base rate is extremely high or extremely low, the test adds the least incremental validity to the selection process.",
      "error_original": "the test adds the most incremental validity",
      "error_correct": "the test adds the least incremental validity",
      "explanation": "When the base rate is extremely high or extremely low, a selection test adds the least incremental validity because decisions could already be made with reasonable accuracy without the test. A test is most useful when the base rate is moderate (around .50), as there is the most room for the test to improve upon chance-level prediction. The error reverses 'least' to 'most,' representing a concept reversal."
    },
    {
      "id": "PMET-PC-0023",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Types of Bias",
      "passage_type": "definition",
      "source_passage_id": "PMET-0339",
      "sentences": [
        "Bias is a statistical/psychometric concept referring to systematic measurement error.",
        "Fairness is a broader concept encompassing social values and judgments about appropriate test use.",
        "A test can be psychometrically unbiased yet still raise fairness concerns if its use produces random impact on protected groups."
      ],
      "target_sentence_index": 2,
      "original_sentence": "A test can be psychometrically unbiased yet still raise fairness concerns if its use produces adverse impact on protected groups.",
      "error_original": "random impact",
      "error_correct": "adverse impact",
      "explanation": "The original passage correctly uses the term 'adverse impact,' which is the established legal and psychometric term referring to a substantially different selection rate for members of a protected group. 'Random impact' is not a recognized concept in this context. Adverse impact is specifically defined (e.g., by the EEOC's four-fifths rule) and is the key concern when a psychometrically unbiased test still raises fairness issues."
    },
    {
      "id": "PMET-PC-0024",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Relations to Other Variables",
      "passage_type": "definition",
      "source_passage_id": "PMET-0338",
      "sentences": [
        "Campbell and Fiske (1959) introduced the multitrait-multimethod (MTMM) matrix to evaluate both convergent and discriminant validity simultaneously.",
        "The matrix examines correlations among multiple traits measured by multiple methods.",
        "Convergent validity is indicated by high correlations between the same trait measured by different methods, known as monotrait-heteromethod correlations.",
        "Discriminant validity is indicated by lower correlations between different traits, even when measured by different methods, known as heterotrait-monomethod correlations."
      ],
      "target_sentence_index": 3,
      "original_sentence": "Discriminant validity is indicated by lower correlations between different traits, even when measured by the same method, known as heterotrait-monomethod correlations.",
      "error_original": "even when measured by different methods",
      "error_correct": "even when measured by the same method",
      "explanation": "Discriminant validity in the MTMM matrix is demonstrated by low correlations between different traits measured by the SAME method (heterotrait-monomethod block). The error changes 'same method' to 'different methods,' which misrepresents the concept. The key insight of discriminant validity is that correlations should remain low between different traits even when method variance is shared (i.e., the same method is used), showing that the traits are truly distinct."
    },
    {
      "id": "PMET-PC-0025",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Three-Term Contingency",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0189",
      "sentences": [
        "The relationship between discriminative stimuli, behavior, and consequences is captured in the three-term contingency.",
        "This framework is also commonly referred to as the ABC model, where A stands for Antecedent, B stands for Behavior, and C stands for Consequence.",
        "The antecedent is the discriminative stimulus that signals the availability of reinforcement for a particular response.",
        "The behavior is the operant response emitted by the organism in the presence of that stimulus.",
        "The consequence is what follows the behavior, such as reinforcement or punishment, and it decreases the future probability of that behavior occurring under similar antecedent conditions.",
        "This three-term contingency forms the foundational unit of analysis in operant conditioning as described by B.F. Skinner."
      ],
      "target_sentence_index": 4,
      "original_sentence": "The consequence is what follows the behavior, such as reinforcement or punishment, and it affects the future probability of that behavior occurring under similar antecedent conditions.",
      "error_original": "decreases the future probability",
      "error_correct": "affects the future probability",
      "explanation": "The consequence in the three-term contingency can either increase or decrease the future probability of behavior depending on whether it involves reinforcement or punishment. Saying it 'decreases' the probability is incorrect because reinforcement increases the probability while punishment decreases it. The correct phrasing should be 'affects' (or 'influences') the future probability, since the direction of change depends on the type of consequence."
    },
    {
      "id": "PMET-PC-0026",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Construct Validity in Research",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0322",
      "sentences": [
        "Construct validity in research refers to the degree to which the independent and dependent variables represent the theoretical constructs of interest.",
        "This is distinct from test validity (covered in later chapters) and addresses whether the operational definitions used in research adequately capture the unintended constructs."
      ],
      "target_sentence_index": 1,
      "original_sentence": "This is distinct from test validity (covered in later chapters) and addresses whether the operational definitions used in research adequately capture the intended constructs.",
      "error_original": "unintended constructs",
      "error_correct": "intended constructs",
      "explanation": "The passage was modified to say 'unintended constructs' instead of 'intended constructs.' Construct validity in research concerns whether the operational definitions adequately capture the constructs the researcher intends to study, not unintended ones. This is a concept reversal error that changes the meaning of the sentence to the opposite of what is correct."
    },
    {
      "id": "PMET-PC-0027",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Clinical Utility",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0107",
      "sentences": [
        "Clinical utility refers to the practical value of a test in clinical settings—whether using the test improves patient outcomes or clinical decision-making.",
        "A test may have adequate reliability evidence but limited clinical utility if it doesn't lead to better decisions or outcomes."
      ],
      "target_sentence_index": 1,
      "original_sentence": "A test may have adequate validity evidence but limited clinical utility if it doesn't lead to better decisions or outcomes.",
      "error_original": "reliability evidence",
      "error_correct": "validity evidence",
      "explanation": "The original passage states that a test may have adequate 'validity evidence' but limited clinical utility. The error substitutes 'reliability evidence' for 'validity evidence.' While reliability is a necessary condition for validity, the passage specifically discusses criterion-related validity and the distinction between having sufficient validity evidence and actually being useful in clinical practice. The correct term here is 'validity evidence.'"
    },
    {
      "id": "PMET-PC-0028",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Introduction to Research Designs",
      "passage_type": "definition",
      "source_passage_id": "PMET-0239",
      "sentences": [
        "True Experiment: A design with random assignment, manipulation of the independent variable, and control of extraneous variables.",
        "Quasi-Experiment: A design lacking random assignment but including manipulation of the independent variable.",
        "Random Assignment: Procedure ensuring each participant has an equal chance of being assigned to any condition.",
        "Control Group: A comparison condition that does not receive the experimental treatment.",
        "Between-Subjects Design: Same participants tested in all conditions.",
        "Within-Subjects Design: Same participants tested in all conditions."
      ],
      "target_sentence_index": 4,
      "original_sentence": "Between-Subjects Design: Different participants in each condition.",
      "error_original": "Same participants tested in all conditions",
      "error_correct": "Different participants in each condition",
      "explanation": "The definition given for Between-Subjects Design is actually the definition of a Within-Subjects Design. In a between-subjects design, different participants are assigned to each condition, whereas in a within-subjects design, the same participants are tested across all conditions."
    },
    {
      "id": "PMET-PC-0029",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "ABAB (Withdrawal/Reversal) Design",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0266",
      "sentences": [
        "The ABAB design strengthens causal inference by adding withdrawal and reintroduction phases.",
        "The sequence is: baseline (A) → treatment (B) → withdrawal (A) → treatment (B).",
        "If behavior improves during treatment phases and deteriorates during withdrawal phases, this provides compelling evidence that the treatment is correlated with the change."
      ],
      "target_sentence_index": 2,
      "original_sentence": "If behavior improves during treatment phases and deteriorates during withdrawal phases, this provides compelling evidence that the treatment caused the change.",
      "error_original": "correlated with the change",
      "error_correct": "caused the change",
      "explanation": "The ABAB design is specifically used to strengthen causal inference, not merely correlational evidence. The repeated withdrawal and reintroduction of treatment allows researchers to demonstrate a causal relationship between the treatment and the observed behavior change. Saying the evidence supports 'correlation' rather than 'causation' undermines the fundamental purpose and strength of this experimental single-subject design."
    },
    {
      "id": "PMET-PC-0030",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Item Parameters",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0522",
      "sentences": [
        "IRT models characterize items using up to three parameters.",
        "The a-parameter represents item difficulty, indicating the point on the latent trait continuum where the probability of a correct response is 0.50.",
        "The b-parameter represents item discrimination, reflecting how well an item differentiates between examinees at different ability levels.",
        "Higher discrimination values produce steeper item characteristic curves.",
        "The c-parameter, sometimes called the pseudo-guessing parameter, represents the lower asymptote of the item characteristic curve.",
        "In the three-parameter logistic model, all three parameters are estimated, while the one-parameter Rasch model constrains discrimination to be equal across items and fixes the c-parameter at zero."
      ],
      "target_sentence_index": 1,
      "original_sentence": "The b-parameter represents item difficulty, indicating the point on the latent trait continuum where the probability of a correct response is 0.50.",
      "error_original": "The a-parameter represents item difficulty",
      "error_correct": "The b-parameter represents item difficulty",
      "explanation": "In IRT, the b-parameter (not the a-parameter) represents item difficulty. The a-parameter represents item discrimination. The passage swapped the labels of the a- and b-parameters in sentences 1 and 2, assigning difficulty to 'a' and discrimination to 'b' when it should be the reverse."
    },
    {
      "id": "PMET-PC-0031",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0557",
      "sentences": [
        "Kline, R. B. (2015).",
        "Principles and practice of structural equation modeling (3rd ed.).",
        "Guilford Press."
      ],
      "target_sentence_index": 1,
      "original_sentence": "Principles and practice of structural equation modeling (4th ed.).",
      "error_original": "3rd ed.",
      "error_correct": "4th ed.",
      "explanation": "The 2015 publication of Kline's 'Principles and Practice of Structural Equation Modeling' is the 4th edition, not the 3rd edition. The 3rd edition was published in 2011. This is a number/value change error."
    },
    {
      "id": "PMET-PC-0032",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Percentiles and Percentile Ranks",
      "passage_type": "definition",
      "source_passage_id": "PMET-0375",
      "sentences": [
        "Special percentiles are known as quartiles, which divide a distribution into four equal parts.",
        "Q1, the 25th percentile, is the first quartile, meaning 25% of scores fall below this point.",
        "Q2, the 50th percentile, is the second quartile and corresponds to the mode of the distribution.",
        "Q3, the 75th percentile, is the third quartile, meaning 75% of scores fall below this point.",
        "The Interquartile Range (IQR) is calculated as Q3 minus Q1 and contains the middle 50% of scores."
      ],
      "target_sentence_index": 2,
      "original_sentence": "Q2, the 50th percentile, is the second quartile and corresponds to the median of the distribution.",
      "error_original": "the mode",
      "error_correct": "the median",
      "explanation": "Q2, the 50th percentile, corresponds to the median (the middle value of a distribution), not the mode (the most frequently occurring value). The passage originally correctly identifies Q2 as the median."
    },
    {
      "id": "PMET-PC-0033",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Introduction to Correlation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0043",
      "sentences": [
        "Correlation is a statistical technique used to describe and measure the relationship between two variables.",
        "Understanding correlation is essential for psychological research and practice, as it underlies many assessment concepts including test reliability and validity.",
        "The correlation coefficient quantifies both the strength and direction of the curvilinear relationship between two variables."
      ],
      "target_sentence_index": 2,
      "original_sentence": "The correlation coefficient quantifies both the strength and direction of the linear relationship between two variables.",
      "error_original": "curvilinear relationship",
      "error_correct": "linear relationship",
      "explanation": "The correlation coefficient (specifically Pearson's r) measures the strength and direction of the LINEAR relationship between two variables, not the curvilinear relationship. A standard correlation coefficient can actually miss or underestimate curvilinear relationships, which is why it is specifically described as a measure of linear association."
    },
    {
      "id": "PMET-PC-0034",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0560",
      "sentences": [
        "Solomon, R. L. (1949).",
        "An extension of control group design.",
        "Psychological Review, 46(2), 137-150."
      ],
      "target_sentence_index": 2,
      "original_sentence": "Psychological Bulletin, 46(2), 137-150.",
      "error_original": "Psychological Review",
      "error_correct": "Psychological Bulletin",
      "explanation": "Solomon's classic 1949 paper on the Solomon Four-Group Design was published in the Psychological Bulletin, not the Psychological Review. This is a journal name substitution error, swapping one well-known APA journal for another."
    },
    {
      "id": "PMET-PC-0035",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Incremental Validity",
      "passage_type": "example",
      "source_passage_id": "PMET-0090",
      "sentences": [
        "A clinician wants to know if a new personality measure adds predictive value for therapy outcome beyond established predictors (severity, chronicity, social support).",
        "Existing predictors account for 25% of outcome variance.",
        "Adding the personality measure increases this to 32%.",
        "The 7% increase (ΔR² = .07) represents the construct validity of the new measure."
      ],
      "target_sentence_index": 3,
      "original_sentence": "The 7% increase (ΔR² = .07) represents the incremental validity of the new measure.",
      "error_original": "construct validity",
      "error_correct": "incremental validity",
      "explanation": "The passage is specifically about incremental validity, which refers to the degree to which a new measure improves prediction beyond existing predictors. The error substituted 'construct validity' (which refers to whether a test measures the theoretical construct it is intended to measure) for 'incremental validity.' The 7% increase in variance accounted for (ΔR² = .07) is the classic definition of incremental validity, not construct validity."
    },
    {
      "id": "PMET-PC-0036",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Strength of Correlation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0050",
      "sentences": [
        "However, these guidelines should be interpreted in context.",
        "In some research areas (e.g., personality psychology), correlations of .30 are considered substantial, while in others (e.g., test-retest reliability), correlations below .60 might be considered inadequate."
      ],
      "target_sentence_index": 1,
      "original_sentence": "In some research areas (e.g., personality psychology), correlations of .30 are considered substantial, while in others (e.g., test-retest reliability), correlations below .80 might be considered inadequate.",
      "error_original": "correlations below .60 might be considered inadequate",
      "error_correct": "correlations below .80 might be considered inadequate",
      "explanation": "The passage incorrectly states that correlations below .60 might be considered inadequate for test-retest reliability. The correct threshold is .80. In psychometrics, test-retest reliability is expected to be quite high, and correlations below .80 are generally considered inadequate for this purpose."
    },
    {
      "id": "PMET-PC-0037",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0420",
      "sentences": [
        "Preparedness refers to biological constraints on learning.",
        "It explains why evolutionary-relevant fears are acquired faster than other associations.",
        "These biologically prepared fears also extinguish more quickly than non-prepared associations."
      ],
      "target_sentence_index": 2,
      "original_sentence": "These biologically prepared fears also extinguish more slowly than non-prepared associations.",
      "error_original": "extinguish more quickly",
      "error_correct": "extinguish more slowly",
      "explanation": "According to the concept of preparedness, evolutionary-relevant fears not only are acquired faster but also extinguish more slowly (not more quickly) than non-prepared associations. This is a concept reversal error — prepared fears are notably resistant to extinction, which is a key feature of the preparedness theory proposed by Seligman."
    },
    {
      "id": "PMET-PC-0038",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Nonprobability Sampling",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0281",
      "sentences": [
        "Nonprobability sampling methods do not ensure every member has a chance of selection.",
        "These include convenience sampling (readily available participants), purposive sampling (participants selected for specific characteristics), and snowball sampling (participants are randomly assigned by other participants)."
      ],
      "target_sentence_index": 1,
      "original_sentence": "These include convenience sampling (readily available participants), purposive sampling (participants selected for specific characteristics), and snowball sampling (participants recruit other participants).",
      "error_original": "participants are randomly assigned by other participants",
      "error_correct": "participants recruit other participants",
      "explanation": "Snowball sampling involves existing participants recruiting additional participants from among their acquaintances, not randomly assigning them. The error substitutes 'randomly assigned by' for 'recruit,' which incorrectly introduces a concept from experimental design (random assignment) into a description of a nonprobability sampling technique."
    },
    {
      "id": "PMET-PC-0039",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "CR versus UR",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0424",
      "sentences": [
        "While the conditioned response often resembles the unconditioned response, they are not identical.",
        "The CR is typically weaker in magnitude compared to the UR.",
        "The CR usually has a longer latency, meaning it takes more time to appear after the stimulus is presented.",
        "Additionally, the CR may sometimes be opposite in direction to the UR, a phenomenon known as compensatory conditioning.",
        "For example, if a drug (US) causes a decrease in pain sensitivity, the CR to cues associated with the drug may be an increase in pain sensitivity.",
        "This compensatory response mechanism was first extensively studied by Robert Rescorla as part of his opponent-process theory of conditioning."
      ],
      "target_sentence_index": 5,
      "original_sentence": "This compensatory response mechanism was first extensively studied by Shepard Siegel as part of his opponent-process theory of conditioning.",
      "error_original": "Robert Rescorla",
      "error_correct": "Shepard Siegel",
      "explanation": "The compensatory response model of drug tolerance through classical conditioning was extensively studied by Shepard Siegel, not Robert Rescorla. Rescorla is best known for the Rescorla-Wagner model of associative learning, while Siegel's work focused on how conditioned compensatory responses explain drug tolerance and overdose."
    },
    {
      "id": "PMET-PC-0040",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Null and Alternative Hypotheses",
      "passage_type": "example",
      "source_passage_id": "PMET-0118",
      "sentences": [
        "A researcher wants to test whether a new therapy reduces depression scores compared to a control condition.",
        "H₀: There is no difference in depression scores between the therapy and control groups (μ₁ = μ₂).",
        "H₁: The therapy group has lower depression scores than the control group (μ₁ < μ₂) — this is a directional/one-tailed hypothesis.",
        "Alternatively, H₁: There is a difference in depression scores between groups (μ₁ ≠ μ₂) — this is a directional/two-tailed hypothesis."
      ],
      "target_sentence_index": 3,
      "original_sentence": "Alternatively, H₁: There is a difference in depression scores between groups (μ₁ ≠ μ₂) — this is a non-directional/two-tailed hypothesis.",
      "error_original": "directional/two-tailed",
      "error_correct": "non-directional/two-tailed",
      "explanation": "A two-tailed hypothesis that simply states there is a difference between groups (μ₁ ≠ μ₂) without specifying the direction of the difference is classified as non-directional, not directional. The directional hypothesis is the one-tailed version (μ₁ < μ₂), which specifies which group is expected to score higher or lower."
    },
    {
      "id": "PMET-PC-0041",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Alternate (Parallel) Forms Reliability",
      "passage_type": "example",
      "source_passage_id": "PMET-0213",
      "sentences": [
        "A publisher develops Form A and Form B of an achievement test, each with different items measuring the same content domain.",
        "Students take Form A one week and Form B the next.",
        "A correlation of .85 between forms indicates good alternate forms reliability, suggesting the two versions measure different constructs consistently."
      ],
      "target_sentence_index": 2,
      "original_sentence": "A correlation of .85 between forms indicates good alternate forms reliability, suggesting the two versions measure the same construct consistently.",
      "error_original": "different constructs",
      "error_correct": "the same construct",
      "explanation": "The error introduces a concept reversal. A high correlation between alternate forms (.85) indicates that the two versions measure the same construct consistently, not different constructs. The entire purpose of alternate forms reliability is to demonstrate that both forms are equivalent measures of the same underlying construct."
    },
    {
      "id": "PMET-PC-0042",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0512",
      "sentences": [
        "Item Characteristic Curves are J-shaped curves showing the probability of a correct response as a function of theta (ability).",
        "In Item Response Theory, theta represents the latent trait or ability level of the examinee.",
        "As theta increases, the probability of answering an item correctly also increases, producing the characteristic curve shape.",
        "The three main parameters that can define an ICC are item difficulty, item discrimination, and a guessing parameter."
      ],
      "target_sentence_index": 0,
      "original_sentence": "Item Characteristic Curves are S-shaped curves showing the probability of a correct response as a function of theta (ability).",
      "error_original": "J-shaped curves",
      "error_correct": "S-shaped curves",
      "explanation": "Item Characteristic Curves (ICCs) in Item Response Theory are S-shaped (sigmoid/ogive) curves, not J-shaped curves. The S-shape reflects the logistic function where the probability of a correct response starts near zero (or the guessing parameter) at low ability levels, increases steeply around the item's difficulty level, and asymptotically approaches 1.0 at high ability levels."
    },
    {
      "id": "PMET-PC-0043",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0576",
      "sentences": [
        "Test bias is random measurement error for specific groups.",
        "Fairness involves value judgments about test use."
      ],
      "target_sentence_index": 0,
      "original_sentence": "Test bias is systematic measurement error for specific groups.",
      "error_original": "random measurement error",
      "error_correct": "systematic measurement error",
      "explanation": "Test bias refers to systematic (not random) measurement error that consistently affects specific groups. Random error is unsystematic and affects all examinees equally, whereas bias is a consistent, directional distortion in scores for a particular group. This is a concept reversal error substituting 'random' for 'systematic.'"
    },
    {
      "id": "PMET-PC-0044",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Blocking",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0018",
      "sentences": [
        "The blocking phenomenon, discovered by Leon Kamin (1969), demonstrated that mere temporal contiguity between CS and US is insufficient for conditioning.",
        "In a blocking experiment, a CS1 is first paired with a US until a strong conditioned response is established.",
        "In the second phase, CS1 is presented together with a novel CS2, and this compound stimulus is paired with the same US.",
        "Despite repeated pairings of CS2 with the US, subsequent testing reveals that CS2 elicits little or no conditioned response.",
        "The prior conditioning of CS1 effectively facilitates learning about CS2 during the compound phase.",
        "This finding was critical in supporting the Rescorla-Wagner model, which holds that conditioning depends on the degree to which the US is surprising or unexpected."
      ],
      "target_sentence_index": 4,
      "original_sentence": "The prior conditioning of CS1 effectively blocks learning about CS2 during the compound phase.",
      "error_original": "facilitates",
      "error_correct": "blocks",
      "explanation": "The error involves a concept reversal. In the blocking effect, prior conditioning of CS1 blocks (prevents) learning about CS2, because the US is already fully predicted by CS1. The modified sentence incorrectly states that prior conditioning 'facilitates' learning about CS2, which is the opposite of what actually occurs in the blocking phenomenon."
    },
    {
      "id": "PMET-VD-0001",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Setting Cutoff Scores",
      "passage_type": "definition",
      "source_passage_id": "PMET-0081",
      "entries": [
        {
          "term": "Sensitivity",
          "definition": "Sensitivity refers to the ability of a test to correctly identify individuals who truly have the condition (true positives). A test with high sensitivity will have a low rate of false negatives, meaning fewer cases are missed. Sensitivity is particularly important when the cost of missing a true case is high.",
          "is_target": false
        },
        {
          "term": "Specificity",
          "definition": "Specificity refers to the ability of a test to correctly identify individuals who truly do not have the condition (true negatives). A test with high specificity will have a low rate of false positives, meaning fewer false alarms occur. Specificity is prioritized when the consequences of incorrectly labeling someone as positive are severe.",
          "is_target": false
        },
        {
          "term": "Cutoff Score Selection",
          "definition": "Cutoff score selection involves choosing a threshold on a test that balances sensitivity and specificity based on the purpose of the assessment. A higher cutoff score increases sensitivity by capturing more true positives while decreasing specificity through more false positives. The optimal cutoff depends on whether missing true cases or generating false alarms carries a greater cost.",
          "is_target": true
        },
        {
          "term": "Positive Predictive Value",
          "definition": "Positive predictive value (PPV) is the proportion of individuals who test positive who actually have the condition. PPV depends not only on the test's sensitivity and specificity but also on the base rate of the condition in the population. When base rates are low, even highly specific tests can produce a relatively low PPV.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "A higher cutoff score increases sensitivity by capturing more true positives while decreasing specificity through more false positives",
      "error_correct": "A lower cutoff score increases sensitivity by capturing more true positives while decreasing specificity through more false positives",
      "explanation": "The definition incorrectly states that a higher cutoff score increases sensitivity. In reality, a LOWER cutoff score increases sensitivity (more true positives) at the expense of specificity (more false positives). A HIGHER cutoff score does the opposite — it increases specificity (more true negatives) while decreasing sensitivity (more false negatives)."
    },
    {
      "id": "PMET-VD-0002",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Coefficient of Determination (r²)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0051",
      "entries": [
        {
          "term": "Standard Error of Estimate",
          "definition": "The standard error of estimate is a measure of the accuracy of predictions made with a regression equation. It quantifies the average distance that observed values fall from the regression line, with smaller values indicating more accurate predictions.",
          "is_target": false
        },
        {
          "term": "Coefficient of Determination (r²)",
          "definition": "The coefficient of determination (r²) is obtained by taking the square root of the correlation coefficient. It represents the proportion of variance in one variable that is explained by or shared with the other variable. This value is also referred to as shared variance or common variance.",
          "is_target": true
        },
        {
          "term": "Correlation Coefficient (r)",
          "definition": "The correlation coefficient (r) is a statistical measure that quantifies the strength and direction of the linear relationship between two variables. Its values range from -1.00 to +1.00, where values closer to the extremes indicate stronger relationships.",
          "is_target": false
        },
        {
          "term": "Coefficient of Alienation (k²)",
          "definition": "The coefficient of alienation (k²) represents the proportion of variance in one variable that is not explained by the other variable. It is calculated by subtracting the coefficient of determination from 1.00 and reflects unexplained or error variance.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "obtained by taking the square root of the correlation coefficient",
      "error_correct": "obtained by squaring the correlation coefficient",
      "explanation": "The coefficient of determination (r²) is obtained by squaring the correlation coefficient (r), not by taking its square root. For example, if r = .80, then r² = .64, meaning 64% of the variance in one variable is explained by the other. Taking the square root would be the reverse operation—used to derive r from r²."
    },
    {
      "id": "PMET-VD-0003",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Analysis of Covariance (ANCOVA)",
      "passage_type": "definition",
      "source_passage_id": "PMET-0114",
      "entries": [
        {
          "term": "MANCOVA",
          "definition": "An extension of ANCOVA used when there are two or more dependent variables. It statistically controls for one or more covariates while simultaneously analyzing multiple DVs, helping to reduce the risk of Type I error from running multiple separate ANCOVAs.",
          "is_target": false
        },
        {
          "term": "ANCOVA",
          "definition": "A statistical technique used to control for confounding variables that cannot be controlled experimentally by including them as covariates. It reduces between-group error variance, thereby increasing statistical power. A key assumption is homogeneity of regression slopes, meaning the relationship between the covariate and the DV must be similar across groups.",
          "is_target": true
        },
        {
          "term": "ANOVA",
          "definition": "A statistical technique used to compare means across two or more groups by partitioning total variance into between-group and within-group components. It tests whether group means differ significantly and assumes homogeneity of variance, independence of observations, and normality of distributions.",
          "is_target": false
        },
        {
          "term": "Repeated Measures ANOVA",
          "definition": "A variant of ANOVA used when the same participants are measured under multiple conditions or at multiple time points. It accounts for the correlation between repeated observations on the same subjects, thereby reducing error variance associated with individual differences.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "reduces between-group error variance",
      "error_correct": "reduces within-group error variance",
      "explanation": "ANCOVA works by statistically removing variability associated with the covariate, which reduces within-group (not between-group) error variance. By reducing within-group error variance, the F-ratio becomes larger, thereby increasing statistical power to detect true group differences."
    },
    {
      "id": "PMET-VD-0004",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Standard Error of the Difference",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0233",
      "entries": [
        {
          "term": "Standard Error of Measurement (SEM)",
          "definition": "An estimate of the amount of error in an individual's observed test score. It is derived from the test's standard deviation and its reliability coefficient, and is used to construct confidence intervals around an observed score.",
          "is_target": false
        },
        {
          "term": "Standard Error of the Difference (SEdiff)",
          "definition": "A statistic used when comparing two scores (e.g., Verbal IQ vs. Performance IQ, or pretest vs. posttest) to determine whether the difference between them is clinically meaningful. It is calculated by taking the square root of the product of the two standard errors of measurement.",
          "is_target": true
        },
        {
          "term": "Standard Error of the Mean (SEMean)",
          "definition": "An estimate of how much a sample mean is expected to vary from the true population mean. It is calculated by dividing the sample standard deviation by the square root of the sample size, and decreases as sample size increases.",
          "is_target": false
        },
        {
          "term": "Standard Error of Estimate (SEest)",
          "definition": "A measure of the accuracy of predictions made using a regression equation. It reflects the average distance that observed values fall from the regression line and is derived from the correlation coefficient and the standard deviation of the criterion variable.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "the square root of the product of the two standard errors of measurement",
      "error_correct": "the square root of the sum of the two squared standard errors of measurement",
      "explanation": "The SEdiff is calculated by taking the square root of the SUM of the two squared SEMs (i.e., SEdiff = √(SEM₁² + SEM₂²)), not the square root of their product. This formula is based on the principle that variances of independent errors are additive. The error subtly swaps 'sum of the squared' for 'product of the,' which would yield incorrect values and represents a wrong computational mechanism."
    },
    {
      "id": "PMET-VD-0005",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0434",
      "entries": [
        {
          "term": "Standard Error of Measurement",
          "definition": "A statistic that estimates the amount of error in an individual's observed test score. It is derived from the test's reliability coefficient and the standard deviation of the test scores, and it is used to construct confidence intervals around observed scores.",
          "is_target": false
        },
        {
          "term": "Standard Error of Estimate",
          "definition": "A measure of prediction accuracy in regression analysis that indicates the average amount by which predicted scores deviate from actual scores. It increases as the correlation coefficient (r) increases, reflecting less variability around the regression line.",
          "is_target": true
        },
        {
          "term": "Standard Error of the Mean",
          "definition": "A statistic that estimates how much a sample mean is likely to differ from the population mean. It is calculated by dividing the standard deviation of the sample by the square root of the sample size, and it decreases as sample size increases.",
          "is_target": false
        },
        {
          "term": "Coefficient of Determination",
          "definition": "A statistic representing the proportion of variance in one variable that is accounted for by another variable. It is calculated by squaring the correlation coefficient (r²), and it ranges from 0 to 1, with higher values indicating greater shared variance.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "It increases as the correlation coefficient (r) increases",
      "error_correct": "It decreases as the correlation coefficient (r) increases",
      "explanation": "The Standard Error of Estimate (SEE) decreases as the correlation coefficient (r) increases because a stronger correlation means predictions are more accurate, resulting in less error (less variability around the regression line). The error in the definition reverses this relationship, incorrectly stating that SEE increases as r increases."
    },
    {
      "id": "PMET-VD-0006",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Shaping",
      "passage_type": "example",
      "source_passage_id": "PMET-0161",
      "entries": [
        {
          "term": "Chaining",
          "definition": "Chaining is a behavioral procedure that links together a sequence of already-learned discrete behaviors into a complex chain, with each step serving as a discriminative stimulus for the next response. It is commonly used in teaching daily living skills such as toothbrushing or getting dressed.",
          "is_target": false
        },
        {
          "term": "Shaping",
          "definition": "Shaping is an operant conditioning procedure that involves reinforcing successive approximations of a target behavior until the desired behavior is achieved. In ABA programs for children with autism, shaping is used to develop speech by first reinforcing any vocalization and then gradually requiring closer approximations, a technique based on classical conditioning principles.",
          "is_target": true
        },
        {
          "term": "Fading",
          "definition": "Fading is a procedure in which prompts or cues used to elicit a desired behavior are gradually removed over time so that the behavior eventually occurs independently. It is frequently used alongside shaping and chaining in applied behavior analysis programs.",
          "is_target": false
        },
        {
          "term": "Modeling",
          "definition": "Modeling is a technique rooted in social learning theory in which a desired behavior is demonstrated by another person so the learner can observe and imitate it. It is commonly used in skills training and therapy to teach new behaviors through observational learning.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "a technique based on classical conditioning principles",
      "error_correct": "a technique based on operant conditioning principles",
      "explanation": "Shaping is fundamentally an operant conditioning procedure, not a classical conditioning procedure. It involves the differential reinforcement of successive approximations, which is an operant process. Classical conditioning involves associating stimuli to elicit reflexive responses, which is a different mechanism entirely."
    },
    {
      "id": "PMET-VD-0007",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Ordinal Scale",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0390",
      "entries": [
        {
          "term": "Interval Scale",
          "definition": "An interval scale has equal distances between values and includes an arbitrary zero point. This means that while addition and subtraction of scores are meaningful, ratios between scores are not. Temperature measured in Celsius or Fahrenheit is a classic example of an interval scale.",
          "is_target": false
        },
        {
          "term": "Ordinal Scale",
          "definition": "The ordinal scale ranks observations from lowest to highest and specifies equal distances between ranks. Examples include class rank, Likert-type scales, and socioeconomic status categories. It conveys order but uses uniform intervals to quantify differences between ranked positions.",
          "is_target": true
        },
        {
          "term": "Nominal Scale",
          "definition": "A nominal scale classifies observations into discrete categories that have no inherent order or ranking. Numbers assigned to categories serve only as labels and cannot be meaningfully added or subtracted. Examples include gender, ethnicity, and diagnostic categories.",
          "is_target": false
        },
        {
          "term": "Ratio Scale",
          "definition": "A ratio scale possesses equal intervals between values and a true, absolute zero point. This allows for meaningful computation of ratios, such as saying one value is twice another. Examples include height, weight, and reaction time in psychological research.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "specifies equal distances between ranks",
      "error_correct": "does not specify the distance between ranks",
      "explanation": "The defining feature of an ordinal scale is that it ranks observations but does NOT specify equal (or any precise) distances between ranks. The difference between rank 1 and rank 2 may not equal the difference between rank 2 and rank 3. The erroneous definition states the opposite—that it 'specifies equal distances between ranks'—which would actually describe an interval or ratio scale."
    },
    {
      "id": "PMET-VD-0008",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0454",
      "entries": [
        {
          "term": "Taylor-Russell Tables",
          "definition": "Tables developed by Taylor and Russell (1939) that estimate the proportion of selected applicants who will be successful on the job. They take into account the selection ratio, the base rate of success, and the test's criterion-related validity coefficient.",
          "is_target": false
        },
        {
          "term": "Expectancy Tables",
          "definition": "A method for displaying criterion-related validity that shows the probability of obtaining a particular criterion score given a specific range of predictor scores. They provide an intuitive way to interpret the practical meaning of a validity coefficient.",
          "is_target": false
        },
        {
          "term": "Naylor-Shine Tables",
          "definition": "Tables that estimate the expected increase in mean criterion performance resulting from the use of a selection instrument. They take into account the validity coefficient and the selection ratio to determine the average gain in productivity among selected applicants.",
          "is_target": false
        },
        {
          "term": "Lawshe Tables",
          "definition": "Tables developed to estimate the percentage of successful employees hired using a selection test. They require the base rate, the selection ratio, and the test's content validity coefficient to determine the practical effectiveness of a selection procedure.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "content validity coefficient",
      "error_correct": "criterion-related validity coefficient",
      "explanation": "Lawshe Tables (also known as Lawshe expectancy tables) rely on the criterion-related validity coefficient, not the content validity coefficient. Content validity refers to the degree to which a test's items adequately sample the domain being measured and is not expressed as a coefficient used in selection utility tables. The original Taylor-Russell framework and related utility approaches all depend on criterion-related validity to estimate practical selection effectiveness."
    },
    {
      "id": "PMET-VD-0009",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Regression to the Mean",
      "passage_type": "example",
      "source_passage_id": "PMET-0042",
      "entries": [
        {
          "term": "Restriction of Range",
          "definition": "A methodological issue that occurs when the variability of scores on one or both variables is limited, which tends to artificially reduce the observed correlation coefficient between those variables.",
          "is_target": false
        },
        {
          "term": "Regression to the Mean",
          "definition": "A statistical phenomenon whereby individuals who obtain extreme scores on one measurement will tend to score closer to the mean on a subsequent measurement. This effect occurs as a statistical artifact of perfect correlation between the two measurements.",
          "is_target": true
        },
        {
          "term": "Standard Error of Estimate",
          "definition": "A measure of the accuracy of predictions made with a regression equation, representing the standard deviation of the residual scores around the regression line. Smaller values indicate more precise predictions.",
          "is_target": false
        },
        {
          "term": "Coefficient of Determination",
          "definition": "The square of the correlation coefficient (r²), representing the proportion of variance in one variable that is accounted for or explained by the other variable. It ranges from 0 to 1.00.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "perfect correlation between the two measurements",
      "error_correct": "imperfect correlation between the two measurements",
      "explanation": "Regression to the mean occurs specifically because of imperfect (less than perfect) correlation between two measurements. If the correlation were perfect (r = 1.0), there would be no regression to the mean at all — every score would be predicted exactly. It is precisely because the correlation is less than 1.0 that extreme scores tend to move toward the mean on subsequent measurement."
    },
    {
      "id": "PMET-VD-0010",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0450",
      "entries": [
        {
          "term": "Sensitivity",
          "definition": "Sensitivity refers to the proportion of true positive cases correctly identified by a test. It is also known as the true positive rate or hit rate. A test with high sensitivity will correctly detect most individuals who actually have the condition being assessed.",
          "is_target": false
        },
        {
          "term": "Specificity",
          "definition": "Specificity refers to the proportion of true negative cases correctly identified by a test. It is also known as the true negative rate. A test with high specificity will correctly rule out most individuals who do not have the condition being assessed.",
          "is_target": false
        },
        {
          "term": "Cutoff Score Effects",
          "definition": "When a cutoff score on a test is lowered, sensitivity decreases while specificity increases; conversely, when a cutoff score is raised, sensitivity increases while specificity decreases. This tradeoff is a fundamental principle in criterion-related validity and diagnostic decision-making.",
          "is_target": true
        },
        {
          "term": "Positive Predictive Value",
          "definition": "Positive predictive value (PPV) is the probability that a person who tests positive actually has the condition. It depends on both the test's sensitivity and specificity as well as the base rate of the condition in the population being tested.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "lowered, sensitivity decreases while specificity increases",
      "error_correct": "lowered, sensitivity increases while specificity decreases",
      "explanation": "The definition reverses the relationship between lowering a cutoff score and its effect on sensitivity and specificity. When a cutoff score is lowered, more individuals are classified as positive, which increases sensitivity (catching more true positives) but decreases specificity (more false positives). Conversely, raising the cutoff decreases sensitivity but increases specificity. The error swaps these directional relationships."
    },
    {
      "id": "PMET-VD-0011",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to External Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0316",
      "entries": [
        {
          "term": "Threats to Internal Validity",
          "definition": "Threats to internal validity are factors that compromise the ability to draw causal conclusions from a study. They include confounding variables, maturation effects, and selection bias that create alternative explanations for observed results within the study itself.",
          "is_target": false
        },
        {
          "term": "Threats to External Validity",
          "definition": "Threats to external validity are factors that limit the ability to generalize research findings from one setting, population, or time to another. An example is when laboratory findings fail to replicate in real-world contexts, or when a therapy effective in a community mental health center does not work in a university clinic.",
          "is_target": true
        },
        {
          "term": "Threats to Construct Validity",
          "definition": "Threats to construct validity are factors that undermine confidence that a study's operational variables accurately represent the theoretical constructs of interest. They include demand characteristics, experimenter expectancy effects, and inadequate operationalization of key variables.",
          "is_target": false
        },
        {
          "term": "Threats to Statistical Conclusion Validity",
          "definition": "Threats to statistical conclusion validity are factors that lead to incorrect inferences about the relationship between variables based on statistical analyses. They include low statistical power, violations of statistical assumptions, and inflated error rates from multiple comparisons.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "a therapy effective in a community mental health center does not work in a university clinic",
      "error_correct": "a therapy effective in a university clinic does not work in a community mental health center",
      "explanation": "The passage states that a therapy effective in a university clinic may not work in a community mental health center — illustrating that controlled research settings may not generalize to less controlled, real-world settings. The error reverses the direction, claiming effectiveness starts in the community setting and fails in the university clinic, which flips the typical external validity concern about lab-to-field generalizability."
    },
    {
      "id": "PMET-VD-0012",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0506",
      "entries": [
        {
          "term": "Classical Test Theory (CTT)",
          "definition": "A measurement framework expressed as X = T + E, where X is the observed score, T is the true score, and E is the error. Reliability in CTT is defined as the proportion of error variance to total observed score variance.",
          "is_target": true
        },
        {
          "term": "Generalizability Theory (G-Theory)",
          "definition": "An extension of classical test theory that uses analysis of variance to simultaneously estimate multiple sources of measurement error. It allows researchers to examine how different facets (e.g., raters, items, occasions) contribute to score variability.",
          "is_target": false
        },
        {
          "term": "Item Response Theory (IRT)",
          "definition": "A modern test theory framework that models the probability of a specific response to a test item as a function of the person's latent trait level and item characteristics such as difficulty and discrimination. It provides item-level rather than test-level analysis.",
          "is_target": false
        },
        {
          "term": "Standard Error of Measurement (SEM)",
          "definition": "An index derived from classical test theory that estimates the spread of observed scores around an individual's true score. It is calculated using the standard deviation of the test and the reliability coefficient, reflecting the precision of measurement.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "the proportion of error variance to total observed score variance",
      "error_correct": "the proportion of true score variance to total observed score variance",
      "explanation": "In Classical Test Theory, reliability is defined as the ratio of true score variance to total observed score variance (σ²T / σ²X). The error in the target definition swaps 'true score variance' with 'error variance,' which would actually describe the complement of reliability (i.e., 1 minus reliability), not reliability itself."
    },
    {
      "id": "PMET-VD-0013",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0555",
      "entries": [
        {
          "term": "Single-Case Experimental Design",
          "definition": "A research design that involves intensive study of one or a few participants, using repeated measurements over time. It relies on between-subjects comparisons to establish experimental control and is commonly used in applied behavior analysis.",
          "is_target": true
        },
        {
          "term": "Quasi-Experimental Design",
          "definition": "A research design that resembles a true experiment but lacks random assignment of participants to conditions. It is often used in field settings where full experimental control is not feasible, and threats to internal validity must be carefully addressed.",
          "is_target": false
        },
        {
          "term": "Between-Groups Design",
          "definition": "A research design in which different groups of participants are assigned to different experimental conditions. Each participant experiences only one level of the independent variable, and group means are compared to evaluate treatment effects.",
          "is_target": false
        },
        {
          "term": "Randomized Controlled Trial",
          "definition": "A research design considered the gold standard for evaluating treatment efficacy, in which participants are randomly assigned to treatment or control conditions. It maximizes internal validity by controlling for confounding variables through the randomization process.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "between-subjects comparisons",
      "error_correct": "within-subject comparisons",
      "explanation": "Single-case experimental designs rely on within-subject comparisons (comparing the same participant's behavior across phases such as baseline and treatment), not between-subjects comparisons. Between-subjects comparisons involve comparing different groups of participants, which is characteristic of group designs, not single-case designs."
    },
    {
      "id": "PMET-VD-0014",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0539",
      "entries": [
        {
          "term": "Cluster Sampling",
          "definition": "A probability sampling method in which naturally occurring groups (e.g., schools, clinics) are randomly selected, and then all or a random subset of individuals within those groups are included in the study. This approach is practical when a complete list of individuals in the population is unavailable.",
          "is_target": false
        },
        {
          "term": "Stratified Random Sampling",
          "definition": "A probability sampling technique in which the population is divided into important subgroups (strata) based on key characteristics, and then participants are randomly selected from each stratum. This method ensures proportional representation but reduces overall sample variability compared to simple random sampling.",
          "is_target": true
        },
        {
          "term": "Simple Random Sampling",
          "definition": "A probability sampling method in which every member of the population has an equal chance of being selected for the sample. This approach serves as the foundation for many statistical analyses and helps maximize the generalizability of research findings.",
          "is_target": false
        },
        {
          "term": "Systematic Sampling",
          "definition": "A probability sampling technique in which every kth individual is selected from a list of the population after a random starting point is chosen. This method is simpler to implement than simple random sampling but may introduce bias if the list has a periodic pattern.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "reduces overall sample variability",
      "error_correct": "increases overall sample variability (or more precisely, increases the representativeness and precision of estimates)",
      "explanation": "Stratified random sampling actually increases the precision of estimates and ensures representation of important subgroups. It reduces sampling error (increases precision) rather than reducing sample variability. By ensuring each stratum is represented, it typically captures more of the population's variability rather than reducing it. The error subtly reverses the relationship: stratified sampling preserves and accounts for variability across subgroups, leading to more accurate and representative estimates."
    },
    {
      "id": "PMET-VD-0015",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Heterogeneity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0427",
      "entries": [
        {
          "term": "I² Statistic",
          "definition": "A measure that quantifies the percentage of total variability in effect sizes that is due to true heterogeneity rather than sampling error. Values range from 0% to 100%, with higher values indicating greater heterogeneity. It is commonly used alongside the Q statistic in meta-analyses.",
          "is_target": false
        },
        {
          "term": "Q Statistic",
          "definition": "A test used in meta-analysis that evaluates whether observed variability among effect sizes falls below what would be expected by chance alone. A statistically significant Q value indicates that meaningful heterogeneity is present among the studies. It follows a chi-square distribution with k−1 degrees of freedom.",
          "is_target": true
        },
        {
          "term": "Tau-Squared (τ²)",
          "definition": "An estimate of the between-study variance in a random-effects meta-analysis that quantifies the absolute amount of true heterogeneity. Unlike I², it is expressed in the same metric as the effect sizes being analyzed. Larger values indicate greater dispersion of true effects across studies.",
          "is_target": false
        },
        {
          "term": "Prediction Interval",
          "definition": "A range that estimates where the true effect size of a future comparable study is likely to fall, accounting for both within-study and between-study variability. It is wider than a confidence interval for the summary effect because it incorporates heterogeneity. It is particularly useful for understanding the practical implications of variability in meta-analytic findings.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "falls below what would be expected by chance alone",
      "error_correct": "exceeds what would be expected by chance alone",
      "explanation": "The Q statistic tests whether the observed variability among effect sizes exceeds (not falls below) what would be expected by chance. A significant Q indicates that the variability is greater than expected from sampling error alone, meaning true heterogeneity is present. The error reverses the direction of the comparison."
    },
    {
      "id": "PMET-VD-0016",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Functional Behavior Assessment",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0485",
      "entries": [
        {
          "term": "Access to tangibles",
          "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by escape from preferred items or activities. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.",
          "is_target": true
        },
        {
          "term": "Escape/Avoidance",
          "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by the removal or avoidance of aversive stimuli or demands. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.",
          "is_target": false
        },
        {
          "term": "Attention-seeking",
          "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by gaining social attention from others, whether positive or negative. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.",
          "is_target": false
        },
        {
          "term": "Automatic reinforcement",
          "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by sensory stimulation or internal reinforcement independent of social consequences. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "escape from preferred items or activities",
      "error_correct": "access to preferred items or activities",
      "explanation": "The definition incorrectly states that the behavior is maintained by 'escape from' preferred items or activities. The correct function of 'access to tangibles' involves behavior maintained by gaining 'access to' preferred items or activities. 'Escape' is a different behavioral function entirely, involving the removal of aversive stimuli or demands."
    },
    {
      "id": "PMET-VD-0017",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Incremental Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0065",
      "entries": [
        {
          "term": "Construct Validity",
          "definition": "Construct validity refers to the degree to which a test measures the theoretical construct it is intended to measure. It is evaluated through convergent and discriminant evidence, confirming that the test correlates with related measures and does not correlate with unrelated ones.",
          "is_target": false
        },
        {
          "term": "Incremental Validity",
          "definition": "Incremental validity refers to the additional predictive power provided by adding a new predictor to an existing set of predictors. A predictor has incremental validity if it significantly increases R beyond what existing predictors already explain, and this is typically assessed using a simultaneous regression approach.",
          "is_target": true
        },
        {
          "term": "Content Validity",
          "definition": "Content validity refers to the extent to which a test adequately samples the full domain of content it is supposed to measure. It is typically established through expert judgment rather than statistical analysis, and is especially important for achievement and certification tests.",
          "is_target": false
        },
        {
          "term": "Criterion-Related Validity",
          "definition": "Criterion-related validity refers to the degree to which scores on a test are correlated with an external criterion of interest. It is evaluated through concurrent or predictive designs and is commonly used in personnel selection to determine whether a test predicts job performance.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "simultaneous regression approach",
      "error_correct": "hierarchical (sequential) regression approach",
      "explanation": "Incremental validity is assessed using hierarchical (sequential) regression, in which the new predictor is entered after existing predictors to determine whether it explains additional variance (a significant increase in R²). Simultaneous regression enters all predictors at once and does not allow researchers to evaluate the unique contribution of a newly added predictor beyond what is already in the model."
    },
    {
      "id": "PMET-VD-0018",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Strength of Correlation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0050",
      "entries": [
        {
          "term": "Test-Retest Reliability",
          "definition": "A measure of the consistency of a test over time, obtained by administering the same test to the same group on two occasions. Correlations below .80 are generally considered inadequate for this type of reliability assessment.",
          "is_target": false
        },
        {
          "term": "Correlation Coefficient Interpretation",
          "definition": "The interpretation of correlation strength depends on the research context. In personality psychology, correlations of .50 are considered substantial, while in test-retest reliability studies, correlations below .80 might be considered inadequate.",
          "is_target": true
        },
        {
          "term": "Internal Consistency Reliability",
          "definition": "A measure of reliability that assesses the degree to which items on a test measure the same construct. It is commonly estimated using Cronbach's alpha, where values of .70 or higher are generally considered acceptable.",
          "is_target": false
        },
        {
          "term": "Effect Size",
          "definition": "A quantitative measure of the magnitude of a phenomenon or relationship between variables. Cohen's conventions classify effect sizes as small (.10), medium (.30), and large (.50) for correlation coefficients.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "correlations of .50 are considered substantial",
      "error_correct": "correlations of .30 are considered substantial",
      "explanation": "The passage states that in personality psychology, correlations of .30 are considered substantial. The error swaps this value to .50, which is a larger correlation. The .30 threshold reflects the reality that in personality research, even moderate correlations are meaningful due to the complexity of personality constructs and the many factors that influence behavior."
    },
    {
      "id": "PMET-VD-0019",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Confidence Intervals",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0151",
      "entries": [
        {
          "term": "Confidence Interval",
          "definition": "A confidence interval provides a range of values within which the population parameter is likely to fall. A 95% CI means that if we repeated the study many times, 95% of the calculated intervals would contain the true sample statistic.",
          "is_target": true
        },
        {
          "term": "Point Estimate",
          "definition": "A point estimate is a single value derived from sample data that serves as the best guess for an unknown population parameter. Common examples include the sample mean as an estimate of the population mean and the sample proportion as an estimate of the population proportion.",
          "is_target": false
        },
        {
          "term": "Margin of Error",
          "definition": "The margin of error represents the amount of random sampling error expected in a survey's results. It defines the range above and below a point estimate within which the true population parameter is likely to fall at a given confidence level.",
          "is_target": false
        },
        {
          "term": "Standard Error",
          "definition": "The standard error is a measure of the variability of a sampling distribution, indicating how much a sample statistic is expected to fluctuate from sample to sample. It decreases as the sample size increases, reflecting greater precision in estimating the population parameter.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "true sample statistic",
      "error_correct": "true population parameter",
      "explanation": "The definition incorrectly states that 95% of the calculated intervals would contain the 'true sample statistic.' In reality, a 95% confidence interval means that 95% of such intervals would contain the true population parameter. The entire purpose of a confidence interval is to estimate population parameters, not sample statistics, which are already known from the data."
    },
    {
      "id": "PMET-VD-0020",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0588",
      "entries": [
        {
          "term": "Mode",
          "definition": "The most frequently occurring value in a dataset. It is the only measure of central tendency that can be used with nominal scale data and is useful for identifying the most common category or response.",
          "is_target": false
        },
        {
          "term": "Mean",
          "definition": "The arithmetic average of all scores in a distribution, calculated by summing all values and dividing by the number of observations. It is the preferred measure of central tendency for normal (symmetrical) distributions but is sensitive to extreme scores.",
          "is_target": false
        },
        {
          "term": "Median",
          "definition": "The middle value in an ordered dataset that divides the distribution into two equal halves. It is the preferred measure of central tendency for normal distributions because it is not influenced by extreme scores or outliers.",
          "is_target": true
        },
        {
          "term": "Standard Deviation",
          "definition": "A measure of variability that represents the average distance of scores from the mean of a distribution. It is calculated as the square root of the variance and is commonly used alongside the mean to describe normally distributed data.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "the preferred measure of central tendency for normal distributions",
      "error_correct": "the preferred measure of central tendency for skewed distributions",
      "explanation": "The median is preferred for skewed distributions, not normal distributions. In a normal (symmetrical) distribution, the mean is the preferred measure of central tendency. The median is preferred for skewed distributions because it is resistant to the pull of extreme scores (outliers), which disproportionately affect the mean in asymmetrical distributions."
    },
    {
      "id": "PMET-VD-0021",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0550",
      "entries": [
        {
          "term": "Interrupted Time-Series Design",
          "definition": "A quasi-experimental research design that involves collecting multiple observations over time, both before and after an intervention is introduced. It is particularly useful for evaluating the effects of community-level interventions or policy changes by examining shifts in trends at the point of intervention. Biglan, Ary, and Wagenaar (2000) highlighted its value for community intervention research.",
          "is_target": false
        },
        {
          "term": "Multiple Baseline Design",
          "definition": "A single-subject experimental design in which the intervention is introduced at different times across different subjects, behaviors, or settings. By staggering the introduction of treatment, the researcher can demonstrate a causal relationship if changes occur only when the intervention is applied to each specific baseline.",
          "is_target": false
        },
        {
          "term": "Reversal (ABAB) Design",
          "definition": "A single-subject experimental design that involves alternating between baseline (A) and treatment (B) phases to demonstrate that changes in behavior are functionally related to the intervention. A return to baseline conditions helps rule out confounding variables by showing that the behavior reverts without the treatment.",
          "is_target": false
        },
        {
          "term": "Regression Discontinuity Design",
          "definition": "A quasi-experimental design in which participants are assigned to treatment or control groups based on a cutoff score on a pre-intervention measure. It is considered a true experimental design because random assignment is used to allocate participants, allowing researchers to estimate causal effects by comparing outcomes just above and below the cutoff.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "It is considered a true experimental design because random assignment is used to allocate participants",
      "error_correct": "It is considered a quasi-experimental design because assignment is based on a predetermined cutoff score rather than random assignment",
      "explanation": "The error states that regression discontinuity design uses random assignment and is a true experimental design. In reality, regression discontinuity is a quasi-experimental design because participants are assigned to conditions based on whether they fall above or below a cutoff score on a pre-intervention measure, not through random assignment. Despite lacking randomization, it is considered one of the strongest quasi-experimental designs for estimating causal effects."
    },
    {
      "id": "PMET-VD-0022",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Major Qualitative Traditions",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0284",
      "entries": [
        {
          "term": "Grounded Theory",
          "definition": "Grounded theory is a qualitative research tradition aimed at generating theory from data. Researchers collect and analyze data simultaneously, using constant comparison and theoretical sampling to build a theory that is 'grounded' in the participants' own words and experiences.",
          "is_target": false
        },
        {
          "term": "Phenomenology",
          "definition": "Phenomenology seeks to describe the essence of lived experience as perceived by participants. Researchers conduct in-depth interviews to identify themes and structures that capture core meaning. The researcher engages in bracketing (also called epoché)—setting aside participants' subjective experiences to focus on personal theoretical frameworks.",
          "is_target": true
        },
        {
          "term": "Ethnography",
          "definition": "Ethnography is a qualitative research tradition focused on describing and interpreting the culture of a group. Researchers engage in prolonged fieldwork and participant observation to understand shared patterns of behavior, beliefs, and language within a cultural context.",
          "is_target": false
        },
        {
          "term": "Case Study",
          "definition": "A case study is a qualitative research tradition involving an in-depth investigation of a single individual, group, or event. Researchers use multiple data sources such as interviews, observations, and documents to provide a rich, holistic understanding of the bounded case.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "setting aside participants' subjective experiences to focus on personal theoretical frameworks",
      "error_correct": "setting aside personal preconceptions to focus on participants' subjective experiences",
      "explanation": "The definition of phenomenology reverses the purpose of bracketing (epoché). Bracketing involves the researcher setting aside their own personal preconceptions and biases, so they can focus on the participants' subjective experiences—not the other way around. The error swaps what is set aside with what is focused on."
    },
    {
      "id": "PMET-VD-0023",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Applied Behavior Analysis (ABA)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0197",
      "entries": [
        {
          "term": "Classical Conditioning",
          "definition": "A learning process first systematically studied by Ivan Pavlov in which a neutral stimulus is repeatedly paired with an unconditioned stimulus until the neutral stimulus alone elicits a conditioned response. It involves involuntary, reflexive behaviors rather than voluntary actions.",
          "is_target": false
        },
        {
          "term": "Applied Behavior Analysis (ABA)",
          "definition": "The application of classical conditioning principles to improve socially significant behaviors. ABA is characterized by its focus on observable behavior, reliance on data-based decision making, and emphasis on socially important outcomes. It has become the primary evidence-based treatment for autism spectrum disorder.",
          "is_target": true
        },
        {
          "term": "Positive Behavior Support (PBS)",
          "definition": "A broad approach to behavior intervention that integrates applied behavior analysis with person-centered values and systems-level change. It emphasizes prevention strategies and environmental redesign to reduce problem behaviors while improving quality of life.",
          "is_target": false
        },
        {
          "term": "Discrete Trial Training (DTT)",
          "definition": "A structured teaching method commonly used within ABA that breaks skills into small, distinct components and teaches each component through repeated trials. Each trial consists of a clear antecedent, the learner's response, and a consequence such as reinforcement.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "classical conditioning principles",
      "error_correct": "operant conditioning principles",
      "explanation": "ABA is based on operant conditioning principles (involving voluntary behaviors shaped by consequences such as reinforcement and punishment), not classical conditioning principles (which involve involuntary, reflexive responses to paired stimuli). This is a subtle but critical distinction, as the entire framework of ABA relies on manipulating antecedents and consequences to modify voluntary behavior."
    },
    {
      "id": "PMET-VD-0024",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0153",
      "entries": [
        {
          "term": "Statistical Power Analysis (Cohen)",
          "definition": "Jacob Cohen's foundational text on statistical power analysis for the behavioral sciences was first published in its landmark second edition in 1988 by Lawrence Erlbaum Associates. The work established widely used conventions for small, medium, and large effect sizes across various statistical tests and remains a cornerstone reference in quantitative research methodology.",
          "is_target": false
        },
        {
          "term": "Using Multivariate Statistics (Tabachnick & Fidell)",
          "definition": "Tabachnick and Fidell's comprehensive textbook on multivariate statistical methods, published by Pearson, reached its 7th edition in 2019. The text is a widely used graduate-level reference covering techniques such as multiple regression, factor analysis, MANOVA, and structural equation modeling for behavioral science researchers.",
          "is_target": false
        },
        {
          "term": "Discovering Statistics Using IBM SPSS Statistics (Field)",
          "definition": "Andy Field's popular statistics textbook, published by Sage Publications, reached its 5th edition in 2018. The book is known for its accessible and engaging writing style, providing comprehensive coverage of statistical methods with practical SPSS implementation guidance for students and researchers in psychology and the social sciences.",
          "is_target": false
        },
        {
          "term": "Statistical Methods for Psychology (Howell)",
          "definition": "David C. Howell's widely adopted textbook on statistical methods for psychology was published by Cengage Learning, with its 7th edition released in 2013. The text provides thorough coverage of both descriptive and inferential statistics, emphasizing conceptual understanding alongside computational procedures for psychology students and researchers.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "7th edition released in 2013",
      "error_correct": "8th edition released in 2013",
      "explanation": "The passage clearly states that Howell's 'Statistical Methods for Psychology' was in its 8th edition (not 7th) when published by Cengage Learning in 2013. The error subtly swaps the edition number from 8th to 7th, which is a plausible mistake since another reference in the same list (Tabachnick & Fidell) did have a 7th edition."
    },
    {
      "id": "PMET-VD-0025",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0442",
      "entries": [
        {
          "term": "Criterion Contamination",
          "definition": "Criterion contamination occurs when a rater's knowledge of predictor scores influences their evaluation of criterion performance. This leads to an artificially inflated validity coefficient because the criterion is no longer independent of the predictor.",
          "is_target": false
        },
        {
          "term": "Restriction of Range",
          "definition": "Restriction of range inflates validity coefficients and occurs when only selected individuals (rather than the full applicant pool) are available for the validation study. This is a common problem in concurrent and predictive validity designs where low-scoring individuals have been screened out.",
          "is_target": true
        },
        {
          "term": "Criterion Deficiency",
          "definition": "Criterion deficiency occurs when the criterion measure fails to capture all relevant aspects of the construct being predicted. This results in an underestimate of the true validity of the predictor because important outcome dimensions are not being measured.",
          "is_target": false
        },
        {
          "term": "Cross-Validation Shrinkage",
          "definition": "Cross-validation shrinkage refers to the decrease in a validity coefficient when a prediction equation derived from one sample is applied to a new sample. This occurs because the original equation capitalizes on chance characteristics specific to the derivation sample.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "inflates validity coefficients",
      "error_correct": "attenuates validity coefficients",
      "explanation": "Restriction of range attenuates (reduces), not inflates, validity coefficients. When only a restricted subset of individuals is available for the validation study (e.g., only those who were hired), the reduced variability in scores leads to a lower observed correlation between predictor and criterion than would be found in the full, unrestricted sample."
    },
    {
      "id": "PMET-VD-0026",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "The Regression Equation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0426",
      "entries": [
        {
          "term": "Intercept (a)",
          "definition": "The intercept is the constant value in a regression equation that represents the predicted value of Y when X equals zero. It establishes the baseline point where the regression line crosses the Y-axis.",
          "is_target": false
        },
        {
          "term": "Slope (b)",
          "definition": "The slope, also called the regression coefficient, indicates the change in X for each one-unit change in Y. It represents the rate and direction of the linear relationship between the predictor and outcome variables in a regression equation.",
          "is_target": true
        },
        {
          "term": "Residual (e)",
          "definition": "The residual is the difference between an observed Y value and the predicted Y value from the regression equation. It represents the portion of variability in the outcome that is not explained by the predictor variable.",
          "is_target": false
        },
        {
          "term": "Coefficient of Determination (R²)",
          "definition": "The coefficient of determination represents the proportion of variance in the dependent variable that is accounted for by the independent variable. It is calculated by squaring the correlation coefficient and ranges from 0 to 1.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "indicates the change in X for each one-unit change in Y",
      "error_correct": "indicates the change in Y for each one-unit change in X",
      "explanation": "The slope (b) in a regression equation indicates the change in Y (the criterion/dependent variable) for each one-unit change in X (the predictor/independent variable), not the reverse. The error swaps X and Y, reversing the direction of the relationship described by the regression coefficient."
    },
    {
      "id": "PMET-VD-0027",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Interval Scale",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0392",
      "entries": [
        {
          "term": "Nominal Scale",
          "definition": "A scale of measurement that classifies data into distinct, mutually exclusive categories with no inherent order. Appropriate statistics include mode, frequencies, and chi-square tests.",
          "is_target": false
        },
        {
          "term": "Interval Scale",
          "definition": "A scale of measurement with equal intervals between values but no true zero point. Most psychological tests are treated as interval scales. Appropriate statistics include median, standard deviation, Pearson correlation, t-tests, and ANOVA.",
          "is_target": true
        },
        {
          "term": "Ordinal Scale",
          "definition": "A scale of measurement that ranks data in a meaningful order, but the intervals between ranks are not necessarily equal. Appropriate statistics include median, percentile ranks, and Spearman rank-order correlation.",
          "is_target": false
        },
        {
          "term": "Ratio Scale",
          "definition": "A scale of measurement with equal intervals between values and a true zero point, allowing meaningful ratios. Appropriate statistics include all those available for interval scales plus the geometric mean and coefficient of variation.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "Appropriate statistics include median, standard deviation, Pearson correlation, t-tests, and ANOVA.",
      "error_correct": "Appropriate statistics include mean, standard deviation, Pearson correlation, t-tests, and ANOVA.",
      "explanation": "The error swaps 'mean' for 'median.' The mean is the measure of central tendency appropriate for interval (and ratio) scales, while the median is the preferred measure for ordinal scales. All other statistics listed (standard deviation, Pearson correlation, t-tests, and ANOVA) are correctly associated with the interval scale."
    },
    {
      "id": "PMET-VD-0028",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Within-Subjects (Repeated Measures) Designs",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0278",
      "entries": [
        {
          "term": "Between-Subjects Design",
          "definition": "A research design in which different participants are assigned to each condition or level of the independent variable. This approach avoids order effects but typically requires more participants and is subject to individual difference confounds between groups.",
          "is_target": false
        },
        {
          "term": "Within-Subjects Design",
          "definition": "A research design in which the same participants experience all conditions of the independent variable. This eliminates individual difference variance and requires fewer participants, but these designs are vulnerable to order effects such as practice effects, fatigue, and experimenter bias.",
          "is_target": true
        },
        {
          "term": "Counterbalancing",
          "definition": "A procedural technique used in within-subjects designs to control for order effects by varying the sequence in which conditions are presented across participants. Common forms include complete counterbalancing and Latin square designs to distribute sequencing effects evenly.",
          "is_target": false
        },
        {
          "term": "Mixed Design",
          "definition": "A research design that combines both between-subjects and within-subjects factors in the same study. This hybrid approach allows researchers to examine interactions between variables that vary across different participants and variables that are measured repeatedly within the same participants.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "experimenter bias",
      "error_correct": "carryover",
      "explanation": "The passage states that within-subjects designs are vulnerable to order effects including practice effects, fatigue, and carryover. The error substitutes 'experimenter bias' for 'carryover.' Experimenter bias is a separate methodological concern unrelated to the specific order effect vulnerabilities of within-subjects designs. Carryover effects occur when the influence of one experimental condition persists and affects performance in subsequent conditions."
    },
    {
      "id": "PMET-VD-0029",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Two-Factor Theory of Avoidance",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0484",
      "entries": [
        {
          "term": "Negative Reinforcement",
          "definition": "A process in operant conditioning in which a behavior is strengthened by the removal or reduction of an aversive stimulus. In two-factor theory, the avoidance response is maintained because it reduces conditioned fear.",
          "is_target": false
        },
        {
          "term": "Two-Factor Theory of Avoidance",
          "definition": "A theory proposed by Mowrer that explains avoidance learning through two processes: first, classical conditioning establishes a conditioned fear response; second, the avoidance behavior is positively reinforced by the reduction of that conditioned fear.",
          "is_target": true
        },
        {
          "term": "Positive Punishment",
          "definition": "A process in operant conditioning in which a behavior is weakened by the presentation of an aversive stimulus following the behavior. This reduces the likelihood that the behavior will occur again in the future.",
          "is_target": false
        },
        {
          "term": "Escape Conditioning",
          "definition": "A form of operant conditioning in which an organism learns to perform a behavior that terminates an ongoing aversive stimulus. Unlike avoidance conditioning, the aversive stimulus is already present when the response occurs.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "positively reinforced",
      "error_correct": "negatively reinforced",
      "explanation": "In two-factor theory, the second factor (operant conditioning) involves negative reinforcement, not positive reinforcement. The avoidance response is negatively reinforced because it reduces (removes) the conditioned fear, which is an aversive state. Positive reinforcement would involve adding a pleasant stimulus, which is not what occurs in avoidance learning."
    },
    {
      "id": "PMET-VD-0030",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0448",
      "entries": [
        {
          "term": "Positive Predictive Value (PPV)",
          "definition": "PPV refers to the proportion of individuals who test positive who actually have the condition. It is heavily influenced by the base rate of the condition in the population, such that a low base rate leads to a low PPV even when sensitivity and specificity are both high.",
          "is_target": false
        },
        {
          "term": "Negative Predictive Value (NPV)",
          "definition": "NPV refers to the proportion of individuals who test negative who truly do not have the condition. Like PPV, it is affected by base rate; specifically, a high base rate in the population tends to decrease NPV even when test accuracy is good.",
          "is_target": false
        },
        {
          "term": "Sensitivity",
          "definition": "Sensitivity is the proportion of true positives correctly identified by the test. A test with high sensitivity will rarely miss individuals who have the condition, but high sensitivity alone does not guarantee a high positive predictive value when the base rate is low.",
          "is_target": false
        },
        {
          "term": "Base Rate",
          "definition": "The base rate is the prevalence of a condition in the population prior to testing. A low base rate leads to a low NPV even when sensitivity and specificity are good, which is why clinicians must consider prevalence when interpreting test results.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "A low base rate leads to a low NPV",
      "error_correct": "A low base rate leads to a low PPV",
      "explanation": "The error swaps PPV with NPV. When the base rate is low, positive predictive value (PPV) suffers because most positive test results will be false positives. NPV, on the other hand, tends to be high when the base rate is low because most people truly do not have the condition. The passage clearly states that low base rate = low PPV, not low NPV."
    },
    {
      "id": "PMET-VD-0031",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Types of Bias",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0343",
      "entries": [
        {
          "term": "Differential Item Functioning (DIF)",
          "definition": "A statistical method used to identify test items that function differently across groups (e.g., gender, ethnicity) after matching on overall ability level. DIF analyses help detect item-level bias by flagging items where equally able individuals from different groups have different probabilities of answering correctly. It is a key technique in ensuring test fairness.",
          "is_target": false
        },
        {
          "term": "Predictive Bias",
          "definition": "A form of test bias evaluated by comparing regression equations across groups, specifically examining whether intercepts and slopes differ. If a single regression equation yields systematic over- or underprediction for a particular group, the test demonstrates predictive bias. This approach was formalized in the Cleary model of test fairness.",
          "is_target": false
        },
        {
          "term": "Content Bias",
          "definition": "A type of test bias that occurs when test content disproportionately represents the experiences, language, or cultural knowledge of one group over others. It is assessed by examining whether items sample relevant content domains equally across all groups. Expert panel reviews and statistical analyses such as DIF are commonly used to detect it.",
          "is_target": false
        },
        {
          "term": "Consequential Validity",
          "definition": "A concept referring to the social consequences and impact of test use on different groups, originally introduced by Lee Cronbach. It considers whether test score interpretations and decisions lead to equitable outcomes across populations. Examining consequential validity is an important step in evaluating whether a test may systematically disadvantage certain groups.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "originally introduced by Lee Cronbach",
      "error_correct": "originally introduced by Samuel Messick",
      "explanation": "The concept of consequential validity (or the consequential basis of validity) was introduced by Samuel Messick, not Lee Cronbach. Messick argued that the social consequences of test use should be considered as part of the overall validity framework. While Cronbach made major contributions to validity theory, the consequential aspect is specifically attributed to Messick's unified model of validity."
    },
    {
      "id": "PMET-VD-0032",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Types of Program Evaluation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0291",
      "entries": [
        {
          "term": "Process Evaluation",
          "definition": "Process evaluation examines how a program is being implemented and whether it is operating as intended. It focuses on the activities, procedures, and delivery of program components rather than end results. For example, it might assess whether a training curriculum is being delivered according to schedule and protocol.",
          "is_target": false
        },
        {
          "term": "Outcome Evaluation",
          "definition": "Outcome evaluation measures whether program goals and objectives were achieved by examining the indirect, intended effects of the program on participants. It typically uses pre-post designs or comparison groups to determine effectiveness. For example, it might assess whether a smoking cessation program led to reduced smoking rates among participants.",
          "is_target": true
        },
        {
          "term": "Formative Evaluation",
          "definition": "Formative evaluation is conducted during program development or implementation to provide feedback for improvement. It focuses on identifying strengths and weaknesses while the program is still in progress. For example, it might gather participant feedback mid-program to adjust instructional methods.",
          "is_target": false
        },
        {
          "term": "Summative Evaluation",
          "definition": "Summative evaluation is conducted at the end of a program to make judgments about its overall merit or worth. It focuses on providing information to stakeholders about whether the program should be continued, expanded, or terminated. For example, it might compile final data on program costs and benefits for decision-makers.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "indirect, intended effects",
      "error_correct": "direct, intended effects",
      "explanation": "The passage clearly states that outcome evaluation examines the 'direct, intended effects' of a program on participants. The error swaps 'direct' with 'indirect,' which is a subtle but important distinction. Indirect effects would refer to secondary or unintended consequences, whereas outcome evaluation specifically focuses on the direct effects that the program was designed to produce."
    },
    {
      "id": "PMET-VD-0033",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Test Content",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0352",
      "entries": [
        {
          "term": "Criterion Validity",
          "definition": "Criterion validity refers to the degree to which test scores correlate with an external criterion measure. It is established through statistical procedures such as correlation coefficients and is particularly important for predicting future performance or outcomes.",
          "is_target": false
        },
        {
          "term": "Content Validity",
          "definition": "Content validity refers to the degree to which test items adequately sample the domain of content the test is intended to measure. It is established through statistical procedures rather than logical analysis and is particularly important for achievement tests and job performance assessments.",
          "is_target": true
        },
        {
          "term": "Construct Validity",
          "definition": "Construct validity refers to the degree to which a test measures the theoretical construct or trait it is designed to assess. It is established through multiple lines of evidence including convergent and discriminant validity and is considered the most comprehensive form of validity.",
          "is_target": false
        },
        {
          "term": "Face Validity",
          "definition": "Face validity refers to the degree to which a test appears to measure what it claims to measure based on surface-level inspection. It is assessed through subjective judgment rather than empirical methods and is considered the least rigorous form of validity evidence.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "established through statistical procedures rather than logical analysis",
      "error_correct": "established through logical analysis rather than statistical procedures",
      "explanation": "The error reverses the method by which content validity is established. Content validity is established through logical analysis (e.g., expert judgment of item relevance and representativeness) rather than statistical procedures. The definition swapped the two, incorrectly stating it relies on statistical procedures."
    },
    {
      "id": "PMET-VD-0034",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Threats to Construct Validity",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0342",
      "entries": [
        {
          "term": "Construct Irrelevant Variance",
          "definition": "A threat to construct validity that occurs when test scores are systematically influenced by factors unrelated to the construct being measured. It is addressed by minimizing irrelevant difficulty sources, providing accommodations when appropriate, and evaluating differential item functioning across groups.",
          "is_target": false
        },
        {
          "term": "Construct Underrepresentation",
          "definition": "A threat to construct validity that occurs when a test fails to capture important aspects of the construct it is intended to measure. It is addressed by ensuring comprehensive sampling of the construct domain and by using a single standardized item format to maintain consistency.",
          "is_target": true
        },
        {
          "term": "Criterion Contamination",
          "definition": "A threat to validity that occurs when the criterion measure is influenced by knowledge of predictor scores or other extraneous information. It is addressed by keeping criterion raters blind to predictor scores and ensuring independence between the two measures.",
          "is_target": false
        },
        {
          "term": "Method Variance",
          "definition": "A source of systematic error in measurement that arises when scores are influenced by the particular method of assessment rather than the construct itself. It is commonly detected and addressed through the use of multitrait-multimethod matrix analysis.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "using a single standardized item format to maintain consistency",
      "error_correct": "using multiple item formats",
      "explanation": "The passage explicitly states that construct underrepresentation should be addressed by using 'multiple item formats' to ensure comprehensive coverage of the construct domain. The error replaces this with 'a single standardized item format,' which is the opposite of the correct recommendation. Using multiple formats helps ensure that different facets of the construct are adequately sampled."
    },
    {
      "id": "PMET-VD-0035",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Standard Error of Measurement",
      "passage_type": "definition",
      "source_passage_id": "PMET-0207",
      "entries": [
        {
          "term": "Standard Error of Estimate (SEE)",
          "definition": "The SEE quantifies the margin of error in predicted scores when using a regression equation. It is calculated using the standard deviation of the criterion variable and the validity coefficient, reflecting the accuracy of predictions from one variable to another.",
          "is_target": false
        },
        {
          "term": "Standard Error of Measurement (SEM)",
          "definition": "The SEM estimates the amount of error in an individual's observed test score. It is calculated using the formula SEM = SD × √(1 + rxx), where SD is the standard deviation of test scores and rxx is the reliability coefficient. A smaller SEM indicates more precise measurement.",
          "is_target": true
        },
        {
          "term": "Standard Error of the Mean",
          "definition": "The standard error of the mean estimates how much sample means vary from the true population mean across repeated sampling. It is calculated by dividing the population standard deviation by the square root of the sample size, and it decreases as sample size increases.",
          "is_target": false
        },
        {
          "term": "Confidence Interval",
          "definition": "A confidence interval provides a range of scores around an observed score within which the true score is likely to fall. It is constructed by adding and subtracting a multiple of the SEM from the observed score, with wider intervals reflecting greater levels of confidence.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "√(1 + rxx)",
      "error_correct": "√(1 - rxx)",
      "explanation": "The SEM formula uses subtraction, not addition, under the square root: SEM = SD × √(1 - rxx). The error swapped the minus sign to a plus sign. Using (1 - rxx) means that as reliability increases (rxx approaches 1), the SEM decreases toward zero, indicating more precise measurement. Using (1 + rxx) would incorrectly suggest that higher reliability leads to greater measurement error."
    },
    {
      "id": "PMET-VD-0036",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Nonprobability Sampling",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0281",
      "entries": [
        {
          "term": "Purposive Sampling",
          "definition": "A nonprobability sampling method in which the researcher deliberately selects participants based on specific characteristics relevant to the study. This approach is used when the researcher needs subjects who meet particular criteria, and it does not guarantee every population member has a chance of being selected.",
          "is_target": false
        },
        {
          "term": "Snowball Sampling",
          "definition": "A nonprobability sampling method in which existing study participants recruit additional participants from among their acquaintances. This technique is especially useful for reaching hard-to-access or hidden populations, and it does not ensure that every member of the population has an equal chance of selection.",
          "is_target": false
        },
        {
          "term": "Convenience Sampling",
          "definition": "A nonprobability sampling method in which the researcher selects participants who are most readily available and accessible. Because participants are chosen based on ease of access, this method ensures every member of the population has at least some chance of being selected.",
          "is_target": true
        },
        {
          "term": "Quota Sampling",
          "definition": "A nonprobability sampling method in which the researcher identifies specific subgroups and then selects a predetermined number of participants from each subgroup using non-random methods. This approach aims to ensure representation of key characteristics but does not guarantee every population member has a chance of being chosen.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "this method ensures every member of the population has at least some chance of being selected",
      "error_correct": "this method does not ensure every member of the population has a chance of being selected",
      "explanation": "Convenience sampling is a nonprobability sampling method, meaning it does NOT ensure that every member of the population has a chance of being selected. The passage explicitly states that nonprobability sampling methods do not ensure every member has a chance of selection. The error reverses this defining characteristic, falsely claiming that convenience sampling provides universal selection opportunity."
    },
    {
      "id": "PMET-VD-0037",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Statistical Conclusion Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0330",
      "entries": [
        {
          "term": "Statistical Power",
          "definition": "Statistical power is the probability of correctly rejecting a false null hypothesis, thereby avoiding a Type II error. It is increased by larger sample sizes, smaller variability, and larger effect sizes. Conventionally, a power level of .80 is considered adequate for most research designs.",
          "is_target": false
        },
        {
          "term": "Type I Error",
          "definition": "A Type I error occurs when a researcher incorrectly rejects a true null hypothesis, concluding that an effect exists when it does not. The probability of a Type I error is denoted by alpha, which is conventionally set at .05. Larger sample sizes and stricter significance thresholds help reduce the risk of this error.",
          "is_target": false
        },
        {
          "term": "Type II Error",
          "definition": "A Type II error occurs when a researcher incorrectly fails to reject a false null hypothesis, concluding no effect exists when one is actually present. The probability of a Type II error is denoted by beta, and it is primarily increased by large samples and low variability. Insufficient statistical power is the most common contributor to this type of error.",
          "is_target": true
        },
        {
          "term": "Effect Size",
          "definition": "Effect size is a quantitative measure of the magnitude of a phenomenon or the strength of the relationship between variables. Common measures include Cohen's d for mean differences and eta-squared for variance explained. Larger effect sizes make it easier to detect true effects and increase statistical power.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "increased by large samples and low variability",
      "error_correct": "increased by small samples and high variability",
      "explanation": "The definition of Type II error contains a subtle reversal: it states that the probability of a Type II error is increased by 'large samples and low variability,' when in fact small samples and high variability increase the probability of a Type II error. Large samples and low variability actually reduce Type II error risk by increasing statistical power."
    },
    {
      "id": "PMET-VD-0038",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Item Response Theory (IRT)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0221",
      "entries": [
        {
          "term": "Classical Test Theory (CTT)",
          "definition": "A measurement framework that operates at the test level, decomposing an observed score into true score and error score components. CTT assumes that measurement error is constant across all ability levels and that item statistics are sample-dependent.",
          "is_target": false
        },
        {
          "term": "Item Response Theory (IRT)",
          "definition": "A measurement framework that operates at the test level, modeling the probability that a person with a given ability level will respond correctly to a specific item. IRT addresses several limitations of CTT by providing more flexible measurement properties and item parameters that are sample-independent.",
          "is_target": true
        },
        {
          "term": "Generalizability Theory (G Theory)",
          "definition": "A measurement framework that extends classical test theory by partitioning total variance into multiple sources of error simultaneously. G Theory uses analysis of variance procedures to estimate variance components associated with different facets of measurement such as raters, occasions, and items.",
          "is_target": false
        },
        {
          "term": "Rasch Model",
          "definition": "A specific one-parameter model within the IRT framework that estimates only item difficulty while assuming equal discrimination across all items. The Rasch model provides person and item measures on the same logit scale, enabling direct comparison of ability and difficulty.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "operates at the test level",
      "error_correct": "operates at the item level",
      "explanation": "The passage explicitly states that IRT operates at the item level, in contrast to Classical Test Theory which operates at the test level. The error swaps this key distinction by stating IRT operates at the test level, which is actually the defining characteristic of CTT."
    },
    {
      "id": "PMET-VD-0039",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Criterion Deficiency",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0098",
      "entries": [
        {
          "term": "Criterion Contamination",
          "definition": "Criterion contamination occurs when a criterion measure is systematically influenced by factors unrelated to the construct being assessed. This introduces irrelevant variance into the criterion scores, which can artificially inflate or distort validity coefficients.",
          "is_target": false
        },
        {
          "term": "Criterion Deficiency",
          "definition": "Criterion deficiency occurs when the criterion measure fails to capture all relevant aspects of the outcome domain. A deficient criterion overestimates true validity because the test may validly predict aspects of performance not captured by the criterion.",
          "is_target": true
        },
        {
          "term": "Criterion Relevance",
          "definition": "Criterion relevance refers to the degree to which the criterion measure adequately represents the construct domain of interest. A highly relevant criterion captures the essential components of the performance or outcome it is intended to measure.",
          "is_target": false
        },
        {
          "term": "Content Validity",
          "definition": "Content validity refers to the extent to which a measure adequately samples the full domain of content it is designed to assess. It is typically established through expert judgment rather than through statistical analysis of test scores.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "overestimates true validity",
      "error_correct": "underestimates true validity",
      "explanation": "The passage states that a deficient criterion 'underestimates true validity' because the test may validly predict aspects of performance that the criterion fails to capture. The error swaps 'underestimates' with 'overestimates.' Since the criterion is missing relevant aspects the test actually predicts, the obtained validity coefficient will be lower than the test's actual predictive power, meaning true validity is underestimated, not overestimated."
    },
    {
      "id": "PMET-VD-0040",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Introduction to Classical Conditioning",
      "passage_type": "definition",
      "source_passage_id": "PMET-0001",
      "entries": [
        {
          "term": "Unconditioned Response (UR)",
          "definition": "The unlearned, naturally occurring response to the unconditioned stimulus. It requires no prior conditioning or training to occur. For example, salivation in response to food placed in the mouth is a UR.",
          "is_target": false
        },
        {
          "term": "Conditioned Stimulus (CS)",
          "definition": "A previously neutral stimulus that, after association with the unconditioned stimulus, comes to trigger a conditioned response. Before conditioning, this stimulus produces no specific response. For example, a bell becomes a CS after repeated pairing with food.",
          "is_target": false
        },
        {
          "term": "Conditioned Response (CR)",
          "definition": "The learned response to the conditioned stimulus that emerges after repeated pairing of the CS and US. It is typically similar in form to the unconditioned response. For example, salivation in response to a bell after conditioning is a CR.",
          "is_target": false
        },
        {
          "term": "Unconditioned Stimulus (US)",
          "definition": "A stimulus that, after repeated exposure, automatically triggers a response through learned association with a neutral stimulus. It serves as the basis for classical conditioning. For example, food causing salivation is considered a US.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "after repeated exposure, automatically triggers a response through learned association with a neutral stimulus",
      "error_correct": "naturally and automatically triggers a response without prior learning",
      "explanation": "The definition of the Unconditioned Stimulus (US) incorrectly states that it triggers a response 'after repeated exposure' and 'through learned association.' By definition, the US naturally and automatically triggers a response WITHOUT any prior learning or repeated exposure — that is precisely what makes it 'unconditioned.' The erroneous definition confuses the US with the process that creates a Conditioned Stimulus."
    },
    {
      "id": "PMET-VD-0041",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0579",
      "entries": [
        {
          "term": "Content Validity",
          "definition": "Content validity refers to the degree to which items on a test adequately and representatively sample the full domain of content the test is intended to measure. It is typically established through expert judgment rather than statistical analysis. This form of validity is especially important for achievement and licensing examinations.",
          "is_target": false
        },
        {
          "term": "Construct Validity",
          "definition": "Construct validity refers to the degree to which a test measures the theoretical construct or trait it is intended to measure. It encompasses multiple lines of evidence including convergent and discriminant validity. Messick (1989) argued that construct validity is the overarching framework that subsumes all other forms of validity evidence.",
          "is_target": false
        },
        {
          "term": "Messick's Unified Validity Framework",
          "definition": "Messick's (1989) unified validity framework was presented in his seminal chapter in R. L. Linn's Educational Measurement (2nd edition). He argued that validity is a single, integrative concept centered on construct validity and that it must also consider the social consequences of test use. This framework fundamentally reshaped how validity is conceptualized in modern psychometrics.",
          "is_target": true
        },
        {
          "term": "Criterion Validity",
          "definition": "Criterion validity refers to the degree to which scores on a test are related to some external criterion measure. It includes both predictive validity, where the criterion is measured in the future, and concurrent validity, where the criterion is measured at the same time. This type of validity evidence is commonly used in personnel selection contexts.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "2nd edition",
      "error_correct": "3rd edition",
      "explanation": "Messick's (1989) seminal chapter on validity appeared in the 3rd edition of Educational Measurement edited by R. L. Linn, not the 2nd edition. The 2nd edition was published in 1971 and edited by Robert L. Thorndike. This is a subtle but factually important bibliographic error."
    },
    {
      "id": "PMET-VD-0042",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Two-Factor Theory of Avoidance",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0483",
      "entries": [
        {
          "term": "Two-Factor Theory of Avoidance",
          "definition": "A theory proposing that avoidance behavior is acquired and maintained through two learning processes. Factor 1 involves classical conditioning, where a neutral stimulus becomes a conditioned stimulus for fear through pairing with an aversive unconditioned stimulus. Factor 2 involves operant conditioning, where the organism learns to avoid the conditioned stimulus, and this avoidance is reinforced by fear reduction.",
          "is_target": false
        },
        {
          "term": "Escape Learning",
          "definition": "A form of operant conditioning in which an organism learns to perform a behavior that terminates an aversive stimulus that is already present. Unlike avoidance learning, the organism must first experience the unpleasant event before responding. The behavior is maintained through negative reinforcement, as the removal of the aversive stimulus strengthens the escape response.",
          "is_target": false
        },
        {
          "term": "Learned Helplessness",
          "definition": "A phenomenon first described by Seligman in which an organism exposed to inescapable aversive stimuli later fails to attempt escape or avoidance even when such behavior becomes possible. Factor 1 involves classical conditioning, where a neutral stimulus becomes a conditioned stimulus for fear through pairing with an appetitive unconditioned stimulus. This concept has been applied as a model for understanding depression in humans.",
          "is_target": true
        },
        {
          "term": "Avoidance Conditioning",
          "definition": "A form of learning in which an organism learns to make a response that prevents an aversive stimulus from occurring. The response is typically cued by a warning signal that precedes the aversive event. This behavior is maintained through negative reinforcement, as the successful avoidance of the aversive stimulus reinforces the avoidance response.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "pairing with an appetitive unconditioned stimulus",
      "error_correct": "pairing with an aversive unconditioned stimulus",
      "explanation": "The definition of Learned Helplessness incorrectly states that fear conditioning involves pairing with an 'appetitive' (rewarding) unconditioned stimulus. In the Two-Factor Theory's classical conditioning component (which the definition borrows from), the neutral stimulus becomes a conditioned stimulus for fear through pairing with an 'aversive' (unpleasant) unconditioned stimulus. An appetitive stimulus would produce approach behavior, not fear."
    },
    {
      "id": "PMET-VD-0043",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0532",
      "entries": [
        {
          "term": "Random Assignment",
          "definition": "Random assignment is the process of allocating participants to different conditions of an experiment using chance procedures. It is the defining feature that distinguishes true experiments from quasi-experiments and is essential for establishing internal validity and causal inferences.",
          "is_target": false
        },
        {
          "term": "Random Selection",
          "definition": "Random selection is a sampling technique in which every member of a population has an equal chance of being chosen for inclusion in a study. It is primarily used to enhance external validity and the generalizability of research findings to the broader population.",
          "is_target": false
        },
        {
          "term": "Quasi-Experimental Design",
          "definition": "A quasi-experimental design is a research approach that resembles a true experiment but lacks random selection of participants into conditions. Because of this limitation, it is more vulnerable to confounding variables and threats to internal validity than a true experiment.",
          "is_target": true
        },
        {
          "term": "Counterbalancing",
          "definition": "Counterbalancing is a procedural technique used in within-subjects designs to control for order effects by varying the sequence in which conditions are presented. It helps ensure that the order of treatments does not systematically bias the results of the study.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "lacks random selection of participants into conditions",
      "error_correct": "lacks random assignment of participants into conditions",
      "explanation": "The definition incorrectly states that quasi-experimental designs lack 'random selection.' The actual distinguishing feature is the lack of 'random assignment' (allocation of participants to conditions). Random selection refers to how participants are chosen from a population (affecting external validity), whereas random assignment refers to how participants are allocated to experimental conditions (affecting internal validity). It is the absence of random assignment that defines a quasi-experiment."
    },
    {
      "id": "PMET-VD-0044",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Introduction to Reliability",
      "passage_type": "definition",
      "source_passage_id": "PMET-0201",
      "entries": [
        {
          "term": "True Score",
          "definition": "A hypothetical score representing an examinee's actual level on the measured attribute. In Classical Test Theory, an individual's observed score is the sum of the true score and measurement error.",
          "is_target": false
        },
        {
          "term": "Measurement Error",
          "definition": "Random fluctuations in scores due to factors unrelated to the construct being measured. These unsystematic influences reduce the reliability of a test and increase the discrepancy between observed and true scores.",
          "is_target": false
        },
        {
          "term": "Reliability Coefficient",
          "definition": "A correlation coefficient indicating the proportion of score variance that is due to measurement error. It ranges from 0 to 1, with higher values indicating greater consistency and less influence from random factors.",
          "is_target": true
        },
        {
          "term": "Standard Error of Measurement",
          "definition": "An index of the amount of error in individual test scores, estimated from the reliability coefficient and the standard deviation of scores. Smaller values indicate more precise measurement of the examinee's true score.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "the proportion of score variance that is due to measurement error",
      "error_correct": "the proportion of score variance that is systematic (true score variance)",
      "explanation": "The reliability coefficient indicates the proportion of score variance that is systematic, i.e., true score variance — not variance due to measurement error. A higher reliability coefficient means more of the variance in observed scores is attributable to true differences among examinees, not to random error."
    },
    {
      "id": "PMET-VD-0045",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0472",
      "entries": [
        {
          "term": "MANCOVA",
          "definition": "An extension of ANCOVA that incorporates multiple dependent variables while statistically controlling for one or more covariates. It combines the features of MANOVA and ANCOVA to reduce error variance and control for confounding variables.",
          "is_target": false
        },
        {
          "term": "MANOVA",
          "definition": "An extension of ANOVA used when there are multiple dependent variables. It controls for Type I error inflation that would occur if separate ANOVAs were run. Key statistics include Wilks' Lambda and Pillai's Trace, and significant results are typically followed up with factor analysis.",
          "is_target": true
        },
        {
          "term": "ANOVA",
          "definition": "A statistical technique used to compare means across two or more groups on a single dependent variable. It partitions total variance into between-group and within-group components and produces an F-ratio to test the null hypothesis of equal group means.",
          "is_target": false
        },
        {
          "term": "Discriminant Function Analysis",
          "definition": "A multivariate statistical procedure used to determine which variables best distinguish between two or more naturally occurring groups. It creates linear combinations of predictor variables that maximize the separation between group centroids.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "followed up with factor analysis",
      "error_correct": "followed up with discriminant function analysis",
      "explanation": "The passage states that MANOVA results are followed up with discriminant function analysis, not factor analysis. Discriminant function analysis identifies which dependent variables contribute most to significant group differences found by MANOVA. Factor analysis is a different technique used to identify underlying latent constructs among a set of observed variables."
    },
    {
      "id": "PMET-VD-0046",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Significance Level (Alpha)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0128",
      "entries": [
        {
          "term": "Type II Error",
          "definition": "A Type II error (beta) occurs when a researcher fails to reject the null hypothesis when it is actually false. The probability of making this error is denoted by β, and it is inversely related to statistical power. Researchers can reduce Type II error by increasing sample size or effect size.",
          "is_target": false
        },
        {
          "term": "Significance Level (Alpha)",
          "definition": "The significance level (α) is the probability of rejecting the null hypothesis when it is actually true. Conventionally, α is set at .05, meaning researchers accept a 5% chance of making a Type II error. More stringent levels such as .01 or .001 may be used when the consequences of a false positive are particularly serious.",
          "is_target": true
        },
        {
          "term": "Statistical Power",
          "definition": "Statistical power is the probability of correctly rejecting the null hypothesis when it is false, calculated as 1 minus beta. Power is influenced by sample size, effect size, and the chosen significance level. A commonly accepted minimum power level in behavioral research is .80.",
          "is_target": false
        },
        {
          "term": "Effect Size",
          "definition": "Effect size is a quantitative measure of the magnitude of a phenomenon or the strength of a relationship between variables. Unlike significance testing, effect size is not directly influenced by sample size. Common measures include Cohen's d for mean differences and r² for variance explained.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "5% chance of making a Type II error",
      "error_correct": "5% chance of making a Type I error",
      "explanation": "The significance level (alpha) represents the probability of making a Type I error—incorrectly rejecting a true null hypothesis (a false positive). The definition incorrectly states it is the chance of making a Type II error, which is a different concept (failing to reject a false null hypothesis, denoted by beta)."
    },
    {
      "id": "PMET-VD-0047",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Posttest-Only Control Group Design",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0256",
      "entries": [
        {
          "term": "Pretest-Posttest Control Group Design",
          "definition": "A true experimental design in which participants are randomly assigned to groups, measured before treatment (pretest), exposed to the treatment or control condition, and then measured again (posttest). The pretest allows researchers to verify that groups were equivalent prior to the intervention.",
          "is_target": false
        },
        {
          "term": "Posttest-Only Control Group Design",
          "definition": "A true experimental design that omits the pretest and relies on matched assignment to equate groups prior to treatment. It is especially useful when pretesting might sensitize participants to the treatment or when the pretest itself could introduce a confound.",
          "is_target": true
        },
        {
          "term": "Solomon Four-Group Design",
          "definition": "A true experimental design that combines both pretest-posttest and posttest-only control group designs, using four groups total. This design allows researchers to assess the effects of pretesting itself as well as the interaction between pretesting and treatment.",
          "is_target": false
        },
        {
          "term": "Factorial Design",
          "definition": "An experimental design in which two or more independent variables are simultaneously manipulated, allowing researchers to examine main effects and interaction effects. Each combination of levels across the independent variables is represented in the design.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "relies on matched assignment to equate groups prior to treatment",
      "error_correct": "relies on random assignment to equate groups prior to treatment",
      "explanation": "The posttest-only control group design relies on random assignment, not matched assignment, to equate groups before treatment. Random assignment is what makes this a true experimental design and justifies omitting the pretest, because randomization is expected to produce equivalent groups on both known and unknown variables."
    },
    {
      "id": "PMET-VD-0048",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0451",
      "entries": [
        {
          "term": "Standards for Educational and Psychological Testing",
          "definition": "A comprehensive set of guidelines published jointly by AERA, APA, and NCME that provides criteria for evaluating tests, testing practices, and the effects of test use. The most recent edition was published in 2014 and addresses topics such as validity, reliability, and fairness in testing.",
          "is_target": false
        },
        {
          "term": "Criterion-Related Validity",
          "definition": "A type of validity evidence that assesses how well a test score predicts or correlates with an external criterion measure. It includes two subtypes: predictive validity, which examines the relationship between test scores and future criterion performance, and concurrent validity, which examines the relationship between test scores and criterion measures obtained at approximately the same time.",
          "is_target": false
        },
        {
          "term": "Content Validity",
          "definition": "A type of validity evidence based on the degree to which the content of a test adequately represents the domain it is intended to measure. It is typically established through expert judgment and systematic examination of the test items to ensure they are relevant and representative of the construct.",
          "is_target": false
        },
        {
          "term": "Construct Validity",
          "definition": "A type of validity evidence that evaluates whether a test measures the theoretical construct it claims to measure. It was originally articulated by Cronbach and Henry in 1955 and is established through multiple lines of evidence including convergent and discriminant validity as described in multitrait-multimethod analyses.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "Cronbach and Henry in 1955",
      "error_correct": "Cronbach and Meehl in 1955",
      "explanation": "The seminal paper on construct validity was authored by Lee Cronbach and Paul Meehl in 1955, not 'Cronbach and Henry.' This is a classic name swap error. Their paper 'Construct Validity in Psychological Tests' is one of the most influential works in psychometrics and is foundational to understanding how psychological constructs are validated."
    },
    {
      "id": "PMET-VD-0049",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Historical Foundations",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0166",
      "entries": [
        {
          "term": "Law of Contiguity",
          "definition": "A principle of associative learning stating that events or stimuli that occur close together in time tend to become linked in the mind. This concept was central to early behavioral theories of classical conditioning and stimulus-response associations.",
          "is_target": false
        },
        {
          "term": "Law of Effect",
          "definition": "Formulated by Edward Thorndike based on his research with dogs in puzzle boxes, this principle states that responses followed by satisfying consequences are strengthened, while responses followed by annoying consequences are weakened. It laid the groundwork for the later development of operant conditioning.",
          "is_target": true
        },
        {
          "term": "Law of Exercise",
          "definition": "A principle proposed by Edward Thorndike stating that the more frequently a stimulus-response connection is practiced, the stronger it becomes. Thorndike later revised this law after finding that repetition alone, without reinforcement, does not reliably strengthen associations.",
          "is_target": false
        },
        {
          "term": "Law of Readiness",
          "definition": "A principle proposed by Edward Thorndike suggesting that learning is facilitated when an organism is prepared to respond to a stimulus. When a learner is ready to act, doing so is satisfying, whereas being prevented from acting produces frustration.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "dogs in puzzle boxes",
      "error_correct": "cats in puzzle boxes",
      "explanation": "Thorndike's famous puzzle box experiments were conducted with cats, not dogs. He observed cats gradually learning to escape from puzzle boxes more quickly over successive trials, which led him to formulate the Law of Effect."
    },
    {
      "id": "PMET-VD-0050",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Ratio Scale",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0596",
      "entries": [
        {
          "term": "Interval Scale",
          "definition": "An interval scale has equal distances between values and an arbitrary zero point. It supports addition and subtraction but not meaningful ratios. Examples include temperature measured in Celsius or Fahrenheit.",
          "is_target": false
        },
        {
          "term": "Ratio Scale",
          "definition": "A ratio scale has equal intervals between values and a true zero point, permitting all mathematical operations. It is the only scale where meaningful ratios can be formed. However, only parametric statistical procedures are appropriate for ratio data.",
          "is_target": true
        },
        {
          "term": "Ordinal Scale",
          "definition": "An ordinal scale ranks observations in order but does not ensure equal intervals between ranks. It supports median and percentile calculations but not arithmetic means. Examples include class rankings or Likert-type response categories.",
          "is_target": false
        },
        {
          "term": "Nominal Scale",
          "definition": "A nominal scale classifies data into distinct categories without any inherent order or numeric value. It permits only mode and frequency-based statistics. Examples include gender, ethnicity, or diagnostic categories.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "only parametric statistical procedures are appropriate for ratio data",
      "error_correct": "all statistical procedures are appropriate for ratio data",
      "explanation": "The passage states that all statistical procedures are appropriate for ratio data, not only parametric ones. Because a ratio scale is the highest level of measurement, it supports every type of statistical analysis—both parametric and nonparametric. Restricting it to only parametric procedures is incorrect."
    },
    {
      "id": "PMET-0407",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Percentiles and Percentile Ranks",
      "passage_type": "paragraph",
      "original_passage": "A percentile point (or percentile) is the score at or below which a specified percentage of scores falls. The 75th percentile point is the score that 75% of examinees scored at or below",
      "modified_passage": "A percentile point (or percentile) is the score at or above which a specified percentage of scores falls. The 75th percentile point is the score that 75% of examinees scored at or below.",
      "error_original": "at or above",
      "error_correct": "at or below",
      "options": [
        "The passage incorrectly states '75th percentile point'; it should say '75th percentile rank'",
        "The passage incorrectly states 'at or above which a specified percentage of scores falls'; it should say 'at or below which a specified percentage of scores falls'",
        "The passage incorrectly states '75% of examinees'; it should say '25% of examinees'",
        "The passage incorrectly states 'a specified percentage of scores falls'; it should say 'a specified percentage of scores is excluded'"
      ],
      "correct_option_index": 1,
      "explanation": "The definition of a percentile point (or percentile) is the score at or BELOW which a specified percentage of scores falls, not 'at or above.' The passage introduced the error by reversing 'below' to 'above' in the first sentence. The second sentence correctly states 'at or below,' but the core definition in the first sentence was altered. A percentile indicates the percentage of scores falling at or below a given score, not at or above it."
    },
    {
      "id": "PMET-0231",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Inter-Rater (Inter-Scorer) Reliability",
      "passage_type": "paragraph",
      "original_passage": "Inter-rater reliability assesses the consistency of scores assigned by different raters or scorers. This is essential for tests involving subjective judgment, such as essay tests, behavioral observations, projective tests, and structured interviews",
      "modified_passage": "Inter-rater reliability assesses the consistency of scores assigned by different raters or scorers. This is essential for tests involving objective judgment, such as essay tests, behavioral observations, projective tests, and structured interviews.",
      "error_original": "objective judgment",
      "error_correct": "subjective judgment",
      "options": [
        "The passage incorrectly states 'behavioral observations'; it should say 'behavioral checklists'",
        "The passage incorrectly states 'objective judgment'; it should say 'subjective judgment'",
        "The passage incorrectly states 'consistency of scores'; it should say 'accuracy of scores'",
        "The passage incorrectly states 'structured interviews'; it should say 'unstructured interviews'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly describes inter-rater reliability as essential for tests involving 'objective judgment.' In fact, inter-rater reliability is essential for tests involving 'subjective judgment,' because it is precisely when scoring requires human interpretation and subjective evaluation—such as with essay tests, behavioral observations, projective tests, and structured interviews—that agreement between raters must be assessed. Objective tests (e.g., multiple-choice) have predetermined correct answers and do not typically require inter-rater reliability assessment."
    },
    {
      "id": "PMET-0032",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Exposure Therapy",
      "passage_type": "paragraph",
      "original_passage": "Modern exposure-based treatments emphasize extinction learning rather than counterconditioning. Prolonged exposure to feared stimuli without the feared outcome allows extinction of the fear response . Key principles include:",
      "modified_passage": "Modern exposure-based treatments emphasize counterconditioning rather than extinction learning. Prolonged exposure to feared stimuli without the feared outcome allows extinction of the fear response. Key principles include:",
      "error_original": "counterconditioning rather than extinction learning",
      "error_correct": "extinction learning rather than counterconditioning",
      "options": [
        "The passage incorrectly states that prolonged exposure occurs without the feared outcome; it should say without the conditioned stimulus",
        "The passage incorrectly states that exposure allows extinction of the fear response; it should say habituation of the fear response",
        "The passage incorrectly states 'counterconditioning rather than extinction learning'; it should say 'extinction learning rather than counterconditioning'",
        "The passage incorrectly states these are exposure-based treatments; it should say systematic desensitization treatments"
      ],
      "correct_option_index": 2,
      "explanation": "The original passage states that modern exposure-based treatments emphasize extinction learning rather than counterconditioning. The modified passage reverses this relationship, incorrectly claiming they emphasize counterconditioning rather than extinction learning. Modern exposure therapy (such as prolonged exposure) is grounded in extinction learning principles — the idea that repeated exposure to the CS without the US leads to new inhibitory learning that suppresses the conditioned fear response. Counterconditioning is a different mechanism associated with procedures like systematic desensitization, where a new response (e.g., relaxation) is paired with the feared stimulus."
    },
    {
      "id": "PMET-0449",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "original_passage": "Incremental validity: Improvement in prediction beyond existing measures (assessed with ΔR²)",
      "modified_passage": "Incremental validity: Improvement in prediction beyond existing measures (assessed with ΔF²)",
      "error_original": "ΔF²",
      "error_correct": "ΔR²",
      "options": [
        "The passage incorrectly states 'Improvement in prediction'; it should say 'Improvement in reliability'",
        "The passage incorrectly states 'ΔF²'; it should say 'ΔR²'",
        "The passage incorrectly states 'Incremental validity'; it should say 'Incremental reliability'",
        "The passage incorrectly states 'beyond existing measures'; it should say 'beyond base rates'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly identifies the statistic used to assess incremental validity as ΔF² (change in F-squared). The correct statistic is ΔR² (change in R-squared), which represents the change in the proportion of variance accounted for when a new predictor is added to a regression model. ΔR² is the standard metric for evaluating whether a new measure provides a meaningful improvement in prediction beyond what existing measures already offer. While F-tests (ΔF) may be used to determine whether ΔR² is statistically significant, the metric itself for assessing incremental validity is ΔR²."
    },
    {
      "id": "PMET-0429",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Publication Bias",
      "passage_type": "paragraph",
      "original_passage": "Funnel plot: A scatter plot of effect sizes (x-axis) against study precision or sample size (y-axis). In the absence of bias, the plot should be symmetrical around the mean effect. Asymmetry (missing studies in the lower-left corner) suggests publication bias.",
      "modified_passage": "Funnel plot: A scatter plot of effect sizes (x-axis) against study precision or sample size (y-axis). In the absence of bias, the plot should be asymmetrical around the mean effect. Asymmetry (missing studies in the lower-left corner) suggests publication bias.",
      "error_original": "asymmetrical",
      "error_correct": "symmetrical",
      "options": [
        "The passage incorrectly states effect sizes are on the x-axis; they should be on the y-axis",
        "The passage incorrectly states the plot should be asymmetrical around the mean effect; it should say symmetrical",
        "The passage incorrectly states missing studies in the lower-left corner; it should say the upper-right corner",
        "The passage incorrectly states study precision is on the y-axis; it should be on the x-axis"
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to state that in the absence of bias, the funnel plot should be 'asymmetrical' around the mean effect. This is incorrect — a funnel plot free of publication bias should be symmetrical around the mean effect size. Asymmetry is actually the indicator of potential publication bias, as stated correctly in the next sentence. The error is subtle because the word 'asymmetry' appears correctly in the following sentence describing what suggests bias, which could cause a reader to confuse the two uses."
    },
    {
      "id": "PMET-0250",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Interrupted Time-Series Design",
      "passage_type": "example",
      "original_passage": "Time-Series Example A hospital implements a new suicide prevention protocol and tracks monthly suicide attempts for two years before and two years after implementation. By examining the pattern of data points, researchers can determine whether the intervention was associated with a change in level or trend of suicide attempts",
      "modified_passage": "Time-Series Example A hospital implements a new suicide prevention protocol and tracks monthly suicide attempts for two years before and two years after implementation. By examining the pattern of data points, researchers can determine whether the intervention was associated with a change in level or slope of suicide attempts",
      "error_original": "slope",
      "error_correct": "trend",
      "options": [
        "The passage incorrectly states 'two years before and two years after'; it should say 'one year before and one year after'",
        "The passage incorrectly states 'monthly suicide attempts'; it should say 'weekly suicide attempts'",
        "The passage incorrectly states 'slope'; it should say 'trend'",
        "The passage incorrectly states 'level or slope'; it should say 'level or variability'"
      ],
      "correct_option_index": 2,
      "explanation": "In interrupted time-series design, researchers examine whether an intervention is associated with a change in 'level' (an immediate shift in the data) or 'trend' (a change in the direction or rate of the data over time). The passage incorrectly substituted 'slope' for 'trend.' While 'slope' may sound related, the standard terminology used in interrupted time-series analysis is 'trend,' which encompasses changes in the overall pattern of data points over time. The other options describe changes that were not actually made to the passage."
    },
    {
      "id": "PMET-0452",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "References",
      "passage_type": "paragraph",
      "original_passage": "Hunsley, J., & Meyer, G. J. (2003). The incremental validity of psychological testing and assessment: Conceptual, methodological, and statistical issues. Psychological Assessment, 15(4), 446-455.",
      "modified_passage": "Hunsley, J., & Meyer, G. J. (2003). The incremental validity of psychological testing and assessment: Conceptual, methodological, and statistical issues. Psychological Assessment, 15(4), 446-455.",
      "error_original": "15(4), 446-455",
      "error_correct": "15(4), 446-455",
      "options": [
        "The passage incorrectly states the publication year as 2003; it should say 2001",
        "The passage incorrectly states the journal as Psychological Assessment; it should say Psychological Bulletin",
        "The passage incorrectly states the topic as 'incremental validity'; it should say 'predictive validity'",
        "The passage incorrectly states the second author as Meyer, G. J.; it should say Meehl, P. E."
      ],
      "correct_option_index": 2,
      "explanation": "No error was actually introduced in the passage as displayed — let me regenerate properly."
    },
    {
      "id": "PMET-0066",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Regression to the Mean",
      "passage_type": "paragraph",
      "original_passage": "Regression to the mean is a statistical phenomenon wherein extreme scores on one measurement tend to be followed by less extreme scores on subsequent measurements. This occurs whenever the correlation between two variables is less than perfect (r < 1.00). The term \"regression\" was coined by Francis Galton based on this observation",
      "modified_passage": "Regression to the mean is a statistical phenomenon wherein extreme scores on one measurement tend to be followed by less extreme scores on subsequent measurements. This occurs whenever the correlation between two variables is less than perfect (r < 1.00). The term \"regression\" was coined by Karl Pearson based on this observation.",
      "error_original": "Karl Pearson",
      "error_correct": "Francis Galton",
      "options": [
        "The passage incorrectly states that regression to the mean involves 'less extreme scores'; it should say 'more extreme scores'",
        "The passage incorrectly states the correlation threshold as 'r < 1.00'; it should say 'r < 0.50'",
        "The passage incorrectly states the term was coined by 'Karl Pearson'; it should say 'Francis Galton'",
        "The passage incorrectly states 'extreme scores on one measurement'; it should say 'average scores on one measurement'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage attributes the coining of the term 'regression' to Karl Pearson, but it was actually Francis Galton who coined the term based on his observations of hereditary characteristics (e.g., the heights of parents and their children). Karl Pearson was a contemporary who contributed significantly to statistics (e.g., the Pearson correlation coefficient) but did not coin the term 'regression.' The other options describe changes that were not actually made to the passage."
    },
    {
      "id": "PMET-0394",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Descriptive Statistics",
      "passage_type": "paragraph",
      "original_passage": "Descriptive statistics summarize and organize data to make patterns more interpretable. They fall into three main categories: measures of central tendency, measures of variability, and measures of relative position",
      "modified_passage": "Descriptive statistics summarize and organize data to make patterns more interpretable. They fall into three main categories: measures of central tendency, measures of variability, and measures of correlation.",
      "error_original": "measures of correlation",
      "error_correct": "measures of relative position",
      "options": [
        "The passage incorrectly states 'measures of variability'; it should say 'measures of dispersion'",
        "The passage incorrectly states 'summarize and organize'; it should say 'analyze and interpret'",
        "The passage incorrectly states 'measures of correlation'; it should say 'measures of relative position'",
        "The passage incorrectly states 'three main categories'; it should say 'four main categories'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage replaced 'measures of relative position' with 'measures of correlation.' The three main categories of descriptive statistics are measures of central tendency, measures of variability, and measures of relative position (e.g., percentile ranks, z-scores). Measures of correlation are a type of inferential or associational statistic, not one of the three main categories of descriptive statistics. Note that 'measures of variability' and 'measures of dispersion' are synonymous terms, so option A describes a distinction without a real difference. The passage correctly states three categories, not four."
    },
    {
      "id": "PMET-0476",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Problems with Punishment",
      "passage_type": "paragraph",
      "original_passage": "Temporary suppression: Punishment often suppresses behavior only temporarily and in the presence of the punishing agent",
      "modified_passage": "Temporary suppression: Punishment often eliminates behavior only temporarily and in the presence of the punishing agent",
      "error_original": "eliminates",
      "error_correct": "suppresses",
      "options": [
        "The passage incorrectly states 'temporarily'; it should say 'permanently'",
        "The passage incorrectly states 'eliminates'; it should say 'suppresses'",
        "The passage incorrectly states 'in the presence of'; it should say 'in the absence of'",
        "The passage incorrectly states 'punishing agent'; it should say 'reinforcing agent'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly uses the word 'eliminates' when it should say 'suppresses.' A key problem with punishment is that it does not eliminate behavior but merely suppresses it. This is an important distinction in operant conditioning: punishment temporarily suppresses the target behavior, particularly when the punishing agent is present, but the behavior often returns when the punishment is removed or the punishing agent is absent. The word 'eliminates' overstates the effect and misrepresents a fundamental limitation of punishment as a behavioral intervention."
    },
    {
      "id": "PMET-0232",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Standard Error of Measurement",
      "passage_type": "paragraph",
      "original_passage": "The standard error of measurement (SEM) quantifies the amount of error expected in an individual's observed score. While the reliability coefficient describes group-level consistency, the SEM provides information about the precision of individual scores",
      "modified_passage": "The standard error of measurement (SEM) quantifies the amount of error expected in an individual's observed score. While the reliability coefficient describes group-level consistency, the SEM provides information about the accuracy of individual scores",
      "error_original": "accuracy",
      "error_correct": "precision",
      "options": [
        "The passage incorrectly states 'observed score'; it should say 'true score'",
        "The passage incorrectly states 'group-level consistency'; it should say 'individual-level consistency'",
        "The passage incorrectly states 'accuracy of individual scores'; it should say 'precision of individual scores'",
        "The passage incorrectly states 'reliability coefficient'; it should say 'validity coefficient'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage replaced the word 'precision' with 'accuracy.' In psychometrics, the SEM specifically provides information about the precision of individual scores — that is, how close repeated measurements of the same person would cluster together. Precision and accuracy are distinct concepts: precision refers to the consistency or repeatability of measurements, while accuracy refers to how close a measurement is to the true value. The SEM is a measure of precision, not accuracy. The other options describe changes that were not actually made in the passage."
    },
    {
      "id": "PMET-0577",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "References",
      "passage_type": "paragraph",
      "original_passage": "Campbell, D. T., & Fiske, D. W. (1959). Convergent and discriminant validation by the multitrait-multimethod matrix. Psychological Bulletin, 56(2), 81-105.",
      "modified_passage": "Campbell, D. T., & Fiske, D. W. (1959). Convergent and discriminant validation by the multitrait-multimethod matrix. Psychological Review, 56(2), 81-105.",
      "error_original": "Psychological Review",
      "error_correct": "Psychological Bulletin",
      "options": [
        "The passage incorrectly states the publication year as 1959; it should say 1957",
        "The passage incorrectly states the journal as Psychological Review; it should say Psychological Bulletin",
        "The passage incorrectly states the volume number as 56; it should say 58",
        "The passage incorrectly states the second author as Fiske, D. W.; it should say Cronbach, L. J."
      ],
      "correct_option_index": 1,
      "explanation": "The seminal Campbell and Fiske (1959) article on convergent and discriminant validation using the multitrait-multimethod matrix was published in Psychological Bulletin, not Psychological Review. Both are well-known APA journals, making this a subtle but verifiable error. The correct citation is: Psychological Bulletin, 56(2), 81-105."
    },
    {
      "id": "PMET-0401",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "The Normal Distribution",
      "passage_type": "paragraph",
      "original_passage": "The normal distribution (also called the Gaussian distribution or bell curve) is a theoretical probability distribution that is symmetric, unimodal, and asymptotic. Many psychological variables approximate normal distributions in large samples, and the normal distribution underlies many statistical procedures",
      "modified_passage": "The normal distribution (also called the Gaussian distribution or bell curve) is a theoretical probability distribution that is symmetric, bimodal, and asymptotic. Many psychological variables approximate normal distributions in large samples, and the normal distribution underlies many statistical procedures.",
      "error_original": "bimodal",
      "error_correct": "unimodal",
      "options": [
        "The passage incorrectly states the distribution is 'symmetric'; it should say 'asymmetric'",
        "The passage incorrectly states 'bimodal'; it should say 'unimodal'",
        "The passage incorrectly states 'Gaussian distribution'; it should say 'Galton distribution'",
        "The passage incorrectly states 'asymptotic'; it should say 'platykurtic'"
      ],
      "correct_option_index": 1,
      "explanation": "The normal distribution is unimodal, meaning it has a single peak or mode. The passage incorrectly describes it as 'bimodal,' which would mean it has two peaks. A bimodal distribution has two distinct modes and does not describe the classic bell curve shape of the normal distribution. The three defining characteristics of the normal distribution are that it is symmetric, unimodal, and asymptotic (the tails approach but never touch the x-axis)."
    },
    {
      "id": "PMET-0522",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Item Parameters",
      "passage_type": "paragraph",
      "original_passage": "IRT models characterize items using up to three parameters:",
      "modified_passage": "IRT models characterize items using up to three parameters: the a-parameter represents item difficulty and indicates the location on the latent trait continuum where the item functions best; the b-parameter represents item discrimination and reflects how well the item differentiates between examinees at different trait levels; and the c-parameter represents the pseudo-guessing parameter, which accounts for the probability of a low-ability examinee answering correctly by chance. In the one-parameter (Rasch) model, only item difficulty is estimated, while the two-parameter model adds discrimination, and the three-parameter model includes all three. Higher discrimination values produce steeper item characteristic curves, meaning the item more sharply distinguishes between examinees above and below the difficulty threshold.",
      "error_original": "the a-parameter represents item difficulty and indicates the location on the latent trait continuum where the item functions best; the b-parameter represents item discrimination",
      "error_correct": "the a-parameter represents item discrimination; the b-parameter represents item difficulty and indicates the location on the latent trait continuum where the item functions best",
      "options": [
        "The passage incorrectly states that higher discrimination values produce steeper item characteristic curves; it should say flatter item characteristic curves",
        "The passage incorrectly states that the c-parameter represents the pseudo-guessing parameter; it should say the pseudo-ceiling parameter",
        "The passage incorrectly states that the a-parameter represents item difficulty and the b-parameter represents item discrimination; it should say the a-parameter represents discrimination and the b-parameter represents difficulty",
        "The passage incorrectly states that the one-parameter (Rasch) model estimates only item difficulty; it should say it estimates only item discrimination"
      ],
      "correct_option_index": 2,
      "explanation": "In Item Response Theory, the a-parameter represents item discrimination (how well the item differentiates between examinees), while the b-parameter represents item difficulty (the location on the trait continuum). The passage swapped these two definitions. The a-parameter corresponds to the slope of the item characteristic curve (discrimination), and the b-parameter corresponds to the inflection point on the trait scale (difficulty). The other options describe changes that were not made in the passage."
    },
    {
      "id": "PMET-0035",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Standard Error of Estimate",
      "passage_type": "definition",
      "original_passage": "Key Regression Concepts Predictor Variable (X): The variable used to make predictions; also called the independent variable in regression. Criterion Variable (Y): The variable being predicted; also called the dependent variable in regression. Residual: The difference between the actual Y score and the predicted Y score (Y - Y'). SEE: The standard deviation of the residuals; indicates prediction accuracy",
      "modified_passage": "Key Regression Concepts Predictor Variable (X): The variable used to make predictions; also called the independent variable in regression. Criterion Variable (Y): The variable being predicted; also called the dependent variable in regression. Residual: The difference between the actual Y score and the predicted Y score (Y - Y'). SEE: The variance of the residuals; indicates prediction accuracy",
      "error_original": "The variance of the residuals",
      "error_correct": "The standard deviation of the residuals",
      "options": [
        "The passage incorrectly states that the Criterion Variable is 'also called the dependent variable'; it should say 'also called the control variable'",
        "The passage incorrectly states that the Residual is '(Y - Y')'; it should say '(Y' - Y)'",
        "The passage incorrectly states that the SEE is 'the variance of the residuals'; it should say 'the standard deviation of the residuals'",
        "The passage incorrectly states that the Predictor Variable is 'also called the independent variable'; it should say 'also called the extraneous variable'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage incorrectly defines the Standard Error of Estimate (SEE) as 'the variance of the residuals.' The SEE is actually the standard deviation of the residuals, not the variance. The standard deviation and variance are related but distinct measures — the variance is the square of the standard deviation. The SEE specifically represents the standard deviation of the distribution of residuals (prediction errors) around the regression line, providing a measure of how accurate predictions are in the original units of the criterion variable."
    },
    {
      "id": "PMET-0432",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "original_passage": "Correlation ≠ Causation: Correlation describes relationships but cannot establish cause and effect",
      "modified_passage": "Correlation ≠ Causation: Correlation describes relationships but cannot establish cause or effect independently; however, a sufficiently high correlation coefficient (e.g., r > .80) can be used to infer directionality of the relationship between two variables.",
      "error_original": "a sufficiently high correlation coefficient (e.g., r > .80) can be used to infer directionality of the relationship between two variables",
      "error_correct": "even a high correlation coefficient cannot be used to infer directionality or causation between two variables",
      "options": [
        "The passage incorrectly states that correlation 'describes relationships'; it should say correlation 'describes differences'",
        "The passage incorrectly states that a sufficiently high correlation can be used to infer directionality; in fact, no correlation, regardless of magnitude, can establish directionality or causation",
        "The passage incorrectly uses r > .80 as the threshold; the correct threshold for a high correlation is r > .70",
        "The passage incorrectly states 'Correlation ≠ Causation'; it should state 'Correlation ≈ Causation' under certain conditions"
      ],
      "correct_option_index": 1,
      "explanation": "The fundamental principle in correlational research is that correlation never implies causation or directionality, regardless of the magnitude of the correlation coefficient. A correlation of r = .99 is just as incapable of establishing cause-and-effect or directionality as a correlation of r = .10. The passage erroneously claims that a high correlation (r > .80) can be used to infer directionality, which directly contradicts this core principle. The other options are distractors: correlation does describe relationships (not differences), the specific threshold of .80 vs. .70 is irrelevant since no magnitude permits causal inference, and the notation 'Correlation ≠ Causation' is the correct and well-established principle."
    },
    {
      "id": "PMET-0249",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Factorial Designs",
      "passage_type": "example",
      "original_passage": "Factorial Design Example A researcher examines the effects of therapy type (CBT vs. psychodynamic) and medication (present vs. absent) on anxiety symptoms using a 2 × 2 factorial design. This allows examination of: (1) the main effect of therapy type, (2) the main effect of medication, and (3) the interaction between therapy and medication (e.g., whether CBT is particularly effective when combined with medication)",
      "modified_passage": "Factorial Design Example A researcher examines the effects of therapy type (CBT vs. psychodynamic) and medication (present vs. absent) on anxiety symptoms using a 2 × 2 factorial design. This allows examination of: (1) the main effect of therapy type, (2) the main effect of medication, and (3) the correlation between therapy and medication (e.g., whether CBT is particularly effective when combined with medication)",
      "error_original": "the correlation between therapy and medication",
      "error_correct": "the interaction between therapy and medication",
      "options": [
        "The passage incorrectly states 'psychodynamic'; it should say 'humanistic'",
        "The passage incorrectly states '2 × 2 factorial design'; it should say '2 × 3 factorial design'",
        "The passage incorrectly states 'the correlation between therapy and medication'; it should say 'the interaction between therapy and medication'",
        "The passage incorrectly states 'main effect of therapy type'; it should say 'simple effect of therapy type'"
      ],
      "correct_option_index": 2,
      "explanation": "In a factorial design, the third element examined (beyond the two main effects) is the interaction between the independent variables, not the correlation. An interaction effect tests whether the effect of one independent variable depends on the level of another independent variable. Correlation is a statistical concept describing the linear relationship between two continuous variables, which is not what is being examined in a factorial ANOVA design. The passage should read 'the interaction between therapy and medication.'"
    },
    {
      "id": "PMET-0415",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Systematic Desensitization",
      "passage_type": "paragraph",
      "original_passage": "Progressing through the hierarchy until the most feared item can be imagined while relaxed",
      "modified_passage": "Progressing through the hierarchy until the least feared item can be imagined while relaxed",
      "error_original": "least feared item",
      "error_correct": "most feared item",
      "options": [
        "The passage incorrectly states 'imagined while relaxed'; it should say 'imagined while aroused'",
        "The passage incorrectly states 'least feared item'; it should say 'most feared item'",
        "The passage incorrectly states 'progressing through the hierarchy'; it should say 'regressing through the hierarchy'",
        "The passage incorrectly states 'imagined'; it should say 'experienced in vivo'"
      ],
      "correct_option_index": 1,
      "explanation": "In systematic desensitization, the client progressively works through the anxiety hierarchy from least to most feared items. The goal is to eventually be able to remain relaxed while imagining the MOST feared item, not the least feared item. The passage incorrectly substituted 'least' for 'most,' reversing the endpoint of the procedure. The least feared item is where the client begins, not where they finish."
    },
    {
      "id": "PMET-0536",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "original_passage": "Know the difference between cross-sectional (one time point, different groups) and longitudinal (multiple time points, same participants) designs",
      "modified_passage": "Know the difference between cross-sectional (one time point, different groups) and longitudinal (multiple time points, different participants) designs",
      "error_original": "different participants",
      "error_correct": "same participants",
      "options": [
        "The passage incorrectly states that cross-sectional designs use 'different groups'; it should say 'same groups'",
        "The passage incorrectly states that cross-sectional designs use 'one time point'; it should say 'multiple time points'",
        "The passage incorrectly states that longitudinal designs use 'different participants'; it should say 'same participants'",
        "The passage incorrectly states that longitudinal designs use 'multiple time points'; it should say 'one time point'"
      ],
      "correct_option_index": 2,
      "explanation": "The error introduced was changing 'same participants' to 'different participants' in the description of longitudinal designs. A defining feature of longitudinal research is that the same participants are studied across multiple time points. Studying different participants at multiple time points would instead describe a cross-sequential or repeated cross-sectional design, not a true longitudinal design."
    },
    {
      "id": "PMET-0550",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "References",
      "passage_type": "paragraph",
      "original_passage": "Biglan, A., Ary, D., & Wagenaar, A. C. (2000). The value of interrupted time-series experiments for community intervention research. Prevention Science, 1(1), 31-49.",
      "modified_passage": "Biglan, A., Ary, D., & Wagenaar, A. C. (2000). The value of interrupted time-series experiments for community intervention research. Prevention Science, 2(1), 31-49.",
      "error_original": "2(1)",
      "error_correct": "1(1)",
      "options": [
        "The passage incorrectly states the journal name as 'Prevention Science'; it should say 'Prevention Research'",
        "The passage incorrectly states the volume number as '2(1)'; it should say '1(1)'",
        "The passage incorrectly states the page range as '31-49'; it should say '31-59'",
        "The passage incorrectly states the publication year as '2000'; it should say '2001'"
      ],
      "correct_option_index": 1,
      "explanation": "The volume number was changed from 1(1) to 2(1). The correct citation indicates this article was published in Volume 1, Issue 1 of Prevention Science. This was the inaugural issue of the journal, published in 2000. All other details in the reference — the authors, year, article title, journal name, and page numbers — are correct as stated."
    },
    {
      "id": "PMET-0208",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Applications of IRT",
      "passage_type": "clinical_note",
      "original_passage": "IRT and the EPPP The EPPP itself uses IRT-based scoring. Items are calibrated using IRT models, and examinee scores are estimated on a theta scale. This means the EPPP can draw from a large item bank while maintaining score comparability across administrations. The pass/fail decision is based on a theta cutoff rather than a raw percentage correct, which accounts for differences in item difficulty across test forms",
      "modified_passage": "IRT and the EPPP The EPPP itself uses IRT-based scoring. Items are calibrated using IRT models, and examinee scores are estimated on a delta scale. This means the EPPP can draw from a large item bank while maintaining score comparability across administrations. The pass/fail decision is based on a theta cutoff rather than a raw percentage correct, which accounts for differences in item difficulty across test forms.",
      "error_original": "delta scale",
      "error_correct": "theta scale",
      "options": [
        "The passage incorrectly states that the pass/fail decision accounts for differences in item difficulty; it should say differences in item discrimination.",
        "The passage incorrectly states scores are estimated on a delta scale; it should say a theta scale.",
        "The passage incorrectly states the pass/fail decision is based on a theta cutoff; it should say a stanine cutoff.",
        "The passage incorrectly states the EPPP uses IRT-based scoring; it should say CTT-based scoring."
      ],
      "correct_option_index": 1,
      "explanation": "In IRT, examinee ability is estimated on a theta (θ) scale. The passage was modified to say 'delta scale' instead of the correct term 'theta scale.' Delta is not the standard term used in IRT for the ability parameter — theta is the conventional notation. The other options describe changes that were not actually made to the passage."
    },
    {
      "id": "PMET-0398",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Measures of Variability",
      "passage_type": "paragraph",
      "original_passage": "The range is the difference between the highest and lowest scores. It is simple but unstable because it is based on only two scores and is highly sensitive to outliers",
      "modified_passage": "The range is the difference between the highest and lowest scores. It is simple but unstable because it is based on only two scores and is highly sensitive to skewness",
      "error_original": "skewness",
      "error_correct": "outliers",
      "options": [
        "The passage incorrectly states the range is the difference between the highest and lowest scores; it should say it is the difference between the mean and the lowest score",
        "The passage incorrectly states the range is based on only two scores; it should say it is based on three scores",
        "The passage incorrectly states the range is sensitive to skewness; it should say it is sensitive to outliers",
        "The passage incorrectly states the range is simple but unstable; it should say it is complex but stable"
      ],
      "correct_option_index": 2,
      "explanation": "The original passage correctly states that the range is 'highly sensitive to outliers.' In the modified passage, 'outliers' was replaced with 'skewness.' While skewness is a related statistical concept, the specific vulnerability of the range as a measure of variability is its sensitivity to outliers (extreme scores), since it relies solely on the two most extreme values in a distribution. Skewness describes the asymmetry of a distribution and is a different concept altogether."
    },
    {
      "id": "PMET-0064",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Multiple Correlation Coefficient (R)",
      "passage_type": "paragraph",
      "original_passage": "R² indicates the proportion of variance in the criterion variable explained by the combination of all predictor variables. R² always increases (or stays the same) when additional predictors are added, even if those predictors have no true relationship with the criterion. For this reason, adjusted R² is often reported, which accounts for the number of predictors and sample size",
      "modified_passage": "R² indicates the proportion of variance in the criterion variable explained by the combination of all predictor variables. R² always increases (or stays the same) when additional predictors are added, even if those predictors have no true relationship with the criterion. For this reason, adjusted R² is often reported, which accounts for the number of predictors and effect size",
      "error_original": "effect size",
      "error_correct": "sample size",
      "options": [
        "The passage incorrectly states that R² 'always increases (or stays the same)'; it should say 'always decreases (or stays the same)'",
        "The passage incorrectly states 'effect size'; it should say 'sample size'",
        "The passage incorrectly states 'criterion variable'; it should say 'predictor variable'",
        "The passage incorrectly states 'proportion of variance'; it should say 'standard deviation of variance'"
      ],
      "correct_option_index": 1,
      "explanation": "The original passage correctly states that adjusted R² accounts for the number of predictors and sample size. The modified passage substituted 'effect size' for 'sample size.' Adjusted R² penalizes for the number of predictors relative to the sample size, not effect size. The other options describe errors that were not actually introduced: R² does always increase or stay the same with additional predictors, it does refer to the criterion variable, and it does refer to proportion of variance."
    },
    {
      "id": "PMET-0302",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Introduction to Research Validity",
      "passage_type": "paragraph",
      "original_passage": "Research validity refers to the accuracy and trustworthiness of research conclusions. In the context of research methodology, validity concerns whether a study truly measures what it claims to measure and whether its conclusions are justified. Understanding different types of validity and the threats to each is essential for evaluating psychological research and designing rigorous studies",
      "modified_passage": "Research validity refers to the accuracy and trustworthiness of research conclusions. In the context of research methodology, validity concerns whether a study truly measures what it claims to measure and whether its conclusions are justified. Understanding different types of reliability and the threats to each is essential for evaluating psychological research and designing rigorous studies.",
      "error_original": "reliability",
      "error_correct": "validity",
      "options": [
        "The passage incorrectly states 'accuracy and trustworthiness'; it should say 'precision and consistency'",
        "The passage incorrectly states 'reliability' in the final sentence; it should say 'validity'",
        "The passage incorrectly states 'truly measures what it claims to measure'; this defines reliability, not validity",
        "The passage incorrectly states 'designing rigorous studies'; it should say 'designing reliable studies'"
      ],
      "correct_option_index": 1,
      "explanation": "The original passage consistently discusses 'validity' throughout. In the modified passage, the final sentence incorrectly substitutes 'reliability' for 'validity,' stating 'Understanding different types of reliability and the threats to each...' The passage is entirely about research validity—not reliability. Reliability refers to the consistency or repeatability of measurements, which is a distinct psychometric concept. The correct term should be 'validity,' as the passage is focused on whether studies accurately measure what they intend to measure and whether conclusions are justified."
    },
    {
      "id": "PMET-0070",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Meta-Analysis",
      "passage_type": "paragraph",
      "original_passage": "Meta-analysis is a quantitative method for systematically combining results from multiple studies to arrive at an overall estimate of an effect. Unlike a narrative literature review, meta-analysis uses statistical techniques to integrate findings, weight studies by quality or sample size, and assess variability across studies . Correlation coefficients can be converted to other effect size metrics and combined to estimate population-level relationships while accounting for sampling error and study artifacts (Hunter & Schmidt, 2004)",
      "modified_passage": "Meta-analysis is a quantitative method for systematically combining results from multiple studies to arrive at an overall estimate of an effect. Unlike a narrative literature review, meta-analysis uses statistical techniques to integrate findings, weight studies by quality or sample size, and assess variability across studies. Correlation coefficients can be converted to other effect size metrics and combined to estimate sample-level relationships while accounting for sampling error and study artifacts (Hunter & Schmidt, 2004)",
      "error_original": "sample-level relationships",
      "error_correct": "population-level relationships",
      "options": [
        "The passage incorrectly states 'quantitative method'; it should say 'qualitative method'",
        "The passage incorrectly states 'sample-level relationships'; it should say 'population-level relationships'",
        "The passage incorrectly states 'Hunter & Schmidt, 2004'; it should say 'Cohen & Cohen, 2004'",
        "The passage incorrectly states 'sampling error'; it should say 'measurement error'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to say 'sample-level relationships' when it should say 'population-level relationships.' A key purpose of meta-analysis, particularly as described by Hunter and Schmidt (2004), is to estimate population-level parameters by aggregating across multiple samples. The goal is to move beyond individual sample estimates to infer the true underlying population relationship, correcting for sampling error and other study artifacts."
    },
    {
      "id": "PMET-0360",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Consequences of Testing",
      "passage_type": "paragraph",
      "original_passage": "This evidence considers the intended and unintended consequences of test use, including potential benefits and harms. It addresses whether test use achieves its intended outcomes and whether any negative consequences are related to construct-irrelevant factors such as bias",
      "modified_passage": "This evidence considers the intended and unintended consequences of test use, including potential benefits and harms. It addresses whether test use achieves its intended outcomes and whether any negative consequences are related to construct-underrepresentation factors such as bias",
      "error_original": "construct-underrepresentation factors",
      "error_correct": "construct-irrelevant factors",
      "options": [
        "The passage incorrectly states 'intended and unintended consequences'; it should say 'intended and anticipated consequences'",
        "The passage incorrectly states 'construct-underrepresentation factors'; it should say 'construct-irrelevant factors'",
        "The passage incorrectly states 'potential benefits and harms'; it should say 'potential reliability and validity concerns'",
        "The passage incorrectly states 'intended outcomes'; it should say 'predicted outcomes'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly uses 'construct-underrepresentation factors' when it should say 'construct-irrelevant factors.' In validity theory, construct-irrelevant variance refers to systematic variance in test scores that is attributable to factors outside the construct being measured (e.g., bias), which can lead to negative consequences of test use. Construct underrepresentation is a different validity threat that refers to a test failing to capture important aspects of the construct. The original passage correctly identified construct-irrelevant factors (such as bias) as the source of negative consequences in consequential evidence of validity."
    },
    {
      "id": "PMET-0403",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Skewness and Kurtosis",
      "passage_type": "paragraph",
      "original_passage": "Real data often deviate from the normal distribution. Two important characteristics describe the shape of distributions: skewness and kurtosis",
      "modified_passage": "Real data often deviate from the normal distribution. Two important characteristics describe the shape of distributions: skewness and variance.",
      "error_original": "variance",
      "error_correct": "kurtosis",
      "options": [
        "The passage incorrectly states that real data 'often' deviate from the normal distribution; it should say 'rarely'",
        "The passage incorrectly states 'variance' as a shape characteristic; it should say 'kurtosis'",
        "The passage incorrectly states there are 'two' important shape characteristics; it should say 'three'",
        "The passage incorrectly states these characteristics describe the 'shape' of distributions; it should say 'central tendency'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage replaced 'kurtosis' with 'variance.' Variance is a measure of dispersion (spread) in a distribution, not a measure of its shape. The two key characteristics that describe the shape of a distribution are skewness (which measures asymmetry) and kurtosis (which measures the degree of peakedness or heaviness of the tails relative to a normal distribution). Together, skewness and kurtosis are the standard descriptors of distributional shape in statistics and psychometrics."
    },
    {
      "id": "PMET-0022",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Experimental Neurosis",
      "passage_type": "paragraph",
      "original_passage": "Pavlov observed that when discrimination training becomes too difficult - when the CS+ and CS- become increasingly similar - animals may develop behavioral disturbances including agitation, aggression, and stereotyped behaviors. He termed this experimental neurosis",
      "modified_passage": "Pavlov observed that when generalization training becomes too difficult - when the CS+ and CS- become increasingly similar - animals may develop behavioral disturbances including agitation, aggression, and stereotyped behaviors. He termed this experimental neurosis",
      "error_original": "generalization training",
      "error_correct": "discrimination training",
      "options": [
        "The passage incorrectly states that Pavlov observed this phenomenon; it was actually Watson who first described experimental neurosis",
        "The passage incorrectly states 'generalization training'; it should say 'discrimination training'",
        "The passage incorrectly states that CS+ and CS- become increasingly similar; they actually become increasingly different",
        "The passage incorrectly states the term was 'experimental neurosis'; Pavlov actually called it 'conditioned anxiety'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage substituted 'generalization training' for 'discrimination training.' Experimental neurosis occurs specifically when discrimination training becomes too difficult — that is, when the organism is required to discriminate between a CS+ (reinforced stimulus) and a CS- (non-reinforced stimulus) that are made increasingly similar. Generalization is the opposite process, where responding spreads to similar stimuli, and is not the training procedure that produces experimental neurosis. Pavlov demonstrated that it was the increasingly difficult discrimination task that led to the behavioral disturbances he termed experimental neurosis."
    },
    {
      "id": "PMET-0239",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Introduction to Research Designs",
      "passage_type": "definition",
      "original_passage": "Key Terminology True Experiment: A design with random assignment, manipulation of the independent variable, and control of extraneous variables Quasi-Experiment: A design lacking random assignment but including manipulation of the independent variable Random Assignment: Procedure ensuring each participant has an equal chance of being assigned to any condition Control Group: A comparison condition that does not receive the experimental treatment Between-Subjects Design: Different participants in each condition Within-Subjects Design: Same participants tested in all conditions",
      "modified_passage": "Key Terminology True Experiment: A design with random assignment, manipulation of the independent variable, and control of extraneous variables Quasi-Experiment: A design lacking random assignment but including manipulation of the independent variable Random Assignment: Procedure ensuring each participant has an equal chance of being assigned to any condition Control Group: A comparison condition that does not receive the experimental treatment Between-Subjects Design: Different participants in each condition Within-Subjects Design: Same participants tested in all conditions. A quasi-experiment differs from a true experiment primarily because it lacks manipulation of the independent variable.",
      "error_original": "it lacks manipulation of the independent variable",
      "error_correct": "it lacks random assignment",
      "options": [
        "The passage incorrectly states that a control group 'does not receive the experimental treatment'; it should say 'receives a placebo treatment'",
        "The passage incorrectly states that a quasi-experiment 'lacks manipulation of the independent variable'; it should say 'lacks random assignment'",
        "The passage incorrectly states that random assignment ensures 'an equal chance of being assigned to any condition'; it should say 'a proportional chance of being assigned to any condition'",
        "The passage incorrectly states that a within-subjects design has 'same participants tested in all conditions'; it should say 'different participants tested in all conditions'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage's final sentence incorrectly states that a quasi-experiment differs from a true experiment because it lacks manipulation of the independent variable. According to the definitions provided earlier in the passage itself (and standard research methodology), a quasi-experiment DOES include manipulation of the independent variable — what it lacks is random assignment. This is the defining distinction between true experiments and quasi-experiments."
    },
    {
      "id": "PMET-0144",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Analysis of Covariance (ANCOVA)",
      "passage_type": "paragraph",
      "original_passage": "ANCOVA combines ANOVA with regression by including one or more covariates —continuous variables that are statistically controlled to reduce error variance and remove the influence of confounding variables. ANCOVA adjusts group means on the DV to account for pre-existing differences on the covariate",
      "modified_passage": "ANCOVA combines ANOVA with regression by including one or more covariates —continuous variables that are statistically controlled to reduce error variance and remove the influence of confounding variables. ANCOVA adjusts group means on the DV to account for pre-existing differences on the covariate",
      "error_original": "reduce error variance",
      "error_correct": "reduce error variance",
      "options": [
        "The passage incorrectly states that ANCOVA combines ANOVA with regression; it should say ANCOVA combines ANOVA with correlation",
        "The passage incorrectly states that covariates are continuous variables; they can also be categorical variables",
        "The passage incorrectly states that ANCOVA adjusts group means on the DV; it should say ANCOVA adjusts group means on the IV",
        "The passage incorrectly states that ANCOVA removes the influence of confounding variables; it should say mediating variables"
      ],
      "correct_option_index": 0,
      "explanation": "The passage incorrectly states that ANCOVA combines ANOVA with regression. While ANCOVA does use regression-like procedures to partial out the covariate, this is actually the correct characterization. Let me re-do this properly."
    },
    {
      "id": "PMET-0060",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Standard Error of Estimate",
      "passage_type": "paragraph",
      "original_passage": "The relationship between SEE and correlation is: as r increases, SEE decreases. When r = 1.00 (perfect correlation), SEE = 0 (no prediction error). When r = 0, the SEE equals the standard deviation of the Y variable (knowing X provides no improvement over just guessing the mean of Y)",
      "modified_passage": "The relationship between SEE and correlation is: as r increases, SEE decreases. When r = 1.00 (perfect correlation), SEE = 0 (no prediction error). When r = 0, the SEE equals the standard deviation of the X variable (knowing X provides no improvement over just guessing the mean of Y)",
      "error_original": "the standard deviation of the X variable",
      "error_correct": "the standard deviation of the Y variable",
      "options": [
        "The passage incorrectly states that when r = 1.00, SEE = 0; it should say SEE = 1.00",
        "The passage incorrectly states that as r increases, SEE decreases; it should say SEE increases",
        "The passage incorrectly states that SEE equals the standard deviation of the X variable when r = 0; it should say the standard deviation of the Y variable",
        "The passage incorrectly states 'no prediction error' for perfect correlation; it should say 'no measurement error'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage was modified to say 'standard deviation of the X variable' when r = 0, but the correct statement is that when r = 0, the SEE equals the standard deviation of the Y variable (the criterion variable). This is because when there is no correlation between X and Y, knowing X provides no predictive advantage, so the best prediction for any value of X is simply the mean of Y, and the error of that prediction equals the standard deviation of Y."
    },
    {
      "id": "PMET-0062",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Multiple Regression",
      "passage_type": "paragraph",
      "original_passage": "Each predictor variable has its own regression coefficient (b) indicating its unique contribution to predicting Y while controlling for other predictors",
      "modified_passage": "Each predictor variable has its own regression coefficient (b) indicating its total contribution to predicting Y while controlling for other predictors",
      "error_original": "total contribution",
      "error_correct": "unique contribution",
      "options": [
        "The passage incorrectly states 'regression coefficient (b)'; it should say 'correlation coefficient (r)'",
        "The passage incorrectly states 'total contribution'; it should say 'unique contribution'",
        "The passage incorrectly states 'controlling for other predictors'; it should say 'adding other predictors'",
        "The passage incorrectly states 'predicting Y'; it should say 'predicting X'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly describes the regression coefficient (b) as indicating the 'total contribution' to predicting Y. In multiple regression, each predictor's regression coefficient (b) represents its 'unique contribution' to predicting Y — that is, the portion of variance it explains while statistically controlling for (holding constant) all other predictors in the model. 'Total contribution' would imply no partialing out of shared variance with other predictors, which contradicts the fundamental purpose of the multiple regression coefficient."
    },
    {
      "id": "PMET-0565",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "original_passage": "External Validity: Can we generalize findings? Population, ecological, and temporal generalizability",
      "modified_passage": "External Validity: Can we generalize findings? External validity refers to the extent to which research findings can be generalized beyond the specific conditions of the study. There are three main types of external validity to consider: population generalizability (whether findings apply to other groups of people), ecological generalizability (whether findings apply to other settings or conditions), and temporal generalizability (whether findings remain stable over time). Threats to external validity include selection bias, the Hawthorne effect, and order effects. Campbell and Stanley (1963) originally distinguished between internal and external validity, and Cook and Campbell (1979) later expanded the framework to include four types of validity: internal validity, external validity, construct validity, and statistical validity.",
      "error_original": "statistical validity",
      "error_correct": "statistical conclusion validity",
      "options": [
        "The passage incorrectly states there are three main types of external validity; there are actually four main types",
        "The passage incorrectly states that Campbell and Stanley originally distinguished between internal and external validity; this distinction was made by Cook and Campbell",
        "The passage incorrectly states 'statistical validity'; it should say 'statistical conclusion validity'",
        "The passage incorrectly lists the Hawthorne effect as a threat to external validity; it is actually a threat to internal validity"
      ],
      "correct_option_index": 2,
      "explanation": "Cook and Campbell (1979) expanded the validity framework to include four types: internal validity, external validity, construct validity, and statistical conclusion validity (not simply 'statistical validity'). Statistical conclusion validity specifically refers to the validity of conclusions about the covariation between variables, including whether the relationship is statistically significant and the strength of that relationship. The term 'statistical conclusion validity' is the precise and correct name used by Cook and Campbell, and this distinction is commonly tested on the EPPP."
    },
    {
      "id": "PMET-0010",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Acquisition",
      "passage_type": "paragraph",
      "original_passage": "Delay conditioning is the most effective temporal arrangement for establishing conditioned responses. In this procedure, the CS begins before the US and continues until the US is presented. The CS \"overlaps\" with the US onset, creating optimal conditions for association formation",
      "modified_passage": "Delay conditioning is the most effective temporal arrangement for establishing conditioned responses. In this procedure, the CS begins before the US and continues until the US is presented. The CS \"overlaps\" with the US offset, creating optimal conditions for association formation.",
      "error_original": "US offset",
      "error_correct": "US onset",
      "options": [
        "The passage incorrectly states 'continues until the US is presented'; it should say 'terminates before the US is presented'",
        "The passage incorrectly states 'US offset'; it should say 'US onset'",
        "The passage incorrectly states 'Delay conditioning'; it should say 'Trace conditioning'",
        "The passage incorrectly states 'CS begins before the US'; it should say 'CS begins after the US'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to say 'US offset' instead of the correct term 'US onset.' In delay conditioning, the CS overlaps with the onset (beginning) of the US, not the offset (end). The overlap with the US onset is what creates optimal conditions for association formation. Option A is incorrect because the CS does indeed continue until the US is presented in delay conditioning. Option C is incorrect because delay conditioning (not trace conditioning) is correctly identified as the most effective temporal arrangement. Option D is incorrect because the CS does begin before the US in delay conditioning."
    },
    {
      "id": "PMET-0110",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "ROC Curves",
      "passage_type": "paragraph",
      "original_passage": "Receiver Operating Characteristic (ROC) curves plot sensitivity against (1 - specificity) across all possible cutoff scores. The area under the curve (AUC) provides an overall index of diagnostic accuracy independent of any particular cutoff. AUC of .50 indicates chance performance; AUC of 1.0 indicates perfect discrimination",
      "modified_passage": "Receiver Operating Characteristic (ROC) curves plot sensitivity against specificity across all possible cutoff scores. The area under the curve (AUC) provides an overall index of diagnostic accuracy independent of any particular cutoff. AUC of .50 indicates chance performance; AUC of 1.0 indicates perfect discrimination.",
      "error_original": "sensitivity against specificity",
      "error_correct": "sensitivity against (1 - specificity)",
      "options": [
        "The passage incorrectly states that AUC of .50 indicates chance performance; it should say AUC of .00 indicates chance performance",
        "The passage incorrectly states that ROC curves plot sensitivity against specificity; it should say sensitivity against (1 - specificity)",
        "The passage incorrectly states that AUC of 1.0 indicates perfect discrimination; it should say AUC of 1.0 indicates perfect reliability",
        "The passage incorrectly states that AUC is independent of any particular cutoff; it should say AUC is dependent on the optimal cutoff"
      ],
      "correct_option_index": 1,
      "explanation": "ROC curves plot sensitivity (true positive rate) on the y-axis against (1 - specificity), which is the false positive rate, on the x-axis. The modified passage incorrectly states that sensitivity is plotted against specificity rather than against (1 - specificity). This is a key distinction because plotting sensitivity against specificity would reverse the x-axis and fundamentally change the interpretation of the curve. A perfect classifier's ROC curve goes to the upper-left corner precisely because the x-axis represents the false positive rate (1 - specificity), not specificity itself."
    },
    {
      "id": "PMET-0528",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "CIPP Model (Stufflebeam)",
      "passage_type": "paragraph",
      "original_passage": "Context evaluation: Assesses needs, problems, and opportunities in the program environment to guide planning decisions",
      "modified_passage": "Context evaluation: Assesses needs, problems, and opportunities in the program environment to guide structuring decisions",
      "error_original": "structuring decisions",
      "error_correct": "planning decisions",
      "options": [
        "The passage incorrectly states 'opportunities'; it should say 'assets'",
        "The passage incorrectly states 'structuring decisions'; it should say 'planning decisions'",
        "The passage incorrectly states 'Context evaluation'; it should say 'Input evaluation'",
        "The passage incorrectly states 'program environment'; it should say 'organizational climate'"
      ],
      "correct_option_index": 1,
      "explanation": "In Stufflebeam's CIPP Model, Context evaluation is specifically designed to guide planning decisions by assessing needs, problems, and opportunities. The passage incorrectly states 'structuring decisions,' which is actually the purpose of Input evaluation in the CIPP framework. Context evaluation informs planning decisions, Input evaluation informs structuring decisions, Process evaluation informs implementing decisions, and Product evaluation informs recycling decisions."
    },
    {
      "id": "PMET-0162",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Discriminative Stimuli",
      "passage_type": "example",
      "original_passage": "Example: Stimulus Control in Everyday Life A ringing telephone (S D ) signals that answering will be reinforced by conversation. The silent telephone (S Δ ) signals that picking up the receiver will not produce reinforcement. Similarly, an \"open\" sign on a store (S D ) signals that entering will be reinforced, while a \"closed\" sign (S Δ ) signals it will not",
      "modified_passage": "Example: Stimulus Control in Everyday Life A ringing telephone (S Δ ) signals that answering will be reinforced by conversation. The silent telephone (S D ) signals that picking up the receiver will not produce reinforcement. Similarly, an \"open\" sign on a store (S D ) signals that entering will be reinforced, while a \"closed\" sign (S Δ ) signals it will not",
      "error_original": "A ringing telephone (S Δ ) signals that answering will be reinforced by conversation. The silent telephone (S D )",
      "error_correct": "A ringing telephone (S D ) signals that answering will be reinforced by conversation. The silent telephone (S Δ )",
      "options": [
        "The passage incorrectly states that the \"open\" sign is an S D; it should be an S Δ",
        "The passage incorrectly labels the ringing telephone as S Δ and the silent telephone as S D; these labels should be reversed",
        "The passage incorrectly states that answering the phone will be reinforced by conversation; it should say reinforced by the ringing stopping",
        "The passage incorrectly states that a \"closed\" sign signals behavior will not be reinforced; it should say behavior will be punished"
      ],
      "correct_option_index": 1,
      "explanation": "The passage swapped the discriminative stimulus labels for the telephone example. A ringing telephone is an S D (discriminative stimulus) because it signals that answering will be reinforced by conversation. The silent telephone is an S Δ (S-delta) because it signals that picking up the receiver will not produce reinforcement. The modified passage incorrectly reversed these two labels while leaving the store example correct."
    },
    {
      "id": "PMET-0501",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Advantages of IRT",
      "passage_type": "paragraph",
      "original_passage": "Item parameter invariance: Item parameters remain stable across different examinee samples (provided the model fits), enabling items to be calibrated once and used across contexts",
      "modified_passage": "Item parameter invariance: Item parameters remain stable across different examinee samples (provided the model fits), enabling items to be calibrated repeatedly and used across contexts",
      "error_original": "calibrated repeatedly",
      "error_correct": "calibrated once",
      "options": [
        "The passage incorrectly states 'Item parameter invariance'; it should say 'Item characteristic invariance'",
        "The passage incorrectly states 'calibrated repeatedly'; it should say 'calibrated once'",
        "The passage incorrectly states 'remain stable across different examinee samples'; it should say 'remain stable across different test forms'",
        "The passage incorrectly states 'provided the model fits'; it should say 'provided the sample is large enough'"
      ],
      "correct_option_index": 1,
      "explanation": "A key advantage of IRT's item parameter invariance is that items need only be calibrated once and can then be used across different contexts and examinee populations. The passage was modified to say 'calibrated repeatedly,' which contradicts this fundamental efficiency advantage of IRT. The correct phrasing is 'calibrated once,' reflecting that stable item parameters do not require recalibration for each new sample."
    },
    {
      "id": "PMET-0028",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Prepared Fears",
      "passage_type": "paragraph",
      "original_passage": "In contrast, fears of evolutionarily recent threats (cars, electrical outlets, guns) are relatively rare despite causing more deaths in modern society. This preparedness may explain the non-random distribution of phobias in clinical populations (Ohman & Mineka, 2001)",
      "modified_passage": "In contrast, fears of evolutionarily recent threats (cars, electrical outlets, guns) are relatively rare despite causing more deaths in modern society. This preparedness may explain the non-random distribution of phobias in clinical populations (Seligman & Mineka, 2001)",
      "error_original": "Seligman & Mineka, 2001",
      "error_correct": "Ohman & Mineka, 2001",
      "options": [
        "The passage incorrectly states that fears of evolutionarily recent threats are 'relatively rare'; it should say 'relatively common'",
        "The passage incorrectly states that these threats cause 'more deaths'; it should say 'fewer deaths'",
        "The passage incorrectly attributes the citation to 'Seligman & Mineka, 2001'; it should say 'Ohman & Mineka, 2001'",
        "The passage incorrectly states 'non-random distribution'; it should say 'random distribution'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage incorrectly attributes the 2001 citation on prepared fears and phobias to Seligman & Mineka. While Seligman is well-known for his earlier work on preparedness theory (1971), the 2001 publication referenced here was authored by Ohman & Mineka. This is a plausible error because Seligman's name is closely associated with preparedness in learning, making it a tempting but incorrect substitution."
    },
    {
      "id": "PMET-0100",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Base Rate",
      "passage_type": "paragraph",
      "original_passage": "The base rate is the proportion of people in the population who would succeed on the criterion without selection (or who have the condition being assessed). Base rate affects how much a test can improve upon chance prediction",
      "modified_passage": "The base rate is the proportion of people in the population who would succeed on the criterion without selection (or who have the condition being assessed). Base rate affects how much a test can improve upon random prediction",
      "error_original": "random prediction",
      "error_correct": "chance prediction",
      "options": [
        "The passage incorrectly states 'proportion of people'; it should say 'percentage of people'",
        "The passage incorrectly states 'succeed on the criterion'; it should say 'fail on the criterion'",
        "The passage incorrectly states 'without selection'; it should say 'with selection'",
        "The passage incorrectly states 'random prediction'; it should say 'chance prediction'"
      ],
      "correct_option_index": 3,
      "explanation": "The original passage uses the specific term 'chance prediction,' which is the standard terminology in psychometrics when discussing base rates and their relationship to test utility. The modified passage substituted 'random prediction' for 'chance prediction.' While these terms may seem similar in everyday language, 'chance prediction' is the established term used in the context of criterion-related validity and base rate discussions. The other options describe changes that were not actually made to the passage."
    },
    {
      "id": "PMET-0048",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Phi Coefficient",
      "passage_type": "paragraph",
      "original_passage": "The phi coefficient (φ) is used when both variables are true dichotomies. It is commonly used in 2×2 contingency tables and is related to chi-square statistics",
      "modified_passage": "The phi coefficient (φ) is used when both variables are artificial dichotomies. It is commonly used in 2×2 contingency tables and is related to chi-square statistics.",
      "error_original": "artificial dichotomies",
      "error_correct": "true dichotomies",
      "options": [
        "The passage incorrectly states that the phi coefficient is used in 2×2 contingency tables; it should say 3×3 contingency tables.",
        "The passage incorrectly states that the phi coefficient is related to chi-square statistics; it should say related to t-test statistics.",
        "The passage incorrectly states that both variables are artificial dichotomies; it should say true dichotomies.",
        "The passage incorrectly states the symbol is φ; it should say ρ."
      ],
      "correct_option_index": 2,
      "explanation": "The phi coefficient (φ) is specifically used when both variables are true (naturally occurring) dichotomies, such as male/female or alive/dead. The passage incorrectly states 'artificial dichotomies,' which would instead call for a different correlation coefficient, such as the tetrachoric correlation. True dichotomies are variables that naturally fall into two categories, whereas artificial dichotomies are created by splitting a continuous variable into two groups."
    },
    {
      "id": "PMET-0257",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Solomon Four-Group Design",
      "passage_type": "paragraph",
      "original_passage": "The Solomon four-group design combines elements of both previous designs, using four groups: two receive pretests (one treatment, one control) and two do not (one treatment, one control). This design allows researchers to detect and control for testing effects and sensitization",
      "modified_passage": "The Solomon four-group design combines elements of both previous designs, using four groups: two receive pretests (one treatment, one control) and two do not (one treatment, one control). This design allows researchers to detect and control for testing effects and habituation",
      "error_original": "habituation",
      "error_correct": "sensitization",
      "options": [
        "The passage incorrectly states 'two receive pretests'; it should say 'three receive pretests'",
        "The passage incorrectly states 'four groups'; it should say 'six groups'",
        "The passage incorrectly states 'habituation'; it should say 'sensitization'",
        "The passage incorrectly states 'testing effects'; it should say 'maturation effects'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage replaced 'sensitization' with 'habituation.' The Solomon four-group design is specifically noted for its ability to detect and control for testing effects and sensitization — the phenomenon where taking a pretest may sensitize participants to the treatment, altering their responses. Habituation refers to a decrease in response to repeated stimulation, which is a different concept entirely. The other options describe changes that were not actually made to the passage: the design does use four groups, two groups do receive pretests, and the design does address testing effects (not maturation effects)."
    },
    {
      "id": "PMET-0299",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Controlling External Validity Threats",
      "passage_type": "clinical_note",
      "original_passage": "Understanding the Distinction Random assignment controls internal validity threats by ensuring groups are equivalent. Random selection (from the population to the sample) controls external validity threats by ensuring the sample represents the population. Many students confuse these concepts on the EPPP. Random assignment creates equivalent groups; random selection creates representative samples",
      "modified_passage": "Understanding the Distinction Random assignment controls external validity threats by ensuring groups are equivalent. Random selection (from the population to the sample) controls external validity threats by ensuring the sample represents the population. Many students confuse these concepts on the EPPP. Random assignment creates equivalent groups; random selection creates representative samples",
      "error_original": "Random assignment controls external validity threats",
      "error_correct": "Random assignment controls internal validity threats",
      "options": [
        "The passage incorrectly states that random selection controls external validity; it should say random selection controls internal validity",
        "The passage incorrectly states that random assignment controls external validity threats; it should say it controls internal validity threats",
        "The passage incorrectly states that random assignment creates equivalent groups; it should say it creates representative samples",
        "The passage incorrectly states that random selection goes from the population to the sample; it should say from the sample to the population"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly states that random assignment controls external validity threats. In reality, random assignment controls internal validity threats by ensuring that groups in an experiment are equivalent prior to the manipulation. Random selection is the technique that controls external validity threats by ensuring the sample is representative of the population. This is a common confusion point tested on the EPPP, and the passage itself later correctly notes that 'random assignment creates equivalent groups,' which is the hallmark of internal validity control."
    },
    {
      "id": "PMET-0361",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Construct Validity",
      "passage_type": "paragraph",
      "original_passage": "Construct validity is the overarching concept that encompasses all validity evidence. It addresses the fundamental question: Does the test measure the theoretical construct it purports to measure? All other forms of validity evidence contribute to construct validity (Cronbach & Meehl, 1955)",
      "modified_passage": "Construct validity is the overarching concept that encompasses all validity evidence. It addresses the fundamental question: Does the test measure the theoretical construct it purports to measure? All other forms of validity evidence contribute to construct validity (Cronbach & Loevinger, 1955)",
      "error_original": "Cronbach & Loevinger, 1955",
      "error_correct": "Cronbach & Meehl, 1955",
      "options": [
        "The passage incorrectly states that construct validity 'encompasses all validity evidence'; it should say 'encompasses all reliability evidence'",
        "The passage incorrectly states the citation as 'Cronbach & Loevinger, 1955'; it should say 'Cronbach & Meehl, 1955'",
        "The passage incorrectly states that construct validity is 'the overarching concept'; it should say 'a subordinate concept'",
        "The passage incorrectly states the year as '1955'; it should say '1966'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage substituted 'Loevinger' for 'Meehl' in the classic citation. The seminal paper on construct validity was authored by Lee Cronbach and Paul Meehl in 1955, not Cronbach and Loevinger. While Jane Loevinger was a prominent psychometrician who also contributed to validity theory, the foundational 1955 paper establishing the concept of construct validity and the nomological network was by Cronbach and Meehl."
    },
    {
      "id": "PMET-0083",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Positive and Negative Predictive Value",
      "passage_type": "clinical_note",
      "original_passage": "Base Rate and Predictive Values Predictive values are strongly influenced by base rate: When base rate is low, PPV is low (most positives are false positives) even with good sensitivity/specificity When base rate is high, NPV is low (most negatives are false negatives) This is why screening tests in low-prevalence populations often require confirmation testing",
      "modified_passage": "Base Rate and Predictive Values Predictive values are strongly influenced by base rate: When base rate is low, PPV is low (most positives are false positives) even with good sensitivity/specificity When base rate is high, NPV is low (most negatives are false negatives) This is why screening tests in high-prevalence populations often require confirmation testing",
      "error_original": "high-prevalence populations",
      "error_correct": "low-prevalence populations",
      "options": [
        "The passage incorrectly states that when base rate is low, PPV is low; it should say NPV is low",
        "The passage incorrectly states that most positives are false positives when base rate is low; it should say most positives are true positives",
        "The passage incorrectly states 'high-prevalence populations'; it should say 'low-prevalence populations'",
        "The passage incorrectly states that when base rate is high, NPV is low; it should say PPV is low"
      ],
      "correct_option_index": 2,
      "explanation": "The passage was modified to say 'high-prevalence populations' instead of 'low-prevalence populations.' The correct statement is that screening tests in low-prevalence populations often require confirmation testing. This is because when base rate (prevalence) is low, the positive predictive value (PPV) is low, meaning most positive test results are false positives. Therefore, confirmation testing is needed to verify positive results in low-prevalence settings. In high-prevalence populations, PPV tends to be adequate, so confirmation is less critical for positive results."
    },
    {
      "id": "PMET-0402",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Z-Scores and Standard Scores",
      "passage_type": "paragraph",
      "original_passage": "A z-score (standard score) expresses a raw score in terms of how many standard deviations it falls above or below the mean. Z-scores allow comparison of scores from different distributions by converting them to a common scale",
      "modified_passage": "A z-score (standard score) expresses a raw score in terms of how many standard deviations it falls above or below the median. Z-scores allow comparison of scores from different distributions by converting them to a common scale.",
      "error_original": "median",
      "error_correct": "mean",
      "options": [
        "The passage incorrectly states 'standard score'; it should say 'scaled score'",
        "The passage incorrectly states 'median'; it should say 'mean'",
        "The passage incorrectly states 'standard deviations'; it should say 'standard errors'",
        "The passage incorrectly states 'different distributions'; it should say 'identical distributions'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly states that a z-score expresses how many standard deviations a raw score falls above or below the 'median.' In fact, z-scores are calculated relative to the mean of the distribution, not the median. The z-score formula is z = (X - M) / SD, where M is the mean. While the mean and median can coincide in a perfectly normal distribution, the defining reference point for a z-score is always the mean."
    },
    {
      "id": "PMET-0190",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Three-Term Contingency",
      "passage_type": "paragraph",
      "original_passage": "This framework is fundamental to functional behavior assessment and the development of behavior intervention plans in clinical and educational settings",
      "modified_passage": "The three-term contingency, also known as the ABD model (Antecedent-Behavior-Consequence), is the foundational unit of analysis in operant conditioning. The antecedent is the stimulus or event that precedes the behavior, the behavior is the observable response, and the consequence is the event that follows and either strengthens or weakens the behavior. This framework is fundamental to functional behavior assessment and the development of behavior intervention plans in clinical and educational settings.",
      "error_original": "ABD model",
      "error_correct": "ABC model",
      "options": [
        "The passage incorrectly states that the consequence 'strengthens or weakens' behavior; it should say 'strengthens or maintains' behavior",
        "The passage incorrectly states that the antecedent 'precedes' the behavior; it should say 'follows' the behavior",
        "The passage incorrectly states it is the 'ABD model'; it should say 'ABC model'",
        "The passage incorrectly states the three-term contingency is the foundational unit of 'operant conditioning'; it should say 'classical conditioning'"
      ],
      "correct_option_index": 2,
      "explanation": "The three-term contingency is commonly referred to as the ABC model, where A stands for Antecedent, B stands for Behavior, and C stands for Consequence. The passage incorrectly labels it the 'ABD model,' which is not a recognized term in behavior analysis. The correct abbreviation is ABC, directly corresponding to the three components described in the passage itself (Antecedent-Behavior-Consequence)."
    },
    {
      "id": "PMET-0163",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Contingency Management",
      "passage_type": "example",
      "original_passage": "Example: Voucher-Based Contingency Management In voucher-based contingency management for cocaine dependence, patients receive vouchers exchangeable for retail goods contingent on cocaine-negative urine specimens. Voucher values typically start low and increase with consecutive negative specimens, with a reset contingency for positive specimens. Meta-analyses demonstrate significant effects on drug abstinence outcomes",
      "modified_passage": "Example: Voucher-Based Contingency Management In voucher-based contingency management for cocaine dependence, patients receive vouchers exchangeable for retail goods contingent on cocaine-negative urine specimens. Voucher values typically start high and decrease with consecutive negative specimens, with a reset contingency for positive specimens. Meta-analyses demonstrate significant effects on drug abstinence outcomes",
      "error_original": "start high and decrease with consecutive negative specimens",
      "error_correct": "start low and increase with consecutive negative specimens",
      "options": [
        "The passage incorrectly states vouchers are exchangeable for retail goods; it should say they are exchangeable for cash",
        "The passage incorrectly states there is a reset contingency for positive specimens; it should say there is a reset contingency for negative specimens",
        "The passage incorrectly states voucher values start high and decrease with consecutive negative specimens; it should say they start low and increase with consecutive negative specimens",
        "The passage incorrectly states the intervention targets cocaine dependence; it should say it targets alcohol dependence"
      ],
      "correct_option_index": 2,
      "explanation": "The passage reverses the direction of the voucher escalation schedule. In voucher-based contingency management, voucher values typically start low and increase (escalate) with each consecutive cocaine-negative urine specimen. This escalating reinforcement schedule is a core feature of the approach, designed to incentivize sustained abstinence. The modified passage incorrectly states that values start high and decrease, which would actually reduce motivation for continued abstinence over time and contradicts the established protocol."
    },
    {
      "id": "PMET-0061",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Multiple Regression",
      "passage_type": "paragraph",
      "original_passage": "Multiple regression extends simple regression by using two or more predictor variables to predict a single criterion variable. The multiple regression equation is: Y' = a + b₁X₁ + b₂X₂ + ... + bₖXₖ",
      "modified_passage": "Multiple regression extends simple regression by using two or more predictor variables to predict a single criterion variable. The multiple regression equation is: Y' = a + b₁X₁ × b₂X₂ × ... × bₖXₖ",
      "error_original": "Y' = a + b₁X₁ × b₂X₂ × ... × bₖXₖ",
      "error_correct": "Y' = a + b₁X₁ + b₂X₂ + ... + bₖXₖ",
      "options": [
        "The passage incorrectly states that multiple regression uses two or more predictor variables; it should say three or more predictor variables",
        "The passage incorrectly states that the equation predicts a single criterion variable; it should say multiple criterion variables",
        "The passage incorrectly uses multiplication signs (×) between the weighted predictor terms; it should use addition signs (+)",
        "The passage incorrectly refers to 'a' as the constant; it should refer to it as the slope coefficient"
      ],
      "correct_option_index": 2,
      "explanation": "The multiple regression equation combines the weighted predictor terms additively, not multiplicatively. The correct equation is Y' = a + b₁X₁ + b₂X₂ + ... + bₖXₖ, where each predictor variable (X) is multiplied by its own regression coefficient (b), and these products are then summed together along with the intercept constant (a). The modified passage incorrectly changed the addition signs between the b₁X₁, b₂X₂, etc. terms to multiplication signs, which would represent an entirely different (and incorrect) mathematical model."
    },
    {
      "id": "PMET-0426",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "The Regression Equation",
      "passage_type": "paragraph",
      "original_passage": "b: The slope (regression coefficient) indicating the change in Y for each one-unit change in X",
      "modified_passage": "b: The slope (regression coefficient) indicating the change in X for each one-unit change in Y",
      "error_original": "the change in X for each one-unit change in Y",
      "error_correct": "the change in Y for each one-unit change in X",
      "options": [
        "The passage incorrectly refers to 'b' as the slope; it should be called the intercept",
        "The passage incorrectly states the change in X for each one-unit change in Y; it should say the change in Y for each one-unit change in X",
        "The passage incorrectly uses the term 'regression coefficient'; it should say 'correlation coefficient'",
        "The passage incorrectly implies a one-unit change; it should say a one-standard-deviation change"
      ],
      "correct_option_index": 1,
      "explanation": "The passage reverses the relationship between X and Y. In a standard regression equation, 'b' (the slope or regression coefficient) indicates how much the predicted value of Y changes for each one-unit increase in X, not the other way around. The correct statement is: 'b: The slope (regression coefficient) indicating the change in Y for each one-unit change in X.'"
    },
    {
      "id": "PMET-SC-0001",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Methods of Assessment",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0505",
      "modified_sentence": "The intraclass correlation coefficient (ICC) is used for continuous ratings to assess inter-rater reliability, and unlike the Spearman correlation it accounts for absolute agreement, making it a preferred measure when evaluating consistency among multiple raters.",
      "phrases": [
        "The intraclass correlation coefficient (ICC) is used for continuous ratings to assess inter-rater reliability,",
        " and unlike the Spearman correlation it accounts for absolute agreement,",
        " making it a preferred measure when evaluating consistency among multiple raters."
      ],
      "target_phrase_index": 1,
      "error_original": "unlike the Spearman correlation",
      "error_correct": "unlike the Pearson correlation",
      "explanation": "The ICC is typically contrasted with the Pearson correlation, not the Spearman correlation. The standard Pearson correlation captures only relative consistency (i.e., whether scores maintain their rank order) but does not account for absolute agreement between raters. The ICC improves upon the Pearson correlation by capturing both consistency and absolute agreement, making it the preferred statistic for inter-rater reliability with continuous data."
    },
    {
      "id": "PMET-SC-0002",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Acquisition",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0408",
      "modified_sentence": "Temporal Contiguity: The CS and US must occur close together in time, and the optimal interval varies by response system but is typically 0.5 seconds for many conditioned responses (Wagner, 1988).",
      "phrases": [
        "Temporal Contiguity: The CS and US must occur close together in time,",
        " and the optimal interval varies by response system",
        " but is typically 0.5 seconds for many conditioned responses (Wagner, 1988)."
      ],
      "target_phrase_index": 2,
      "error_original": "(Wagner, 1988)",
      "error_correct": "(Rescorla, 1988)",
      "explanation": "The passage attributes this finding to Rescorla (1988), not Wagner. While Allan Wagner is a prominent learning theorist who often collaborated with Rescorla (e.g., the Rescorla-Wagner model), the specific 1988 citation about temporal contiguity and optimal CS-US intervals belongs to Robert Rescorla."
    },
    {
      "id": "PMET-SC-0003",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Heteroscedasticity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0056",
      "modified_sentence": "Heteroscedasticity occurs when the variability of one variable differs across levels of the other variable, and the variance in job performance might be larger for people with high test scores than for people with low scores, which violates an assumption of Pearson's r and can affect the accuracy of the correlation coefficient.",
      "phrases": [
        "Heteroscedasticity occurs when the variability of one variable differs across levels of the other variable,",
        " and the variance in job performance might be larger for people with high test scores than for people with low scores,",
        " which violates an assumption of Pearson's r",
        " and can affect the accuracy of the correlation coefficient."
      ],
      "target_phrase_index": 1,
      "error_original": "larger for people with high test scores than for people with low scores",
      "error_correct": "larger for people with low test scores than for people with high scores",
      "explanation": "The passage gives the classic example that variance in job performance is larger for people with LOW test scores than for people with HIGH scores (i.e., low scorers show more spread in performance). The error reverses which group has greater variability, requiring specific recall of this well-known heteroscedasticity example in psychometrics."
    },
    {
      "id": "PMET-SC-0004",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0437",
      "modified_sentence": "Fixed effects models assume one true effect size across all studies, while random effects models account for variability across studies and are generally preferred in medical research for meta-analyses.",
      "phrases": [
        "Fixed effects models assume one true effect size across all studies,",
        " while random effects models account for variability across studies",
        " and are generally preferred in medical research for meta-analyses."
      ],
      "target_phrase_index": 2,
      "error_original": "preferred in medical research",
      "error_correct": "preferred in psychology",
      "explanation": "The passage states that random effects models are preferred in psychology, not in medical research. Psychology research typically favors random effects models because psychological studies are expected to have heterogeneous true effects across different populations, settings, and methods."
    },
    {
      "id": "PMET-SC-0005",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Other Standard Score Transformations",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0599",
      "modified_sentence": "Z-scores can be linearly transformed to other standard score scales that eliminate negative values and decimals,such as T-scores which have a mean of 50 and a standard deviation of 15,or IQ scores which have a mean of 100and a standard deviation of 15.",
      "phrases": [
        "Z-scores can be linearly transformed to other standard score scales that eliminate negative values and decimals,",
        "such as T-scores which have a mean of 50 and a standard deviation of 15,",
        "or IQ scores which have a mean of 100",
        "and a standard deviation of 15."
      ],
      "target_phrase_index": 1,
      "error_original": "a standard deviation of 15",
      "error_correct": "a standard deviation of 10",
      "explanation": "T-scores have a mean of 50 and a standard deviation of 10, not 15. The standard deviation of 15 is associated with IQ scores (Wechsler scale), not T-scores. This is a common source of confusion on the EPPP, as both are linear transformations of z-scores but use different parameters."
    },
    {
      "id": "PMET-SC-0006",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0516",
      "modified_sentence": "Embretson and Reise published their influential text on item response theory for psychologists in 2000, which was released by Lawrence Erlbaum Associates and helped popularize IRT methods as alternatives to generalizability theory approaches to reliability.",
      "phrases": [
        "Embretson and Reise published their influential text on item response theory for psychologists in 2000,",
        " which was released by Lawrence Erlbaum Associates",
        " and helped popularize IRT methods as alternatives to generalizability theory approaches to reliability."
      ],
      "target_phrase_index": 2,
      "error_original": "as alternatives to generalizability theory",
      "error_correct": "as alternatives to classical test theory",
      "explanation": "Item Response Theory (IRT) was popularized as an alternative to Classical Test Theory (CTT), not generalizability theory. While generalizability theory is also a framework for understanding reliability, IRT is specifically positioned in the psychometric literature as a modern alternative to CTT's approach to measurement. Embretson and Reise's (2000) text explicitly contrasts IRT with classical test theory methods."
    },
    {
      "id": "PMET-SC-0007",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Inter-Rater (Inter-Scorer) Reliability",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0231",
      "modified_sentence": "Inter-rater reliability assesses the consistency of scores assigned by different raters or scorers, and is essential for tests involving objective judgment, such as essay tests, behavioral observations, projective tests, and structured interviews.",
      "phrases": [
        "Inter-rater reliability assesses the consistency of scores assigned by different raters or scorers,",
        " and is essential for tests involving objective judgment,",
        " such as essay tests, behavioral observations,",
        " projective tests, and structured interviews."
      ],
      "target_phrase_index": 1,
      "error_original": "objective judgment",
      "error_correct": "subjective judgment",
      "explanation": "Inter-rater reliability is specifically important for tests that involve subjective judgment, not objective judgment. The whole point of assessing inter-rater reliability is to determine whether different raters can arrive at consistent scores when the scoring process requires subjective interpretation, as in essay tests, behavioral observations, and projective tests. Objective tests (e.g., multiple-choice) have predetermined correct answers and do not require inter-rater reliability assessment."
    },
    {
      "id": "PMET-SC-0008",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Decision Theory and Cutoff Scores",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0108",
      "modified_sentence": "In decision theory, raising the cutoff score reduces false positives but increases false negatives, and the positive predictive value of the test is primarily influenced by the selection ratio of the condition in the population.",
      "phrases": [
        "In decision theory, raising the cutoff score reduces false positives",
        " but increases false negatives, and the positive predictive value of the test",
        " is primarily influenced by the selection ratio",
        " of the condition in the population."
      ],
      "target_phrase_index": 2,
      "error_original": "the selection ratio",
      "error_correct": "the base rate",
      "explanation": "The positive predictive value of a test is primarily influenced by the base rate (prevalence) of the condition in the population, not the selection ratio. The base rate refers to the proportion of people in the population who actually have the condition, while the selection ratio refers to the proportion of applicants who are selected. Even a highly accurate test will have poor positive predictive value when the base rate is very low, as demonstrated by Bayesian reasoning applied to diagnostic decisions."
    },
    {
      "id": "PMET-SC-0009",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0494",
      "modified_sentence": "Two-Factor Theory proposes that classical conditioning establishes the fear response, and operant conditioning through positive reinforcement maintains avoidance behavior in anxiety disorders.",
      "phrases": [
        "Two-Factor Theory proposes that classical conditioning establishes the fear response,",
        " and operant conditioning through positive reinforcement",
        " maintains avoidance behavior in anxiety disorders."
      ],
      "target_phrase_index": 1,
      "error_original": "positive reinforcement",
      "error_correct": "negative reinforcement",
      "explanation": "In Mowrer's Two-Factor Theory, avoidance behavior is maintained through negative reinforcement (the reduction or removal of the aversive fear/anxiety state when the person avoids the feared stimulus), not positive reinforcement. The relief from anxiety negatively reinforces the avoidance response, thereby maintaining the anxiety disorder."
    },
    {
      "id": "PMET-SC-0010",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "The Normal Distribution",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0401",
      "modified_sentence": "The normal distribution, also called the Gaussian distribution or bell curve, is a theoretical probability distribution that is symmetric, bimodal, and asymptotic to the horizontal axis.",
      "phrases": [
        "The normal distribution, also called the Gaussian distribution or bell curve,",
        " is a theoretical probability distribution that is symmetric, bimodal,",
        " and asymptotic to the horizontal axis."
      ],
      "target_phrase_index": 1,
      "error_original": "bimodal",
      "error_correct": "unimodal",
      "explanation": "The normal distribution is unimodal, meaning it has a single peak (one mode). 'Bimodal' refers to a distribution with two peaks, which is incorrect. The normal (Gaussian) distribution is defined by being symmetric, unimodal, and asymptotic."
    },
    {
      "id": "PMET-SC-0011",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0575",
      "modified_sentence": "Validity generalization, a method developed by Schmidt and Hunter,shows that validity coefficients can be generalized across settingswhen corrected for artifacts such as criterion contamination,range restriction, and sampling error.",
      "phrases": [
        "Validity generalization, a method developed by Schmidt and Hunter,",
        "shows that validity coefficients can be generalized across settings",
        "when corrected for artifacts such as criterion contamination,",
        "range restriction, and sampling error."
      ],
      "target_phrase_index": 2,
      "error_original": "criterion contamination",
      "error_correct": "criterion unreliability",
      "explanation": "Validity generalization corrects for three primary statistical artifacts: sampling error, range restriction, and criterion unreliability (not criterion contamination). Criterion contamination is a separate concept referring to when a criterion measure is influenced by knowledge of predictor scores. The artifact corrections in validity generalization address measurement error in the criterion variable, which is criterion unreliability."
    },
    {
      "id": "PMET-SC-0012",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0547",
      "modified_sentence": "The CIPP Model, developed by Stufflebeam, stands for Context, Implementation, Process, and Product, and it serves as a comprehensive evaluation framework used in program evaluation research.",
      "phrases": [
        "The CIPP Model, developed by Stufflebeam,",
        " stands for Context, Implementation, Process, and Product,",
        " and it serves as a comprehensive evaluation framework",
        " used in program evaluation research."
      ],
      "target_phrase_index": 1,
      "error_original": "Implementation",
      "error_correct": "Input",
      "explanation": "In Stufflebeam's CIPP Model, the 'I' stands for 'Input,' not 'Implementation.' The four components of the CIPP evaluation framework are Context, Input, Process, and Product. 'Implementation' is a plausible-sounding alternative but is incorrect; the Input phase focuses on evaluating resources, strategies, and procedural designs needed to meet identified needs."
    },
    {
      "id": "PMET-SC-0013",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Introduction to Research Designs",
      "passage_type": "definition",
      "source_passage_id": "PMET-0239",
      "modified_sentence": "A quasi-experiment is a design lacking random assignmentbut including control of extraneous variables,while a true experiment includes random assignment, manipulation of the independent variable,and control of extraneous variables.",
      "phrases": [
        "A quasi-experiment is a design lacking random assignment",
        "but including control of extraneous variables,",
        "while a true experiment includes random assignment, manipulation of the independent variable,",
        "and control of extraneous variables."
      ],
      "target_phrase_index": 1,
      "error_original": "but including control of extraneous variables",
      "error_correct": "but including manipulation of the independent variable",
      "explanation": "A quasi-experiment lacks random assignment but still includes manipulation of the independent variable. The passage erroneously states that a quasi-experiment includes 'control of extraneous variables' instead of 'manipulation of the independent variable.' Control of extraneous variables is a feature of true experiments, not the defining characteristic retained by quasi-experiments."
    },
    {
      "id": "PMET-SC-0014",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Standard Error of the Difference",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0233",
      "modified_sentence": "The standard error of the difference is calculated by taking the square root of the sum of the two squared standard errors of measurement, and a difference between two scores is typically considered statistically significant when it exceeds 1.645 times the SEdiff at the .05 level of confidence.",
      "phrases": [
        "The standard error of the difference is calculated by taking the square root",
        " of the sum of the two squared standard errors of measurement,",
        " and a difference between two scores is typically considered statistically significant",
        " when it exceeds 1.645 times the SEdiff at the .05 level of confidence."
      ],
      "target_phrase_index": 3,
      "error_original": "1.645 times the SEdiff at the .05 level of confidence",
      "error_correct": "1.96 times the SEdiff at the .05 level of confidence",
      "explanation": "The value 1.645 corresponds to the critical z-value for a one-tailed test at the .05 significance level. For a two-tailed test at the .05 level of confidence, which is the standard used when comparing two scores for statistical significance, the correct critical z-value is 1.96. This is a commonly tested distinction on the EPPP."
    },
    {
      "id": "PMET-SC-0015",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Functional Behavior Assessment",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0200",
      "modified_sentence": "Functional Behavior Assessment (FBA) is a process for identifying the environmental variables that maintain problem behavior, and by identifying the structure of behavior clinicians can develop function-based interventions that address the maintaining contingencies rather than simply suppressing behavior.",
      "phrases": [
        "Functional Behavior Assessment (FBA) is a process for identifying the environmental variables that maintain problem behavior,",
        " and by identifying the structure of behavior clinicians can develop function-based interventions",
        " that address the maintaining contingencies",
        " rather than simply suppressing behavior."
      ],
      "target_phrase_index": 1,
      "error_original": "the structure of behavior",
      "error_correct": "the function of behavior",
      "explanation": "In Functional Behavior Assessment (FBA), the key goal is to identify the function of behavior — that is, what reinforces it. The passage incorrectly states 'the structure of behavior,' but FBA is specifically concerned with identifying the function (i.e., the purpose the behavior serves and the reinforcement contingencies maintaining it), not its structure. The very name 'Functional Behavior Assessment' emphasizes the focus on function."
    },
    {
      "id": "PMET-SC-0016",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Item Characteristic Curves (ICCs)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0222",
      "modified_sentence": "The Item Characteristic Curve (ICC) is an S-shaped (logistic) curvethat plots the probability of a correct response (y-axis) as a function of examinee ability, denoted theta (x-axis),and the curve's position along the theta axis reflects discrimination (b), its steepness reflects difficulty (a),and its lower asymptote reflects guessing (c).",
      "phrases": [
        "The Item Characteristic Curve (ICC) is an S-shaped (logistic) curve",
        "that plots the probability of a correct response (y-axis) as a function of examinee ability, denoted theta (x-axis),",
        "and the curve's position along the theta axis reflects discrimination (b), its steepness reflects difficulty (a),",
        "and its lower asymptote reflects guessing (c)."
      ],
      "target_phrase_index": 2,
      "error_original": "the curve's position along the theta axis reflects discrimination (b), its steepness reflects difficulty (a)",
      "error_correct": "the curve's position along the theta axis reflects difficulty (b), its steepness reflects discrimination (a)",
      "explanation": "In Item Response Theory, the b parameter represents item difficulty and is indicated by the curve's position along the theta axis, while the a parameter represents item discrimination and is indicated by the curve's steepness (slope). The error swaps these two associations: it incorrectly states that position reflects discrimination (b) and steepness reflects difficulty (a), when in fact position reflects difficulty (b) and steepness reflects discrimination (a)."
    },
    {
      "id": "PMET-SC-0017",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Posttest-Only Control Group Design",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0256",
      "modified_sentence": "When pretesting may sensitize participants to the treatmentor when random assignment is expected to produce equivalent groups,researchers may use a posttest-only control group design, which omits the pretestand relies on matched assignment to equate groups prior to treatment.",
      "phrases": [
        "When pretesting may sensitize participants to the treatment",
        "or when random assignment is expected to produce equivalent groups,",
        "researchers may use a posttest-only control group design, which omits the pretest",
        "and relies on matched assignment to equate groups prior to treatment."
      ],
      "target_phrase_index": 3,
      "error_original": "matched assignment",
      "error_correct": "random assignment",
      "explanation": "The posttest-only control group design relies on random assignment—not matched assignment—to equate groups prior to treatment. Random assignment is the key mechanism that justifies omitting the pretest, as it is expected to produce equivalent groups without the need for pretest comparisons."
    },
    {
      "id": "PMET-SC-0018",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0558",
      "modified_sentence": "Schaie, K. W. (1965) proposed a general modelfor the study of developmental problems,which was published in the American Psychologist,and appeared in volume 64, issue 2, pages 92-107.",
      "phrases": [
        "Schaie, K. W. (1965) proposed a general model",
        "for the study of developmental problems,",
        "which was published in the American Psychologist,",
        "and appeared in volume 64, issue 2, pages 92-107."
      ],
      "target_phrase_index": 2,
      "error_original": "the American Psychologist",
      "error_correct": "the Psychological Bulletin",
      "explanation": "Schaie's 1965 article 'A general model for the study of developmental problems' was published in Psychological Bulletin, not the American Psychologist. Both are APA journals, but the Psychological Bulletin is the correct source for this landmark article on developmental research designs."
    },
    {
      "id": "PMET-SC-0019",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Meta-Analysis",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0071",
      "modified_sentence": "Common effect size metrics include Cohen's d (standardized mean difference), Pearson's r (correlation), and odds ratios (for binary outcomes), and each study's effect size is calculated and then combined into an unweighted average with larger studies typically receiving more weight.",
      "phrases": [
        "Common effect size metrics include Cohen's d (standardized mean difference),",
        " Pearson's r (correlation), and odds ratios (for binary outcomes),",
        " and each study's effect size is calculated and then combined into an unweighted average",
        " with larger studies typically receiving more weight."
      ],
      "target_phrase_index": 2,
      "error_original": "combined into an unweighted average",
      "error_correct": "combined into a weighted average",
      "explanation": "In meta-analysis, individual study effect sizes are combined into a weighted average, not an unweighted average. The weighting is a critical feature of meta-analysis, as it gives greater influence to studies with larger sample sizes (and thus more precise estimates). An unweighted average would treat all studies equally regardless of sample size or precision, which would be methodologically inappropriate."
    },
    {
      "id": "PMET-SC-0020",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Null and Alternative Hypotheses",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0127",
      "modified_sentence": "The alternative hypothesis (H₁ or Hₐ) states that there is an effect, difference, or relationship, and it represents what the researcher typically hopes to demonstrate, with the alternative hypothesis being either directional (two-tailed) or non-directional (one-tailed).",
      "phrases": [
        "The alternative hypothesis (H₁ or Hₐ) states that there is an effect, difference, or relationship,",
        " and it represents what the researcher typically hopes to demonstrate,",
        " with the alternative hypothesis being either directional (two-tailed) or non-directional (one-tailed)."
      ],
      "target_phrase_index": 2,
      "error_original": "directional (two-tailed)",
      "error_correct": "directional (one-tailed)",
      "explanation": "A directional alternative hypothesis corresponds to a one-tailed test, not a two-tailed test. A one-tailed test specifies the direction of the expected effect (e.g., greater than or less than), while a two-tailed test corresponds to a non-directional hypothesis that simply predicts a difference without specifying direction. The passage swapped the parenthetical labels for directional and non-directional hypotheses."
    },
    {
      "id": "PMET-SC-0021",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Skewness",
      "passage_type": "example",
      "source_passage_id": "PMET-0383",
      "modified_sentence": "In a positively skewed distribution such as income, many low-to-moderate values cluster together, while few very high incomes pull the median above the mode and mean.",
      "phrases": [
        "In a positively skewed distribution such as income,",
        " many low-to-moderate values cluster together,",
        " while few very high incomes pull the median",
        " above the mode and mean."
      ],
      "target_phrase_index": 2,
      "error_original": "pull the median",
      "error_correct": "pull the mean",
      "explanation": "In a positively skewed distribution, it is the mean—not the median—that is most strongly pulled in the direction of the extreme high values. The typical ordering in a positive skew is mode < median < mean, meaning the mean is pulled upward above both the median and mode, not the other way around."
    },
    {
      "id": "PMET-SC-0022",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Chi-Square Tests",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0463",
      "modified_sentence": "The chi-square goodness of fit test evaluates whether observed frequencies match expected frequencies in a single categorical variable, and it is a parametric test that compares the distribution of observed data to a theoretical or hypothesized distribution.",
      "phrases": [
        "The chi-square goodness of fit test evaluates whether observed frequencies match expected frequencies",
        " in a single categorical variable, and it is a parametric test",
        " that compares the distribution of observed data",
        " to a theoretical or hypothesized distribution."
      ],
      "target_phrase_index": 1,
      "error_original": "it is a parametric test",
      "error_correct": "it is a nonparametric test",
      "explanation": "The chi-square goodness of fit test is a nonparametric (distribution-free) test, not a parametric test. It analyzes categorical (nominal) frequency data and does not assume the data are normally distributed or measured on interval/ratio scales, which are hallmarks of parametric tests."
    },
    {
      "id": "PMET-SC-0023",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Ratio Scale",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0393",
      "modified_sentence": "The ratio scale has all the properties of the ordinal scale plus a true zero point representing the absence of the measured attribute, and ratios are meaningful—for example, 10 seconds is twice as long as 5 seconds.",
      "phrases": [
        "The ratio scale has all the properties",
        " of the ordinal scale plus a true zero point representing the absence of the measured attribute,",
        " and ratios are meaningful—for example,",
        " 10 seconds is twice as long as 5 seconds."
      ],
      "target_phrase_index": 1,
      "error_original": "of the ordinal scale plus a true zero point",
      "error_correct": "of the interval scale plus a true zero point",
      "explanation": "The ratio scale builds on the interval scale (not the ordinal scale) by adding a true zero point. The interval scale already possesses equal intervals between values and the properties of ordinal and nominal scales; the ratio scale adds the true zero point to all of these. Substituting 'ordinal' for 'interval' omits the critical equal-interval property that the ratio scale inherits."
    },
    {
      "id": "PMET-SC-0024",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Multiple Correlation Coefficient (R)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0064",
      "modified_sentence": "R² always increases (or stays the same) when additional predictors are added, even if those predictors have no true relationship with the criterion, and for this reason adjusted R² is often reported, which accounts for the number of predictors and effect size.",
      "phrases": [
        "R² always increases (or stays the same) when additional predictors are added,",
        " even if those predictors have no true relationship with the criterion,",
        " and for this reason adjusted R² is often reported,",
        " which accounts for the number of predictors and effect size."
      ],
      "target_phrase_index": 3,
      "error_original": "effect size",
      "error_correct": "sample size",
      "explanation": "The passage states that adjusted R² accounts for the number of predictors and sample size, not effect size. Adjusted R² applies a penalty based on the number of predictors relative to the sample size (N), correcting for the artificial inflation of R² that occurs when predictors are added. Effect size is an unrelated statistical concept and is not what adjusted R² corrects for."
    },
    {
      "id": "PMET-SC-0025",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Incremental Validity",
      "passage_type": "definition",
      "source_passage_id": "PMET-0080",
      "modified_sentence": "Incremental validity is typically assessed using hierarchical multiple regression, where existing predictors are entered first and the resulting R² is noted, then the new test is added and the change in R (ΔR) is evaluated for statistical significance and practical meaningfulness.",
      "phrases": [
        "Incremental validity is typically assessed using hierarchical multiple regression,",
        " where existing predictors are entered first and the resulting R² is noted,",
        " then the new test is added and the change in R (ΔR) is evaluated",
        " for statistical significance and practical meaningfulness."
      ],
      "target_phrase_index": 2,
      "error_original": "the change in R (ΔR)",
      "error_correct": "the change in R² (ΔR²)",
      "explanation": "In hierarchical multiple regression for assessing incremental validity, it is the change in R² (ΔR²), not the change in R (ΔR), that is evaluated. R² represents the proportion of variance explained, and it is the increment in this variance-explained statistic (ΔR²) that determines whether a new predictor adds meaningful predictive power beyond existing predictors."
    },
    {
      "id": "PMET-SC-0026",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Internal Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0312",
      "modified_sentence": "Mortality (or attrition) refers to differential dropout from groups during the study, and if participants who drop out are similar systematically to those who remain and dropout rates differ between groups, results may be biased.",
      "phrases": [
        "Mortality (or attrition) refers to differential dropout from groups during the study,",
        " and if participants who drop out are similar systematically to those who remain",
        " and dropout rates differ between groups, results may be biased."
      ],
      "target_phrase_index": 1,
      "error_original": "are similar systematically to those who remain",
      "error_correct": "differ systematically from those who remain",
      "explanation": "The mortality (attrition) threat to internal validity occurs when participants who drop out DIFFER systematically from those who remain, not when they are similar. If dropouts were similar to remaining participants, the loss would not introduce systematic bias. The key concern is that differential characteristics of those leaving the study skew the composition of the remaining sample, potentially confounding results."
    },
    {
      "id": "PMET-SC-0027",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0588",
      "modified_sentence": "The median is the preferred measure of central tendency for skewed distributions, and unlike the mean, it is not pulled by extreme scores, making it especially useful when data are measured on at least an interval scale.",
      "phrases": [
        "The median is the preferred measure of central tendency for skewed distributions,",
        " and unlike the mean, it is not pulled by extreme scores,",
        " making it especially useful when data are measured",
        " on at least an interval scale."
      ],
      "target_phrase_index": 3,
      "error_original": "on at least an interval scale",
      "error_correct": "on at least an ordinal scale",
      "explanation": "The median requires data measured on at least an ordinal scale (i.e., data that can be rank-ordered). It does not require interval-level measurement. The median can be computed for ordinal, interval, and ratio data. Stating that it requires 'at least an interval scale' overstates the measurement requirement and is incorrect."
    },
    {
      "id": "PMET-SC-0028",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "The Regression Equation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0426",
      "modified_sentence": "In a simple linear regression equation, the slope coefficient b indicates the predicted change in Y for each one-standard-deviation change in X, and the intercept a represents the predicted value of Y when X equals zero.",
      "phrases": [
        "In a simple linear regression equation,",
        " the slope coefficient b indicates the predicted change in Y for each one-standard-deviation change in X,",
        " and the intercept a represents the predicted value of Y when X equals zero."
      ],
      "target_phrase_index": 1,
      "error_original": "one-standard-deviation change in X",
      "error_correct": "one-unit change in X",
      "explanation": "The unstandardized regression coefficient b indicates the predicted change in Y for each one-unit change in X, not a one-standard-deviation change. A one-standard-deviation change in X corresponds to the standardized regression coefficient (beta weight), not the raw slope b. This is a critical distinction in regression analysis between unstandardized and standardized coefficients."
    },
    {
      "id": "PMET-SC-0029",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Base Rates and Selection Ratios",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0457",
      "modified_sentence": "According to the Taylor-Russell tables, a test adds the most incremental validity when the base rate is extreme (near 0 or 1), and the selection ratio is low relative to the number of applicants.",
      "phrases": [
        "According to the Taylor-Russell tables,",
        " a test adds the most incremental validity when the base rate is extreme (near 0 or 1),",
        " and the selection ratio is low",
        " relative to the number of applicants."
      ],
      "target_phrase_index": 1,
      "error_original": "the base rate is extreme (near 0 or 1)",
      "error_correct": "the base rate is moderate (around .50)",
      "explanation": "According to the Taylor-Russell tables, a test's incremental validity is maximized when the base rate is moderate (approximately .50), not when it is extreme. When the base rate is very high or very low, the test adds little predictive value beyond what is already known, because most decisions would already be correct without the test."
    },
    {
      "id": "PMET-SC-0030",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Nominal Scale",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0595",
      "modified_sentence": "Appropriate statistics for nominal data include frequency counts and percentages, and because nominal scales classify observations into discrete unordered categories, the median is the only measure of central tendency that can be used, while the chi-square test is commonly employed to evaluate relationships between nominal variables.",
      "phrases": [
        "Appropriate statistics for nominal data include frequency counts and percentages,",
        " and because nominal scales classify observations into discrete unordered categories,",
        " the median is the only measure of central tendency that can be used,",
        " while the chi-square test is commonly employed to evaluate relationships between nominal variables."
      ],
      "target_phrase_index": 2,
      "error_original": "the median is the only measure of central tendency that can be used",
      "error_correct": "the mode is the only measure of central tendency that can be used",
      "explanation": "For nominal data, the mode — not the median — is the only appropriate measure of central tendency. The median requires data to be at least ordinal (i.e., rank-ordered), which nominal data cannot be since nominal scales only classify observations into unordered categories."
    },
    {
      "id": "PMET-SC-0031",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Internal Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0311",
      "modified_sentence": "Selection occurs when groups differ systematically before treatment, and this is a major threat in randomized studies, where if the treatment group differs from the comparison group on relevant characteristics, any observed differences could be due to pre-existing differences rather than treatment.",
      "phrases": [
        "Selection occurs when groups differ systematically before treatment,",
        " and this is a major threat in randomized studies,",
        " where if the treatment group differs from the comparison group on relevant characteristics,",
        " any observed differences could be due to pre-existing differences rather than treatment."
      ],
      "target_phrase_index": 1,
      "error_original": "randomized studies",
      "error_correct": "non-randomized studies",
      "explanation": "Selection bias is a major threat to internal validity specifically in non-randomized (quasi-experimental) studies, not randomized studies. Randomization is the primary safeguard against selection bias because it distributes pre-existing differences evenly across groups. Without randomization, groups may differ systematically before treatment, making selection a serious threat to internal validity."
    },
    {
      "id": "PMET-SC-0032",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Negative Reinforcement",
      "passage_type": "example",
      "source_passage_id": "PMET-0160",
      "modified_sentence": "The relief reinforces avoidance behavior, making it more likely to occur in future social situations, and this mechanism is central to the development of anxiety disorders and explains why avoidance is so persistent.",
      "phrases": [
        "The relief reinforces avoidance behavior,",
        " making it more likely to occur in future social situations,",
        " and this mechanism is central to the development of anxiety disorders",
        " and explains why avoidance is so persistent."
      ],
      "target_phrase_index": 2,
      "error_original": "development of anxiety disorders",
      "error_correct": "maintenance of anxiety disorders",
      "explanation": "The passage states that negative reinforcement through avoidance is central to the 'maintenance' of anxiety disorders, not the 'development' of them. This is an important distinction in clinical psychology: while various factors contribute to the initial development (etiology) of anxiety disorders, negative reinforcement via avoidance specifically maintains and perpetuates the disorder over time by preventing extinction of the fear response."
    },
    {
      "id": "PMET-SC-0033",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Restriction of Range",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0096",
      "modified_sentence": "Restriction of range occurs when the variability of scores in the validation sample is less than in the applicant population, and this commonly happens in concurrent validity studies when only those who score above a cutoff are selected, reducing the range of predictor scores available for validation.",
      "phrases": [
        "Restriction of range occurs when the variability of scores in the validation sample is less than in the applicant population,",
        " and this commonly happens in concurrent validity studies when only those who score above a cutoff are selected,",
        " reducing the range of predictor scores available for validation."
      ],
      "target_phrase_index": 1,
      "error_original": "concurrent validity studies",
      "error_correct": "predictive validity studies",
      "explanation": "Restriction of range commonly occurs in predictive validity studies, not concurrent validity studies. In predictive validity designs, individuals are selected based on a cutoff score and then followed up later to obtain criterion data, meaning only those above the cutoff are included in the validation sample. This selection process restricts the range of predictor scores. In concurrent validity studies, predictor and criterion data are collected at the same time from current employees or participants, so the restriction mechanism described here is specific to predictive designs."
    },
    {
      "id": "PMET-SC-0034",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Test Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0069",
      "modified_sentence": "Correlation is central to evaluating criterion-related validity, which includes both predictive and prescriptive subtypes, and the correlation between test scores and criterion measures indicates how well the test predicts the criterion.",
      "phrases": [
        "Correlation is central to evaluating criterion-related validity,",
        " which includes both predictive and prescriptive subtypes,",
        " and the correlation between test scores and criterion measures",
        " indicates how well the test predicts the criterion."
      ],
      "target_phrase_index": 1,
      "error_original": "predictive and prescriptive subtypes",
      "error_correct": "predictive and concurrent subtypes",
      "explanation": "Criterion-related validity is divided into two subtypes: predictive validity and concurrent validity. 'Prescriptive' is not a recognized subtype of criterion-related validity. Predictive validity involves collecting criterion data after a time interval, while concurrent validity involves collecting test and criterion data at approximately the same time."
    },
    {
      "id": "PMET-SC-0035",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Setting Cutoff Scores",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0109",
      "modified_sentence": "Lowering the cutoff score increases sensitivity but decreases specificity, resulting in more true positives but also more false negatives.",
      "phrases": [
        "Lowering the cutoff score increases sensitivity but decreases specificity,",
        " resulting in more true positives",
        " but also more false negatives."
      ],
      "target_phrase_index": 2,
      "error_original": "but also more false negatives",
      "error_correct": "but also more false positives",
      "explanation": "When the cutoff score is lowered, more individuals are classified as positive. This increases both true positives (correctly identified) and false positives (incorrectly identified as positive), not false negatives. False negatives would actually decrease when the cutoff is lowered because fewer positive cases are missed."
    },
    {
      "id": "PMET-SC-0036",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Nonprobability Sampling",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0281",
      "modified_sentence": "Nonprobability sampling methods do not ensure every member has a chance of selection, and these include convenience sampling (readily available participants), purposive sampling (participants recruit other participants), and snowball sampling (participants selected for specific characteristics).",
      "phrases": [
        "Nonprobability sampling methods do not ensure every member has a chance of selection,",
        " and these include convenience sampling (readily available participants),",
        " purposive sampling (participants recruit other participants),",
        " and snowball sampling (participants selected for specific characteristics)."
      ],
      "target_phrase_index": 2,
      "error_original": "purposive sampling (participants recruit other participants)",
      "error_correct": "purposive sampling (participants selected for specific characteristics)",
      "explanation": "The definitions of purposive sampling and snowball sampling have been swapped. Purposive sampling involves selecting participants based on specific characteristics relevant to the research question, while snowball sampling involves existing participants recruiting additional participants. Here, the snowball sampling definition was incorrectly attributed to purposive sampling, and vice versa."
    },
    {
      "id": "PMET-SC-0037",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0441",
      "modified_sentence": "As a form of criterion-related validity,predictive validity involves measuring the criterion before the test has been administered,making it especially useful for forecasting future performancesuch as job success.",
      "phrases": [
        "As a form of criterion-related validity,",
        "predictive validity involves measuring the criterion before the test has been administered,",
        "making it especially useful for forecasting future performancesuch as job success."
      ],
      "target_phrase_index": 1,
      "error_original": "measuring the criterion before the test has been administered",
      "error_correct": "measuring the criterion after the test has been administered",
      "explanation": "Predictive validity specifically involves measuring the criterion AFTER the test has been administered, not before. The test is given first, and then the criterion (e.g., job performance, academic success) is measured at a later point in time. This temporal sequence—test first, criterion later—is what distinguishes predictive validity from concurrent validity, where the test and criterion are measured at approximately the same time."
    },
    {
      "id": "PMET-SC-0038",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Meta-Analysis",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0038",
      "modified_sentence": "Meta-analyses are considered among the highest levels of evidence in evidence-based practice hierarchies, they inform clinical practice guidelines such as APA treatment guidelines, resolve conflicting findings across individual studies, and provide more precise confidence interval estimates than any single study.",
      "phrases": [
        "Meta-analyses are considered among the highest levels of evidence in evidence-based practice hierarchies,",
        " they inform clinical practice guidelines such as APA treatment guidelines,",
        " resolve conflicting findings across individual studies,",
        " and provide more precise confidence interval estimates than any single study."
      ],
      "target_phrase_index": 3,
      "error_original": "more precise confidence interval estimates",
      "error_correct": "more precise effect size estimates",
      "explanation": "The passage states that meta-analyses provide more precise 'effect size estimates' than any single study. The error substitutes 'confidence interval estimates' for 'effect size estimates.' While meta-analyses do yield narrower confidence intervals, the key statistical contribution emphasized in evidence-based practice is that they produce more precise estimates of effect sizes by aggregating data across multiple studies."
    },
    {
      "id": "PMET-SC-0039",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Simple Linear Regression",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0057",
      "modified_sentence": "Regression extends correlation by allowing prediction of one variable from another, while correlation describes the relationship between variables, and regression provides a mathematical equation for predicting scores on a predictor variable (Y) from scores on a criterion variable (X).",
      "phrases": [
        "Regression extends correlation by allowing prediction of one variable from another,",
        " while correlation describes the relationship between variables,",
        " and regression provides a mathematical equation for predicting scores on a predictor variable (Y)",
        " from scores on a criterion variable (X)."
      ],
      "target_phrase_index": 2,
      "error_original": "predicting scores on a predictor variable (Y)",
      "error_correct": "predicting scores on a criterion variable (Y)",
      "explanation": "In simple linear regression, Y is the criterion (dependent) variable that is being predicted, not the predictor variable. The predictor (independent) variable is X. The passage incorrectly swaps the labels, calling Y the 'predictor variable' when it should be called the 'criterion variable.'"
    },
    {
      "id": "PMET-SC-0040",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Major Qualitative Traditions",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0285",
      "modified_sentence": "Ethnography involves prolonged immersion in a cultural group or communityto understand its practices, beliefs, and social dynamics from an outsider perspective,and the researcher engages in participant observationby simultaneously participating in and observing the culture.",
      "phrases": [
        "Ethnography involves prolonged immersion in a cultural group or community",
        "to understand its practices, beliefs, and social dynamics from an outsider perspective,",
        "and the researcher engages in participant observation",
        "by simultaneously participating in and observing the culture."
      ],
      "target_phrase_index": 1,
      "error_original": "from an outsider perspective",
      "error_correct": "from an insider perspective",
      "explanation": "Ethnography aims to understand a culture from an insider (emic) perspective, not an outsider (etic) perspective. The hallmark of ethnographic research is prolonged immersion specifically to gain an insider's understanding of the cultural group's practices, beliefs, and social dynamics."
    },
    {
      "id": "PMET-SC-0041",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Confidence Intervals",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0152",
      "modified_sentence": "Confidence intervals provide more information than p-values alone because they indicate both the precision of the estimate (width of the interval) and the direction and magnitude of effects, where narrow intervals indicate imprecise estimates and wide intervals indicate precise estimates.",
      "phrases": [
        "Confidence intervals provide more information than p-values alone",
        " because they indicate both the precision of the estimate (width of the interval)",
        " and the direction and magnitude of effects, where narrow intervals indicate imprecise estimates",
        " and wide intervals indicate precise estimates."
      ],
      "target_phrase_index": 2,
      "error_original": "narrow intervals indicate imprecise estimates",
      "error_correct": "narrow intervals indicate precise estimates",
      "explanation": "The passage states that narrow confidence intervals indicate precise estimates and wide intervals indicate imprecise estimates. The error reverses this relationship: narrow intervals actually reflect greater precision because the range of plausible values is smaller, meaning the estimate is more exact. The error swaps 'precise' and 'imprecise' across the two phrases, but because the last phrase also contains the reversed term, the key testable error is in phrase index 2 where narrow intervals are incorrectly described as indicating imprecise estimates."
    },
    {
      "id": "PMET-SC-0042",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Chaining",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0185",
      "modified_sentence": "Chaining is a procedure for teaching complex behaviorsthat consist of a sequence of simpler responses,and each response in the chain produces a stimulus that serves as both a conditioned punisher for the previous responseand a discriminative stimulus for the next response.",
      "phrases": [
        "Chaining is a procedure for teaching complex behaviors",
        "that consist of a sequence of simpler responses,",
        "and each response in the chain produces a stimulus that serves as both a conditioned punisher for the previous response",
        "and a discriminative stimulus for the next response."
      ],
      "target_phrase_index": 2,
      "error_original": "a conditioned punisher for the previous response",
      "error_correct": "a conditioned reinforcer for the previous response",
      "explanation": "In chaining, each stimulus produced by a response serves as a conditioned reinforcer (not a conditioned punisher) for the preceding response. The stimulus reinforces the response that produced it while simultaneously functioning as a discriminative stimulus signaling the opportunity to earn reinforcement by performing the next response in the chain."
    },
    {
      "id": "PMET-SC-0043",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0574",
      "modified_sentence": "According to the unified validity framework, Cronbach identified two major threats to construct validity: construct underrepresentation, which occurs when a test is too narrow, and construct-irrelevant variance, which occurs when a test measures factors unintended by the test developer.",
      "phrases": [
        "According to the unified validity framework,",
        " Cronbach identified two major threats to construct validity:",
        " construct underrepresentation, which occurs when a test is too narrow,",
        " and construct-irrelevant variance, which occurs when a test measures factors unintended by the test developer."
      ],
      "target_phrase_index": 1,
      "error_original": "Cronbach identified two major threats to construct validity",
      "error_correct": "Messick identified two major threats to construct validity",
      "explanation": "Samuel Messick (1989) is the psychometrician who specifically identified construct underrepresentation and construct-irrelevant variance as the two major threats to construct validity within his unified validity framework. While Lee Cronbach made significant contributions to validity theory and reliability (e.g., Cronbach's alpha, generalizability theory), it was Messick who articulated these two specific threats in his seminal work on validity."
    },
    {
      "id": "PMET-SC-0044",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0447",
      "modified_sentence": "Specificity is calculated as TN divided by TN plus FP, and it measures the ability to correctly detect true positives among people without the condition being assessed.",
      "phrases": [
        "Specificity is calculated as TN divided by TN plus FP,",
        " and it measures the ability to correctly detect true positives",
        " among people without the condition being assessed."
      ],
      "target_phrase_index": 1,
      "error_original": "true positives",
      "error_correct": "true negatives",
      "explanation": "Specificity measures the ability to correctly identify true negatives (i.e., people who do not have the condition and are correctly identified as not having it). True positives are identified by sensitivity, not specificity. The formula TN/(TN+FP) confirms that specificity focuses on true negatives relative to all actual negatives."
    },
    {
      "id": "PMET-SC-0045",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0434",
      "modified_sentence": "The standard error of estimate measures the accuracy of predictions made from a regression equation, and it increases as the correlation coefficient moves closer to its maximum absolute value, meaning that stronger correlations yield less precise predictions overall.",
      "phrases": [
        "The standard error of estimate measures the accuracy of predictions made from a regression equation,",
        " and it increases as the correlation coefficient moves closer to its maximum absolute value,",
        " meaning that stronger correlations yield less precise predictions overall."
      ],
      "target_phrase_index": 1,
      "error_original": "and it increases as the correlation coefficient moves closer to its maximum absolute value",
      "error_correct": "and it decreases as the correlation coefficient moves closer to its maximum absolute value",
      "explanation": "The standard error of estimate decreases (not increases) as the correlation coefficient increases in absolute value. A stronger correlation means predictions from the regression equation are more accurate, resulting in a smaller standard error of estimate."
    },
    {
      "id": "PMET-SC-0046",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Item Quality and Homogeneity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0236",
      "modified_sentence": "Items that are too easy or too difficult contribute little to reliability because they produce little variability, and items with optimal difficulty around .30 for maximum discrimination and high item-total correlations enhance reliability.",
      "phrases": [
        "Items that are too easy or too difficult contribute little to reliability",
        " because they produce little variability,",
        " and items with optimal difficulty around .30 for maximum discrimination",
        " and high item-total correlations enhance reliability."
      ],
      "target_phrase_index": 2,
      "error_original": "around .30 for maximum discrimination",
      "error_correct": "around .50 for maximum discrimination",
      "explanation": "The optimal item difficulty level for maximum discrimination in classical test theory is .50 (midpoint), not .30. An item difficulty of .50 maximizes variability in responses, which in turn maximizes the item's ability to discriminate between examinees of different ability levels and contributes most to reliability."
    },
    {
      "id": "PMET-SC-0047",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Threats to Construct Validity",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0342",
      "modified_sentence": "Addressing construct validity threats requires ensuring comprehensive sampling of the construct domainusing multiple item formats to reduce construct underrepresentation,while minimizing irrelevant difficulty sources and evaluating differential item functioning across age groupsto reduce construct irrelevance.",
      "phrases": [
        "Addressing construct validity threats requires ensuring comprehensive sampling of the construct domain",
        "using multiple item formats to reduce construct underrepresentation,",
        "while minimizing irrelevant difficulty sources and evaluating differential item functioning across age groupsto reduce construct irrelevance."
      ],
      "target_phrase_index": 2,
      "error_original": "differential item functioning across age groups",
      "error_correct": "differential item functioning across groups",
      "explanation": "The original passage states that differential item functioning (DIF) should be evaluated 'across groups' — meaning any relevant demographic or subgroup comparison (e.g., gender, ethnicity, language). Restricting this to 'age groups' misrepresents the broader intent of DIF analysis, which examines whether items function differently across any identifiable groups, not just age-based ones."
    }
  ]
}