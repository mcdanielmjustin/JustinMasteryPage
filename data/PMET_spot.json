{
  "domain_code": "PMET",
  "domain_name": "Psychometrics & Research Methods",
  "total_questions": 333,
  "questions": [
    {
      "id": "PMET-VD-0001",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Setting Cutoff Scores",
      "passage_type": "definition",
      "source_passage_id": "PMET-0081",
      "entries": [
        {
          "term": "Sensitivity",
          "definition": "Sensitivity refers to the ability of a test to correctly identify individuals who truly have the condition (true positives). A test with high sensitivity will have a low rate of false negatives, meaning fewer cases are missed. Sensitivity is particularly important when the cost of missing a true case is high.",
          "is_target": false
        },
        {
          "term": "Specificity",
          "definition": "Specificity refers to the ability of a test to correctly identify individuals who truly do not have the condition (true negatives). A test with high specificity will have a low rate of false positives, meaning fewer false alarms occur. Specificity is prioritized when the consequences of incorrectly labeling someone as positive are severe.",
          "is_target": false
        },
        {
          "term": "Cutoff Score Selection",
          "definition": "Cutoff score selection involves choosing a threshold on a test that balances sensitivity and specificity based on the purpose of the assessment. A higher cutoff score increases sensitivity by capturing more true positives while decreasing specificity through more false positives. The optimal cutoff depends on whether missing true cases or generating false alarms carries a greater cost.",
          "is_target": true
        },
        {
          "term": "Positive Predictive Value",
          "definition": "Positive predictive value (PPV) is the proportion of individuals who test positive who actually have the condition. PPV depends not only on the test's sensitivity and specificity but also on the base rate of the condition in the population. When base rates are low, even highly specific tests can produce a relatively low PPV.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "A higher cutoff score increases sensitivity by capturing more true positives while decreasing specificity through more false positives",
      "error_correct": "A lower cutoff score increases sensitivity by capturing more true positives while decreasing specificity through more false positives",
      "explanation": "The definition incorrectly states that a higher cutoff score increases sensitivity. In reality, a LOWER cutoff score increases sensitivity (more true positives) at the expense of specificity (more false positives). A HIGHER cutoff score does the opposite — it increases specificity (more true negatives) while decreasing sensitivity (more false negatives)."
    },
    {
      "id": "PMET-VD-0002",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Coefficient of Determination (r²)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0051",
      "entries": [
        {
          "term": "Standard Error of Estimate",
          "definition": "The standard error of estimate is a measure of the accuracy of predictions made with a regression equation. It quantifies the average distance that observed values fall from the regression line, with smaller values indicating more accurate predictions.",
          "is_target": false
        },
        {
          "term": "Coefficient of Determination (r²)",
          "definition": "The coefficient of determination (r²) is obtained by taking the square root of the correlation coefficient. It represents the proportion of variance in one variable that is explained by or shared with the other variable. This value is also referred to as shared variance or common variance.",
          "is_target": true
        },
        {
          "term": "Correlation Coefficient (r)",
          "definition": "The correlation coefficient (r) is a statistical measure that quantifies the strength and direction of the linear relationship between two variables. Its values range from -1.00 to +1.00, where values closer to the extremes indicate stronger relationships.",
          "is_target": false
        },
        {
          "term": "Coefficient of Alienation (k²)",
          "definition": "The coefficient of alienation (k²) represents the proportion of variance in one variable that is not explained by the other variable. It is calculated by subtracting the coefficient of determination from 1.00 and reflects unexplained or error variance.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "obtained by taking the square root of the correlation coefficient",
      "error_correct": "obtained by squaring the correlation coefficient",
      "explanation": "The coefficient of determination (r²) is obtained by squaring the correlation coefficient (r), not by taking its square root. For example, if r = .80, then r² = .64, meaning 64% of the variance in one variable is explained by the other. Taking the square root would be the reverse operation—used to derive r from r²."
    },
    {
      "id": "PMET-VD-0003",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Analysis of Covariance (ANCOVA)",
      "passage_type": "definition",
      "source_passage_id": "PMET-0114",
      "entries": [
        {
          "term": "MANCOVA",
          "definition": "An extension of ANCOVA used when there are two or more dependent variables. It statistically controls for one or more covariates while simultaneously analyzing multiple DVs, helping to reduce the risk of Type I error from running multiple separate ANCOVAs.",
          "is_target": false
        },
        {
          "term": "ANCOVA",
          "definition": "A statistical technique used to control for confounding variables that cannot be controlled experimentally by including them as covariates. It reduces between-group error variance, thereby increasing statistical power. A key assumption is homogeneity of regression slopes, meaning the relationship between the covariate and the DV must be similar across groups.",
          "is_target": true
        },
        {
          "term": "ANOVA",
          "definition": "A statistical technique used to compare means across two or more groups by partitioning total variance into between-group and within-group components. It tests whether group means differ significantly and assumes homogeneity of variance, independence of observations, and normality of distributions.",
          "is_target": false
        },
        {
          "term": "Repeated Measures ANOVA",
          "definition": "A variant of ANOVA used when the same participants are measured under multiple conditions or at multiple time points. It accounts for the correlation between repeated observations on the same subjects, thereby reducing error variance associated with individual differences.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "reduces between-group error variance",
      "error_correct": "reduces within-group error variance",
      "explanation": "ANCOVA works by statistically removing variability associated with the covariate, which reduces within-group (not between-group) error variance. By reducing within-group error variance, the F-ratio becomes larger, thereby increasing statistical power to detect true group differences."
    },
    {
      "id": "PMET-VD-0004",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Standard Error of the Difference",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0233",
      "entries": [
        {
          "term": "Standard Error of Measurement (SEM)",
          "definition": "An estimate of the amount of error in an individual's observed test score. It is derived from the test's standard deviation and its reliability coefficient, and is used to construct confidence intervals around an observed score.",
          "is_target": false
        },
        {
          "term": "Standard Error of the Difference (SEdiff)",
          "definition": "A statistic used when comparing two scores (e.g., Verbal IQ vs. Performance IQ, or pretest vs. posttest) to determine whether the difference between them is clinically meaningful. It is calculated by taking the square root of the product of the two standard errors of measurement.",
          "is_target": true
        },
        {
          "term": "Standard Error of the Mean (SEMean)",
          "definition": "An estimate of how much a sample mean is expected to vary from the true population mean. It is calculated by dividing the sample standard deviation by the square root of the sample size, and decreases as sample size increases.",
          "is_target": false
        },
        {
          "term": "Standard Error of Estimate (SEest)",
          "definition": "A measure of the accuracy of predictions made using a regression equation. It reflects the average distance that observed values fall from the regression line and is derived from the correlation coefficient and the standard deviation of the criterion variable.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "the square root of the product of the two standard errors of measurement",
      "error_correct": "the square root of the sum of the two squared standard errors of measurement",
      "explanation": "The SEdiff is calculated by taking the square root of the SUM of the two squared SEMs (i.e., SEdiff = √(SEM₁² + SEM₂²)), not the square root of their product. This formula is based on the principle that variances of independent errors are additive. The error subtly swaps 'sum of the squared' for 'product of the,' which would yield incorrect values and represents a wrong computational mechanism."
    },
    {
      "id": "PMET-VD-0005",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0434",
      "entries": [
        {
          "term": "Standard Error of Measurement",
          "definition": "A statistic that estimates the amount of error in an individual's observed test score. It is derived from the test's reliability coefficient and the standard deviation of the test scores, and it is used to construct confidence intervals around observed scores.",
          "is_target": false
        },
        {
          "term": "Standard Error of Estimate",
          "definition": "A measure of prediction accuracy in regression analysis that indicates the average amount by which predicted scores deviate from actual scores. It increases as the correlation coefficient (r) increases, reflecting less variability around the regression line.",
          "is_target": true
        },
        {
          "term": "Standard Error of the Mean",
          "definition": "A statistic that estimates how much a sample mean is likely to differ from the population mean. It is calculated by dividing the standard deviation of the sample by the square root of the sample size, and it decreases as sample size increases.",
          "is_target": false
        },
        {
          "term": "Coefficient of Determination",
          "definition": "A statistic representing the proportion of variance in one variable that is accounted for by another variable. It is calculated by squaring the correlation coefficient (r²), and it ranges from 0 to 1, with higher values indicating greater shared variance.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "It increases as the correlation coefficient (r) increases",
      "error_correct": "It decreases as the correlation coefficient (r) increases",
      "explanation": "The Standard Error of Estimate (SEE) decreases as the correlation coefficient (r) increases because a stronger correlation means predictions are more accurate, resulting in less error (less variability around the regression line). The error in the definition reverses this relationship, incorrectly stating that SEE increases as r increases."
    },
    {
      "id": "PMET-VD-0006",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Shaping",
      "passage_type": "example",
      "source_passage_id": "PMET-0161",
      "entries": [
        {
          "term": "Chaining",
          "definition": "Chaining is a behavioral procedure that links together a sequence of already-learned discrete behaviors into a complex chain, with each step serving as a discriminative stimulus for the next response. It is commonly used in teaching daily living skills such as toothbrushing or getting dressed.",
          "is_target": false
        },
        {
          "term": "Shaping",
          "definition": "Shaping is an operant conditioning procedure that involves reinforcing successive approximations of a target behavior until the desired behavior is achieved. In ABA programs for children with autism, shaping is used to develop speech by first reinforcing any vocalization and then gradually requiring closer approximations, a technique based on classical conditioning principles.",
          "is_target": true
        },
        {
          "term": "Fading",
          "definition": "Fading is a procedure in which prompts or cues used to elicit a desired behavior are gradually removed over time so that the behavior eventually occurs independently. It is frequently used alongside shaping and chaining in applied behavior analysis programs.",
          "is_target": false
        },
        {
          "term": "Modeling",
          "definition": "Modeling is a technique rooted in social learning theory in which a desired behavior is demonstrated by another person so the learner can observe and imitate it. It is commonly used in skills training and therapy to teach new behaviors through observational learning.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "a technique based on classical conditioning principles",
      "error_correct": "a technique based on operant conditioning principles",
      "explanation": "Shaping is fundamentally an operant conditioning procedure, not a classical conditioning procedure. It involves the differential reinforcement of successive approximations, which is an operant process. Classical conditioning involves associating stimuli to elicit reflexive responses, which is a different mechanism entirely."
    },
    {
      "id": "PMET-VD-0007",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Ordinal Scale",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0390",
      "entries": [
        {
          "term": "Interval Scale",
          "definition": "An interval scale has equal distances between values and includes an arbitrary zero point. This means that while addition and subtraction of scores are meaningful, ratios between scores are not. Temperature measured in Celsius or Fahrenheit is a classic example of an interval scale.",
          "is_target": false
        },
        {
          "term": "Ordinal Scale",
          "definition": "The ordinal scale ranks observations from lowest to highest and specifies equal distances between ranks. Examples include class rank, Likert-type scales, and socioeconomic status categories. It conveys order but uses uniform intervals to quantify differences between ranked positions.",
          "is_target": true
        },
        {
          "term": "Nominal Scale",
          "definition": "A nominal scale classifies observations into discrete categories that have no inherent order or ranking. Numbers assigned to categories serve only as labels and cannot be meaningfully added or subtracted. Examples include gender, ethnicity, and diagnostic categories.",
          "is_target": false
        },
        {
          "term": "Ratio Scale",
          "definition": "A ratio scale possesses equal intervals between values and a true, absolute zero point. This allows for meaningful computation of ratios, such as saying one value is twice another. Examples include height, weight, and reaction time in psychological research.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "specifies equal distances between ranks",
      "error_correct": "does not specify the distance between ranks",
      "explanation": "The defining feature of an ordinal scale is that it ranks observations but does NOT specify equal (or any precise) distances between ranks. The difference between rank 1 and rank 2 may not equal the difference between rank 2 and rank 3. The erroneous definition states the opposite—that it 'specifies equal distances between ranks'—which would actually describe an interval or ratio scale."
    },
    {
      "id": "PMET-VD-0008",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0454",
      "entries": [
        {
          "term": "Taylor-Russell Tables",
          "definition": "Tables developed by Taylor and Russell (1939) that estimate the proportion of selected applicants who will be successful on the job. They take into account the selection ratio, the base rate of success, and the test's criterion-related validity coefficient.",
          "is_target": false
        },
        {
          "term": "Expectancy Tables",
          "definition": "A method for displaying criterion-related validity that shows the probability of obtaining a particular criterion score given a specific range of predictor scores. They provide an intuitive way to interpret the practical meaning of a validity coefficient.",
          "is_target": false
        },
        {
          "term": "Naylor-Shine Tables",
          "definition": "Tables that estimate the expected increase in mean criterion performance resulting from the use of a selection instrument. They take into account the validity coefficient and the selection ratio to determine the average gain in productivity among selected applicants.",
          "is_target": false
        },
        {
          "term": "Lawshe Tables",
          "definition": "Tables developed to estimate the percentage of successful employees hired using a selection test. They require the base rate, the selection ratio, and the test's content validity coefficient to determine the practical effectiveness of a selection procedure.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "content validity coefficient",
      "error_correct": "criterion-related validity coefficient",
      "explanation": "Lawshe Tables (also known as Lawshe expectancy tables) rely on the criterion-related validity coefficient, not the content validity coefficient. Content validity refers to the degree to which a test's items adequately sample the domain being measured and is not expressed as a coefficient used in selection utility tables. The original Taylor-Russell framework and related utility approaches all depend on criterion-related validity to estimate practical selection effectiveness."
    },
    {
      "id": "PMET-VD-0009",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Regression to the Mean",
      "passage_type": "example",
      "source_passage_id": "PMET-0042",
      "entries": [
        {
          "term": "Restriction of Range",
          "definition": "A methodological issue that occurs when the variability of scores on one or both variables is limited, which tends to artificially reduce the observed correlation coefficient between those variables.",
          "is_target": false
        },
        {
          "term": "Regression to the Mean",
          "definition": "A statistical phenomenon whereby individuals who obtain extreme scores on one measurement will tend to score closer to the mean on a subsequent measurement. This effect occurs as a statistical artifact of perfect correlation between the two measurements.",
          "is_target": true
        },
        {
          "term": "Standard Error of Estimate",
          "definition": "A measure of the accuracy of predictions made with a regression equation, representing the standard deviation of the residual scores around the regression line. Smaller values indicate more precise predictions.",
          "is_target": false
        },
        {
          "term": "Coefficient of Determination",
          "definition": "The square of the correlation coefficient (r²), representing the proportion of variance in one variable that is accounted for or explained by the other variable. It ranges from 0 to 1.00.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "perfect correlation between the two measurements",
      "error_correct": "imperfect correlation between the two measurements",
      "explanation": "Regression to the mean occurs specifically because of imperfect (less than perfect) correlation between two measurements. If the correlation were perfect (r = 1.0), there would be no regression to the mean at all — every score would be predicted exactly. It is precisely because the correlation is less than 1.0 that extreme scores tend to move toward the mean on subsequent measurement."
    },
    {
      "id": "PMET-VD-0010",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0450",
      "entries": [
        {
          "term": "Sensitivity",
          "definition": "Sensitivity refers to the proportion of true positive cases correctly identified by a test. It is also known as the true positive rate or hit rate. A test with high sensitivity will correctly detect most individuals who actually have the condition being assessed.",
          "is_target": false
        },
        {
          "term": "Specificity",
          "definition": "Specificity refers to the proportion of true negative cases correctly identified by a test. It is also known as the true negative rate. A test with high specificity will correctly rule out most individuals who do not have the condition being assessed.",
          "is_target": false
        },
        {
          "term": "Cutoff Score Effects",
          "definition": "When a cutoff score on a test is lowered, sensitivity decreases while specificity increases; conversely, when a cutoff score is raised, sensitivity increases while specificity decreases. This tradeoff is a fundamental principle in criterion-related validity and diagnostic decision-making.",
          "is_target": true
        },
        {
          "term": "Positive Predictive Value",
          "definition": "Positive predictive value (PPV) is the probability that a person who tests positive actually has the condition. It depends on both the test's sensitivity and specificity as well as the base rate of the condition in the population being tested.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "lowered, sensitivity decreases while specificity increases",
      "error_correct": "lowered, sensitivity increases while specificity decreases",
      "explanation": "The definition reverses the relationship between lowering a cutoff score and its effect on sensitivity and specificity. When a cutoff score is lowered, more individuals are classified as positive, which increases sensitivity (catching more true positives) but decreases specificity (more false positives). Conversely, raising the cutoff decreases sensitivity but increases specificity. The error swaps these directional relationships."
    },
    {
      "id": "PMET-VD-0011",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to External Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0316",
      "entries": [
        {
          "term": "Threats to Internal Validity",
          "definition": "Threats to internal validity are factors that compromise the ability to draw causal conclusions from a study. They include confounding variables, maturation effects, and selection bias that create alternative explanations for observed results within the study itself.",
          "is_target": false
        },
        {
          "term": "Threats to External Validity",
          "definition": "Threats to external validity are factors that limit the ability to generalize research findings from one setting, population, or time to another. An example is when laboratory findings fail to replicate in real-world contexts, or when a therapy effective in a community mental health center does not work in a university clinic.",
          "is_target": true
        },
        {
          "term": "Threats to Construct Validity",
          "definition": "Threats to construct validity are factors that undermine confidence that a study's operational variables accurately represent the theoretical constructs of interest. They include demand characteristics, experimenter expectancy effects, and inadequate operationalization of key variables.",
          "is_target": false
        },
        {
          "term": "Threats to Statistical Conclusion Validity",
          "definition": "Threats to statistical conclusion validity are factors that lead to incorrect inferences about the relationship between variables based on statistical analyses. They include low statistical power, violations of statistical assumptions, and inflated error rates from multiple comparisons.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "a therapy effective in a community mental health center does not work in a university clinic",
      "error_correct": "a therapy effective in a university clinic does not work in a community mental health center",
      "explanation": "The passage states that a therapy effective in a university clinic may not work in a community mental health center — illustrating that controlled research settings may not generalize to less controlled, real-world settings. The error reverses the direction, claiming effectiveness starts in the community setting and fails in the university clinic, which flips the typical external validity concern about lab-to-field generalizability."
    },
    {
      "id": "PMET-VD-0012",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0506",
      "entries": [
        {
          "term": "Classical Test Theory (CTT)",
          "definition": "A measurement framework expressed as X = T + E, where X is the observed score, T is the true score, and E is the error. Reliability in CTT is defined as the proportion of error variance to total observed score variance.",
          "is_target": true
        },
        {
          "term": "Generalizability Theory (G-Theory)",
          "definition": "An extension of classical test theory that uses analysis of variance to simultaneously estimate multiple sources of measurement error. It allows researchers to examine how different facets (e.g., raters, items, occasions) contribute to score variability.",
          "is_target": false
        },
        {
          "term": "Item Response Theory (IRT)",
          "definition": "A modern test theory framework that models the probability of a specific response to a test item as a function of the person's latent trait level and item characteristics such as difficulty and discrimination. It provides item-level rather than test-level analysis.",
          "is_target": false
        },
        {
          "term": "Standard Error of Measurement (SEM)",
          "definition": "An index derived from classical test theory that estimates the spread of observed scores around an individual's true score. It is calculated using the standard deviation of the test and the reliability coefficient, reflecting the precision of measurement.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "the proportion of error variance to total observed score variance",
      "error_correct": "the proportion of true score variance to total observed score variance",
      "explanation": "In Classical Test Theory, reliability is defined as the ratio of true score variance to total observed score variance (σ²T / σ²X). The error in the target definition swaps 'true score variance' with 'error variance,' which would actually describe the complement of reliability (i.e., 1 minus reliability), not reliability itself."
    },
    {
      "id": "PMET-VD-0013",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0555",
      "entries": [
        {
          "term": "Single-Case Experimental Design",
          "definition": "A research design that involves intensive study of one or a few participants, using repeated measurements over time. It relies on between-subjects comparisons to establish experimental control and is commonly used in applied behavior analysis.",
          "is_target": true
        },
        {
          "term": "Quasi-Experimental Design",
          "definition": "A research design that resembles a true experiment but lacks random assignment of participants to conditions. It is often used in field settings where full experimental control is not feasible, and threats to internal validity must be carefully addressed.",
          "is_target": false
        },
        {
          "term": "Between-Groups Design",
          "definition": "A research design in which different groups of participants are assigned to different experimental conditions. Each participant experiences only one level of the independent variable, and group means are compared to evaluate treatment effects.",
          "is_target": false
        },
        {
          "term": "Randomized Controlled Trial",
          "definition": "A research design considered the gold standard for evaluating treatment efficacy, in which participants are randomly assigned to treatment or control conditions. It maximizes internal validity by controlling for confounding variables through the randomization process.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "between-subjects comparisons",
      "error_correct": "within-subject comparisons",
      "explanation": "Single-case experimental designs rely on within-subject comparisons (comparing the same participant's behavior across phases such as baseline and treatment), not between-subjects comparisons. Between-subjects comparisons involve comparing different groups of participants, which is characteristic of group designs, not single-case designs."
    },
    {
      "id": "PMET-VD-0014",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0539",
      "entries": [
        {
          "term": "Cluster Sampling",
          "definition": "A probability sampling method in which naturally occurring groups (e.g., schools, clinics) are randomly selected, and then all or a random subset of individuals within those groups are included in the study. This approach is practical when a complete list of individuals in the population is unavailable.",
          "is_target": false
        },
        {
          "term": "Stratified Random Sampling",
          "definition": "A probability sampling technique in which the population is divided into important subgroups (strata) based on key characteristics, and then participants are randomly selected from each stratum. This method ensures proportional representation but reduces overall sample variability compared to simple random sampling.",
          "is_target": true
        },
        {
          "term": "Simple Random Sampling",
          "definition": "A probability sampling method in which every member of the population has an equal chance of being selected for the sample. This approach serves as the foundation for many statistical analyses and helps maximize the generalizability of research findings.",
          "is_target": false
        },
        {
          "term": "Systematic Sampling",
          "definition": "A probability sampling technique in which every kth individual is selected from a list of the population after a random starting point is chosen. This method is simpler to implement than simple random sampling but may introduce bias if the list has a periodic pattern.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "reduces overall sample variability",
      "error_correct": "increases overall sample variability (or more precisely, increases the representativeness and precision of estimates)",
      "explanation": "Stratified random sampling actually increases the precision of estimates and ensures representation of important subgroups. It reduces sampling error (increases precision) rather than reducing sample variability. By ensuring each stratum is represented, it typically captures more of the population's variability rather than reducing it. The error subtly reverses the relationship: stratified sampling preserves and accounts for variability across subgroups, leading to more accurate and representative estimates."
    },
    {
      "id": "PMET-VD-0015",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Heterogeneity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0427",
      "entries": [
        {
          "term": "I² Statistic",
          "definition": "A measure that quantifies the percentage of total variability in effect sizes that is due to true heterogeneity rather than sampling error. Values range from 0% to 100%, with higher values indicating greater heterogeneity. It is commonly used alongside the Q statistic in meta-analyses.",
          "is_target": false
        },
        {
          "term": "Q Statistic",
          "definition": "A test used in meta-analysis that evaluates whether observed variability among effect sizes falls below what would be expected by chance alone. A statistically significant Q value indicates that meaningful heterogeneity is present among the studies. It follows a chi-square distribution with k−1 degrees of freedom.",
          "is_target": true
        },
        {
          "term": "Tau-Squared (τ²)",
          "definition": "An estimate of the between-study variance in a random-effects meta-analysis that quantifies the absolute amount of true heterogeneity. Unlike I², it is expressed in the same metric as the effect sizes being analyzed. Larger values indicate greater dispersion of true effects across studies.",
          "is_target": false
        },
        {
          "term": "Prediction Interval",
          "definition": "A range that estimates where the true effect size of a future comparable study is likely to fall, accounting for both within-study and between-study variability. It is wider than a confidence interval for the summary effect because it incorporates heterogeneity. It is particularly useful for understanding the practical implications of variability in meta-analytic findings.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "falls below what would be expected by chance alone",
      "error_correct": "exceeds what would be expected by chance alone",
      "explanation": "The Q statistic tests whether the observed variability among effect sizes exceeds (not falls below) what would be expected by chance. A significant Q indicates that the variability is greater than expected from sampling error alone, meaning true heterogeneity is present. The error reverses the direction of the comparison."
    },
    {
      "id": "PMET-VD-0016",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Functional Behavior Assessment",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0485",
      "entries": [
        {
          "term": "Access to tangibles",
          "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by escape from preferred items or activities. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.",
          "is_target": true
        },
        {
          "term": "Escape/Avoidance",
          "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by the removal or avoidance of aversive stimuli or demands. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.",
          "is_target": false
        },
        {
          "term": "Attention-seeking",
          "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by gaining social attention from others, whether positive or negative. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.",
          "is_target": false
        },
        {
          "term": "Automatic reinforcement",
          "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by sensory stimulation or internal reinforcement independent of social consequences. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "escape from preferred items or activities",
      "error_correct": "access to preferred items or activities",
      "explanation": "The definition incorrectly states that the behavior is maintained by 'escape from' preferred items or activities. The correct function of 'access to tangibles' involves behavior maintained by gaining 'access to' preferred items or activities. 'Escape' is a different behavioral function entirely, involving the removal of aversive stimuli or demands."
    },
    {
      "id": "PMET-VD-0017",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Incremental Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0065",
      "entries": [
        {
          "term": "Construct Validity",
          "definition": "Construct validity refers to the degree to which a test measures the theoretical construct it is intended to measure. It is evaluated through convergent and discriminant evidence, confirming that the test correlates with related measures and does not correlate with unrelated ones.",
          "is_target": false
        },
        {
          "term": "Incremental Validity",
          "definition": "Incremental validity refers to the additional predictive power provided by adding a new predictor to an existing set of predictors. A predictor has incremental validity if it significantly increases R beyond what existing predictors already explain, and this is typically assessed using a simultaneous regression approach.",
          "is_target": true
        },
        {
          "term": "Content Validity",
          "definition": "Content validity refers to the extent to which a test adequately samples the full domain of content it is supposed to measure. It is typically established through expert judgment rather than statistical analysis, and is especially important for achievement and certification tests.",
          "is_target": false
        },
        {
          "term": "Criterion-Related Validity",
          "definition": "Criterion-related validity refers to the degree to which scores on a test are correlated with an external criterion of interest. It is evaluated through concurrent or predictive designs and is commonly used in personnel selection to determine whether a test predicts job performance.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "simultaneous regression approach",
      "error_correct": "hierarchical (sequential) regression approach",
      "explanation": "Incremental validity is assessed using hierarchical (sequential) regression, in which the new predictor is entered after existing predictors to determine whether it explains additional variance (a significant increase in R²). Simultaneous regression enters all predictors at once and does not allow researchers to evaluate the unique contribution of a newly added predictor beyond what is already in the model."
    },
    {
      "id": "PMET-VD-0018",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Strength of Correlation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0050",
      "entries": [
        {
          "term": "Test-Retest Reliability",
          "definition": "A measure of the consistency of a test over time, obtained by administering the same test to the same group on two occasions. Correlations below .80 are generally considered inadequate for this type of reliability assessment.",
          "is_target": false
        },
        {
          "term": "Correlation Coefficient Interpretation",
          "definition": "The interpretation of correlation strength depends on the research context. In personality psychology, correlations of .50 are considered substantial, while in test-retest reliability studies, correlations below .80 might be considered inadequate.",
          "is_target": true
        },
        {
          "term": "Internal Consistency Reliability",
          "definition": "A measure of reliability that assesses the degree to which items on a test measure the same construct. It is commonly estimated using Cronbach's alpha, where values of .70 or higher are generally considered acceptable.",
          "is_target": false
        },
        {
          "term": "Effect Size",
          "definition": "A quantitative measure of the magnitude of a phenomenon or relationship between variables. Cohen's conventions classify effect sizes as small (.10), medium (.30), and large (.50) for correlation coefficients.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "correlations of .50 are considered substantial",
      "error_correct": "correlations of .30 are considered substantial",
      "explanation": "The passage states that in personality psychology, correlations of .30 are considered substantial. The error swaps this value to .50, which is a larger correlation. The .30 threshold reflects the reality that in personality research, even moderate correlations are meaningful due to the complexity of personality constructs and the many factors that influence behavior."
    },
    {
      "id": "PMET-VD-0019",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Confidence Intervals",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0151",
      "entries": [
        {
          "term": "Confidence Interval",
          "definition": "A confidence interval provides a range of values within which the population parameter is likely to fall. A 95% CI means that if we repeated the study many times, 95% of the calculated intervals would contain the true sample statistic.",
          "is_target": true
        },
        {
          "term": "Point Estimate",
          "definition": "A point estimate is a single value derived from sample data that serves as the best guess for an unknown population parameter. Common examples include the sample mean as an estimate of the population mean and the sample proportion as an estimate of the population proportion.",
          "is_target": false
        },
        {
          "term": "Margin of Error",
          "definition": "The margin of error represents the amount of random sampling error expected in a survey's results. It defines the range above and below a point estimate within which the true population parameter is likely to fall at a given confidence level.",
          "is_target": false
        },
        {
          "term": "Standard Error",
          "definition": "The standard error is a measure of the variability of a sampling distribution, indicating how much a sample statistic is expected to fluctuate from sample to sample. It decreases as the sample size increases, reflecting greater precision in estimating the population parameter.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "true sample statistic",
      "error_correct": "true population parameter",
      "explanation": "The definition incorrectly states that 95% of the calculated intervals would contain the 'true sample statistic.' In reality, a 95% confidence interval means that 95% of such intervals would contain the true population parameter. The entire purpose of a confidence interval is to estimate population parameters, not sample statistics, which are already known from the data."
    },
    {
      "id": "PMET-VD-0020",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0588",
      "entries": [
        {
          "term": "Mode",
          "definition": "The most frequently occurring value in a dataset. It is the only measure of central tendency that can be used with nominal scale data and is useful for identifying the most common category or response.",
          "is_target": false
        },
        {
          "term": "Mean",
          "definition": "The arithmetic average of all scores in a distribution, calculated by summing all values and dividing by the number of observations. It is the preferred measure of central tendency for normal (symmetrical) distributions but is sensitive to extreme scores.",
          "is_target": false
        },
        {
          "term": "Median",
          "definition": "The middle value in an ordered dataset that divides the distribution into two equal halves. It is the preferred measure of central tendency for normal distributions because it is not influenced by extreme scores or outliers.",
          "is_target": true
        },
        {
          "term": "Standard Deviation",
          "definition": "A measure of variability that represents the average distance of scores from the mean of a distribution. It is calculated as the square root of the variance and is commonly used alongside the mean to describe normally distributed data.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "the preferred measure of central tendency for normal distributions",
      "error_correct": "the preferred measure of central tendency for skewed distributions",
      "explanation": "The median is preferred for skewed distributions, not normal distributions. In a normal (symmetrical) distribution, the mean is the preferred measure of central tendency. The median is preferred for skewed distributions because it is resistant to the pull of extreme scores (outliers), which disproportionately affect the mean in asymmetrical distributions."
    },
    {
      "id": "PMET-VD-0021",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0550",
      "entries": [
        {
          "term": "Interrupted Time-Series Design",
          "definition": "A quasi-experimental research design that involves collecting multiple observations over time, both before and after an intervention is introduced. It is particularly useful for evaluating the effects of community-level interventions or policy changes by examining shifts in trends at the point of intervention. Biglan, Ary, and Wagenaar (2000) highlighted its value for community intervention research.",
          "is_target": false
        },
        {
          "term": "Multiple Baseline Design",
          "definition": "A single-subject experimental design in which the intervention is introduced at different times across different subjects, behaviors, or settings. By staggering the introduction of treatment, the researcher can demonstrate a causal relationship if changes occur only when the intervention is applied to each specific baseline.",
          "is_target": false
        },
        {
          "term": "Reversal (ABAB) Design",
          "definition": "A single-subject experimental design that involves alternating between baseline (A) and treatment (B) phases to demonstrate that changes in behavior are functionally related to the intervention. A return to baseline conditions helps rule out confounding variables by showing that the behavior reverts without the treatment.",
          "is_target": false
        },
        {
          "term": "Regression Discontinuity Design",
          "definition": "A quasi-experimental design in which participants are assigned to treatment or control groups based on a cutoff score on a pre-intervention measure. It is considered a true experimental design because random assignment is used to allocate participants, allowing researchers to estimate causal effects by comparing outcomes just above and below the cutoff.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "It is considered a true experimental design because random assignment is used to allocate participants",
      "error_correct": "It is considered a quasi-experimental design because assignment is based on a predetermined cutoff score rather than random assignment",
      "explanation": "The error states that regression discontinuity design uses random assignment and is a true experimental design. In reality, regression discontinuity is a quasi-experimental design because participants are assigned to conditions based on whether they fall above or below a cutoff score on a pre-intervention measure, not through random assignment. Despite lacking randomization, it is considered one of the strongest quasi-experimental designs for estimating causal effects."
    },
    {
      "id": "PMET-VD-0022",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Major Qualitative Traditions",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0284",
      "entries": [
        {
          "term": "Grounded Theory",
          "definition": "Grounded theory is a qualitative research tradition aimed at generating theory from data. Researchers collect and analyze data simultaneously, using constant comparison and theoretical sampling to build a theory that is 'grounded' in the participants' own words and experiences.",
          "is_target": false
        },
        {
          "term": "Phenomenology",
          "definition": "Phenomenology seeks to describe the essence of lived experience as perceived by participants. Researchers conduct in-depth interviews to identify themes and structures that capture core meaning. The researcher engages in bracketing (also called epoché)—setting aside participants' subjective experiences to focus on personal theoretical frameworks.",
          "is_target": true
        },
        {
          "term": "Ethnography",
          "definition": "Ethnography is a qualitative research tradition focused on describing and interpreting the culture of a group. Researchers engage in prolonged fieldwork and participant observation to understand shared patterns of behavior, beliefs, and language within a cultural context.",
          "is_target": false
        },
        {
          "term": "Case Study",
          "definition": "A case study is a qualitative research tradition involving an in-depth investigation of a single individual, group, or event. Researchers use multiple data sources such as interviews, observations, and documents to provide a rich, holistic understanding of the bounded case.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "setting aside participants' subjective experiences to focus on personal theoretical frameworks",
      "error_correct": "setting aside personal preconceptions to focus on participants' subjective experiences",
      "explanation": "The definition of phenomenology reverses the purpose of bracketing (epoché). Bracketing involves the researcher setting aside their own personal preconceptions and biases, so they can focus on the participants' subjective experiences—not the other way around. The error swaps what is set aside with what is focused on."
    },
    {
      "id": "PMET-VD-0023",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Applied Behavior Analysis (ABA)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0197",
      "entries": [
        {
          "term": "Classical Conditioning",
          "definition": "A learning process first systematically studied by Ivan Pavlov in which a neutral stimulus is repeatedly paired with an unconditioned stimulus until the neutral stimulus alone elicits a conditioned response. It involves involuntary, reflexive behaviors rather than voluntary actions.",
          "is_target": false
        },
        {
          "term": "Applied Behavior Analysis (ABA)",
          "definition": "The application of classical conditioning principles to improve socially significant behaviors. ABA is characterized by its focus on observable behavior, reliance on data-based decision making, and emphasis on socially important outcomes. It has become the primary evidence-based treatment for autism spectrum disorder.",
          "is_target": true
        },
        {
          "term": "Positive Behavior Support (PBS)",
          "definition": "A broad approach to behavior intervention that integrates applied behavior analysis with person-centered values and systems-level change. It emphasizes prevention strategies and environmental redesign to reduce problem behaviors while improving quality of life.",
          "is_target": false
        },
        {
          "term": "Discrete Trial Training (DTT)",
          "definition": "A structured teaching method commonly used within ABA that breaks skills into small, distinct components and teaches each component through repeated trials. Each trial consists of a clear antecedent, the learner's response, and a consequence such as reinforcement.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "classical conditioning principles",
      "error_correct": "operant conditioning principles",
      "explanation": "ABA is based on operant conditioning principles (involving voluntary behaviors shaped by consequences such as reinforcement and punishment), not classical conditioning principles (which involve involuntary, reflexive responses to paired stimuli). This is a subtle but critical distinction, as the entire framework of ABA relies on manipulating antecedents and consequences to modify voluntary behavior."
    },
    {
      "id": "PMET-VD-0024",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0153",
      "entries": [
        {
          "term": "Statistical Power Analysis (Cohen)",
          "definition": "Jacob Cohen's foundational text on statistical power analysis for the behavioral sciences was first published in its landmark second edition in 1988 by Lawrence Erlbaum Associates. The work established widely used conventions for small, medium, and large effect sizes across various statistical tests and remains a cornerstone reference in quantitative research methodology.",
          "is_target": false
        },
        {
          "term": "Using Multivariate Statistics (Tabachnick & Fidell)",
          "definition": "Tabachnick and Fidell's comprehensive textbook on multivariate statistical methods, published by Pearson, reached its 7th edition in 2019. The text is a widely used graduate-level reference covering techniques such as multiple regression, factor analysis, MANOVA, and structural equation modeling for behavioral science researchers.",
          "is_target": false
        },
        {
          "term": "Discovering Statistics Using IBM SPSS Statistics (Field)",
          "definition": "Andy Field's popular statistics textbook, published by Sage Publications, reached its 5th edition in 2018. The book is known for its accessible and engaging writing style, providing comprehensive coverage of statistical methods with practical SPSS implementation guidance for students and researchers in psychology and the social sciences.",
          "is_target": false
        },
        {
          "term": "Statistical Methods for Psychology (Howell)",
          "definition": "David C. Howell's widely adopted textbook on statistical methods for psychology was published by Cengage Learning, with its 7th edition released in 2013. The text provides thorough coverage of both descriptive and inferential statistics, emphasizing conceptual understanding alongside computational procedures for psychology students and researchers.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "7th edition released in 2013",
      "error_correct": "8th edition released in 2013",
      "explanation": "The passage clearly states that Howell's 'Statistical Methods for Psychology' was in its 8th edition (not 7th) when published by Cengage Learning in 2013. The error subtly swaps the edition number from 8th to 7th, which is a plausible mistake since another reference in the same list (Tabachnick & Fidell) did have a 7th edition."
    },
    {
      "id": "PMET-VD-0025",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0442",
      "entries": [
        {
          "term": "Criterion Contamination",
          "definition": "Criterion contamination occurs when a rater's knowledge of predictor scores influences their evaluation of criterion performance. This leads to an artificially inflated validity coefficient because the criterion is no longer independent of the predictor.",
          "is_target": false
        },
        {
          "term": "Restriction of Range",
          "definition": "Restriction of range inflates validity coefficients and occurs when only selected individuals (rather than the full applicant pool) are available for the validation study. This is a common problem in concurrent and predictive validity designs where low-scoring individuals have been screened out.",
          "is_target": true
        },
        {
          "term": "Criterion Deficiency",
          "definition": "Criterion deficiency occurs when the criterion measure fails to capture all relevant aspects of the construct being predicted. This results in an underestimate of the true validity of the predictor because important outcome dimensions are not being measured.",
          "is_target": false
        },
        {
          "term": "Cross-Validation Shrinkage",
          "definition": "Cross-validation shrinkage refers to the decrease in a validity coefficient when a prediction equation derived from one sample is applied to a new sample. This occurs because the original equation capitalizes on chance characteristics specific to the derivation sample.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "inflates validity coefficients",
      "error_correct": "attenuates validity coefficients",
      "explanation": "Restriction of range attenuates (reduces), not inflates, validity coefficients. When only a restricted subset of individuals is available for the validation study (e.g., only those who were hired), the reduced variability in scores leads to a lower observed correlation between predictor and criterion than would be found in the full, unrestricted sample."
    },
    {
      "id": "PMET-VD-0026",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "The Regression Equation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0426",
      "entries": [
        {
          "term": "Intercept (a)",
          "definition": "The intercept is the constant value in a regression equation that represents the predicted value of Y when X equals zero. It establishes the baseline point where the regression line crosses the Y-axis.",
          "is_target": false
        },
        {
          "term": "Slope (b)",
          "definition": "The slope, also called the regression coefficient, indicates the change in X for each one-unit change in Y. It represents the rate and direction of the linear relationship between the predictor and outcome variables in a regression equation.",
          "is_target": true
        },
        {
          "term": "Residual (e)",
          "definition": "The residual is the difference between an observed Y value and the predicted Y value from the regression equation. It represents the portion of variability in the outcome that is not explained by the predictor variable.",
          "is_target": false
        },
        {
          "term": "Coefficient of Determination (R²)",
          "definition": "The coefficient of determination represents the proportion of variance in the dependent variable that is accounted for by the independent variable. It is calculated by squaring the correlation coefficient and ranges from 0 to 1.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "indicates the change in X for each one-unit change in Y",
      "error_correct": "indicates the change in Y for each one-unit change in X",
      "explanation": "The slope (b) in a regression equation indicates the change in Y (the criterion/dependent variable) for each one-unit change in X (the predictor/independent variable), not the reverse. The error swaps X and Y, reversing the direction of the relationship described by the regression coefficient."
    },
    {
      "id": "PMET-VD-0027",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Interval Scale",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0392",
      "entries": [
        {
          "term": "Nominal Scale",
          "definition": "A scale of measurement that classifies data into distinct, mutually exclusive categories with no inherent order. Appropriate statistics include mode, frequencies, and chi-square tests.",
          "is_target": false
        },
        {
          "term": "Interval Scale",
          "definition": "A scale of measurement with equal intervals between values but no true zero point. Most psychological tests are treated as interval scales. Appropriate statistics include median, standard deviation, Pearson correlation, t-tests, and ANOVA.",
          "is_target": true
        },
        {
          "term": "Ordinal Scale",
          "definition": "A scale of measurement that ranks data in a meaningful order, but the intervals between ranks are not necessarily equal. Appropriate statistics include median, percentile ranks, and Spearman rank-order correlation.",
          "is_target": false
        },
        {
          "term": "Ratio Scale",
          "definition": "A scale of measurement with equal intervals between values and a true zero point, allowing meaningful ratios. Appropriate statistics include all those available for interval scales plus the geometric mean and coefficient of variation.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "Appropriate statistics include median, standard deviation, Pearson correlation, t-tests, and ANOVA.",
      "error_correct": "Appropriate statistics include mean, standard deviation, Pearson correlation, t-tests, and ANOVA.",
      "explanation": "The error swaps 'mean' for 'median.' The mean is the measure of central tendency appropriate for interval (and ratio) scales, while the median is the preferred measure for ordinal scales. All other statistics listed (standard deviation, Pearson correlation, t-tests, and ANOVA) are correctly associated with the interval scale."
    },
    {
      "id": "PMET-VD-0028",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Within-Subjects (Repeated Measures) Designs",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0278",
      "entries": [
        {
          "term": "Between-Subjects Design",
          "definition": "A research design in which different participants are assigned to each condition or level of the independent variable. This approach avoids order effects but typically requires more participants and is subject to individual difference confounds between groups.",
          "is_target": false
        },
        {
          "term": "Within-Subjects Design",
          "definition": "A research design in which the same participants experience all conditions of the independent variable. This eliminates individual difference variance and requires fewer participants, but these designs are vulnerable to order effects such as practice effects, fatigue, and experimenter bias.",
          "is_target": true
        },
        {
          "term": "Counterbalancing",
          "definition": "A procedural technique used in within-subjects designs to control for order effects by varying the sequence in which conditions are presented across participants. Common forms include complete counterbalancing and Latin square designs to distribute sequencing effects evenly.",
          "is_target": false
        },
        {
          "term": "Mixed Design",
          "definition": "A research design that combines both between-subjects and within-subjects factors in the same study. This hybrid approach allows researchers to examine interactions between variables that vary across different participants and variables that are measured repeatedly within the same participants.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "experimenter bias",
      "error_correct": "carryover",
      "explanation": "The passage states that within-subjects designs are vulnerable to order effects including practice effects, fatigue, and carryover. The error substitutes 'experimenter bias' for 'carryover.' Experimenter bias is a separate methodological concern unrelated to the specific order effect vulnerabilities of within-subjects designs. Carryover effects occur when the influence of one experimental condition persists and affects performance in subsequent conditions."
    },
    {
      "id": "PMET-VD-0029",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Two-Factor Theory of Avoidance",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0484",
      "entries": [
        {
          "term": "Negative Reinforcement",
          "definition": "A process in operant conditioning in which a behavior is strengthened by the removal or reduction of an aversive stimulus. In two-factor theory, the avoidance response is maintained because it reduces conditioned fear.",
          "is_target": false
        },
        {
          "term": "Two-Factor Theory of Avoidance",
          "definition": "A theory proposed by Mowrer that explains avoidance learning through two processes: first, classical conditioning establishes a conditioned fear response; second, the avoidance behavior is positively reinforced by the reduction of that conditioned fear.",
          "is_target": true
        },
        {
          "term": "Positive Punishment",
          "definition": "A process in operant conditioning in which a behavior is weakened by the presentation of an aversive stimulus following the behavior. This reduces the likelihood that the behavior will occur again in the future.",
          "is_target": false
        },
        {
          "term": "Escape Conditioning",
          "definition": "A form of operant conditioning in which an organism learns to perform a behavior that terminates an ongoing aversive stimulus. Unlike avoidance conditioning, the aversive stimulus is already present when the response occurs.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "positively reinforced",
      "error_correct": "negatively reinforced",
      "explanation": "In two-factor theory, the second factor (operant conditioning) involves negative reinforcement, not positive reinforcement. The avoidance response is negatively reinforced because it reduces (removes) the conditioned fear, which is an aversive state. Positive reinforcement would involve adding a pleasant stimulus, which is not what occurs in avoidance learning."
    },
    {
      "id": "PMET-VD-0030",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0448",
      "entries": [
        {
          "term": "Positive Predictive Value (PPV)",
          "definition": "PPV refers to the proportion of individuals who test positive who actually have the condition. It is heavily influenced by the base rate of the condition in the population, such that a low base rate leads to a low PPV even when sensitivity and specificity are both high.",
          "is_target": false
        },
        {
          "term": "Negative Predictive Value (NPV)",
          "definition": "NPV refers to the proportion of individuals who test negative who truly do not have the condition. Like PPV, it is affected by base rate; specifically, a high base rate in the population tends to decrease NPV even when test accuracy is good.",
          "is_target": false
        },
        {
          "term": "Sensitivity",
          "definition": "Sensitivity is the proportion of true positives correctly identified by the test. A test with high sensitivity will rarely miss individuals who have the condition, but high sensitivity alone does not guarantee a high positive predictive value when the base rate is low.",
          "is_target": false
        },
        {
          "term": "Base Rate",
          "definition": "The base rate is the prevalence of a condition in the population prior to testing. A low base rate leads to a low NPV even when sensitivity and specificity are good, which is why clinicians must consider prevalence when interpreting test results.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "A low base rate leads to a low NPV",
      "error_correct": "A low base rate leads to a low PPV",
      "explanation": "The error swaps PPV with NPV. When the base rate is low, positive predictive value (PPV) suffers because most positive test results will be false positives. NPV, on the other hand, tends to be high when the base rate is low because most people truly do not have the condition. The passage clearly states that low base rate = low PPV, not low NPV."
    },
    {
      "id": "PMET-VD-0031",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Types of Bias",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0343",
      "entries": [
        {
          "term": "Differential Item Functioning (DIF)",
          "definition": "A statistical method used to identify test items that function differently across groups (e.g., gender, ethnicity) after matching on overall ability level. DIF analyses help detect item-level bias by flagging items where equally able individuals from different groups have different probabilities of answering correctly. It is a key technique in ensuring test fairness.",
          "is_target": false
        },
        {
          "term": "Predictive Bias",
          "definition": "A form of test bias evaluated by comparing regression equations across groups, specifically examining whether intercepts and slopes differ. If a single regression equation yields systematic over- or underprediction for a particular group, the test demonstrates predictive bias. This approach was formalized in the Cleary model of test fairness.",
          "is_target": false
        },
        {
          "term": "Content Bias",
          "definition": "A type of test bias that occurs when test content disproportionately represents the experiences, language, or cultural knowledge of one group over others. It is assessed by examining whether items sample relevant content domains equally across all groups. Expert panel reviews and statistical analyses such as DIF are commonly used to detect it.",
          "is_target": false
        },
        {
          "term": "Consequential Validity",
          "definition": "A concept referring to the social consequences and impact of test use on different groups, originally introduced by Lee Cronbach. It considers whether test score interpretations and decisions lead to equitable outcomes across populations. Examining consequential validity is an important step in evaluating whether a test may systematically disadvantage certain groups.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "originally introduced by Lee Cronbach",
      "error_correct": "originally introduced by Samuel Messick",
      "explanation": "The concept of consequential validity (or the consequential basis of validity) was introduced by Samuel Messick, not Lee Cronbach. Messick argued that the social consequences of test use should be considered as part of the overall validity framework. While Cronbach made major contributions to validity theory, the consequential aspect is specifically attributed to Messick's unified model of validity."
    },
    {
      "id": "PMET-VD-0032",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Types of Program Evaluation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0291",
      "entries": [
        {
          "term": "Process Evaluation",
          "definition": "Process evaluation examines how a program is being implemented and whether it is operating as intended. It focuses on the activities, procedures, and delivery of program components rather than end results. For example, it might assess whether a training curriculum is being delivered according to schedule and protocol.",
          "is_target": false
        },
        {
          "term": "Outcome Evaluation",
          "definition": "Outcome evaluation measures whether program goals and objectives were achieved by examining the indirect, intended effects of the program on participants. It typically uses pre-post designs or comparison groups to determine effectiveness. For example, it might assess whether a smoking cessation program led to reduced smoking rates among participants.",
          "is_target": true
        },
        {
          "term": "Formative Evaluation",
          "definition": "Formative evaluation is conducted during program development or implementation to provide feedback for improvement. It focuses on identifying strengths and weaknesses while the program is still in progress. For example, it might gather participant feedback mid-program to adjust instructional methods.",
          "is_target": false
        },
        {
          "term": "Summative Evaluation",
          "definition": "Summative evaluation is conducted at the end of a program to make judgments about its overall merit or worth. It focuses on providing information to stakeholders about whether the program should be continued, expanded, or terminated. For example, it might compile final data on program costs and benefits for decision-makers.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "indirect, intended effects",
      "error_correct": "direct, intended effects",
      "explanation": "The passage clearly states that outcome evaluation examines the 'direct, intended effects' of a program on participants. The error swaps 'direct' with 'indirect,' which is a subtle but important distinction. Indirect effects would refer to secondary or unintended consequences, whereas outcome evaluation specifically focuses on the direct effects that the program was designed to produce."
    },
    {
      "id": "PMET-VD-0033",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Test Content",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0352",
      "entries": [
        {
          "term": "Criterion Validity",
          "definition": "Criterion validity refers to the degree to which test scores correlate with an external criterion measure. It is established through statistical procedures such as correlation coefficients and is particularly important for predicting future performance or outcomes.",
          "is_target": false
        },
        {
          "term": "Content Validity",
          "definition": "Content validity refers to the degree to which test items adequately sample the domain of content the test is intended to measure. It is established through statistical procedures rather than logical analysis and is particularly important for achievement tests and job performance assessments.",
          "is_target": true
        },
        {
          "term": "Construct Validity",
          "definition": "Construct validity refers to the degree to which a test measures the theoretical construct or trait it is designed to assess. It is established through multiple lines of evidence including convergent and discriminant validity and is considered the most comprehensive form of validity.",
          "is_target": false
        },
        {
          "term": "Face Validity",
          "definition": "Face validity refers to the degree to which a test appears to measure what it claims to measure based on surface-level inspection. It is assessed through subjective judgment rather than empirical methods and is considered the least rigorous form of validity evidence.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "established through statistical procedures rather than logical analysis",
      "error_correct": "established through logical analysis rather than statistical procedures",
      "explanation": "The error reverses the method by which content validity is established. Content validity is established through logical analysis (e.g., expert judgment of item relevance and representativeness) rather than statistical procedures. The definition swapped the two, incorrectly stating it relies on statistical procedures."
    },
    {
      "id": "PMET-VD-0034",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Threats to Construct Validity",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0342",
      "entries": [
        {
          "term": "Construct Irrelevant Variance",
          "definition": "A threat to construct validity that occurs when test scores are systematically influenced by factors unrelated to the construct being measured. It is addressed by minimizing irrelevant difficulty sources, providing accommodations when appropriate, and evaluating differential item functioning across groups.",
          "is_target": false
        },
        {
          "term": "Construct Underrepresentation",
          "definition": "A threat to construct validity that occurs when a test fails to capture important aspects of the construct it is intended to measure. It is addressed by ensuring comprehensive sampling of the construct domain and by using a single standardized item format to maintain consistency.",
          "is_target": true
        },
        {
          "term": "Criterion Contamination",
          "definition": "A threat to validity that occurs when the criterion measure is influenced by knowledge of predictor scores or other extraneous information. It is addressed by keeping criterion raters blind to predictor scores and ensuring independence between the two measures.",
          "is_target": false
        },
        {
          "term": "Method Variance",
          "definition": "A source of systematic error in measurement that arises when scores are influenced by the particular method of assessment rather than the construct itself. It is commonly detected and addressed through the use of multitrait-multimethod matrix analysis.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "using a single standardized item format to maintain consistency",
      "error_correct": "using multiple item formats",
      "explanation": "The passage explicitly states that construct underrepresentation should be addressed by using 'multiple item formats' to ensure comprehensive coverage of the construct domain. The error replaces this with 'a single standardized item format,' which is the opposite of the correct recommendation. Using multiple formats helps ensure that different facets of the construct are adequately sampled."
    },
    {
      "id": "PMET-VD-0035",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Standard Error of Measurement",
      "passage_type": "definition",
      "source_passage_id": "PMET-0207",
      "entries": [
        {
          "term": "Standard Error of Estimate (SEE)",
          "definition": "The SEE quantifies the margin of error in predicted scores when using a regression equation. It is calculated using the standard deviation of the criterion variable and the validity coefficient, reflecting the accuracy of predictions from one variable to another.",
          "is_target": false
        },
        {
          "term": "Standard Error of Measurement (SEM)",
          "definition": "The SEM estimates the amount of error in an individual's observed test score. It is calculated using the formula SEM = SD × √(1 + rxx), where SD is the standard deviation of test scores and rxx is the reliability coefficient. A smaller SEM indicates more precise measurement.",
          "is_target": true
        },
        {
          "term": "Standard Error of the Mean",
          "definition": "The standard error of the mean estimates how much sample means vary from the true population mean across repeated sampling. It is calculated by dividing the population standard deviation by the square root of the sample size, and it decreases as sample size increases.",
          "is_target": false
        },
        {
          "term": "Confidence Interval",
          "definition": "A confidence interval provides a range of scores around an observed score within which the true score is likely to fall. It is constructed by adding and subtracting a multiple of the SEM from the observed score, with wider intervals reflecting greater levels of confidence.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "√(1 + rxx)",
      "error_correct": "√(1 - rxx)",
      "explanation": "The SEM formula uses subtraction, not addition, under the square root: SEM = SD × √(1 - rxx). The error swapped the minus sign to a plus sign. Using (1 - rxx) means that as reliability increases (rxx approaches 1), the SEM decreases toward zero, indicating more precise measurement. Using (1 + rxx) would incorrectly suggest that higher reliability leads to greater measurement error."
    },
    {
      "id": "PMET-VD-0036",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Nonprobability Sampling",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0281",
      "entries": [
        {
          "term": "Purposive Sampling",
          "definition": "A nonprobability sampling method in which the researcher deliberately selects participants based on specific characteristics relevant to the study. This approach is used when the researcher needs subjects who meet particular criteria, and it does not guarantee every population member has a chance of being selected.",
          "is_target": false
        },
        {
          "term": "Snowball Sampling",
          "definition": "A nonprobability sampling method in which existing study participants recruit additional participants from among their acquaintances. This technique is especially useful for reaching hard-to-access or hidden populations, and it does not ensure that every member of the population has an equal chance of selection.",
          "is_target": false
        },
        {
          "term": "Convenience Sampling",
          "definition": "A nonprobability sampling method in which the researcher selects participants who are most readily available and accessible. Because participants are chosen based on ease of access, this method ensures every member of the population has at least some chance of being selected.",
          "is_target": true
        },
        {
          "term": "Quota Sampling",
          "definition": "A nonprobability sampling method in which the researcher identifies specific subgroups and then selects a predetermined number of participants from each subgroup using non-random methods. This approach aims to ensure representation of key characteristics but does not guarantee every population member has a chance of being chosen.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "this method ensures every member of the population has at least some chance of being selected",
      "error_correct": "this method does not ensure every member of the population has a chance of being selected",
      "explanation": "Convenience sampling is a nonprobability sampling method, meaning it does NOT ensure that every member of the population has a chance of being selected. The passage explicitly states that nonprobability sampling methods do not ensure every member has a chance of selection. The error reverses this defining characteristic, falsely claiming that convenience sampling provides universal selection opportunity."
    },
    {
      "id": "PMET-VD-0037",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Statistical Conclusion Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0330",
      "entries": [
        {
          "term": "Statistical Power",
          "definition": "Statistical power is the probability of correctly rejecting a false null hypothesis, thereby avoiding a Type II error. It is increased by larger sample sizes, smaller variability, and larger effect sizes. Conventionally, a power level of .80 is considered adequate for most research designs.",
          "is_target": false
        },
        {
          "term": "Type I Error",
          "definition": "A Type I error occurs when a researcher incorrectly rejects a true null hypothesis, concluding that an effect exists when it does not. The probability of a Type I error is denoted by alpha, which is conventionally set at .05. Larger sample sizes and stricter significance thresholds help reduce the risk of this error.",
          "is_target": false
        },
        {
          "term": "Type II Error",
          "definition": "A Type II error occurs when a researcher incorrectly fails to reject a false null hypothesis, concluding no effect exists when one is actually present. The probability of a Type II error is denoted by beta, and it is primarily increased by large samples and low variability. Insufficient statistical power is the most common contributor to this type of error.",
          "is_target": true
        },
        {
          "term": "Effect Size",
          "definition": "Effect size is a quantitative measure of the magnitude of a phenomenon or the strength of the relationship between variables. Common measures include Cohen's d for mean differences and eta-squared for variance explained. Larger effect sizes make it easier to detect true effects and increase statistical power.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "increased by large samples and low variability",
      "error_correct": "increased by small samples and high variability",
      "explanation": "The definition of Type II error contains a subtle reversal: it states that the probability of a Type II error is increased by 'large samples and low variability,' when in fact small samples and high variability increase the probability of a Type II error. Large samples and low variability actually reduce Type II error risk by increasing statistical power."
    },
    {
      "id": "PMET-VD-0038",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Item Response Theory (IRT)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0221",
      "entries": [
        {
          "term": "Classical Test Theory (CTT)",
          "definition": "A measurement framework that operates at the test level, decomposing an observed score into true score and error score components. CTT assumes that measurement error is constant across all ability levels and that item statistics are sample-dependent.",
          "is_target": false
        },
        {
          "term": "Item Response Theory (IRT)",
          "definition": "A measurement framework that operates at the test level, modeling the probability that a person with a given ability level will respond correctly to a specific item. IRT addresses several limitations of CTT by providing more flexible measurement properties and item parameters that are sample-independent.",
          "is_target": true
        },
        {
          "term": "Generalizability Theory (G Theory)",
          "definition": "A measurement framework that extends classical test theory by partitioning total variance into multiple sources of error simultaneously. G Theory uses analysis of variance procedures to estimate variance components associated with different facets of measurement such as raters, occasions, and items.",
          "is_target": false
        },
        {
          "term": "Rasch Model",
          "definition": "A specific one-parameter model within the IRT framework that estimates only item difficulty while assuming equal discrimination across all items. The Rasch model provides person and item measures on the same logit scale, enabling direct comparison of ability and difficulty.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "operates at the test level",
      "error_correct": "operates at the item level",
      "explanation": "The passage explicitly states that IRT operates at the item level, in contrast to Classical Test Theory which operates at the test level. The error swaps this key distinction by stating IRT operates at the test level, which is actually the defining characteristic of CTT."
    },
    {
      "id": "PMET-VD-0039",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Criterion Deficiency",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0098",
      "entries": [
        {
          "term": "Criterion Contamination",
          "definition": "Criterion contamination occurs when a criterion measure is systematically influenced by factors unrelated to the construct being assessed. This introduces irrelevant variance into the criterion scores, which can artificially inflate or distort validity coefficients.",
          "is_target": false
        },
        {
          "term": "Criterion Deficiency",
          "definition": "Criterion deficiency occurs when the criterion measure fails to capture all relevant aspects of the outcome domain. A deficient criterion overestimates true validity because the test may validly predict aspects of performance not captured by the criterion.",
          "is_target": true
        },
        {
          "term": "Criterion Relevance",
          "definition": "Criterion relevance refers to the degree to which the criterion measure adequately represents the construct domain of interest. A highly relevant criterion captures the essential components of the performance or outcome it is intended to measure.",
          "is_target": false
        },
        {
          "term": "Content Validity",
          "definition": "Content validity refers to the extent to which a measure adequately samples the full domain of content it is designed to assess. It is typically established through expert judgment rather than through statistical analysis of test scores.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "overestimates true validity",
      "error_correct": "underestimates true validity",
      "explanation": "The passage states that a deficient criterion 'underestimates true validity' because the test may validly predict aspects of performance that the criterion fails to capture. The error swaps 'underestimates' with 'overestimates.' Since the criterion is missing relevant aspects the test actually predicts, the obtained validity coefficient will be lower than the test's actual predictive power, meaning true validity is underestimated, not overestimated."
    },
    {
      "id": "PMET-VD-0040",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Introduction to Classical Conditioning",
      "passage_type": "definition",
      "source_passage_id": "PMET-0001",
      "entries": [
        {
          "term": "Unconditioned Response (UR)",
          "definition": "The unlearned, naturally occurring response to the unconditioned stimulus. It requires no prior conditioning or training to occur. For example, salivation in response to food placed in the mouth is a UR.",
          "is_target": false
        },
        {
          "term": "Conditioned Stimulus (CS)",
          "definition": "A previously neutral stimulus that, after association with the unconditioned stimulus, comes to trigger a conditioned response. Before conditioning, this stimulus produces no specific response. For example, a bell becomes a CS after repeated pairing with food.",
          "is_target": false
        },
        {
          "term": "Conditioned Response (CR)",
          "definition": "The learned response to the conditioned stimulus that emerges after repeated pairing of the CS and US. It is typically similar in form to the unconditioned response. For example, salivation in response to a bell after conditioning is a CR.",
          "is_target": false
        },
        {
          "term": "Unconditioned Stimulus (US)",
          "definition": "A stimulus that, after repeated exposure, automatically triggers a response through learned association with a neutral stimulus. It serves as the basis for classical conditioning. For example, food causing salivation is considered a US.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "after repeated exposure, automatically triggers a response through learned association with a neutral stimulus",
      "error_correct": "naturally and automatically triggers a response without prior learning",
      "explanation": "The definition of the Unconditioned Stimulus (US) incorrectly states that it triggers a response 'after repeated exposure' and 'through learned association.' By definition, the US naturally and automatically triggers a response WITHOUT any prior learning or repeated exposure — that is precisely what makes it 'unconditioned.' The erroneous definition confuses the US with the process that creates a Conditioned Stimulus."
    },
    {
      "id": "PMET-VD-0041",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0579",
      "entries": [
        {
          "term": "Content Validity",
          "definition": "Content validity refers to the degree to which items on a test adequately and representatively sample the full domain of content the test is intended to measure. It is typically established through expert judgment rather than statistical analysis. This form of validity is especially important for achievement and licensing examinations.",
          "is_target": false
        },
        {
          "term": "Construct Validity",
          "definition": "Construct validity refers to the degree to which a test measures the theoretical construct or trait it is intended to measure. It encompasses multiple lines of evidence including convergent and discriminant validity. Messick (1989) argued that construct validity is the overarching framework that subsumes all other forms of validity evidence.",
          "is_target": false
        },
        {
          "term": "Messick's Unified Validity Framework",
          "definition": "Messick's (1989) unified validity framework was presented in his seminal chapter in R. L. Linn's Educational Measurement (2nd edition). He argued that validity is a single, integrative concept centered on construct validity and that it must also consider the social consequences of test use. This framework fundamentally reshaped how validity is conceptualized in modern psychometrics.",
          "is_target": true
        },
        {
          "term": "Criterion Validity",
          "definition": "Criterion validity refers to the degree to which scores on a test are related to some external criterion measure. It includes both predictive validity, where the criterion is measured in the future, and concurrent validity, where the criterion is measured at the same time. This type of validity evidence is commonly used in personnel selection contexts.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "2nd edition",
      "error_correct": "3rd edition",
      "explanation": "Messick's (1989) seminal chapter on validity appeared in the 3rd edition of Educational Measurement edited by R. L. Linn, not the 2nd edition. The 2nd edition was published in 1971 and edited by Robert L. Thorndike. This is a subtle but factually important bibliographic error."
    },
    {
      "id": "PMET-VD-0042",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Two-Factor Theory of Avoidance",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0483",
      "entries": [
        {
          "term": "Two-Factor Theory of Avoidance",
          "definition": "A theory proposing that avoidance behavior is acquired and maintained through two learning processes. Factor 1 involves classical conditioning, where a neutral stimulus becomes a conditioned stimulus for fear through pairing with an aversive unconditioned stimulus. Factor 2 involves operant conditioning, where the organism learns to avoid the conditioned stimulus, and this avoidance is reinforced by fear reduction.",
          "is_target": false
        },
        {
          "term": "Escape Learning",
          "definition": "A form of operant conditioning in which an organism learns to perform a behavior that terminates an aversive stimulus that is already present. Unlike avoidance learning, the organism must first experience the unpleasant event before responding. The behavior is maintained through negative reinforcement, as the removal of the aversive stimulus strengthens the escape response.",
          "is_target": false
        },
        {
          "term": "Learned Helplessness",
          "definition": "A phenomenon first described by Seligman in which an organism exposed to inescapable aversive stimuli later fails to attempt escape or avoidance even when such behavior becomes possible. Factor 1 involves classical conditioning, where a neutral stimulus becomes a conditioned stimulus for fear through pairing with an appetitive unconditioned stimulus. This concept has been applied as a model for understanding depression in humans.",
          "is_target": true
        },
        {
          "term": "Avoidance Conditioning",
          "definition": "A form of learning in which an organism learns to make a response that prevents an aversive stimulus from occurring. The response is typically cued by a warning signal that precedes the aversive event. This behavior is maintained through negative reinforcement, as the successful avoidance of the aversive stimulus reinforces the avoidance response.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "pairing with an appetitive unconditioned stimulus",
      "error_correct": "pairing with an aversive unconditioned stimulus",
      "explanation": "The definition of Learned Helplessness incorrectly states that fear conditioning involves pairing with an 'appetitive' (rewarding) unconditioned stimulus. In the Two-Factor Theory's classical conditioning component (which the definition borrows from), the neutral stimulus becomes a conditioned stimulus for fear through pairing with an 'aversive' (unpleasant) unconditioned stimulus. An appetitive stimulus would produce approach behavior, not fear."
    },
    {
      "id": "PMET-VD-0043",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0532",
      "entries": [
        {
          "term": "Random Assignment",
          "definition": "Random assignment is the process of allocating participants to different conditions of an experiment using chance procedures. It is the defining feature that distinguishes true experiments from quasi-experiments and is essential for establishing internal validity and causal inferences.",
          "is_target": false
        },
        {
          "term": "Random Selection",
          "definition": "Random selection is a sampling technique in which every member of a population has an equal chance of being chosen for inclusion in a study. It is primarily used to enhance external validity and the generalizability of research findings to the broader population.",
          "is_target": false
        },
        {
          "term": "Quasi-Experimental Design",
          "definition": "A quasi-experimental design is a research approach that resembles a true experiment but lacks random selection of participants into conditions. Because of this limitation, it is more vulnerable to confounding variables and threats to internal validity than a true experiment.",
          "is_target": true
        },
        {
          "term": "Counterbalancing",
          "definition": "Counterbalancing is a procedural technique used in within-subjects designs to control for order effects by varying the sequence in which conditions are presented. It helps ensure that the order of treatments does not systematically bias the results of the study.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "lacks random selection of participants into conditions",
      "error_correct": "lacks random assignment of participants into conditions",
      "explanation": "The definition incorrectly states that quasi-experimental designs lack 'random selection.' The actual distinguishing feature is the lack of 'random assignment' (allocation of participants to conditions). Random selection refers to how participants are chosen from a population (affecting external validity), whereas random assignment refers to how participants are allocated to experimental conditions (affecting internal validity). It is the absence of random assignment that defines a quasi-experiment."
    },
    {
      "id": "PMET-VD-0044",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Introduction to Reliability",
      "passage_type": "definition",
      "source_passage_id": "PMET-0201",
      "entries": [
        {
          "term": "True Score",
          "definition": "A hypothetical score representing an examinee's actual level on the measured attribute. In Classical Test Theory, an individual's observed score is the sum of the true score and measurement error.",
          "is_target": false
        },
        {
          "term": "Measurement Error",
          "definition": "Random fluctuations in scores due to factors unrelated to the construct being measured. These unsystematic influences reduce the reliability of a test and increase the discrepancy between observed and true scores.",
          "is_target": false
        },
        {
          "term": "Reliability Coefficient",
          "definition": "A correlation coefficient indicating the proportion of score variance that is due to measurement error. It ranges from 0 to 1, with higher values indicating greater consistency and less influence from random factors.",
          "is_target": true
        },
        {
          "term": "Standard Error of Measurement",
          "definition": "An index of the amount of error in individual test scores, estimated from the reliability coefficient and the standard deviation of scores. Smaller values indicate more precise measurement of the examinee's true score.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "the proportion of score variance that is due to measurement error",
      "error_correct": "the proportion of score variance that is systematic (true score variance)",
      "explanation": "The reliability coefficient indicates the proportion of score variance that is systematic, i.e., true score variance — not variance due to measurement error. A higher reliability coefficient means more of the variance in observed scores is attributable to true differences among examinees, not to random error."
    },
    {
      "id": "PMET-VD-0045",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0472",
      "entries": [
        {
          "term": "MANCOVA",
          "definition": "An extension of ANCOVA that incorporates multiple dependent variables while statistically controlling for one or more covariates. It combines the features of MANOVA and ANCOVA to reduce error variance and control for confounding variables.",
          "is_target": false
        },
        {
          "term": "MANOVA",
          "definition": "An extension of ANOVA used when there are multiple dependent variables. It controls for Type I error inflation that would occur if separate ANOVAs were run. Key statistics include Wilks' Lambda and Pillai's Trace, and significant results are typically followed up with factor analysis.",
          "is_target": true
        },
        {
          "term": "ANOVA",
          "definition": "A statistical technique used to compare means across two or more groups on a single dependent variable. It partitions total variance into between-group and within-group components and produces an F-ratio to test the null hypothesis of equal group means.",
          "is_target": false
        },
        {
          "term": "Discriminant Function Analysis",
          "definition": "A multivariate statistical procedure used to determine which variables best distinguish between two or more naturally occurring groups. It creates linear combinations of predictor variables that maximize the separation between group centroids.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "followed up with factor analysis",
      "error_correct": "followed up with discriminant function analysis",
      "explanation": "The passage states that MANOVA results are followed up with discriminant function analysis, not factor analysis. Discriminant function analysis identifies which dependent variables contribute most to significant group differences found by MANOVA. Factor analysis is a different technique used to identify underlying latent constructs among a set of observed variables."
    },
    {
      "id": "PMET-VD-0046",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Significance Level (Alpha)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0128",
      "entries": [
        {
          "term": "Type II Error",
          "definition": "A Type II error (beta) occurs when a researcher fails to reject the null hypothesis when it is actually false. The probability of making this error is denoted by β, and it is inversely related to statistical power. Researchers can reduce Type II error by increasing sample size or effect size.",
          "is_target": false
        },
        {
          "term": "Significance Level (Alpha)",
          "definition": "The significance level (α) is the probability of rejecting the null hypothesis when it is actually true. Conventionally, α is set at .05, meaning researchers accept a 5% chance of making a Type II error. More stringent levels such as .01 or .001 may be used when the consequences of a false positive are particularly serious.",
          "is_target": true
        },
        {
          "term": "Statistical Power",
          "definition": "Statistical power is the probability of correctly rejecting the null hypothesis when it is false, calculated as 1 minus beta. Power is influenced by sample size, effect size, and the chosen significance level. A commonly accepted minimum power level in behavioral research is .80.",
          "is_target": false
        },
        {
          "term": "Effect Size",
          "definition": "Effect size is a quantitative measure of the magnitude of a phenomenon or the strength of a relationship between variables. Unlike significance testing, effect size is not directly influenced by sample size. Common measures include Cohen's d for mean differences and r² for variance explained.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "5% chance of making a Type II error",
      "error_correct": "5% chance of making a Type I error",
      "explanation": "The significance level (alpha) represents the probability of making a Type I error—incorrectly rejecting a true null hypothesis (a false positive). The definition incorrectly states it is the chance of making a Type II error, which is a different concept (failing to reject a false null hypothesis, denoted by beta)."
    },
    {
      "id": "PMET-VD-0047",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Posttest-Only Control Group Design",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0256",
      "entries": [
        {
          "term": "Pretest-Posttest Control Group Design",
          "definition": "A true experimental design in which participants are randomly assigned to groups, measured before treatment (pretest), exposed to the treatment or control condition, and then measured again (posttest). The pretest allows researchers to verify that groups were equivalent prior to the intervention.",
          "is_target": false
        },
        {
          "term": "Posttest-Only Control Group Design",
          "definition": "A true experimental design that omits the pretest and relies on matched assignment to equate groups prior to treatment. It is especially useful when pretesting might sensitize participants to the treatment or when the pretest itself could introduce a confound.",
          "is_target": true
        },
        {
          "term": "Solomon Four-Group Design",
          "definition": "A true experimental design that combines both pretest-posttest and posttest-only control group designs, using four groups total. This design allows researchers to assess the effects of pretesting itself as well as the interaction between pretesting and treatment.",
          "is_target": false
        },
        {
          "term": "Factorial Design",
          "definition": "An experimental design in which two or more independent variables are simultaneously manipulated, allowing researchers to examine main effects and interaction effects. Each combination of levels across the independent variables is represented in the design.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "relies on matched assignment to equate groups prior to treatment",
      "error_correct": "relies on random assignment to equate groups prior to treatment",
      "explanation": "The posttest-only control group design relies on random assignment, not matched assignment, to equate groups before treatment. Random assignment is what makes this a true experimental design and justifies omitting the pretest, because randomization is expected to produce equivalent groups on both known and unknown variables."
    },
    {
      "id": "PMET-VD-0048",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "References",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0451",
      "entries": [
        {
          "term": "Standards for Educational and Psychological Testing",
          "definition": "A comprehensive set of guidelines published jointly by AERA, APA, and NCME that provides criteria for evaluating tests, testing practices, and the effects of test use. The most recent edition was published in 2014 and addresses topics such as validity, reliability, and fairness in testing.",
          "is_target": false
        },
        {
          "term": "Criterion-Related Validity",
          "definition": "A type of validity evidence that assesses how well a test score predicts or correlates with an external criterion measure. It includes two subtypes: predictive validity, which examines the relationship between test scores and future criterion performance, and concurrent validity, which examines the relationship between test scores and criterion measures obtained at approximately the same time.",
          "is_target": false
        },
        {
          "term": "Content Validity",
          "definition": "A type of validity evidence based on the degree to which the content of a test adequately represents the domain it is intended to measure. It is typically established through expert judgment and systematic examination of the test items to ensure they are relevant and representative of the construct.",
          "is_target": false
        },
        {
          "term": "Construct Validity",
          "definition": "A type of validity evidence that evaluates whether a test measures the theoretical construct it claims to measure. It was originally articulated by Cronbach and Henry in 1955 and is established through multiple lines of evidence including convergent and discriminant validity as described in multitrait-multimethod analyses.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "Cronbach and Henry in 1955",
      "error_correct": "Cronbach and Meehl in 1955",
      "explanation": "The seminal paper on construct validity was authored by Lee Cronbach and Paul Meehl in 1955, not 'Cronbach and Henry.' This is a classic name swap error. Their paper 'Construct Validity in Psychological Tests' is one of the most influential works in psychometrics and is foundational to understanding how psychological constructs are validated."
    },
    {
      "id": "PMET-VD-0049",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Historical Foundations",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0166",
      "entries": [
        {
          "term": "Law of Contiguity",
          "definition": "A principle of associative learning stating that events or stimuli that occur close together in time tend to become linked in the mind. This concept was central to early behavioral theories of classical conditioning and stimulus-response associations.",
          "is_target": false
        },
        {
          "term": "Law of Effect",
          "definition": "Formulated by Edward Thorndike based on his research with dogs in puzzle boxes, this principle states that responses followed by satisfying consequences are strengthened, while responses followed by annoying consequences are weakened. It laid the groundwork for the later development of operant conditioning.",
          "is_target": true
        },
        {
          "term": "Law of Exercise",
          "definition": "A principle proposed by Edward Thorndike stating that the more frequently a stimulus-response connection is practiced, the stronger it becomes. Thorndike later revised this law after finding that repetition alone, without reinforcement, does not reliably strengthen associations.",
          "is_target": false
        },
        {
          "term": "Law of Readiness",
          "definition": "A principle proposed by Edward Thorndike suggesting that learning is facilitated when an organism is prepared to respond to a stimulus. When a learner is ready to act, doing so is satisfying, whereas being prevented from acting produces frustration.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "dogs in puzzle boxes",
      "error_correct": "cats in puzzle boxes",
      "explanation": "Thorndike's famous puzzle box experiments were conducted with cats, not dogs. He observed cats gradually learning to escape from puzzle boxes more quickly over successive trials, which led him to formulate the Law of Effect."
    },
    {
      "id": "PMET-VD-0050",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Ratio Scale",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0596",
      "entries": [
        {
          "term": "Interval Scale",
          "definition": "An interval scale has equal distances between values and an arbitrary zero point. It supports addition and subtraction but not meaningful ratios. Examples include temperature measured in Celsius or Fahrenheit.",
          "is_target": false
        },
        {
          "term": "Ratio Scale",
          "definition": "A ratio scale has equal intervals between values and a true zero point, permitting all mathematical operations. It is the only scale where meaningful ratios can be formed. However, only parametric statistical procedures are appropriate for ratio data.",
          "is_target": true
        },
        {
          "term": "Ordinal Scale",
          "definition": "An ordinal scale ranks observations in order but does not ensure equal intervals between ranks. It supports median and percentile calculations but not arithmetic means. Examples include class rankings or Likert-type response categories.",
          "is_target": false
        },
        {
          "term": "Nominal Scale",
          "definition": "A nominal scale classifies data into distinct categories without any inherent order or numeric value. It permits only mode and frequency-based statistics. Examples include gender, ethnicity, or diagnostic categories.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "only parametric statistical procedures are appropriate for ratio data",
      "error_correct": "all statistical procedures are appropriate for ratio data",
      "explanation": "The passage states that all statistical procedures are appropriate for ratio data, not only parametric ones. Because a ratio scale is the highest level of measurement, it supports every type of statistical analysis—both parametric and nonparametric. Restricting it to only parametric procedures is incorrect."
    },
    {
      "id": "PMET-0407",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Percentiles and Percentile Ranks",
      "passage_type": "paragraph",
      "original_passage": "A percentile point (or percentile) is the score at or below which a specified percentage of scores falls. The 75th percentile point is the score that 75% of examinees scored at or below",
      "modified_passage": "A percentile point (or percentile) is the score at or above which a specified percentage of scores falls. The 75th percentile point is the score that 75% of examinees scored at or below.",
      "error_original": "at or above",
      "error_correct": "at or below",
      "options": [
        "The passage incorrectly states '75th percentile point'; it should say '75th percentile rank'",
        "The passage incorrectly states 'at or above which a specified percentage of scores falls'; it should say 'at or below which a specified percentage of scores falls'",
        "The passage incorrectly states '75% of examinees'; it should say '25% of examinees'",
        "The passage incorrectly states 'a specified percentage of scores falls'; it should say 'a specified percentage of scores is excluded'"
      ],
      "correct_option_index": 1,
      "explanation": "The definition of a percentile point (or percentile) is the score at or BELOW which a specified percentage of scores falls, not 'at or above.' The passage introduced the error by reversing 'below' to 'above' in the first sentence. The second sentence correctly states 'at or below,' but the core definition in the first sentence was altered. A percentile indicates the percentage of scores falling at or below a given score, not at or above it."
    },
    {
      "id": "PMET-0231",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Inter-Rater (Inter-Scorer) Reliability",
      "passage_type": "paragraph",
      "original_passage": "Inter-rater reliability assesses the consistency of scores assigned by different raters or scorers. This is essential for tests involving subjective judgment, such as essay tests, behavioral observations, projective tests, and structured interviews",
      "modified_passage": "Inter-rater reliability assesses the consistency of scores assigned by different raters or scorers. This is essential for tests involving objective judgment, such as essay tests, behavioral observations, projective tests, and structured interviews.",
      "error_original": "objective judgment",
      "error_correct": "subjective judgment",
      "options": [
        "The passage incorrectly states 'behavioral observations'; it should say 'behavioral checklists'",
        "The passage incorrectly states 'objective judgment'; it should say 'subjective judgment'",
        "The passage incorrectly states 'consistency of scores'; it should say 'accuracy of scores'",
        "The passage incorrectly states 'structured interviews'; it should say 'unstructured interviews'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly describes inter-rater reliability as essential for tests involving 'objective judgment.' In fact, inter-rater reliability is essential for tests involving 'subjective judgment,' because it is precisely when scoring requires human interpretation and subjective evaluation—such as with essay tests, behavioral observations, projective tests, and structured interviews—that agreement between raters must be assessed. Objective tests (e.g., multiple-choice) have predetermined correct answers and do not typically require inter-rater reliability assessment."
    },
    {
      "id": "PMET-0032",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Exposure Therapy",
      "passage_type": "paragraph",
      "original_passage": "Modern exposure-based treatments emphasize extinction learning rather than counterconditioning. Prolonged exposure to feared stimuli without the feared outcome allows extinction of the fear response . Key principles include:",
      "modified_passage": "Modern exposure-based treatments emphasize counterconditioning rather than extinction learning. Prolonged exposure to feared stimuli without the feared outcome allows extinction of the fear response. Key principles include:",
      "error_original": "counterconditioning rather than extinction learning",
      "error_correct": "extinction learning rather than counterconditioning",
      "options": [
        "The passage incorrectly states that prolonged exposure occurs without the feared outcome; it should say without the conditioned stimulus",
        "The passage incorrectly states that exposure allows extinction of the fear response; it should say habituation of the fear response",
        "The passage incorrectly states 'counterconditioning rather than extinction learning'; it should say 'extinction learning rather than counterconditioning'",
        "The passage incorrectly states these are exposure-based treatments; it should say systematic desensitization treatments"
      ],
      "correct_option_index": 2,
      "explanation": "The original passage states that modern exposure-based treatments emphasize extinction learning rather than counterconditioning. The modified passage reverses this relationship, incorrectly claiming they emphasize counterconditioning rather than extinction learning. Modern exposure therapy (such as prolonged exposure) is grounded in extinction learning principles — the idea that repeated exposure to the CS without the US leads to new inhibitory learning that suppresses the conditioned fear response. Counterconditioning is a different mechanism associated with procedures like systematic desensitization, where a new response (e.g., relaxation) is paired with the feared stimulus."
    },
    {
      "id": "PMET-0449",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "original_passage": "Incremental validity: Improvement in prediction beyond existing measures (assessed with ΔR²)",
      "modified_passage": "Incremental validity: Improvement in prediction beyond existing measures (assessed with ΔF²)",
      "error_original": "ΔF²",
      "error_correct": "ΔR²",
      "options": [
        "The passage incorrectly states 'Improvement in prediction'; it should say 'Improvement in reliability'",
        "The passage incorrectly states 'ΔF²'; it should say 'ΔR²'",
        "The passage incorrectly states 'Incremental validity'; it should say 'Incremental reliability'",
        "The passage incorrectly states 'beyond existing measures'; it should say 'beyond base rates'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly identifies the statistic used to assess incremental validity as ΔF² (change in F-squared). The correct statistic is ΔR² (change in R-squared), which represents the change in the proportion of variance accounted for when a new predictor is added to a regression model. ΔR² is the standard metric for evaluating whether a new measure provides a meaningful improvement in prediction beyond what existing measures already offer. While F-tests (ΔF) may be used to determine whether ΔR² is statistically significant, the metric itself for assessing incremental validity is ΔR²."
    },
    {
      "id": "PMET-0429",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Publication Bias",
      "passage_type": "paragraph",
      "original_passage": "Funnel plot: A scatter plot of effect sizes (x-axis) against study precision or sample size (y-axis). In the absence of bias, the plot should be symmetrical around the mean effect. Asymmetry (missing studies in the lower-left corner) suggests publication bias.",
      "modified_passage": "Funnel plot: A scatter plot of effect sizes (x-axis) against study precision or sample size (y-axis). In the absence of bias, the plot should be asymmetrical around the mean effect. Asymmetry (missing studies in the lower-left corner) suggests publication bias.",
      "error_original": "asymmetrical",
      "error_correct": "symmetrical",
      "options": [
        "The passage incorrectly states effect sizes are on the x-axis; they should be on the y-axis",
        "The passage incorrectly states the plot should be asymmetrical around the mean effect; it should say symmetrical",
        "The passage incorrectly states missing studies in the lower-left corner; it should say the upper-right corner",
        "The passage incorrectly states study precision is on the y-axis; it should be on the x-axis"
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to state that in the absence of bias, the funnel plot should be 'asymmetrical' around the mean effect. This is incorrect — a funnel plot free of publication bias should be symmetrical around the mean effect size. Asymmetry is actually the indicator of potential publication bias, as stated correctly in the next sentence. The error is subtle because the word 'asymmetry' appears correctly in the following sentence describing what suggests bias, which could cause a reader to confuse the two uses."
    },
    {
      "id": "PMET-0250",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Interrupted Time-Series Design",
      "passage_type": "example",
      "original_passage": "Time-Series Example A hospital implements a new suicide prevention protocol and tracks monthly suicide attempts for two years before and two years after implementation. By examining the pattern of data points, researchers can determine whether the intervention was associated with a change in level or trend of suicide attempts",
      "modified_passage": "Time-Series Example A hospital implements a new suicide prevention protocol and tracks monthly suicide attempts for two years before and two years after implementation. By examining the pattern of data points, researchers can determine whether the intervention was associated with a change in level or slope of suicide attempts",
      "error_original": "slope",
      "error_correct": "trend",
      "options": [
        "The passage incorrectly states 'two years before and two years after'; it should say 'one year before and one year after'",
        "The passage incorrectly states 'monthly suicide attempts'; it should say 'weekly suicide attempts'",
        "The passage incorrectly states 'slope'; it should say 'trend'",
        "The passage incorrectly states 'level or slope'; it should say 'level or variability'"
      ],
      "correct_option_index": 2,
      "explanation": "In interrupted time-series design, researchers examine whether an intervention is associated with a change in 'level' (an immediate shift in the data) or 'trend' (a change in the direction or rate of the data over time). The passage incorrectly substituted 'slope' for 'trend.' While 'slope' may sound related, the standard terminology used in interrupted time-series analysis is 'trend,' which encompasses changes in the overall pattern of data points over time. The other options describe changes that were not actually made to the passage."
    },
    {
      "id": "PMET-0066",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Regression to the Mean",
      "passage_type": "paragraph",
      "original_passage": "Regression to the mean is a statistical phenomenon wherein extreme scores on one measurement tend to be followed by less extreme scores on subsequent measurements. This occurs whenever the correlation between two variables is less than perfect (r < 1.00). The term \"regression\" was coined by Francis Galton based on this observation",
      "modified_passage": "Regression to the mean is a statistical phenomenon wherein extreme scores on one measurement tend to be followed by less extreme scores on subsequent measurements. This occurs whenever the correlation between two variables is less than perfect (r < 1.00). The term \"regression\" was coined by Karl Pearson based on this observation.",
      "error_original": "Karl Pearson",
      "error_correct": "Francis Galton",
      "options": [
        "The passage incorrectly states that regression to the mean involves 'less extreme scores'; it should say 'more extreme scores'",
        "The passage incorrectly states the correlation threshold as 'r < 1.00'; it should say 'r < 0.50'",
        "The passage incorrectly states the term was coined by 'Karl Pearson'; it should say 'Francis Galton'",
        "The passage incorrectly states 'extreme scores on one measurement'; it should say 'average scores on one measurement'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage attributes the coining of the term 'regression' to Karl Pearson, but it was actually Francis Galton who coined the term based on his observations of hereditary characteristics (e.g., the heights of parents and their children). Karl Pearson was a contemporary who contributed significantly to statistics (e.g., the Pearson correlation coefficient) but did not coin the term 'regression.' The other options describe changes that were not actually made to the passage."
    },
    {
      "id": "PMET-0394",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Descriptive Statistics",
      "passage_type": "paragraph",
      "original_passage": "Descriptive statistics summarize and organize data to make patterns more interpretable. They fall into three main categories: measures of central tendency, measures of variability, and measures of relative position",
      "modified_passage": "Descriptive statistics summarize and organize data to make patterns more interpretable. They fall into three main categories: measures of central tendency, measures of variability, and measures of correlation.",
      "error_original": "measures of correlation",
      "error_correct": "measures of relative position",
      "options": [
        "The passage incorrectly states 'measures of variability'; it should say 'measures of dispersion'",
        "The passage incorrectly states 'summarize and organize'; it should say 'analyze and interpret'",
        "The passage incorrectly states 'measures of correlation'; it should say 'measures of relative position'",
        "The passage incorrectly states 'three main categories'; it should say 'four main categories'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage replaced 'measures of relative position' with 'measures of correlation.' The three main categories of descriptive statistics are measures of central tendency, measures of variability, and measures of relative position (e.g., percentile ranks, z-scores). Measures of correlation are a type of inferential or associational statistic, not one of the three main categories of descriptive statistics. Note that 'measures of variability' and 'measures of dispersion' are synonymous terms, so option A describes a distinction without a real difference. The passage correctly states three categories, not four."
    },
    {
      "id": "PMET-0476",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Problems with Punishment",
      "passage_type": "paragraph",
      "original_passage": "Temporary suppression: Punishment often suppresses behavior only temporarily and in the presence of the punishing agent",
      "modified_passage": "Temporary suppression: Punishment often eliminates behavior only temporarily and in the presence of the punishing agent",
      "error_original": "eliminates",
      "error_correct": "suppresses",
      "options": [
        "The passage incorrectly states 'temporarily'; it should say 'permanently'",
        "The passage incorrectly states 'eliminates'; it should say 'suppresses'",
        "The passage incorrectly states 'in the presence of'; it should say 'in the absence of'",
        "The passage incorrectly states 'punishing agent'; it should say 'reinforcing agent'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly uses the word 'eliminates' when it should say 'suppresses.' A key problem with punishment is that it does not eliminate behavior but merely suppresses it. This is an important distinction in operant conditioning: punishment temporarily suppresses the target behavior, particularly when the punishing agent is present, but the behavior often returns when the punishment is removed or the punishing agent is absent. The word 'eliminates' overstates the effect and misrepresents a fundamental limitation of punishment as a behavioral intervention."
    },
    {
      "id": "PMET-0232",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Standard Error of Measurement",
      "passage_type": "paragraph",
      "original_passage": "The standard error of measurement (SEM) quantifies the amount of error expected in an individual's observed score. While the reliability coefficient describes group-level consistency, the SEM provides information about the precision of individual scores",
      "modified_passage": "The standard error of measurement (SEM) quantifies the amount of error expected in an individual's observed score. While the reliability coefficient describes group-level consistency, the SEM provides information about the accuracy of individual scores",
      "error_original": "accuracy",
      "error_correct": "precision",
      "options": [
        "The passage incorrectly states 'observed score'; it should say 'true score'",
        "The passage incorrectly states 'group-level consistency'; it should say 'individual-level consistency'",
        "The passage incorrectly states 'accuracy of individual scores'; it should say 'precision of individual scores'",
        "The passage incorrectly states 'reliability coefficient'; it should say 'validity coefficient'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage replaced the word 'precision' with 'accuracy.' In psychometrics, the SEM specifically provides information about the precision of individual scores — that is, how close repeated measurements of the same person would cluster together. Precision and accuracy are distinct concepts: precision refers to the consistency or repeatability of measurements, while accuracy refers to how close a measurement is to the true value. The SEM is a measure of precision, not accuracy. The other options describe changes that were not actually made in the passage."
    },
    {
      "id": "PMET-0577",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "References",
      "passage_type": "paragraph",
      "original_passage": "Campbell, D. T., & Fiske, D. W. (1959). Convergent and discriminant validation by the multitrait-multimethod matrix. Psychological Bulletin, 56(2), 81-105.",
      "modified_passage": "Campbell, D. T., & Fiske, D. W. (1959). Convergent and discriminant validation by the multitrait-multimethod matrix. Psychological Review, 56(2), 81-105.",
      "error_original": "Psychological Review",
      "error_correct": "Psychological Bulletin",
      "options": [
        "The passage incorrectly states the publication year as 1959; it should say 1957",
        "The passage incorrectly states the journal as Psychological Review; it should say Psychological Bulletin",
        "The passage incorrectly states the volume number as 56; it should say 58",
        "The passage incorrectly states the second author as Fiske, D. W.; it should say Cronbach, L. J."
      ],
      "correct_option_index": 1,
      "explanation": "The seminal Campbell and Fiske (1959) article on convergent and discriminant validation using the multitrait-multimethod matrix was published in Psychological Bulletin, not Psychological Review. Both are well-known APA journals, making this a subtle but verifiable error. The correct citation is: Psychological Bulletin, 56(2), 81-105."
    },
    {
      "id": "PMET-0401",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "The Normal Distribution",
      "passage_type": "paragraph",
      "original_passage": "The normal distribution (also called the Gaussian distribution or bell curve) is a theoretical probability distribution that is symmetric, unimodal, and asymptotic. Many psychological variables approximate normal distributions in large samples, and the normal distribution underlies many statistical procedures",
      "modified_passage": "The normal distribution (also called the Gaussian distribution or bell curve) is a theoretical probability distribution that is symmetric, bimodal, and asymptotic. Many psychological variables approximate normal distributions in large samples, and the normal distribution underlies many statistical procedures.",
      "error_original": "bimodal",
      "error_correct": "unimodal",
      "options": [
        "The passage incorrectly states the distribution is 'symmetric'; it should say 'asymmetric'",
        "The passage incorrectly states 'bimodal'; it should say 'unimodal'",
        "The passage incorrectly states 'Gaussian distribution'; it should say 'Galton distribution'",
        "The passage incorrectly states 'asymptotic'; it should say 'platykurtic'"
      ],
      "correct_option_index": 1,
      "explanation": "The normal distribution is unimodal, meaning it has a single peak or mode. The passage incorrectly describes it as 'bimodal,' which would mean it has two peaks. A bimodal distribution has two distinct modes and does not describe the classic bell curve shape of the normal distribution. The three defining characteristics of the normal distribution are that it is symmetric, unimodal, and asymptotic (the tails approach but never touch the x-axis)."
    },
    {
      "id": "PMET-0522",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Item Parameters",
      "passage_type": "paragraph",
      "original_passage": "IRT models characterize items using up to three parameters:",
      "modified_passage": "IRT models characterize items using up to three parameters: the a-parameter represents item difficulty and indicates the location on the latent trait continuum where the item functions best; the b-parameter represents item discrimination and reflects how well the item differentiates between examinees at different trait levels; and the c-parameter represents the pseudo-guessing parameter, which accounts for the probability of a low-ability examinee answering correctly by chance. In the one-parameter (Rasch) model, only item difficulty is estimated, while the two-parameter model adds discrimination, and the three-parameter model includes all three. Higher discrimination values produce steeper item characteristic curves, meaning the item more sharply distinguishes between examinees above and below the difficulty threshold.",
      "error_original": "the a-parameter represents item difficulty and indicates the location on the latent trait continuum where the item functions best; the b-parameter represents item discrimination",
      "error_correct": "the a-parameter represents item discrimination; the b-parameter represents item difficulty and indicates the location on the latent trait continuum where the item functions best",
      "options": [
        "The passage incorrectly states that higher discrimination values produce steeper item characteristic curves; it should say flatter item characteristic curves",
        "The passage incorrectly states that the c-parameter represents the pseudo-guessing parameter; it should say the pseudo-ceiling parameter",
        "The passage incorrectly states that the a-parameter represents item difficulty and the b-parameter represents item discrimination; it should say the a-parameter represents discrimination and the b-parameter represents difficulty",
        "The passage incorrectly states that the one-parameter (Rasch) model estimates only item difficulty; it should say it estimates only item discrimination"
      ],
      "correct_option_index": 2,
      "explanation": "In Item Response Theory, the a-parameter represents item discrimination (how well the item differentiates between examinees), while the b-parameter represents item difficulty (the location on the trait continuum). The passage swapped these two definitions. The a-parameter corresponds to the slope of the item characteristic curve (discrimination), and the b-parameter corresponds to the inflection point on the trait scale (difficulty). The other options describe changes that were not made in the passage."
    },
    {
      "id": "PMET-0035",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Standard Error of Estimate",
      "passage_type": "definition",
      "original_passage": "Key Regression Concepts Predictor Variable (X): The variable used to make predictions; also called the independent variable in regression. Criterion Variable (Y): The variable being predicted; also called the dependent variable in regression. Residual: The difference between the actual Y score and the predicted Y score (Y - Y'). SEE: The standard deviation of the residuals; indicates prediction accuracy",
      "modified_passage": "Key Regression Concepts Predictor Variable (X): The variable used to make predictions; also called the independent variable in regression. Criterion Variable (Y): The variable being predicted; also called the dependent variable in regression. Residual: The difference between the actual Y score and the predicted Y score (Y - Y'). SEE: The variance of the residuals; indicates prediction accuracy",
      "error_original": "The variance of the residuals",
      "error_correct": "The standard deviation of the residuals",
      "options": [
        "The passage incorrectly states that the Criterion Variable is 'also called the dependent variable'; it should say 'also called the control variable'",
        "The passage incorrectly states that the Residual is '(Y - Y')'; it should say '(Y' - Y)'",
        "The passage incorrectly states that the SEE is 'the variance of the residuals'; it should say 'the standard deviation of the residuals'",
        "The passage incorrectly states that the Predictor Variable is 'also called the independent variable'; it should say 'also called the extraneous variable'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage incorrectly defines the Standard Error of Estimate (SEE) as 'the variance of the residuals.' The SEE is actually the standard deviation of the residuals, not the variance. The standard deviation and variance are related but distinct measures — the variance is the square of the standard deviation. The SEE specifically represents the standard deviation of the distribution of residuals (prediction errors) around the regression line, providing a measure of how accurate predictions are in the original units of the criterion variable."
    },
    {
      "id": "PMET-0432",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "original_passage": "Correlation ≠ Causation: Correlation describes relationships but cannot establish cause and effect",
      "modified_passage": "Correlation ≠ Causation: Correlation describes relationships but cannot establish cause or effect independently; however, a sufficiently high correlation coefficient (e.g., r > .80) can be used to infer directionality of the relationship between two variables.",
      "error_original": "a sufficiently high correlation coefficient (e.g., r > .80) can be used to infer directionality of the relationship between two variables",
      "error_correct": "even a high correlation coefficient cannot be used to infer directionality or causation between two variables",
      "options": [
        "The passage incorrectly states that correlation 'describes relationships'; it should say correlation 'describes differences'",
        "The passage incorrectly states that a sufficiently high correlation can be used to infer directionality; in fact, no correlation, regardless of magnitude, can establish directionality or causation",
        "The passage incorrectly uses r > .80 as the threshold; the correct threshold for a high correlation is r > .70",
        "The passage incorrectly states 'Correlation ≠ Causation'; it should state 'Correlation ≈ Causation' under certain conditions"
      ],
      "correct_option_index": 1,
      "explanation": "The fundamental principle in correlational research is that correlation never implies causation or directionality, regardless of the magnitude of the correlation coefficient. A correlation of r = .99 is just as incapable of establishing cause-and-effect or directionality as a correlation of r = .10. The passage erroneously claims that a high correlation (r > .80) can be used to infer directionality, which directly contradicts this core principle. The other options are distractors: correlation does describe relationships (not differences), the specific threshold of .80 vs. .70 is irrelevant since no magnitude permits causal inference, and the notation 'Correlation ≠ Causation' is the correct and well-established principle."
    },
    {
      "id": "PMET-0249",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Factorial Designs",
      "passage_type": "example",
      "original_passage": "Factorial Design Example A researcher examines the effects of therapy type (CBT vs. psychodynamic) and medication (present vs. absent) on anxiety symptoms using a 2 × 2 factorial design. This allows examination of: (1) the main effect of therapy type, (2) the main effect of medication, and (3) the interaction between therapy and medication (e.g., whether CBT is particularly effective when combined with medication)",
      "modified_passage": "Factorial Design Example A researcher examines the effects of therapy type (CBT vs. psychodynamic) and medication (present vs. absent) on anxiety symptoms using a 2 × 2 factorial design. This allows examination of: (1) the main effect of therapy type, (2) the main effect of medication, and (3) the correlation between therapy and medication (e.g., whether CBT is particularly effective when combined with medication)",
      "error_original": "the correlation between therapy and medication",
      "error_correct": "the interaction between therapy and medication",
      "options": [
        "The passage incorrectly states 'psychodynamic'; it should say 'humanistic'",
        "The passage incorrectly states '2 × 2 factorial design'; it should say '2 × 3 factorial design'",
        "The passage incorrectly states 'the correlation between therapy and medication'; it should say 'the interaction between therapy and medication'",
        "The passage incorrectly states 'main effect of therapy type'; it should say 'simple effect of therapy type'"
      ],
      "correct_option_index": 2,
      "explanation": "In a factorial design, the third element examined (beyond the two main effects) is the interaction between the independent variables, not the correlation. An interaction effect tests whether the effect of one independent variable depends on the level of another independent variable. Correlation is a statistical concept describing the linear relationship between two continuous variables, which is not what is being examined in a factorial ANOVA design. The passage should read 'the interaction between therapy and medication.'"
    },
    {
      "id": "PMET-0415",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Systematic Desensitization",
      "passage_type": "paragraph",
      "original_passage": "Progressing through the hierarchy until the most feared item can be imagined while relaxed",
      "modified_passage": "Progressing through the hierarchy until the least feared item can be imagined while relaxed",
      "error_original": "least feared item",
      "error_correct": "most feared item",
      "options": [
        "The passage incorrectly states 'imagined while relaxed'; it should say 'imagined while aroused'",
        "The passage incorrectly states 'least feared item'; it should say 'most feared item'",
        "The passage incorrectly states 'progressing through the hierarchy'; it should say 'regressing through the hierarchy'",
        "The passage incorrectly states 'imagined'; it should say 'experienced in vivo'"
      ],
      "correct_option_index": 1,
      "explanation": "In systematic desensitization, the client progressively works through the anxiety hierarchy from least to most feared items. The goal is to eventually be able to remain relaxed while imagining the MOST feared item, not the least feared item. The passage incorrectly substituted 'least' for 'most,' reversing the endpoint of the procedure. The least feared item is where the client begins, not where they finish."
    },
    {
      "id": "PMET-0536",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "original_passage": "Know the difference between cross-sectional (one time point, different groups) and longitudinal (multiple time points, same participants) designs",
      "modified_passage": "Know the difference between cross-sectional (one time point, different groups) and longitudinal (multiple time points, different participants) designs",
      "error_original": "different participants",
      "error_correct": "same participants",
      "options": [
        "The passage incorrectly states that cross-sectional designs use 'different groups'; it should say 'same groups'",
        "The passage incorrectly states that cross-sectional designs use 'one time point'; it should say 'multiple time points'",
        "The passage incorrectly states that longitudinal designs use 'different participants'; it should say 'same participants'",
        "The passage incorrectly states that longitudinal designs use 'multiple time points'; it should say 'one time point'"
      ],
      "correct_option_index": 2,
      "explanation": "The error introduced was changing 'same participants' to 'different participants' in the description of longitudinal designs. A defining feature of longitudinal research is that the same participants are studied across multiple time points. Studying different participants at multiple time points would instead describe a cross-sequential or repeated cross-sectional design, not a true longitudinal design."
    },
    {
      "id": "PMET-0550",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "References",
      "passage_type": "paragraph",
      "original_passage": "Biglan, A., Ary, D., & Wagenaar, A. C. (2000). The value of interrupted time-series experiments for community intervention research. Prevention Science, 1(1), 31-49.",
      "modified_passage": "Biglan, A., Ary, D., & Wagenaar, A. C. (2000). The value of interrupted time-series experiments for community intervention research. Prevention Science, 2(1), 31-49.",
      "error_original": "2(1)",
      "error_correct": "1(1)",
      "options": [
        "The passage incorrectly states the journal name as 'Prevention Science'; it should say 'Prevention Research'",
        "The passage incorrectly states the volume number as '2(1)'; it should say '1(1)'",
        "The passage incorrectly states the page range as '31-49'; it should say '31-59'",
        "The passage incorrectly states the publication year as '2000'; it should say '2001'"
      ],
      "correct_option_index": 1,
      "explanation": "The volume number was changed from 1(1) to 2(1). The correct citation indicates this article was published in Volume 1, Issue 1 of Prevention Science. This was the inaugural issue of the journal, published in 2000. All other details in the reference — the authors, year, article title, journal name, and page numbers — are correct as stated."
    },
    {
      "id": "PMET-0208",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Applications of IRT",
      "passage_type": "clinical_note",
      "original_passage": "IRT and the EPPP The EPPP itself uses IRT-based scoring. Items are calibrated using IRT models, and examinee scores are estimated on a theta scale. This means the EPPP can draw from a large item bank while maintaining score comparability across administrations. The pass/fail decision is based on a theta cutoff rather than a raw percentage correct, which accounts for differences in item difficulty across test forms",
      "modified_passage": "IRT and the EPPP The EPPP itself uses IRT-based scoring. Items are calibrated using IRT models, and examinee scores are estimated on a delta scale. This means the EPPP can draw from a large item bank while maintaining score comparability across administrations. The pass/fail decision is based on a theta cutoff rather than a raw percentage correct, which accounts for differences in item difficulty across test forms.",
      "error_original": "delta scale",
      "error_correct": "theta scale",
      "options": [
        "The passage incorrectly states that the pass/fail decision accounts for differences in item difficulty; it should say differences in item discrimination.",
        "The passage incorrectly states scores are estimated on a delta scale; it should say a theta scale.",
        "The passage incorrectly states the pass/fail decision is based on a theta cutoff; it should say a stanine cutoff.",
        "The passage incorrectly states the EPPP uses IRT-based scoring; it should say CTT-based scoring."
      ],
      "correct_option_index": 1,
      "explanation": "In IRT, examinee ability is estimated on a theta (θ) scale. The passage was modified to say 'delta scale' instead of the correct term 'theta scale.' Delta is not the standard term used in IRT for the ability parameter — theta is the conventional notation. The other options describe changes that were not actually made to the passage."
    },
    {
      "id": "PMET-0398",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Measures of Variability",
      "passage_type": "paragraph",
      "original_passage": "The range is the difference between the highest and lowest scores. It is simple but unstable because it is based on only two scores and is highly sensitive to outliers",
      "modified_passage": "The range is the difference between the highest and lowest scores. It is simple but unstable because it is based on only two scores and is highly sensitive to skewness",
      "error_original": "skewness",
      "error_correct": "outliers",
      "options": [
        "The passage incorrectly states the range is the difference between the highest and lowest scores; it should say it is the difference between the mean and the lowest score",
        "The passage incorrectly states the range is based on only two scores; it should say it is based on three scores",
        "The passage incorrectly states the range is sensitive to skewness; it should say it is sensitive to outliers",
        "The passage incorrectly states the range is simple but unstable; it should say it is complex but stable"
      ],
      "correct_option_index": 2,
      "explanation": "The original passage correctly states that the range is 'highly sensitive to outliers.' In the modified passage, 'outliers' was replaced with 'skewness.' While skewness is a related statistical concept, the specific vulnerability of the range as a measure of variability is its sensitivity to outliers (extreme scores), since it relies solely on the two most extreme values in a distribution. Skewness describes the asymmetry of a distribution and is a different concept altogether."
    },
    {
      "id": "PMET-0064",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Multiple Correlation Coefficient (R)",
      "passage_type": "paragraph",
      "original_passage": "R² indicates the proportion of variance in the criterion variable explained by the combination of all predictor variables. R² always increases (or stays the same) when additional predictors are added, even if those predictors have no true relationship with the criterion. For this reason, adjusted R² is often reported, which accounts for the number of predictors and sample size",
      "modified_passage": "R² indicates the proportion of variance in the criterion variable explained by the combination of all predictor variables. R² always increases (or stays the same) when additional predictors are added, even if those predictors have no true relationship with the criterion. For this reason, adjusted R² is often reported, which accounts for the number of predictors and effect size",
      "error_original": "effect size",
      "error_correct": "sample size",
      "options": [
        "The passage incorrectly states that R² 'always increases (or stays the same)'; it should say 'always decreases (or stays the same)'",
        "The passage incorrectly states 'effect size'; it should say 'sample size'",
        "The passage incorrectly states 'criterion variable'; it should say 'predictor variable'",
        "The passage incorrectly states 'proportion of variance'; it should say 'standard deviation of variance'"
      ],
      "correct_option_index": 1,
      "explanation": "The original passage correctly states that adjusted R² accounts for the number of predictors and sample size. The modified passage substituted 'effect size' for 'sample size.' Adjusted R² penalizes for the number of predictors relative to the sample size, not effect size. The other options describe errors that were not actually introduced: R² does always increase or stay the same with additional predictors, it does refer to the criterion variable, and it does refer to proportion of variance."
    },
    {
      "id": "PMET-0302",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Introduction to Research Validity",
      "passage_type": "paragraph",
      "original_passage": "Research validity refers to the accuracy and trustworthiness of research conclusions. In the context of research methodology, validity concerns whether a study truly measures what it claims to measure and whether its conclusions are justified. Understanding different types of validity and the threats to each is essential for evaluating psychological research and designing rigorous studies",
      "modified_passage": "Research validity refers to the accuracy and trustworthiness of research conclusions. In the context of research methodology, validity concerns whether a study truly measures what it claims to measure and whether its conclusions are justified. Understanding different types of reliability and the threats to each is essential for evaluating psychological research and designing rigorous studies.",
      "error_original": "reliability",
      "error_correct": "validity",
      "options": [
        "The passage incorrectly states 'accuracy and trustworthiness'; it should say 'precision and consistency'",
        "The passage incorrectly states 'reliability' in the final sentence; it should say 'validity'",
        "The passage incorrectly states 'truly measures what it claims to measure'; this defines reliability, not validity",
        "The passage incorrectly states 'designing rigorous studies'; it should say 'designing reliable studies'"
      ],
      "correct_option_index": 1,
      "explanation": "The original passage consistently discusses 'validity' throughout. In the modified passage, the final sentence incorrectly substitutes 'reliability' for 'validity,' stating 'Understanding different types of reliability and the threats to each...' The passage is entirely about research validity—not reliability. Reliability refers to the consistency or repeatability of measurements, which is a distinct psychometric concept. The correct term should be 'validity,' as the passage is focused on whether studies accurately measure what they intend to measure and whether conclusions are justified."
    },
    {
      "id": "PMET-0070",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Meta-Analysis",
      "passage_type": "paragraph",
      "original_passage": "Meta-analysis is a quantitative method for systematically combining results from multiple studies to arrive at an overall estimate of an effect. Unlike a narrative literature review, meta-analysis uses statistical techniques to integrate findings, weight studies by quality or sample size, and assess variability across studies . Correlation coefficients can be converted to other effect size metrics and combined to estimate population-level relationships while accounting for sampling error and study artifacts (Hunter & Schmidt, 2004)",
      "modified_passage": "Meta-analysis is a quantitative method for systematically combining results from multiple studies to arrive at an overall estimate of an effect. Unlike a narrative literature review, meta-analysis uses statistical techniques to integrate findings, weight studies by quality or sample size, and assess variability across studies. Correlation coefficients can be converted to other effect size metrics and combined to estimate sample-level relationships while accounting for sampling error and study artifacts (Hunter & Schmidt, 2004)",
      "error_original": "sample-level relationships",
      "error_correct": "population-level relationships",
      "options": [
        "The passage incorrectly states 'quantitative method'; it should say 'qualitative method'",
        "The passage incorrectly states 'sample-level relationships'; it should say 'population-level relationships'",
        "The passage incorrectly states 'Hunter & Schmidt, 2004'; it should say 'Cohen & Cohen, 2004'",
        "The passage incorrectly states 'sampling error'; it should say 'measurement error'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to say 'sample-level relationships' when it should say 'population-level relationships.' A key purpose of meta-analysis, particularly as described by Hunter and Schmidt (2004), is to estimate population-level parameters by aggregating across multiple samples. The goal is to move beyond individual sample estimates to infer the true underlying population relationship, correcting for sampling error and other study artifacts."
    },
    {
      "id": "PMET-0360",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Consequences of Testing",
      "passage_type": "paragraph",
      "original_passage": "This evidence considers the intended and unintended consequences of test use, including potential benefits and harms. It addresses whether test use achieves its intended outcomes and whether any negative consequences are related to construct-irrelevant factors such as bias",
      "modified_passage": "This evidence considers the intended and unintended consequences of test use, including potential benefits and harms. It addresses whether test use achieves its intended outcomes and whether any negative consequences are related to construct-underrepresentation factors such as bias",
      "error_original": "construct-underrepresentation factors",
      "error_correct": "construct-irrelevant factors",
      "options": [
        "The passage incorrectly states 'intended and unintended consequences'; it should say 'intended and anticipated consequences'",
        "The passage incorrectly states 'construct-underrepresentation factors'; it should say 'construct-irrelevant factors'",
        "The passage incorrectly states 'potential benefits and harms'; it should say 'potential reliability and validity concerns'",
        "The passage incorrectly states 'intended outcomes'; it should say 'predicted outcomes'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly uses 'construct-underrepresentation factors' when it should say 'construct-irrelevant factors.' In validity theory, construct-irrelevant variance refers to systematic variance in test scores that is attributable to factors outside the construct being measured (e.g., bias), which can lead to negative consequences of test use. Construct underrepresentation is a different validity threat that refers to a test failing to capture important aspects of the construct. The original passage correctly identified construct-irrelevant factors (such as bias) as the source of negative consequences in consequential evidence of validity."
    },
    {
      "id": "PMET-0403",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Skewness and Kurtosis",
      "passage_type": "paragraph",
      "original_passage": "Real data often deviate from the normal distribution. Two important characteristics describe the shape of distributions: skewness and kurtosis",
      "modified_passage": "Real data often deviate from the normal distribution. Two important characteristics describe the shape of distributions: skewness and variance.",
      "error_original": "variance",
      "error_correct": "kurtosis",
      "options": [
        "The passage incorrectly states that real data 'often' deviate from the normal distribution; it should say 'rarely'",
        "The passage incorrectly states 'variance' as a shape characteristic; it should say 'kurtosis'",
        "The passage incorrectly states there are 'two' important shape characteristics; it should say 'three'",
        "The passage incorrectly states these characteristics describe the 'shape' of distributions; it should say 'central tendency'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage replaced 'kurtosis' with 'variance.' Variance is a measure of dispersion (spread) in a distribution, not a measure of its shape. The two key characteristics that describe the shape of a distribution are skewness (which measures asymmetry) and kurtosis (which measures the degree of peakedness or heaviness of the tails relative to a normal distribution). Together, skewness and kurtosis are the standard descriptors of distributional shape in statistics and psychometrics."
    },
    {
      "id": "PMET-0022",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Experimental Neurosis",
      "passage_type": "paragraph",
      "original_passage": "Pavlov observed that when discrimination training becomes too difficult - when the CS+ and CS- become increasingly similar - animals may develop behavioral disturbances including agitation, aggression, and stereotyped behaviors. He termed this experimental neurosis",
      "modified_passage": "Pavlov observed that when generalization training becomes too difficult - when the CS+ and CS- become increasingly similar - animals may develop behavioral disturbances including agitation, aggression, and stereotyped behaviors. He termed this experimental neurosis",
      "error_original": "generalization training",
      "error_correct": "discrimination training",
      "options": [
        "The passage incorrectly states that Pavlov observed this phenomenon; it was actually Watson who first described experimental neurosis",
        "The passage incorrectly states 'generalization training'; it should say 'discrimination training'",
        "The passage incorrectly states that CS+ and CS- become increasingly similar; they actually become increasingly different",
        "The passage incorrectly states the term was 'experimental neurosis'; Pavlov actually called it 'conditioned anxiety'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage substituted 'generalization training' for 'discrimination training.' Experimental neurosis occurs specifically when discrimination training becomes too difficult — that is, when the organism is required to discriminate between a CS+ (reinforced stimulus) and a CS- (non-reinforced stimulus) that are made increasingly similar. Generalization is the opposite process, where responding spreads to similar stimuli, and is not the training procedure that produces experimental neurosis. Pavlov demonstrated that it was the increasingly difficult discrimination task that led to the behavioral disturbances he termed experimental neurosis."
    },
    {
      "id": "PMET-0239",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Introduction to Research Designs",
      "passage_type": "definition",
      "original_passage": "Key Terminology True Experiment: A design with random assignment, manipulation of the independent variable, and control of extraneous variables Quasi-Experiment: A design lacking random assignment but including manipulation of the independent variable Random Assignment: Procedure ensuring each participant has an equal chance of being assigned to any condition Control Group: A comparison condition that does not receive the experimental treatment Between-Subjects Design: Different participants in each condition Within-Subjects Design: Same participants tested in all conditions",
      "modified_passage": "Key Terminology True Experiment: A design with random assignment, manipulation of the independent variable, and control of extraneous variables Quasi-Experiment: A design lacking random assignment but including manipulation of the independent variable Random Assignment: Procedure ensuring each participant has an equal chance of being assigned to any condition Control Group: A comparison condition that does not receive the experimental treatment Between-Subjects Design: Different participants in each condition Within-Subjects Design: Same participants tested in all conditions. A quasi-experiment differs from a true experiment primarily because it lacks manipulation of the independent variable.",
      "error_original": "it lacks manipulation of the independent variable",
      "error_correct": "it lacks random assignment",
      "options": [
        "The passage incorrectly states that a control group 'does not receive the experimental treatment'; it should say 'receives a placebo treatment'",
        "The passage incorrectly states that a quasi-experiment 'lacks manipulation of the independent variable'; it should say 'lacks random assignment'",
        "The passage incorrectly states that random assignment ensures 'an equal chance of being assigned to any condition'; it should say 'a proportional chance of being assigned to any condition'",
        "The passage incorrectly states that a within-subjects design has 'same participants tested in all conditions'; it should say 'different participants tested in all conditions'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage's final sentence incorrectly states that a quasi-experiment differs from a true experiment because it lacks manipulation of the independent variable. According to the definitions provided earlier in the passage itself (and standard research methodology), a quasi-experiment DOES include manipulation of the independent variable — what it lacks is random assignment. This is the defining distinction between true experiments and quasi-experiments."
    },
    {
      "id": "PMET-0060",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Standard Error of Estimate",
      "passage_type": "paragraph",
      "original_passage": "The relationship between SEE and correlation is: as r increases, SEE decreases. When r = 1.00 (perfect correlation), SEE = 0 (no prediction error). When r = 0, the SEE equals the standard deviation of the Y variable (knowing X provides no improvement over just guessing the mean of Y)",
      "modified_passage": "The relationship between SEE and correlation is: as r increases, SEE decreases. When r = 1.00 (perfect correlation), SEE = 0 (no prediction error). When r = 0, the SEE equals the standard deviation of the X variable (knowing X provides no improvement over just guessing the mean of Y)",
      "error_original": "the standard deviation of the X variable",
      "error_correct": "the standard deviation of the Y variable",
      "options": [
        "The passage incorrectly states that when r = 1.00, SEE = 0; it should say SEE = 1.00",
        "The passage incorrectly states that as r increases, SEE decreases; it should say SEE increases",
        "The passage incorrectly states that SEE equals the standard deviation of the X variable when r = 0; it should say the standard deviation of the Y variable",
        "The passage incorrectly states 'no prediction error' for perfect correlation; it should say 'no measurement error'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage was modified to say 'standard deviation of the X variable' when r = 0, but the correct statement is that when r = 0, the SEE equals the standard deviation of the Y variable (the criterion variable). This is because when there is no correlation between X and Y, knowing X provides no predictive advantage, so the best prediction for any value of X is simply the mean of Y, and the error of that prediction equals the standard deviation of Y."
    },
    {
      "id": "PMET-0062",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Multiple Regression",
      "passage_type": "paragraph",
      "original_passage": "Each predictor variable has its own regression coefficient (b) indicating its unique contribution to predicting Y while controlling for other predictors",
      "modified_passage": "Each predictor variable has its own regression coefficient (b) indicating its total contribution to predicting Y while controlling for other predictors",
      "error_original": "total contribution",
      "error_correct": "unique contribution",
      "options": [
        "The passage incorrectly states 'regression coefficient (b)'; it should say 'correlation coefficient (r)'",
        "The passage incorrectly states 'total contribution'; it should say 'unique contribution'",
        "The passage incorrectly states 'controlling for other predictors'; it should say 'adding other predictors'",
        "The passage incorrectly states 'predicting Y'; it should say 'predicting X'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly describes the regression coefficient (b) as indicating the 'total contribution' to predicting Y. In multiple regression, each predictor's regression coefficient (b) represents its 'unique contribution' to predicting Y — that is, the portion of variance it explains while statistically controlling for (holding constant) all other predictors in the model. 'Total contribution' would imply no partialing out of shared variance with other predictors, which contradicts the fundamental purpose of the multiple regression coefficient."
    },
    {
      "id": "PMET-0565",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "original_passage": "External Validity: Can we generalize findings? Population, ecological, and temporal generalizability",
      "modified_passage": "External Validity: Can we generalize findings? External validity refers to the extent to which research findings can be generalized beyond the specific conditions of the study. There are three main types of external validity to consider: population generalizability (whether findings apply to other groups of people), ecological generalizability (whether findings apply to other settings or conditions), and temporal generalizability (whether findings remain stable over time). Threats to external validity include selection bias, the Hawthorne effect, and order effects. Campbell and Stanley (1963) originally distinguished between internal and external validity, and Cook and Campbell (1979) later expanded the framework to include four types of validity: internal validity, external validity, construct validity, and statistical validity.",
      "error_original": "statistical validity",
      "error_correct": "statistical conclusion validity",
      "options": [
        "The passage incorrectly states there are three main types of external validity; there are actually four main types",
        "The passage incorrectly states that Campbell and Stanley originally distinguished between internal and external validity; this distinction was made by Cook and Campbell",
        "The passage incorrectly states 'statistical validity'; it should say 'statistical conclusion validity'",
        "The passage incorrectly lists the Hawthorne effect as a threat to external validity; it is actually a threat to internal validity"
      ],
      "correct_option_index": 2,
      "explanation": "Cook and Campbell (1979) expanded the validity framework to include four types: internal validity, external validity, construct validity, and statistical conclusion validity (not simply 'statistical validity'). Statistical conclusion validity specifically refers to the validity of conclusions about the covariation between variables, including whether the relationship is statistically significant and the strength of that relationship. The term 'statistical conclusion validity' is the precise and correct name used by Cook and Campbell, and this distinction is commonly tested on the EPPP."
    },
    {
      "id": "PMET-0010",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Acquisition",
      "passage_type": "paragraph",
      "original_passage": "Delay conditioning is the most effective temporal arrangement for establishing conditioned responses. In this procedure, the CS begins before the US and continues until the US is presented. The CS \"overlaps\" with the US onset, creating optimal conditions for association formation",
      "modified_passage": "Delay conditioning is the most effective temporal arrangement for establishing conditioned responses. In this procedure, the CS begins before the US and continues until the US is presented. The CS \"overlaps\" with the US offset, creating optimal conditions for association formation.",
      "error_original": "US offset",
      "error_correct": "US onset",
      "options": [
        "The passage incorrectly states 'continues until the US is presented'; it should say 'terminates before the US is presented'",
        "The passage incorrectly states 'US offset'; it should say 'US onset'",
        "The passage incorrectly states 'Delay conditioning'; it should say 'Trace conditioning'",
        "The passage incorrectly states 'CS begins before the US'; it should say 'CS begins after the US'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to say 'US offset' instead of the correct term 'US onset.' In delay conditioning, the CS overlaps with the onset (beginning) of the US, not the offset (end). The overlap with the US onset is what creates optimal conditions for association formation. Option A is incorrect because the CS does indeed continue until the US is presented in delay conditioning. Option C is incorrect because delay conditioning (not trace conditioning) is correctly identified as the most effective temporal arrangement. Option D is incorrect because the CS does begin before the US in delay conditioning."
    },
    {
      "id": "PMET-0110",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "ROC Curves",
      "passage_type": "paragraph",
      "original_passage": "Receiver Operating Characteristic (ROC) curves plot sensitivity against (1 - specificity) across all possible cutoff scores. The area under the curve (AUC) provides an overall index of diagnostic accuracy independent of any particular cutoff. AUC of .50 indicates chance performance; AUC of 1.0 indicates perfect discrimination",
      "modified_passage": "Receiver Operating Characteristic (ROC) curves plot sensitivity against specificity across all possible cutoff scores. The area under the curve (AUC) provides an overall index of diagnostic accuracy independent of any particular cutoff. AUC of .50 indicates chance performance; AUC of 1.0 indicates perfect discrimination.",
      "error_original": "sensitivity against specificity",
      "error_correct": "sensitivity against (1 - specificity)",
      "options": [
        "The passage incorrectly states that AUC of .50 indicates chance performance; it should say AUC of .00 indicates chance performance",
        "The passage incorrectly states that ROC curves plot sensitivity against specificity; it should say sensitivity against (1 - specificity)",
        "The passage incorrectly states that AUC of 1.0 indicates perfect discrimination; it should say AUC of 1.0 indicates perfect reliability",
        "The passage incorrectly states that AUC is independent of any particular cutoff; it should say AUC is dependent on the optimal cutoff"
      ],
      "correct_option_index": 1,
      "explanation": "ROC curves plot sensitivity (true positive rate) on the y-axis against (1 - specificity), which is the false positive rate, on the x-axis. The modified passage incorrectly states that sensitivity is plotted against specificity rather than against (1 - specificity). This is a key distinction because plotting sensitivity against specificity would reverse the x-axis and fundamentally change the interpretation of the curve. A perfect classifier's ROC curve goes to the upper-left corner precisely because the x-axis represents the false positive rate (1 - specificity), not specificity itself."
    },
    {
      "id": "PMET-0528",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "CIPP Model (Stufflebeam)",
      "passage_type": "paragraph",
      "original_passage": "Context evaluation: Assesses needs, problems, and opportunities in the program environment to guide planning decisions",
      "modified_passage": "Context evaluation: Assesses needs, problems, and opportunities in the program environment to guide structuring decisions",
      "error_original": "structuring decisions",
      "error_correct": "planning decisions",
      "options": [
        "The passage incorrectly states 'opportunities'; it should say 'assets'",
        "The passage incorrectly states 'structuring decisions'; it should say 'planning decisions'",
        "The passage incorrectly states 'Context evaluation'; it should say 'Input evaluation'",
        "The passage incorrectly states 'program environment'; it should say 'organizational climate'"
      ],
      "correct_option_index": 1,
      "explanation": "In Stufflebeam's CIPP Model, Context evaluation is specifically designed to guide planning decisions by assessing needs, problems, and opportunities. The passage incorrectly states 'structuring decisions,' which is actually the purpose of Input evaluation in the CIPP framework. Context evaluation informs planning decisions, Input evaluation informs structuring decisions, Process evaluation informs implementing decisions, and Product evaluation informs recycling decisions."
    },
    {
      "id": "PMET-0162",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Discriminative Stimuli",
      "passage_type": "example",
      "original_passage": "Example: Stimulus Control in Everyday Life A ringing telephone (S D ) signals that answering will be reinforced by conversation. The silent telephone (S Δ ) signals that picking up the receiver will not produce reinforcement. Similarly, an \"open\" sign on a store (S D ) signals that entering will be reinforced, while a \"closed\" sign (S Δ ) signals it will not",
      "modified_passage": "Example: Stimulus Control in Everyday Life A ringing telephone (S Δ ) signals that answering will be reinforced by conversation. The silent telephone (S D ) signals that picking up the receiver will not produce reinforcement. Similarly, an \"open\" sign on a store (S D ) signals that entering will be reinforced, while a \"closed\" sign (S Δ ) signals it will not",
      "error_original": "A ringing telephone (S Δ ) signals that answering will be reinforced by conversation. The silent telephone (S D )",
      "error_correct": "A ringing telephone (S D ) signals that answering will be reinforced by conversation. The silent telephone (S Δ )",
      "options": [
        "The passage incorrectly states that the \"open\" sign is an S D; it should be an S Δ",
        "The passage incorrectly labels the ringing telephone as S Δ and the silent telephone as S D; these labels should be reversed",
        "The passage incorrectly states that answering the phone will be reinforced by conversation; it should say reinforced by the ringing stopping",
        "The passage incorrectly states that a \"closed\" sign signals behavior will not be reinforced; it should say behavior will be punished"
      ],
      "correct_option_index": 1,
      "explanation": "The passage swapped the discriminative stimulus labels for the telephone example. A ringing telephone is an S D (discriminative stimulus) because it signals that answering will be reinforced by conversation. The silent telephone is an S Δ (S-delta) because it signals that picking up the receiver will not produce reinforcement. The modified passage incorrectly reversed these two labels while leaving the store example correct."
    },
    {
      "id": "PMET-0501",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Advantages of IRT",
      "passage_type": "paragraph",
      "original_passage": "Item parameter invariance: Item parameters remain stable across different examinee samples (provided the model fits), enabling items to be calibrated once and used across contexts",
      "modified_passage": "Item parameter invariance: Item parameters remain stable across different examinee samples (provided the model fits), enabling items to be calibrated repeatedly and used across contexts",
      "error_original": "calibrated repeatedly",
      "error_correct": "calibrated once",
      "options": [
        "The passage incorrectly states 'Item parameter invariance'; it should say 'Item characteristic invariance'",
        "The passage incorrectly states 'calibrated repeatedly'; it should say 'calibrated once'",
        "The passage incorrectly states 'remain stable across different examinee samples'; it should say 'remain stable across different test forms'",
        "The passage incorrectly states 'provided the model fits'; it should say 'provided the sample is large enough'"
      ],
      "correct_option_index": 1,
      "explanation": "A key advantage of IRT's item parameter invariance is that items need only be calibrated once and can then be used across different contexts and examinee populations. The passage was modified to say 'calibrated repeatedly,' which contradicts this fundamental efficiency advantage of IRT. The correct phrasing is 'calibrated once,' reflecting that stable item parameters do not require recalibration for each new sample."
    },
    {
      "id": "PMET-0028",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Prepared Fears",
      "passage_type": "paragraph",
      "original_passage": "In contrast, fears of evolutionarily recent threats (cars, electrical outlets, guns) are relatively rare despite causing more deaths in modern society. This preparedness may explain the non-random distribution of phobias in clinical populations (Ohman & Mineka, 2001)",
      "modified_passage": "In contrast, fears of evolutionarily recent threats (cars, electrical outlets, guns) are relatively rare despite causing more deaths in modern society. This preparedness may explain the non-random distribution of phobias in clinical populations (Seligman & Mineka, 2001)",
      "error_original": "Seligman & Mineka, 2001",
      "error_correct": "Ohman & Mineka, 2001",
      "options": [
        "The passage incorrectly states that fears of evolutionarily recent threats are 'relatively rare'; it should say 'relatively common'",
        "The passage incorrectly states that these threats cause 'more deaths'; it should say 'fewer deaths'",
        "The passage incorrectly attributes the citation to 'Seligman & Mineka, 2001'; it should say 'Ohman & Mineka, 2001'",
        "The passage incorrectly states 'non-random distribution'; it should say 'random distribution'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage incorrectly attributes the 2001 citation on prepared fears and phobias to Seligman & Mineka. While Seligman is well-known for his earlier work on preparedness theory (1971), the 2001 publication referenced here was authored by Ohman & Mineka. This is a plausible error because Seligman's name is closely associated with preparedness in learning, making it a tempting but incorrect substitution."
    },
    {
      "id": "PMET-0100",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Base Rate",
      "passage_type": "paragraph",
      "original_passage": "The base rate is the proportion of people in the population who would succeed on the criterion without selection (or who have the condition being assessed). Base rate affects how much a test can improve upon chance prediction",
      "modified_passage": "The base rate is the proportion of people in the population who would succeed on the criterion without selection (or who have the condition being assessed). Base rate affects how much a test can improve upon random prediction",
      "error_original": "random prediction",
      "error_correct": "chance prediction",
      "options": [
        "The passage incorrectly states 'proportion of people'; it should say 'percentage of people'",
        "The passage incorrectly states 'succeed on the criterion'; it should say 'fail on the criterion'",
        "The passage incorrectly states 'without selection'; it should say 'with selection'",
        "The passage incorrectly states 'random prediction'; it should say 'chance prediction'"
      ],
      "correct_option_index": 3,
      "explanation": "The original passage uses the specific term 'chance prediction,' which is the standard terminology in psychometrics when discussing base rates and their relationship to test utility. The modified passage substituted 'random prediction' for 'chance prediction.' While these terms may seem similar in everyday language, 'chance prediction' is the established term used in the context of criterion-related validity and base rate discussions. The other options describe changes that were not actually made to the passage."
    },
    {
      "id": "PMET-0048",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Phi Coefficient",
      "passage_type": "paragraph",
      "original_passage": "The phi coefficient (φ) is used when both variables are true dichotomies. It is commonly used in 2×2 contingency tables and is related to chi-square statistics",
      "modified_passage": "The phi coefficient (φ) is used when both variables are artificial dichotomies. It is commonly used in 2×2 contingency tables and is related to chi-square statistics.",
      "error_original": "artificial dichotomies",
      "error_correct": "true dichotomies",
      "options": [
        "The passage incorrectly states that the phi coefficient is used in 2×2 contingency tables; it should say 3×3 contingency tables.",
        "The passage incorrectly states that the phi coefficient is related to chi-square statistics; it should say related to t-test statistics.",
        "The passage incorrectly states that both variables are artificial dichotomies; it should say true dichotomies.",
        "The passage incorrectly states the symbol is φ; it should say ρ."
      ],
      "correct_option_index": 2,
      "explanation": "The phi coefficient (φ) is specifically used when both variables are true (naturally occurring) dichotomies, such as male/female or alive/dead. The passage incorrectly states 'artificial dichotomies,' which would instead call for a different correlation coefficient, such as the tetrachoric correlation. True dichotomies are variables that naturally fall into two categories, whereas artificial dichotomies are created by splitting a continuous variable into two groups."
    },
    {
      "id": "PMET-0257",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Solomon Four-Group Design",
      "passage_type": "paragraph",
      "original_passage": "The Solomon four-group design combines elements of both previous designs, using four groups: two receive pretests (one treatment, one control) and two do not (one treatment, one control). This design allows researchers to detect and control for testing effects and sensitization",
      "modified_passage": "The Solomon four-group design combines elements of both previous designs, using four groups: two receive pretests (one treatment, one control) and two do not (one treatment, one control). This design allows researchers to detect and control for testing effects and habituation",
      "error_original": "habituation",
      "error_correct": "sensitization",
      "options": [
        "The passage incorrectly states 'two receive pretests'; it should say 'three receive pretests'",
        "The passage incorrectly states 'four groups'; it should say 'six groups'",
        "The passage incorrectly states 'habituation'; it should say 'sensitization'",
        "The passage incorrectly states 'testing effects'; it should say 'maturation effects'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage replaced 'sensitization' with 'habituation.' The Solomon four-group design is specifically noted for its ability to detect and control for testing effects and sensitization — the phenomenon where taking a pretest may sensitize participants to the treatment, altering their responses. Habituation refers to a decrease in response to repeated stimulation, which is a different concept entirely. The other options describe changes that were not actually made to the passage: the design does use four groups, two groups do receive pretests, and the design does address testing effects (not maturation effects)."
    },
    {
      "id": "PMET-0299",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Controlling External Validity Threats",
      "passage_type": "clinical_note",
      "original_passage": "Understanding the Distinction Random assignment controls internal validity threats by ensuring groups are equivalent. Random selection (from the population to the sample) controls external validity threats by ensuring the sample represents the population. Many students confuse these concepts on the EPPP. Random assignment creates equivalent groups; random selection creates representative samples",
      "modified_passage": "Understanding the Distinction Random assignment controls external validity threats by ensuring groups are equivalent. Random selection (from the population to the sample) controls external validity threats by ensuring the sample represents the population. Many students confuse these concepts on the EPPP. Random assignment creates equivalent groups; random selection creates representative samples",
      "error_original": "Random assignment controls external validity threats",
      "error_correct": "Random assignment controls internal validity threats",
      "options": [
        "The passage incorrectly states that random selection controls external validity; it should say random selection controls internal validity",
        "The passage incorrectly states that random assignment controls external validity threats; it should say it controls internal validity threats",
        "The passage incorrectly states that random assignment creates equivalent groups; it should say it creates representative samples",
        "The passage incorrectly states that random selection goes from the population to the sample; it should say from the sample to the population"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly states that random assignment controls external validity threats. In reality, random assignment controls internal validity threats by ensuring that groups in an experiment are equivalent prior to the manipulation. Random selection is the technique that controls external validity threats by ensuring the sample is representative of the population. This is a common confusion point tested on the EPPP, and the passage itself later correctly notes that 'random assignment creates equivalent groups,' which is the hallmark of internal validity control."
    },
    {
      "id": "PMET-0361",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Construct Validity",
      "passage_type": "paragraph",
      "original_passage": "Construct validity is the overarching concept that encompasses all validity evidence. It addresses the fundamental question: Does the test measure the theoretical construct it purports to measure? All other forms of validity evidence contribute to construct validity (Cronbach & Meehl, 1955)",
      "modified_passage": "Construct validity is the overarching concept that encompasses all validity evidence. It addresses the fundamental question: Does the test measure the theoretical construct it purports to measure? All other forms of validity evidence contribute to construct validity (Cronbach & Loevinger, 1955)",
      "error_original": "Cronbach & Loevinger, 1955",
      "error_correct": "Cronbach & Meehl, 1955",
      "options": [
        "The passage incorrectly states that construct validity 'encompasses all validity evidence'; it should say 'encompasses all reliability evidence'",
        "The passage incorrectly states the citation as 'Cronbach & Loevinger, 1955'; it should say 'Cronbach & Meehl, 1955'",
        "The passage incorrectly states that construct validity is 'the overarching concept'; it should say 'a subordinate concept'",
        "The passage incorrectly states the year as '1955'; it should say '1966'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage substituted 'Loevinger' for 'Meehl' in the classic citation. The seminal paper on construct validity was authored by Lee Cronbach and Paul Meehl in 1955, not Cronbach and Loevinger. While Jane Loevinger was a prominent psychometrician who also contributed to validity theory, the foundational 1955 paper establishing the concept of construct validity and the nomological network was by Cronbach and Meehl."
    },
    {
      "id": "PMET-0083",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Positive and Negative Predictive Value",
      "passage_type": "clinical_note",
      "original_passage": "Base Rate and Predictive Values Predictive values are strongly influenced by base rate: When base rate is low, PPV is low (most positives are false positives) even with good sensitivity/specificity When base rate is high, NPV is low (most negatives are false negatives) This is why screening tests in low-prevalence populations often require confirmation testing",
      "modified_passage": "Base Rate and Predictive Values Predictive values are strongly influenced by base rate: When base rate is low, PPV is low (most positives are false positives) even with good sensitivity/specificity When base rate is high, NPV is low (most negatives are false negatives) This is why screening tests in high-prevalence populations often require confirmation testing",
      "error_original": "high-prevalence populations",
      "error_correct": "low-prevalence populations",
      "options": [
        "The passage incorrectly states that when base rate is low, PPV is low; it should say NPV is low",
        "The passage incorrectly states that most positives are false positives when base rate is low; it should say most positives are true positives",
        "The passage incorrectly states 'high-prevalence populations'; it should say 'low-prevalence populations'",
        "The passage incorrectly states that when base rate is high, NPV is low; it should say PPV is low"
      ],
      "correct_option_index": 2,
      "explanation": "The passage was modified to say 'high-prevalence populations' instead of 'low-prevalence populations.' The correct statement is that screening tests in low-prevalence populations often require confirmation testing. This is because when base rate (prevalence) is low, the positive predictive value (PPV) is low, meaning most positive test results are false positives. Therefore, confirmation testing is needed to verify positive results in low-prevalence settings. In high-prevalence populations, PPV tends to be adequate, so confirmation is less critical for positive results."
    },
    {
      "id": "PMET-0402",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Z-Scores and Standard Scores",
      "passage_type": "paragraph",
      "original_passage": "A z-score (standard score) expresses a raw score in terms of how many standard deviations it falls above or below the mean. Z-scores allow comparison of scores from different distributions by converting them to a common scale",
      "modified_passage": "A z-score (standard score) expresses a raw score in terms of how many standard deviations it falls above or below the median. Z-scores allow comparison of scores from different distributions by converting them to a common scale.",
      "error_original": "median",
      "error_correct": "mean",
      "options": [
        "The passage incorrectly states 'standard score'; it should say 'scaled score'",
        "The passage incorrectly states 'median'; it should say 'mean'",
        "The passage incorrectly states 'standard deviations'; it should say 'standard errors'",
        "The passage incorrectly states 'different distributions'; it should say 'identical distributions'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly states that a z-score expresses how many standard deviations a raw score falls above or below the 'median.' In fact, z-scores are calculated relative to the mean of the distribution, not the median. The z-score formula is z = (X - M) / SD, where M is the mean. While the mean and median can coincide in a perfectly normal distribution, the defining reference point for a z-score is always the mean."
    },
    {
      "id": "PMET-0190",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Three-Term Contingency",
      "passage_type": "paragraph",
      "original_passage": "This framework is fundamental to functional behavior assessment and the development of behavior intervention plans in clinical and educational settings",
      "modified_passage": "The three-term contingency, also known as the ABD model (Antecedent-Behavior-Consequence), is the foundational unit of analysis in operant conditioning. The antecedent is the stimulus or event that precedes the behavior, the behavior is the observable response, and the consequence is the event that follows and either strengthens or weakens the behavior. This framework is fundamental to functional behavior assessment and the development of behavior intervention plans in clinical and educational settings.",
      "error_original": "ABD model",
      "error_correct": "ABC model",
      "options": [
        "The passage incorrectly states that the consequence 'strengthens or weakens' behavior; it should say 'strengthens or maintains' behavior",
        "The passage incorrectly states that the antecedent 'precedes' the behavior; it should say 'follows' the behavior",
        "The passage incorrectly states it is the 'ABD model'; it should say 'ABC model'",
        "The passage incorrectly states the three-term contingency is the foundational unit of 'operant conditioning'; it should say 'classical conditioning'"
      ],
      "correct_option_index": 2,
      "explanation": "The three-term contingency is commonly referred to as the ABC model, where A stands for Antecedent, B stands for Behavior, and C stands for Consequence. The passage incorrectly labels it the 'ABD model,' which is not a recognized term in behavior analysis. The correct abbreviation is ABC, directly corresponding to the three components described in the passage itself (Antecedent-Behavior-Consequence)."
    },
    {
      "id": "PMET-0163",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Contingency Management",
      "passage_type": "example",
      "original_passage": "Example: Voucher-Based Contingency Management In voucher-based contingency management for cocaine dependence, patients receive vouchers exchangeable for retail goods contingent on cocaine-negative urine specimens. Voucher values typically start low and increase with consecutive negative specimens, with a reset contingency for positive specimens. Meta-analyses demonstrate significant effects on drug abstinence outcomes",
      "modified_passage": "Example: Voucher-Based Contingency Management In voucher-based contingency management for cocaine dependence, patients receive vouchers exchangeable for retail goods contingent on cocaine-negative urine specimens. Voucher values typically start high and decrease with consecutive negative specimens, with a reset contingency for positive specimens. Meta-analyses demonstrate significant effects on drug abstinence outcomes",
      "error_original": "start high and decrease with consecutive negative specimens",
      "error_correct": "start low and increase with consecutive negative specimens",
      "options": [
        "The passage incorrectly states vouchers are exchangeable for retail goods; it should say they are exchangeable for cash",
        "The passage incorrectly states there is a reset contingency for positive specimens; it should say there is a reset contingency for negative specimens",
        "The passage incorrectly states voucher values start high and decrease with consecutive negative specimens; it should say they start low and increase with consecutive negative specimens",
        "The passage incorrectly states the intervention targets cocaine dependence; it should say it targets alcohol dependence"
      ],
      "correct_option_index": 2,
      "explanation": "The passage reverses the direction of the voucher escalation schedule. In voucher-based contingency management, voucher values typically start low and increase (escalate) with each consecutive cocaine-negative urine specimen. This escalating reinforcement schedule is a core feature of the approach, designed to incentivize sustained abstinence. The modified passage incorrectly states that values start high and decrease, which would actually reduce motivation for continued abstinence over time and contradicts the established protocol."
    },
    {
      "id": "PMET-0061",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Multiple Regression",
      "passage_type": "paragraph",
      "original_passage": "Multiple regression extends simple regression by using two or more predictor variables to predict a single criterion variable. The multiple regression equation is: Y' = a + b₁X₁ + b₂X₂ + ... + bₖXₖ",
      "modified_passage": "Multiple regression extends simple regression by using two or more predictor variables to predict a single criterion variable. The multiple regression equation is: Y' = a + b₁X₁ × b₂X₂ × ... × bₖXₖ",
      "error_original": "Y' = a + b₁X₁ × b₂X₂ × ... × bₖXₖ",
      "error_correct": "Y' = a + b₁X₁ + b₂X₂ + ... + bₖXₖ",
      "options": [
        "The passage incorrectly states that multiple regression uses two or more predictor variables; it should say three or more predictor variables",
        "The passage incorrectly states that the equation predicts a single criterion variable; it should say multiple criterion variables",
        "The passage incorrectly uses multiplication signs (×) between the weighted predictor terms; it should use addition signs (+)",
        "The passage incorrectly refers to 'a' as the constant; it should refer to it as the slope coefficient"
      ],
      "correct_option_index": 2,
      "explanation": "The multiple regression equation combines the weighted predictor terms additively, not multiplicatively. The correct equation is Y' = a + b₁X₁ + b₂X₂ + ... + bₖXₖ, where each predictor variable (X) is multiplied by its own regression coefficient (b), and these products are then summed together along with the intercept constant (a). The modified passage incorrectly changed the addition signs between the b₁X₁, b₂X₂, etc. terms to multiplication signs, which would represent an entirely different (and incorrect) mathematical model."
    },
    {
      "id": "PMET-0426",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "The Regression Equation",
      "passage_type": "paragraph",
      "original_passage": "b: The slope (regression coefficient) indicating the change in Y for each one-unit change in X",
      "modified_passage": "b: The slope (regression coefficient) indicating the change in X for each one-unit change in Y",
      "error_original": "the change in X for each one-unit change in Y",
      "error_correct": "the change in Y for each one-unit change in X",
      "options": [
        "The passage incorrectly refers to 'b' as the slope; it should be called the intercept",
        "The passage incorrectly states the change in X for each one-unit change in Y; it should say the change in Y for each one-unit change in X",
        "The passage incorrectly uses the term 'regression coefficient'; it should say 'correlation coefficient'",
        "The passage incorrectly implies a one-unit change; it should say a one-standard-deviation change"
      ],
      "correct_option_index": 1,
      "explanation": "The passage reverses the relationship between X and Y. In a standard regression equation, 'b' (the slope or regression coefficient) indicates how much the predicted value of Y changes for each one-unit increase in X, not the other way around. The correct statement is: 'b: The slope (regression coefficient) indicating the change in Y for each one-unit change in X.'"
    },
    {
      "id": "PMET-PC-0001",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Restriction of Range",
      "passage_type": "example",
      "source_passage_id": "PMET-0087",
      "sentences": [
        "A company uses a cognitive ability test to select employees, hiring only those who score above the 70th percentile.",
        "When validating the test against job performance, only selected employees (those with high scores) are available.",
        "The restricted range of test scores inflates the observed validity coefficient.",
        "Statistical corrections can estimate what the validity would be in the full applicant population."
      ],
      "target_sentence_index": 2,
      "original_sentence": "The restricted range of test scores reduces the observed validity coefficient.",
      "error_original": "inflates the observed validity coefficient",
      "error_correct": "reduces the observed validity coefficient",
      "explanation": "Restriction of range attenuates (reduces) the observed correlation between predictor and criterion scores, resulting in an underestimate of the true validity coefficient. The error reverses this well-established psychometric principle by stating that restriction of range 'inflates' the validity coefficient."
    },
    {
      "id": "PMET-PC-0002",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Acquisition",
      "passage_type": "example",
      "source_passage_id": "PMET-0004",
      "sentences": [
        "Clinical Example: Fear Acquisition A child is bitten by a dog (US) while at a park.",
        "The pain from the bite naturally produces fear (UR).",
        "The visual appearance of dogs (CS) was present just before and during the bite.",
        "After this single pairing (one-trial learning), the child now experiences fear (CR) when seeing any dog.",
        "The temporal relationship - seeing the dog immediately before and during the painful bite - exemplifies trace conditioning producing rapid fear acquisition."
      ],
      "target_sentence_index": 4,
      "original_sentence": "The temporal relationship - seeing the dog immediately before and during the painful bite - exemplifies delay conditioning producing rapid fear acquisition.",
      "error_original": "trace conditioning",
      "error_correct": "delay conditioning",
      "explanation": "The passage describes the CS (seeing the dog) being present 'immediately before and during' the US (the bite), meaning the CS overlaps with the US. This is the defining feature of delay conditioning, where the CS onset precedes the US and the CS remains present until the US occurs. Trace conditioning, by contrast, involves a temporal gap between the offset of the CS and the onset of the US. The error substitutes 'trace conditioning' for the correct term 'delay conditioning.'"
    },
    {
      "id": "PMET-PC-0003",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Introduction to Inferential Statistics",
      "passage_type": "definition",
      "source_passage_id": "PMET-0111",
      "sentences": [
        "Population: The entire group to which researchers wish to generalize findings.",
        "Sample: A subset of the population that is actually measured.",
        "Parameter: A characteristic of a population (e.g., population mean μ).",
        "Statistic: A characteristic of a population used to estimate parameters (e.g., sample mean M or x̄).",
        "Sampling Distribution: The theoretical distribution of a statistic across all possible samples of a given size."
      ],
      "target_sentence_index": 3,
      "original_sentence": "Statistic: A characteristic of a sample used to estimate parameters (e.g., sample mean M or x̄).",
      "error_original": "A characteristic of a population used to estimate parameters",
      "error_correct": "A characteristic of a sample used to estimate parameters",
      "explanation": "A statistic is a characteristic of a *sample*, not a population. A characteristic of a population is called a parameter. The error swaps 'sample' for 'population' in the definition of a statistic, making it incorrectly identical to the definition of a parameter."
    },
    {
      "id": "PMET-PC-0004",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Standard Error of Estimate",
      "passage_type": "definition",
      "source_passage_id": "PMET-0035",
      "sentences": [
        "Predictor Variable (X): The variable used to make predictions; also called the independent variable in regression.",
        "Criterion Variable (Y): The variable being predicted; also called the dependent variable in regression.",
        "Residual: The difference between the actual Y score and the predicted Y score (Y - Y').",
        "SEE: The standard variance of the residuals; indicates prediction accuracy."
      ],
      "target_sentence_index": 3,
      "original_sentence": "SEE: The standard deviation of the residuals; indicates prediction accuracy.",
      "error_original": "standard variance",
      "error_correct": "standard deviation",
      "explanation": "The Standard Error of Estimate (SEE) is defined as the standard deviation of the residuals, not the standard variance. Variance is the square of the standard deviation, so substituting 'variance' for 'deviation' is a subtle but clearly incorrect term substitution."
    },
    {
      "id": "PMET-PC-0005",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Coefficient of Determination (r²)",
      "passage_type": "example",
      "source_passage_id": "PMET-0039",
      "sentences": [
        "Example: Interpreting r² If the correlation between SAT scores and college GPA is r = .50, then r² = .25.",
        "This means that 25% of the variance in college GPA can be accounted for by SAT scores.",
        "Conversely, 75% of the variance in GPA is due to other factors not captured by SAT scores.",
        "This interpretation helps put correlation coefficients in theoretical perspective."
      ],
      "target_sentence_index": 3,
      "original_sentence": "This interpretation helps put correlation coefficients in practical perspective.",
      "error_original": "theoretical perspective",
      "error_correct": "practical perspective",
      "explanation": "The original passage states that interpreting r² helps put correlation coefficients in 'practical' perspective, emphasizing real-world applicability. The error substitutes 'theoretical' for 'practical,' which reverses the intended meaning. The coefficient of determination (r²) is specifically valued because it gives a concrete, practical understanding of how much variance is shared between two variables, making abstract correlation values more meaningful in applied contexts."
    },
    {
      "id": "PMET-PC-0006",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Incremental Validity",
      "passage_type": "example",
      "source_passage_id": "PMET-0041",
      "sentences": [
        "A researcher examines whether a new anxiety questionnaire adds predictive value for treatment outcomes beyond existing measures.",
        "A regression using only the BDI-II yields R² = .30.",
        "Adding the new anxiety measure increases R² to .38.",
        "The increment of .08 (8% additional variance explained) represents the convergent validity of the new measure.",
        "Statistical tests can determine whether this increment is significant."
      ],
      "target_sentence_index": 3,
      "original_sentence": "The increment of .08 (8% additional variance explained) represents the incremental validity of the new measure.",
      "error_original": "convergent validity",
      "error_correct": "incremental validity",
      "explanation": "The passage describes how a new measure adds predictive value beyond an existing measure, which is the definition of incremental validity — the degree to which a new predictor explains additional variance beyond what is already accounted for. Convergent validity refers to the degree to which two measures of the same construct correlate with each other, which is a different psychometric concept entirely."
    },
    {
      "id": "PMET-PC-0007",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Kurtosis",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0405",
      "sentences": [
        "Kurtosis describes the \"peakedness\" of a distribution and the heaviness of its tails.",
        "Leptokurtic distributions are more peaked with heavier tails than normal (positive kurtosis).",
        "Platykurtic distributions are flatter with lighter tails (negative kurtosis).",
        "Mesokurtic distributions have the same kurtosis as the normal distribution."
      ],
      "target_sentence_index": 2,
      "original_sentence": "Platykurtic distributions are flatter with lighter tails (negative kurtosis).",
      "error_original": "lighter tails",
      "error_correct": "lighter tails",
      "explanation": "This passage is too short to introduce a subtle error without it being immediately revealed by adjacent sentences. Let me re-evaluate."
    },
    {
      "id": "PMET-PC-0008",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Major Qualitative Traditions",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0286",
      "sentences": [
        "The case study approach involves in-depth examination of a single case or a small number of cases.",
        "A \"case\" can be an individual, a program, an organization, or an event.",
        "Case studies use multiple data sources (interviews, documents, observations) and are valuable for exploring complex phenomena in controlled laboratory contexts.",
        "They provide rich detail but have limited generalizability."
      ],
      "target_sentence_index": 2,
      "original_sentence": "Case studies use multiple data sources (interviews, documents, observations) and are valuable for exploring complex phenomena in real-world contexts.",
      "error_original": "controlled laboratory contexts",
      "error_correct": "real-world contexts",
      "explanation": "Case studies are specifically valued for exploring complex phenomena in real-world contexts (naturalistic settings), not controlled laboratory contexts. A hallmark of case study research is its ecological validity and its focus on understanding phenomena as they occur naturally, which is why they trade generalizability for rich, contextually embedded detail."
    },
    {
      "id": "PMET-PC-0009",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Types of Program Evaluation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0290",
      "sentences": [
        "Process evaluation (also called implementation evaluation) monitors whether the program is being delivered as intended.",
        "It assesses implementation fidelity —the degree to which the program follows its designed protocol.",
        "Process evaluation answers: \"Is the program being carried out as planned?\"",
        "\"Are participants being reached?\"",
        "\"Are services being delivered correctly?\""
      ],
      "target_sentence_index": 0,
      "original_sentence": "Process evaluation (also called implementation evaluation) monitors whether the program is being delivered as intended.",
      "error_original": "also called implementation evaluation",
      "error_correct": "also called formative evaluation",
      "explanation": "Process evaluation is also commonly referred to as formative evaluation (or implementation evaluation). However, in this context the original passage correctly uses 'implementation evaluation' as the alternate name. The error here is actually a trick — let me re-examine. The original passage does say 'implementation evaluation,' which is correct. A subtle error would be to swap this alternate name."
    },
    {
      "id": "PMET-PC-0010",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Major Qualitative Traditions",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0284",
      "sentences": [
        "Phenomenology seeks to describe the essence of lived experience as perceived by participants.",
        "Researchers conduct in-depth interviews to understand how individuals experience a particular phenomenon (e.g., living with chronic pain, experiencing grief).",
        "The analyst identifies themes and structures that capture the core meaning of the experience.",
        "The researcher engages in bracketing (also called epoché)—setting aside personal preconceptions to focus on participants' objective experiences."
      ],
      "target_sentence_index": 3,
      "original_sentence": "The researcher engages in bracketing (also called epoché)—setting aside personal preconceptions to focus on participants' subjective experiences.",
      "error_original": "objective experiences",
      "error_correct": "subjective experiences",
      "explanation": "Phenomenology is fundamentally concerned with subjective experience—how individuals personally perceive and make meaning of phenomena. The passage incorrectly states 'objective experiences,' but bracketing (epoché) is performed precisely so the researcher can set aside their own assumptions and attend faithfully to participants' subjective experiences. Replacing 'subjective' with 'objective' contradicts a core tenet of phenomenological inquiry."
    },
    {
      "id": "PMET-PC-0011",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Incremental Validity",
      "passage_type": "example",
      "source_passage_id": "PMET-0090",
      "sentences": [
        "A clinician wants to know if a new personality measure adds predictive value for therapy outcome beyond established predictors (severity, chronicity, social support).",
        "Existing predictors account for 25% of outcome variance.",
        "Adding the personality measure increases this to 32%.",
        "The 7% increase (ΔR² = .07) represents the convergent validity of the new measure."
      ],
      "target_sentence_index": 3,
      "original_sentence": "The 7% increase (ΔR² = .07) represents the incremental validity of the new measure.",
      "error_original": "convergent validity",
      "error_correct": "incremental validity",
      "explanation": "The passage describes the additional variance accounted for by a new measure beyond existing predictors (ΔR²), which is the definition of incremental validity, not convergent validity. Convergent validity refers to the degree to which two measures that theoretically should be related are in fact related. The concept being illustrated here — whether a new predictor adds predictive value beyond what is already explained — is specifically incremental validity."
    },
    {
      "id": "PMET-PC-0012",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Controlling External Validity Threats",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0299",
      "sentences": [
        "Understanding the Distinction Random assignment controls internal validity threats by ensuring groups are equivalent.",
        "Random selection (from the population to the sample) controls external validity threats by ensuring the sample represents the population.",
        "Many students confuse these concepts on the EPPP.",
        "Random assignment creates representative samples; random selection creates equivalent groups."
      ],
      "target_sentence_index": 3,
      "original_sentence": "Random assignment creates equivalent groups; random selection creates representative samples.",
      "error_original": "Random assignment creates representative samples; random selection creates equivalent groups",
      "error_correct": "Random assignment creates equivalent groups; random selection creates representative samples",
      "explanation": "The final sentence reverses the functions of random assignment and random selection. Random assignment creates equivalent groups (controlling internal validity), while random selection creates representative samples (controlling external validity). The error swaps these two outcomes, which is a concept reversal."
    },
    {
      "id": "PMET-PC-0013",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Type II Error (False Negative)",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0115",
      "sentences": [
        "Clinical Implications In clinical contexts, the consequences of Type I and Type II errors differ.",
        "A Type I error in treatment research means concluding a treatment works when it doesn't (potentially exposing patients to ineffective treatments).",
        "A Type II error means missing a truly effective treatment (denying patients beneficial interventions).",
        "Researchers must consider these trade-offs when designing studies."
      ],
      "target_sentence_index": 1,
      "original_sentence": "A Type I error in treatment research means concluding a treatment works when it doesn't (potentially exposing patients to ineffective treatments).",
      "error_original": "A Type I error in treatment research means concluding a treatment works when it doesn't",
      "error_correct": "A Type I error in treatment research means concluding a treatment works when it doesn't",
      "explanation": "This passage has fewer than 4 meaningfully distinct factual claims to modify without the error being revealed by adjacent sentences. However, re-evaluating: the passage does contain 4 sentences."
    },
    {
      "id": "PMET-PC-0014",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "One-Sample t-Test",
      "passage_type": "example",
      "source_passage_id": "PMET-0119",
      "sentences": [
        "Example: Choosing a t-Test Scenario: A researcher measures anxiety before and after a relaxation intervention in the same 30 participants.",
        "Appropriate test: Paired samples t-test, because the same participants are measured twice (within-subjects design).",
        "Scenario: A researcher compares anxiety scores between 30 participants who received treatment and 30 different participants who received placebo.",
        "Appropriate test: Independent samples t-test, because different participants are in each group (within-subjects design)."
      ],
      "target_sentence_index": 3,
      "original_sentence": "Appropriate test: Independent samples t-test, because different participants are in each group (between-subjects design).",
      "error_original": "within-subjects design",
      "error_correct": "between-subjects design",
      "explanation": "An independent samples t-test is used when different participants are in each group, which constitutes a between-subjects design, not a within-subjects design. A within-subjects design involves the same participants being measured under multiple conditions, which would call for a paired samples t-test instead."
    },
    {
      "id": "PMET-PC-0015",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Types of Program Evaluation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0291",
      "sentences": [
        "Outcome evaluation measures whether program goals and objectives were achieved.",
        "It examines the indirect, intended effects of the program on participants.",
        "For example, did a smoking cessation program lead to reduced smoking rates among participants?",
        "Outcome evaluations typically use pre-post designs or comparison groups."
      ],
      "target_sentence_index": 1,
      "original_sentence": "It examines the direct, intended effects of the program on participants.",
      "error_original": "indirect, intended effects",
      "error_correct": "direct, intended effects",
      "explanation": "Outcome evaluation specifically examines the direct, intended effects of a program on its participants. The word 'indirect' was substituted for 'direct,' which is a concept reversal error. Indirect effects would be more associated with impact evaluation, which looks at broader, longer-term, or unintended consequences of a program, rather than outcome evaluation which focuses on the direct results tied to program goals and objectives."
    },
    {
      "id": "PMET-PC-0016",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Avoidance Learning",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0158",
      "sentences": [
        "Two-factor theory provides a behavioral account of anxiety disorder maintenance.",
        "Avoidance prevents exposure to feared situations, thereby preventing extinction of conditioned fear.",
        "This explains why exposure therapy, which prevents avoidance and allows sensitization to occur, is effective for anxiety disorders.",
        "The theory also explains why reassurance-seeking and safety behaviors maintain anxiety."
      ],
      "target_sentence_index": 2,
      "original_sentence": "This explains why exposure therapy, which prevents avoidance and allows extinction to occur, is effective for anxiety disorders.",
      "error_original": "allows sensitization to occur",
      "error_correct": "allows extinction to occur",
      "explanation": "The error substitutes 'sensitization' for 'extinction.' Exposure therapy works by preventing avoidance responses so that the conditioned fear response can undergo extinction (i.e., the conditioned stimulus is presented without the unconditioned stimulus, weakening the association). Sensitization is the opposite process—it refers to an increased response to a stimulus with repeated exposure, which is not the therapeutic mechanism of exposure therapy."
    },
    {
      "id": "PMET-PC-0017",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Introduction to Classical Conditioning",
      "passage_type": "definition",
      "source_passage_id": "PMET-0001",
      "sentences": [
        "Unconditioned Stimulus (US): A stimulus that naturally and automatically triggers a response without prior learning (e.g., food causing salivation).",
        "Unconditioned Response (UR): The unlearned, naturally occurring response to the unconditioned stimulus (e.g., salivation to food).",
        "Conditioned Stimulus (CS): A previously neutral stimulus that, after association with the US, comes to trigger a conditioned response (e.g., bell after pairing with food).",
        "Conditioned Response (CR): The learned response to the unconditioned stimulus (e.g., salivation to the bell)."
      ],
      "target_sentence_index": 3,
      "original_sentence": "Conditioned Response (CR): The learned response to the conditioned stimulus (e.g., salivation to the bell).",
      "error_original": "the unconditioned stimulus",
      "error_correct": "the conditioned stimulus",
      "explanation": "The Conditioned Response (CR) is defined as the learned response to the conditioned stimulus (CS), not the unconditioned stimulus (US). The passage incorrectly substitutes 'unconditioned stimulus' for 'conditioned stimulus.' The response to the unconditioned stimulus is the Unconditioned Response (UR), not the CR."
    },
    {
      "id": "PMET-PC-0018",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Introduction to Research Validity",
      "passage_type": "definition",
      "source_passage_id": "PMET-0295",
      "sentences": [
        "Four Types of Research Validity.",
        "Internal Validity: Did the treatment (IV) actually cause the observed effect (DV)?",
        "External Validity: Can the findings be generalized to other people, settings, and times?",
        "Construct Validity: Are we measuring the constructs we think we're measuring?",
        "Statistical Conclusion Validity: Are our statistical assumptions justified?"
      ],
      "target_sentence_index": 4,
      "original_sentence": "Statistical Conclusion Validity: Are our statistical conclusions accurate?",
      "error_original": "statistical assumptions justified",
      "error_correct": "statistical conclusions accurate",
      "explanation": "Statistical Conclusion Validity concerns whether the statistical conclusions drawn from data are accurate (e.g., correct inferences about covariation between variables). The modified sentence incorrectly substitutes 'assumptions justified' for 'conclusions accurate,' which changes the core definition of this validity type. Justified assumptions relate more to meeting test prerequisites, not to the accuracy of statistical inferences."
    },
    {
      "id": "PMET-PC-0019",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Multiple Baseline Design",
      "passage_type": "example",
      "source_passage_id": "PMET-0251",
      "sentences": [
        "A therapist targets three problem behaviors in a child with autism: hand-flapping, echolalia, and tantrums.",
        "After establishing baselines for all three, she implements a token economy first for hand-flapping.",
        "Once reduction is observed, she extends the intervention to echolalia, then to tantrums.",
        "If each behavior decreases only when the intervention is applied to it, this demonstrates internal validity without requiring withdrawal."
      ],
      "target_sentence_index": 3,
      "original_sentence": "If each behavior decreases only when the intervention is applied to it, this demonstrates experimental control without requiring withdrawal.",
      "error_original": "internal validity",
      "error_correct": "experimental control",
      "explanation": "The passage originally states that the multiple baseline design demonstrates 'experimental control' without requiring withdrawal. The error substitutes 'internal validity' for 'experimental control.' While related concepts, the specific strength of the multiple baseline design is that it demonstrates experimental control (i.e., a functional relationship between the independent and dependent variables) without needing to withdraw treatment, which is the hallmark advantage of this design over reversal/withdrawal designs."
    },
    {
      "id": "PMET-PC-0020",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Components of Clinical Utility",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0085",
      "sentences": [
        "Evaluating Clinical Utility Questions to ask about a test's clinical utility:",
        "Does use of this test lead to different clinical decisions than would be made without it?",
        "Do those different decisions result in better patient outcomes?",
        "Is the test feasible to administer in typical research settings?",
        "Is the test acceptable to patients and clinicians?"
      ],
      "target_sentence_index": 3,
      "original_sentence": "Is the test feasible to administer in typical clinical settings?",
      "error_original": "typical research settings",
      "error_correct": "typical clinical settings",
      "explanation": "The original passage specifies 'clinical settings' as the context for evaluating feasibility of test administration. Clinical utility specifically concerns the practical value of a test in real-world clinical practice, not research settings. Substituting 'research' for 'clinical' subtly changes the meaning, as feasibility in research environments (which may have more resources and fewer time constraints) is a different consideration than feasibility in everyday clinical practice."
    },
    {
      "id": "PMET-PC-0021",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Meta-Analysis",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0072",
      "sentences": [
        "A forest plot is the standard graphical display of meta-analytic results.",
        "Each study is represented by a horizontal line (confidence interval) with a square at the point estimate (the square's size reflects the study's weight).",
        "The overall combined estimate is displayed as a diamond at the top.",
        "Forest plots allow readers to visually assess the consistency and direction of effects across studies."
      ],
      "target_sentence_index": 2,
      "original_sentence": "The overall combined estimate is displayed as a diamond at the bottom.",
      "error_original": "at the top",
      "error_correct": "at the bottom",
      "explanation": "In a standard forest plot, the overall combined (summary) estimate is displayed as a diamond at the bottom of the plot, not at the top. Individual study estimates are listed above it, and the diamond representing the pooled effect size appears at the bottom after all individual studies."
    },
    {
      "id": "PMET-PC-0022",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Restriction of Range",
      "passage_type": "example",
      "source_passage_id": "PMET-0040",
      "sentences": [
        "A company uses a cognitive ability test to select employees, hiring only those who score above the 75th percentile.",
        "When later examining the correlation between test scores and job performance among hired employees, they find a modest correlation of r = .20.",
        "However, if they could have studied all applicants (unrestricted range), the correlation might be r = .50.",
        "The selection process restricted the range of test scores, inflating the observed validity coefficient."
      ],
      "target_sentence_index": 3,
      "original_sentence": "The selection process restricted the range of test scores, attenuating the observed validity coefficient.",
      "error_original": "inflating the observed validity coefficient",
      "error_correct": "attenuating the observed validity coefficient",
      "explanation": "Restriction of range attenuates (reduces) the observed correlation, not inflates it. In the example, the unrestricted correlation would be r = .50, but due to range restriction the observed correlation dropped to r = .20. This demonstrates that restriction of range leads to an underestimate of the true relationship between variables."
    },
    {
      "id": "PMET-PC-0023",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Major Qualitative Traditions",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0283",
      "sentences": [
        "Grounded theory aims to generate theory from data rather than testing pre-existing hypotheses.",
        "Researchers collect data (typically through interviews and observations), code it systematically, and develop theoretical concepts that are \"grounded\" in the data.",
        "The process involves open coding (identifying initial categories), axial coding (relating categories), and selective coding (integrating into a coherent theory).",
        "Data collection and analysis occur simultaneously, and theoretical sampling guides recruitment—new participants are selected to refine emerging theory (Glaser & Corbin, 1967)."
      ],
      "target_sentence_index": 3,
      "original_sentence": "Data collection and analysis occur simultaneously, and theoretical sampling guides recruitment—new participants are selected to refine emerging theory (Glaser & Strauss, 1967).",
      "error_original": "Glaser & Corbin, 1967",
      "error_correct": "Glaser & Strauss, 1967",
      "explanation": "The seminal work on grounded theory was authored by Barney Glaser and Anselm Strauss in 1967 ('The Discovery of Grounded Theory'). The error substitutes 'Corbin' for 'Strauss.' Juliet Corbin was a later collaborator of Strauss, but the original 1967 foundational text was by Glaser and Strauss."
    },
    {
      "id": "PMET-PC-0024",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Internal vs. External Validity Trade-offs",
      "passage_type": "definition",
      "source_passage_id": "PMET-0297",
      "sentences": [
        "The Trade-off High Control (Lab): Strong internal validity, weaker external validity.",
        "We know the treatment works in controlled conditions but may not generalize to the real world.",
        "Naturalistic Settings (Field): Stronger external validity, weaker internal validity.",
        "Results generalize better, but we're less certain the treatment caused the effects."
      ],
      "target_sentence_index": 0,
      "original_sentence": "The Trade-off High Control (Lab): Strong internal validity, weaker external validity.",
      "error_original": "weaker external validity",
      "error_correct": "weaker external validity",
      "explanation": "This passage actually contains no error in sentence 0 — let me re-do this properly."
    },
    {
      "id": "PMET-PC-0025",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Internal Validity",
      "passage_type": "definition",
      "source_passage_id": "PMET-0296",
      "sentences": [
        "Diffusion of Treatment occurs when control group participants learn about or receive aspects of the treatment.",
        "Compensatory Equalization occurs when administrators provide extra services to the control group to compensate for not receiving treatment.",
        "Compensatory Rivalry occurs when control group participants work harder to compete with the treatment group, also known as the Hawthorne effect.",
        "Resentful Demoralization occurs when control group participants become demoralized and perform worse than they otherwise would."
      ],
      "target_sentence_index": 2,
      "original_sentence": "Compensatory Rivalry occurs when control group participants work harder to compete with the treatment group, also known as the John Henry effect.",
      "error_original": "Hawthorne effect",
      "error_correct": "John Henry effect",
      "explanation": "Compensatory Rivalry is also known as the John Henry effect, named after the folk legend of a steel driver who worked harder to compete against a machine. The Hawthorne effect refers to a different phenomenon where participants modify their behavior simply because they know they are being observed, not because they are competing with another group."
    },
    {
      "id": "PMET-PC-0026",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Internal Validity",
      "passage_type": "example",
      "source_passage_id": "PMET-0300",
      "sentences": [
        "A researcher studies the effect of a new anxiety treatment over 8 weeks.",
        "During the study, a major national crisis occurs, elevating anxiety in the population.",
        "Any observed changes in anxiety might be due to the treatment, the crisis, or both.",
        "The external event (crisis) is a maturation threat."
      ],
      "target_sentence_index": 3,
      "original_sentence": "The external event (crisis) is a history threat.",
      "error_original": "maturation threat",
      "error_correct": "history threat",
      "explanation": "The passage describes an external event (a national crisis) occurring during the study that could affect the dependent variable. This is a 'history threat' to internal validity — an outside event coinciding with the study that provides an alternative explanation for results. A 'maturation threat' refers to internal changes within participants over time (e.g., growing older, becoming fatigued), not external events."
    },
    {
      "id": "PMET-PC-0027",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Applications of IRT",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0208",
      "sentences": [
        "The EPPP itself uses IRT-based scoring.",
        "Items are calibrated using IRT models, and examinee scores are estimated on a theta scale.",
        "This means the EPPP can draw from a large item bank while maintaining score comparability across administrations.",
        "The pass/fail decision is based on a theta cutoff rather than a raw percentage correct, which accounts for differences in item discrimination across test forms."
      ],
      "target_sentence_index": 3,
      "original_sentence": "The pass/fail decision is based on a theta cutoff rather than a raw percentage correct, which accounts for differences in item difficulty across test forms.",
      "error_original": "item discrimination",
      "error_correct": "item difficulty",
      "explanation": "The original passage states that the theta cutoff accounts for differences in 'item difficulty' across test forms. The error substitutes 'item discrimination' for 'item difficulty.' While item discrimination is an important IRT parameter, the primary reason for using a theta cutoff rather than a raw percentage correct across different test forms is to account for variations in item difficulty, ensuring that examinees taking forms with harder or easier items are evaluated fairly."
    },
    {
      "id": "PMET-PC-0028",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Introduction to Correlation",
      "passage_type": "definition",
      "source_passage_id": "PMET-0033",
      "sentences": [
        "A correlation coefficient is a numerical index ranging from -1.00 to +1.00 that indicates the strength and direction of the linear relationship between two variables.",
        "A positive correlation means that as one variable increases, the other tends to increase (e.g., height and weight).",
        "A negative correlation means that as one variable increases, the other tends to increase as well (e.g., stress and immune function).",
        "A zero correlation indicates that no systematic linear relationship exists between the variables."
      ],
      "target_sentence_index": 2,
      "original_sentence": "A negative correlation means that as one variable increases, the other tends to decrease (e.g., stress and immune function).",
      "error_original": "the other tends to increase as well",
      "error_correct": "the other tends to decrease",
      "explanation": "A negative correlation indicates an inverse relationship between two variables: as one variable increases, the other tends to decrease. The error reverses this direction by stating the other variable 'tends to increase as well,' which would describe a positive correlation, not a negative one."
    },
    {
      "id": "PMET-PC-0029",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Introduction to Operant Conditioning",
      "passage_type": "definition",
      "source_passage_id": "PMET-0154",
      "sentences": [
        "Operant Behavior: Voluntary behavior that operates on the environment to produce consequences.",
        "Reinforcement: Any consequence that increases the probability of a behavior occurring again.",
        "Punishment: Any consequence that increases the probability of a behavior occurring again.",
        "Contingency: The relationship between a behavior and its consequence."
      ],
      "target_sentence_index": 2,
      "original_sentence": "Punishment: Any consequence that decreases the probability of a behavior occurring again.",
      "error_original": "increases the probability",
      "error_correct": "decreases the probability",
      "explanation": "Punishment is defined as any consequence that decreases (not increases) the probability of a behavior occurring again. The error swaps 'decreases' with 'increases,' making the definition of punishment identical to that of reinforcement, which is incorrect."
    },
    {
      "id": "PMET-PC-0030",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Four Basic Schedules",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0179",
      "sentences": [
        "Reinforcement is delivered after a fixed number of responses.",
        "For example, FR-5 means reinforcement occurs after every 5 responses.",
        "FR schedules produce high, steady response rates with a brief pause before reinforcement (pre-reinforcement pause).",
        "Piecework pay is a real-world example."
      ],
      "target_sentence_index": 2,
      "original_sentence": "FR schedules produce high, steady response rates with a brief pause after reinforcement (post-reinforcement pause).",
      "error_original": "a brief pause before reinforcement (pre-reinforcement pause)",
      "error_correct": "a brief pause after reinforcement (post-reinforcement pause)",
      "explanation": "In fixed-ratio (FR) schedules, the characteristic pause occurs immediately after reinforcement is delivered, not before it. This is known as the post-reinforcement pause (PRP). The organism pauses briefly after receiving the reinforcer before resuming responding at a high rate to meet the next ratio requirement."
    },
    {
      "id": "PMET-PC-0031",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Multivariate Analysis of Variance (MANOVA)",
      "passage_type": "example",
      "source_passage_id": "PMET-0121",
      "sentences": [
        "A researcher compares two therapy groups on three outcome measures simultaneously: depression, anxiety, and quality of life.",
        "Running three separate ANOVAs would inflate the Type II error rate (approximately 1 - .95^3 = .14 rather than .05).",
        "MANOVA tests all three DVs in one analysis, maintaining the overall alpha at .05.",
        "The researcher reports Wilks' Lambda and, if significant, conducts follow-up analyses to determine which DVs contribute to the group difference."
      ],
      "target_sentence_index": 1,
      "original_sentence": "Running three separate ANOVAs would inflate the Type I error rate (approximately 1 - .95^3 = .14 rather than .05).",
      "error_original": "Type II error rate",
      "error_correct": "Type I error rate",
      "explanation": "The error substitutes 'Type II' for 'Type I.' Running multiple separate ANOVAs inflates the Type I error rate (the probability of falsely rejecting a true null hypothesis), not the Type II error rate. This is the primary rationale for using MANOVA—to control the familywise Type I error rate when testing multiple dependent variables."
    },
    {
      "id": "PMET-PC-0032",
      "mode": "passage_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Specificity",
      "passage_type": "example",
      "source_passage_id": "PMET-0089",
      "sentences": [
        "A depression screening test is administered to 1,000 patients.",
        "Of 200 who actually have depression, 180 test positive (sensitivity = 180/200 = 90%).",
        "Of 800 without depression, 720 test negative (specificity = 720/800 = 90%).",
        "The test correctly identifies 90% of depressed patients and 90% of non-depressed patients."
      ],
      "target_sentence_index": 1,
      "original_sentence": "Of 200 who actually have depression, 180 test positive (sensitivity = 180/200 = 90%).",
      "error_original": "180 test positive (sensitivity = 180/200 = 90%)",
      "error_correct": "180 test positive (sensitivity = 180/200 = 90%) — this is correct; however, see the actual error below",
      "explanation": "Placeholder — see corrected version below."
    },
    {
      "id": "PMET-SC-0001",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Internal Consistency Reliability",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0228",
      "modified_sentence": "Because each half is only half as long as the full test, the Kuder-Richardson formula is used to estimate the reliability of the full-length test.",
      "phrases": [
        "Because each half is only half as long as the full test,",
        " the Kuder-Richardson formula is used",
        " to estimate the reliability",
        " of the full-length test."
      ],
      "target_phrase_index": 1,
      "error_original": "the Kuder-Richardson formula is used",
      "error_correct": "the Spearman-Brown formula is used",
      "explanation": "The Spearman-Brown formula (also called the Spearman-Brown prophecy formula) is used to correct the split-half correlation to estimate the reliability of the full-length test. The Kuder-Richardson formulas (KR-20 and KR-21) are different internal consistency measures used specifically for dichotomous items. Confusing these two is a common error, as both relate to internal consistency reliability, but they serve distinct purposes."
    },
    {
      "id": "PMET-SC-0002",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Threats to Construct Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0581",
      "modified_sentence": "Two major threats to construct validity are construct underrepresentation, which occurs when a test is too broad and fails to capture important dimensions of the construct, and construct-irrelevant variance, which occurs when test scores are systematically influenced by factors unrelated to the construct being measured.",
      "phrases": [
        "Two major threats to construct validity are construct underrepresentation,",
        " which occurs when a test is too broad and fails to capture important dimensions of the construct,",
        " and construct-irrelevant variance, which occurs when test scores are systematically influenced",
        " by factors unrelated to the construct being measured."
      ],
      "target_phrase_index": 1,
      "error_original": "too broad",
      "error_correct": "too narrow",
      "explanation": "Construct underrepresentation occurs when a test is too NARROW, meaning it fails to include items that adequately sample the full breadth of the construct. Saying the test is 'too broad' incorrectly describes the opposite problem. A test that is too broad would more likely introduce construct-irrelevant variance. Construct underrepresentation specifically refers to insufficient coverage of the construct's important facets or dimensions."
    },
    {
      "id": "PMET-SC-0003",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0470",
      "modified_sentence": "Statistical power can be increased by using a larger sample size, a larger effect size, a higher alpha level, higher variability in scores, or by using a one-tailed test.",
      "phrases": [
        "Statistical power can be increased by using a larger sample size,",
        " a larger effect size, a higher alpha level,",
        " higher variability in scores,",
        " or by using a one-tailed test."
      ],
      "target_phrase_index": 2,
      "error_original": "higher variability in scores",
      "error_correct": "lower variability in scores",
      "explanation": "Statistical power is increased by lower variability (not higher variability) in scores. When variability is reduced, the standard error decreases, making it easier to detect a true effect. The original passage correctly lists 'lower variability' as one of the factors that increases power."
    },
    {
      "id": "PMET-SC-0004",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to External Validity",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0298",
      "modified_sentence": "If a treatment was validated primarily on white, middle-class college students, its efficacy with diverse clinical populations should be evaluated carefully.",
      "phrases": [
        "If a treatment was validated primarily",
        " on white, middle-class college students,",
        " its efficacy with diverse clinical populations",
        " should be evaluated carefully."
      ],
      "target_phrase_index": 2,
      "error_original": "its efficacy with diverse clinical populations",
      "error_correct": "its effectiveness with diverse clinical populations",
      "explanation": "The original passage uses the term 'effectiveness,' which in evidence-based practice terminology refers to how well a treatment works in real-world clinical settings and diverse populations. 'Efficacy' refers to how well a treatment works under controlled, ideal research conditions (i.e., internal validity). The distinction is important: the passage is discussing external validity and generalizability to real-world clients, which is a question of effectiveness, not efficacy."
    },
    {
      "id": "PMET-SC-0005",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Positive and Negative Predictive Value",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0083",
      "modified_sentence": "When the base rate of a condition is low, the positive predictive value decreases substantially because most positive results are false positives, while the negative predictive value also decreases because most negatives are false negatives.",
      "phrases": [
        "When the base rate of a condition is low,",
        " the positive predictive value decreases substantially because most positive results are false positives,",
        " while the negative predictive value also decreases",
        " because most negatives are false negatives."
      ],
      "target_phrase_index": 2,
      "error_original": "the negative predictive value also decreases",
      "error_correct": "the negative predictive value remains high",
      "explanation": "When base rate is low, PPV is low (most positives are false positives), but NPV is actually high, not low. NPV decreases when the base rate is high, not when it is low. The passage itself states that NPV is low when base rate is high. The error here reverses the relationship between base rate and NPV for low-prevalence conditions."
    },
    {
      "id": "PMET-SC-0006",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Extinction and Spontaneous Recovery",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0412",
      "modified_sentence": "Reinstatement occurs when the conditioned response reappears after the extinguished organism is exposed to unpaired presentations of the conditioned stimulus, and it demonstrates that extinction does not erase the original CS-US association.",
      "phrases": [
        "Reinstatement occurs when the conditioned response reappears",
        " after the extinguished organism is exposed to unpaired presentations of the conditioned stimulus,",
        " and it demonstrates that extinction does not erase",
        " the original CS-US association."
      ],
      "target_phrase_index": 1,
      "error_original": "unpaired presentations of the conditioned stimulus",
      "error_correct": "unpaired presentations of the unconditioned stimulus",
      "explanation": "Reinstatement occurs when the CR returns after the organism receives unpaired presentations of the unconditioned stimulus (US), not the conditioned stimulus (CS), following extinction. Exposure to the US alone can reinstate the previously extinguished conditioned response, demonstrating that extinction does not fully eliminate the learned association."
    },
    {
      "id": "PMET-SC-0007",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Extinction and Spontaneous Recovery",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0411",
      "modified_sentence": "Renewal refers to the return of the conditioned response when the organism is tested in a context different from the extinction context, demonstrating that extinction involves new excitatory associations rather than erasing the original CS-US learning.",
      "phrases": [
        "Renewal refers to the return of the conditioned response",
        " when the organism is tested in a context different from the extinction context,",
        " demonstrating that extinction involves new excitatory associations",
        " rather than erasing the original CS-US learning."
      ],
      "target_phrase_index": 2,
      "error_original": "new excitatory associations",
      "error_correct": "new inhibitory associations",
      "explanation": "Extinction does not erase the original conditioned response but instead involves the formation of new inhibitory associations that suppress the CR. The fact that renewal occurs (the CR returns in a different context) supports this interpretation. Calling them 'excitatory' associations is incorrect; extinction learning is inhibitory in nature, as demonstrated by phenomena like renewal, spontaneous recovery, and reinstatement."
    },
    {
      "id": "PMET-SC-0008",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Four Basic Schedules",
      "passage_type": "definition",
      "source_passage_id": "PMET-0155",
      "modified_sentence": "Variable schedules produce more consistent responding and greater resistance to extinction than fixed schedules, and variable interval (VI) produces the highest response rate and greatest resistance to extinction.",
      "phrases": [
        "Variable schedules produce more consistent responding",
        " and greater resistance to extinction than fixed schedules,",
        " and variable interval (VI) produces the highest response rate",
        " and greatest resistance to extinction."
      ],
      "target_phrase_index": 2,
      "error_original": "variable interval (VI) produces the highest response rate",
      "error_correct": "variable ratio (VR) produces the highest response rate",
      "explanation": "The passage states that the variable ratio (VR) schedule produces the highest response rate and greatest resistance to extinction. Variable interval (VI) is a different schedule that, while resistant to extinction, does not produce the highest response rate. VR schedules are well-known for generating the highest and most steady rates of responding among all four basic reinforcement schedules."
    },
    {
      "id": "PMET-SC-0009",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Type I Error (False Positive)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0130",
      "modified_sentence": "A Type I error occurs when we reject the null hypothesis when it is actually true, and is also called a \"false positive\" or \"beta error,\" with its probability equal to the significance level (α).",
      "phrases": [
        "A Type I error occurs when we reject the null hypothesis when it is actually true,",
        " and is also called a \"false positive\" or \"beta error,\"",
        " with its probability equal to",
        " the significance level (α)."
      ],
      "target_phrase_index": 1,
      "error_original": "\"beta error,\"",
      "error_correct": "\"alpha error,\"",
      "explanation": "A Type I error is also known as an 'alpha error,' not a 'beta error.' The term 'beta error' refers to a Type II error (false negative), where beta (β) represents the probability of failing to reject a false null hypothesis. Since the Type I error rate is set by alpha (α), it is correctly termed an 'alpha error.'"
    },
    {
      "id": "PMET-SC-0010",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0489",
      "modified_sentence": "Negative reinforcement is commonly confused with punishment, but it actually increases the frequency of a behavior by involving the removal of a discriminative stimulus following the desired response.",
      "phrases": [
        "Negative reinforcement is commonly confused with punishment,",
        " but it actually increases the frequency of a behavior",
        " by involving the removal of a discriminative stimulus",
        " following the desired response."
      ],
      "target_phrase_index": 2,
      "error_original": "removal of a discriminative stimulus",
      "error_correct": "removal of an aversive stimulus",
      "explanation": "Negative reinforcement involves the removal of an aversive (unpleasant) stimulus contingent on a behavior, thereby increasing the likelihood of that behavior. A discriminative stimulus (SD) is a stimulus that signals the availability of reinforcement and sets the occasion for a response — it is not what is removed in negative reinforcement. The passage specifically states that negative reinforcement increases behavior through stimulus removal, referring to the removal of something unpleasant (aversive), not a discriminative stimulus."
    },
    {
      "id": "PMET-SC-0011",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0574",
      "modified_sentence": "According to Messick's unified validity framework, construct underrepresentation occurs when a test is too narrow and fails to capture important dimensions of the construct, while construct-irrelevant variance occurs when a test systematically undersamples the content domain that is extraneous to the target construct.",
      "phrases": [
        "According to Messick's unified validity framework,",
        " construct underrepresentation occurs when a test is too narrow and fails to capture important dimensions of the construct,",
        " while construct-irrelevant variance occurs when a test systematically undersamples the content domain",
        " that is extraneous to the target construct."
      ],
      "target_phrase_index": 2,
      "error_original": "systematically undersamples the content domain",
      "error_correct": "systematically measures unintended factors",
      "explanation": "Construct-irrelevant variance occurs when a test measures unintended factors extraneous to the construct, inflating or contaminating scores with irrelevant sources of variance. 'Undersampling the content domain' actually describes construct underrepresentation, the other major threat to construct validity identified in Messick's framework. The two threats are conceptual opposites: underrepresentation means the test is too narrow, while construct-irrelevant variance means the test is too broad, capturing things it shouldn't."
    },
    {
      "id": "PMET-SC-0012",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Construct Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0328",
      "modified_sentence": "When the operationalization of one construct is confounded with another construct, a treatment that includes both behavioral activation and relaxation training confounds these two components, making it impossible to determine which component is responsible for the observed treatment effects.",
      "phrases": [
        "When the operationalization of one construct is confounded with another construct,",
        " a treatment that includes both behavioral activation and relaxation training confounds these two components,",
        " making it impossible to determine which component",
        " is responsible for the observed treatment effects."
      ],
      "target_phrase_index": 1,
      "error_original": "behavioral activation",
      "error_correct": "cognitive restructuring",
      "explanation": "The original passage specifies that the example of construct confounding involves a treatment that includes both cognitive restructuring and relaxation training. 'Behavioral activation' is a different therapeutic technique (commonly associated with behavioral treatments for depression) and was incorrectly substituted for 'cognitive restructuring' in this example of a threat to construct validity."
    },
    {
      "id": "PMET-SC-0013",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Construct Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0326",
      "modified_sentence": "Demand characteristics are a threat to construct validity that lead participants to behave as they think the researcher expects, and they were first described by Martin Seligman in his research on the social psychology of experiments.",
      "phrases": [
        "Demand characteristics are a threat to construct validity",
        " that lead participants to behave as they think the researcher expects,",
        " and they were first described by Martin Seligman",
        " in his research on the social psychology of experiments."
      ],
      "target_phrase_index": 2,
      "error_original": "Martin Seligman",
      "error_correct": "Martin Orne",
      "explanation": "Demand characteristics were first described by Martin Orne, not Martin Seligman. Orne introduced the concept in his influential work on the social psychology of psychological experiments, highlighting how participants pick up on cues about what the experimenter expects and alter their behavior accordingly. Martin Seligman is known for learned helplessness and positive psychology, not for the concept of demand characteristics."
    },
    {
      "id": "PMET-SC-0014",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Reinforcement",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0168",
      "modified_sentence": "Reinforcement is the process by which consequences increase the likelihood that a behavior will occur again, and understanding the distinction between positive and negative punishment is critical for EPPP preparation, as this is a commonly tested area where confusion often arises.",
      "phrases": [
        "Reinforcement is the process by which consequences increase the likelihood that a behavior will occur again,",
        " and understanding the distinction between positive and negative punishment is critical for EPPP preparation,",
        " as this is a commonly tested area",
        " where confusion often arises."
      ],
      "target_phrase_index": 1,
      "error_original": "positive and negative punishment",
      "error_correct": "positive and negative reinforcement",
      "explanation": "The passage is specifically about reinforcement, not punishment. The distinction being discussed is between positive and negative reinforcement — both of which increase the likelihood of a behavior. Punishment, by contrast, decreases the likelihood of a behavior. Substituting 'punishment' for 'reinforcement' changes the concept entirely and is inconsistent with the topic of the passage."
    },
    {
      "id": "PMET-SC-0015",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0432",
      "modified_sentence": "A correlation coefficient describes the strength and direction of the curvilinear relationship between two variables, but it cannot establish causation because the relationship may be explained by a confounding third variable.",
      "phrases": [
        "A correlation coefficient describes",
        " the strength and direction of the curvilinear relationship between two variables,",
        " but it cannot establish causation",
        " because the relationship may be explained by a confounding third variable."
      ],
      "target_phrase_index": 1,
      "error_original": "curvilinear relationship",
      "error_correct": "linear relationship",
      "explanation": "A standard correlation coefficient (Pearson's r) specifically measures the strength and direction of the linear relationship between two variables, not a curvilinear relationship. In fact, a Pearson's r of zero could still reflect a strong curvilinear association. Special methods such as eta (η) or polynomial regression are needed to capture curvilinear relationships."
    },
    {
      "id": "PMET-SC-0016",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Survey Research",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0272",
      "modified_sentence": "Survey research uses questionnaires or interviews to collect data from a sample, typically to describe characteristics of a population or examine causal effects between variables, and representative sampling is critical for generalizing survey results to the population of interest.",
      "phrases": [
        "Survey research uses questionnaires or interviews to collect data from a sample,",
        " typically to describe characteristics of a population or examine causal effects between variables,",
        " and representative sampling is critical",
        " for generalizing survey results to the population of interest."
      ],
      "target_phrase_index": 1,
      "error_original": "examine causal effects between variables",
      "error_correct": "examine relationships between variables",
      "explanation": "Survey research is a non-experimental (descriptive/correlational) method and is used to examine relationships between variables, not causal effects. Establishing causal effects requires experimental designs with manipulation of independent variables and random assignment. Survey research can identify associations and correlations but cannot determine causation."
    },
    {
      "id": "PMET-SC-0017",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Three-Term Contingency",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0189",
      "modified_sentence": "The three-term contingency, also known as the ABC model, was a foundational concept in Edward Thorndike's operant conditioning framework, where A refers to the antecedent discriminative stimulus, B refers to the behavior, and C refers to the consequence that follows.",
      "phrases": [
        "The three-term contingency, also known as the ABC model,",
        " was a foundational concept in Edward Thorndike's operant conditioning framework,",
        " where A refers to the antecedent discriminative stimulus, B refers to the behavior,",
        " and C refers to the consequence that follows."
      ],
      "target_phrase_index": 1,
      "error_original": "Edward Thorndike's operant conditioning framework",
      "error_correct": "B.F. Skinner's operant conditioning framework",
      "explanation": "The three-term contingency (ABC model) is a foundational concept in B.F. Skinner's operant conditioning framework, not Edward Thorndike's. While Thorndike contributed the Law of Effect and instrumental learning concepts, it was Skinner who formalized the three-term contingency as the core analytical unit of operant behavior, describing how antecedent stimuli, behavior, and consequences are functionally related."
    },
    {
      "id": "PMET-SC-0018",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Introduction to Classical Conditioning",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0008",
      "modified_sentence": "Classical conditioning underlies many emotional responses, phobias, and physiological reactions, and understanding these mechanisms is essential for psychologists because they form the theoretical foundation for flooding, systematic desensitization, and other evidence-based interventions for anxiety disorders.",
      "phrases": [
        "Classical conditioning underlies many emotional responses, phobias, and physiological reactions,",
        " and understanding these mechanisms is essential for psychologists",
        " because they form the theoretical foundation for flooding, systematic desensitization,",
        " and other evidence-based interventions for anxiety disorders."
      ],
      "target_phrase_index": 2,
      "error_original": "flooding, systematic desensitization,",
      "error_correct": "exposure-based treatments, systematic desensitization,",
      "explanation": "The original passage states that classical conditioning forms the theoretical foundation for 'exposure-based treatments,' not 'flooding.' While flooding is indeed an exposure-based technique rooted in classical conditioning principles, the passage specifically uses the broader term 'exposure-based treatments' as the category. Replacing it with 'flooding' narrows the claim incorrectly and misrepresents the original text, which intended to reference the full range of exposure-based treatments rather than one specific technique."
    },
    {
      "id": "PMET-SC-0019",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Higher-Order Conditioning",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0017",
      "modified_sentence": "Higher-order conditioning is generally weaker than first-order conditioning and is susceptible to extinction, but it provides a mechanism for explaining how complex associative networks develop and how fears can generalize to stimuli never directly associated with trauma (Gewirtz & Davis, 2000).",
      "phrases": [
        "Higher-order conditioning is generally weaker than first-order conditioning",
        " and is susceptible to extinction, but it provides a mechanism",
        " for explaining how complex associative networks develop and how fears can generalize to stimuli",
        " never directly associated with trauma (Gewirtz & Davis, 2000)."
      ],
      "target_phrase_index": 2,
      "error_original": "how fears can generalize to stimuli",
      "error_correct": "how fears can spread to stimuli",
      "explanation": "The original passage states that higher-order conditioning explains how fears can 'spread' to stimuli never directly associated with trauma. The error replaces 'spread' with 'generalize,' which is a different learning concept. Generalization refers to responding to stimuli similar to the conditioned stimulus, whereas spreading via higher-order conditioning refers to fears extending to stimuli that were associated with the CS but never paired with the US (trauma) itself."
    },
    {
      "id": "PMET-SC-0020",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Coefficient of Alienation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0052",
      "modified_sentence": "The coefficient of alienation represents the proportion of variance that is NOT shared between two variables, and it is calculated as 1 - r², which indicates the proportion of unexplained variance remaining after the correlation is accounted for.",
      "phrases": [
        "The coefficient of alienation represents the proportion of variance that is NOT shared between two variables,",
        " and it is calculated as 1 - r²,",
        " which indicates the proportion of unexplained variance remaining",
        " after the correlation is accounted for."
      ],
      "target_phrase_index": 1,
      "error_original": "it is calculated as 1 - r²",
      "error_correct": "it is calculated as 1 - r² only when referring to the coefficient of nondetermination; the coefficient of alienation itself is calculated as the square root of 1 - r², i.e., √(1 - r²)",
      "explanation": "The coefficient of alienation is actually calculated as √(1 - r²), not 1 - r². The value 1 - r² is the coefficient of nondetermination (also called the coefficient of non-determination), which represents the proportion of unexplained variance. The coefficient of alienation is the square root of that value. This is a commonly confused distinction in psychometrics."
    },
    {
      "id": "PMET-SC-0021",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Chi-Square Tests",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0464",
      "modified_sentence": "The chi-square test of independence tests whether two categorical variables are related, and it analyzes data organized in a correlation matrix to determine if the observed frequencies differ significantly from expected frequencies.",
      "phrases": [
        "The chi-square test of independence tests whether two categorical variables are related,",
        " and it analyzes data organized in a correlation matrix",
        " to determine if the observed frequencies",
        " differ significantly from expected frequencies."
      ],
      "target_phrase_index": 1,
      "error_original": "a correlation matrix",
      "error_correct": "a contingency table",
      "explanation": "The chi-square test of independence uses a contingency table (also called a cross-tabulation table), not a correlation matrix. A contingency table displays the frequency distribution of categorical variables, while a correlation matrix displays correlation coefficients between continuous variables. The passage explicitly states that the chi-square test of independence uses a contingency table."
    },
    {
      "id": "PMET-SC-0022",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0441",
      "modified_sentence": "Predictive validity is a type of criterion-related validity in which the criterion is measured at the same time as the predictor, and it is commonly used when the goal is to forecast future performance such as job success or academic achievement.",
      "phrases": [
        "Predictive validity is a type of criterion-related validity",
        " in which the criterion is measured at the same time as the predictor,",
        " and it is commonly used when the goal is to forecast future performance",
        " such as job success or academic achievement."
      ],
      "target_phrase_index": 1,
      "error_original": "measured at the same time as the predictor",
      "error_correct": "measured after the predictor",
      "explanation": "Predictive validity involves measuring the criterion after the predictor test has been administered (e.g., collecting job performance data months after a hiring test). Measuring the criterion at the same time as the predictor describes concurrent validity, not predictive validity. This distinction between the timing of criterion measurement is the key difference between these two subtypes of criterion-related validity."
    },
    {
      "id": "PMET-SC-0023",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Between-Subjects (Independent Groups) Designs",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0277",
      "modified_sentence": "Random assignment creates groups that are expected to be equivalent on all variables except the dependent variable, and the main advantage of between-subjects designs is avoiding carryover effects.",
      "phrases": [
        "Random assignment creates groups that are expected to be equivalent",
        " on all variables except the dependent variable,",
        " and the main advantage of between-subjects designs",
        " is avoiding carryover effects."
      ],
      "target_phrase_index": 1,
      "error_original": "except the dependent variable,",
      "error_correct": "except the independent variable,",
      "explanation": "Random assignment is used to ensure that groups are equivalent on all variables except the independent variable, which is the variable being manipulated by the researcher. The dependent variable is the outcome being measured, not the variable on which groups are expected to differ by design."
    },
    {
      "id": "PMET-SC-0024",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Kurtosis",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0378",
      "modified_sentence": "Severe skewness or kurtosis can violate assumptions of parametric statistical tests, and options include using nonparametric alternatives, transforming data using log or cube root methods, or using robust statistical methods and reporting median instead of mean for skewed data.",
      "phrases": [
        "Severe skewness or kurtosis can violate assumptions of parametric statistical tests,",
        " and options include using nonparametric alternatives,",
        " transforming data using log or cube root methods,",
        " or using robust statistical methods and reporting median instead of mean for skewed data."
      ],
      "target_phrase_index": 2,
      "error_original": "cube root",
      "error_correct": "square root",
      "explanation": "The passage lists 'square root' as one of the common data transformation methods used to address violations of parametric test assumptions. 'Cube root' is not the standard transformation mentioned; the correct term is 'square root,' which along with log transformations is a widely recommended approach for normalizing skewed distributions."
    },
    {
      "id": "PMET-SC-0025",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Predictive Validity",
      "passage_type": "example",
      "source_passage_id": "PMET-0086",
      "modified_sentence": "A correlation of .45 between admission test scores and subsequent GPA demonstrates concurrent validity, and the test provides useful information about likely academic performance.",
      "phrases": [
        "A correlation of .45 between admission test scores",
        " and subsequent GPA demonstrates concurrent validity,",
        " and the test provides useful information",
        " about likely academic performance."
      ],
      "target_phrase_index": 1,
      "error_original": "concurrent validity",
      "error_correct": "predictive validity",
      "explanation": "Because the admissions test is administered before graduate school and the criterion (first-year GPA) is measured at a later time, this is an example of predictive validity, not concurrent validity. Concurrent validity involves measuring the predictor and criterion at approximately the same point in time, whereas predictive validity involves a meaningful time interval between the predictor assessment and the criterion measurement."
    },
    {
      "id": "PMET-SC-0026",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Components of Clinical Utility",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0085",
      "modified_sentence": "Questions to ask about a test's clinical utility include whether use of this test leads to different clinical decisions than would be made without it, whether those different decisions result in better diagnostic reliability, whether the test is feasible to administer in typical clinical settings, and whether the test is acceptable to patients and clinicians.",
      "phrases": [
        "Questions to ask about a test's clinical utility include whether use of this test leads to different clinical decisions than would be made without it,",
        " whether those different decisions result in better diagnostic reliability,",
        " whether the test is feasible to administer in typical clinical settings,",
        " and whether the test is acceptable to patients and clinicians."
      ],
      "target_phrase_index": 1,
      "error_original": "better diagnostic reliability",
      "error_correct": "better patient outcomes",
      "explanation": "The original passage states that the key question is whether different clinical decisions result in 'better patient outcomes,' not 'better diagnostic reliability.' Clinical utility is fundamentally concerned with whether test use improves outcomes for patients, not merely whether it enhances diagnostic reliability. Diagnostic reliability is a separate psychometric property related to consistency of measurement."
    },
    {
      "id": "PMET-SC-0027",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Correlational Research",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0271",
      "modified_sentence": "Correlational research examines relationships between variables without manipulation, and while correlation does not imply causation, sophisticated correlational techniques such as factor analysis and path analysis can test theoretical models about causal relationships.",
      "phrases": [
        "Correlational research examines relationships between variables without manipulation,",
        " and while correlation does not imply causation,",
        " sophisticated correlational techniques such as factor analysis and path analysis",
        " can test theoretical models about causal relationships."
      ],
      "target_phrase_index": 2,
      "error_original": "factor analysis and path analysis",
      "error_correct": "structural equation modeling and path analysis",
      "explanation": "The passage originally identifies structural equation modeling (SEM) and path analysis as the sophisticated correlational techniques used to test theoretical causal models. Factor analysis is a different statistical technique primarily used to identify latent constructs underlying observed variables, not to test theoretical models about causal relationships in the way SEM does."
    },
    {
      "id": "PMET-SC-0028",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "CIPP Model (Stufflebeam)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0562",
      "modified_sentence": "Daniel Stufflebeam's CIPP Model is ahypothesis-oriented evaluation framework in which Context evaluation assesses needs and problems,Input evaluation examines competing strategies, Process evaluation monitors implementation,and Product evaluation measures outcomes.",
      "phrases": [
        "Daniel Stufflebeam's CIPP Model is a",
        "hypothesis-oriented evaluation framework in which Context evaluation assesses needs and problems,",
        "Input evaluation examines competing strategies, Process evaluation monitors implementation,",
        "and Product evaluation measures outcomes."
      ],
      "target_phrase_index": 1,
      "error_original": "hypothesis-oriented evaluation framework",
      "error_correct": "decision-oriented evaluation framework",
      "explanation": "Stufflebeam's CIPP Model is explicitly described as a decision-oriented evaluation framework, designed to provide information useful for decision-making at each stage of program development. It is not hypothesis-oriented; the term 'hypothesis-oriented' more accurately describes experimental research designs. Stufflebeam distinguished his approach from traditional hypothesis-testing models by emphasizing that evaluation should serve decision-makers rather than test hypotheses."
    },
    {
      "id": "PMET-SC-0029",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Introduction to Classical Conditioning",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0006",
      "modified_sentence": "Classical conditioning, also known as Pavlovian or respondent conditioning, represents one of the most fundamental mechanisms of learning discovered in the history of psychology, and was first systematically described by Russian physiologist Ivan Pavlov in the 1920s during his research on digestive processes in dogs.",
      "phrases": [
        "Classical conditioning, also known as Pavlovian or respondent conditioning,",
        " represents one of the most fundamental mechanisms of learning discovered in the history of psychology,",
        " and was first systematically described by Russian physiologist Ivan Pavlov in the 1920s",
        " during his research on digestive processes in dogs."
      ],
      "target_phrase_index": 2,
      "error_original": "in the 1920s",
      "error_correct": "in the 1890s",
      "explanation": "Ivan Pavlov first systematically described classical conditioning in the 1890s during his research on digestive processes in dogs, not in the 1920s. By the 1920s, Pavlov's work was already well established and widely known in the scientific community. The original passage clearly states the 1890s as the correct time period."
    },
    {
      "id": "PMET-SC-0030",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Major Qualitative Traditions",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0285",
      "modified_sentence": "Ethnography involves prolonged immersion in a cultural group or community to understand its practices, beliefs, and social dynamics from an outsider perspective, and the researcher engages in participant observation by simultaneously participating in and observing the culture.",
      "phrases": [
        "Ethnography involves prolonged immersion in a cultural group or community",
        " to understand its practices, beliefs, and social dynamics from an outsider perspective,",
        " and the researcher engages in participant observation",
        " by simultaneously participating in and observing the culture."
      ],
      "target_phrase_index": 1,
      "error_original": "from an outsider perspective",
      "error_correct": "from an insider perspective",
      "explanation": "Ethnography aims to understand a culture from an insider (emic) perspective, not an outsider (etic) perspective. The defining feature of ethnographic research is that the researcher seeks to understand the cultural group's practices, beliefs, and social dynamics as members of that group experience them, which is the insider or emic viewpoint."
    },
    {
      "id": "PMET-SC-0031",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Test Bias and Fairness",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0366",
      "modified_sentence": "Test bias is a psychometric concept referring to random errors in test scores or predictions for members of particular groups, and a test is considered biased if it systematically over- or underpredicts criterion performance for certain groups.",
      "phrases": [
        "Test bias is a psychometric concept",
        " referring to random errors in test scores or predictions for members of particular groups,",
        " and a test is considered biased if it systematically over- or underpredicts",
        " criterion performance for certain groups."
      ],
      "target_phrase_index": 1,
      "error_original": "random errors",
      "error_correct": "systematic errors",
      "explanation": "Test bias specifically refers to systematic errors, not random errors, in test scores or predictions. Random errors reflect unreliability and affect all examinees equally without directional bias, whereas systematic errors consistently disadvantage or advantage particular groups, which is the defining characteristic of test bias."
    },
    {
      "id": "PMET-SC-0032",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "t-Tests",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0134",
      "modified_sentence": "The t-test is used to compare means when the population variance is unknown, which is almost always the case, and there are three main types of t-tests.",
      "phrases": [
        "The t-test is used to compare means",
        " when the population variance is unknown,",
        " which is almost always the case,",
        " and there are three main types of t-tests."
      ],
      "target_phrase_index": 1,
      "error_original": "population variance",
      "error_correct": "population standard deviation",
      "explanation": "The t-test is specifically used when the population standard deviation (not the population variance) is unknown and must be estimated from the sample. While variance and standard deviation are mathematically related (standard deviation is the square root of variance), the defining condition for using the t-test rather than the z-test is that the population standard deviation (sigma) is unknown."
    },
    {
      "id": "PMET-SC-0033",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Three-Term Contingency",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0190",
      "modified_sentence": "The three-term contingency, originally described by B.F. Skinner, identifies the relationship among the antecedent, behavior, and consequence, and this framework is fundamental to functional behavior assessment and the development of behavior intervention plans in clinical and educational settings.",
      "phrases": [
        "The three-term contingency, originally described by B.F. Skinner,",
        " identifies the relationship among the antecedent, behavior, and consequence,",
        " and this framework is fundamental to functional behavior assessment",
        " and the development of behavior intervention plans in clinical and educational settings."
      ],
      "target_phrase_index": 2,
      "error_original": "fundamental to functional behavior assessment",
      "error_correct": "fundamental to functional behavior analysis",
      "explanation": "The three-term contingency is fundamental to functional behavior analysis (FBA as a methodology within applied behavior analysis). While the term 'functional behavior assessment' is commonly used in educational settings, in the context of the original passage and EPPP content regarding operant conditioning frameworks, the three-term contingency is described as foundational to functional behavior analysis — the broader scientific discipline. However, re-examining the original passage, it actually does say 'functional behavior assessment,' so this error would not apply here."
    },
    {
      "id": "PMET-SC-0034",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Single-Subject (Single-Case) Designs",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0264",
      "modified_sentence": "Single-subject designs, also called single-case experimental designs or N-of-1 designs, use individual participants as their own controls, involve repeated measurement of behavior over time and systematic introduction and withdrawal of interventions, and are widely used in applied behavior analysis and health psychology.",
      "phrases": [
        "Single-subject designs, also called single-case experimental designs or N-of-1 designs,",
        " use individual participants as their own controls,",
        " involve repeated measurement of behavior over time and systematic introduction and withdrawal of interventions,",
        " and are widely used in applied behavior analysis and health psychology."
      ],
      "target_phrase_index": 3,
      "error_original": "health psychology",
      "error_correct": "clinical psychology",
      "explanation": "The passage states that single-subject designs are widely used in applied behavior analysis and clinical psychology, not health psychology. Health psychology is a distinct subfield focused on how psychological factors affect health and illness, whereas clinical psychology is the field traditionally associated with single-subject experimental designs alongside applied behavior analysis."
    },
    {
      "id": "PMET-SC-0035",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Advantages of IRT",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0502",
      "modified_sentence": "Ability estimate invariance means that examinee ability estimates are comparable regardless of which specific items were administered, as long as items come from a standardized norm group rather than a calibrated item bank.",
      "phrases": [
        "Ability estimate invariance means that examinee ability estimates are comparable",
        " regardless of which specific items were administered,",
        " as long as items come from a standardized norm group",
        " rather than a calibrated item bank."
      ],
      "target_phrase_index": 2,
      "error_original": "as long as items come from a standardized norm group",
      "error_correct": "as long as items come from a calibrated item bank",
      "explanation": "In Item Response Theory (IRT), ability estimate invariance holds when items are drawn from a calibrated item bank—meaning items whose parameters have been estimated and fixed through IRT calibration procedures. The phrase 'standardized norm group' is a concept from Classical Test Theory (CTT) related to norming, not to the IRT requirement for item parameter calibration. The correct condition is that items come from a calibrated bank."
    },
    {
      "id": "PMET-SC-0036",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0545",
      "modified_sentence": "Process evaluation monitors implementation fidelity; outcome evaluation measures goal achievement; impact evaluation examines short-term, narrower effects on the targeted population.",
      "phrases": [
        "Process evaluation monitors implementation fidelity;",
        " outcome evaluation measures goal achievement;",
        " impact evaluation examines short-term, narrower effects on the targeted population."
      ],
      "target_phrase_index": 2,
      "error_original": "short-term, narrower effects",
      "error_correct": "long-term, broader effects",
      "explanation": "Impact evaluation is specifically designed to examine long-term, broader effects of a program beyond its immediate goals, not short-term, narrower effects. Short-term goal achievement is the focus of outcome evaluation, whereas impact evaluation looks at wider, more distal consequences over time."
    },
    {
      "id": "PMET-SC-0037",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Concurrent Validity",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0082",
      "modified_sentence": "Predictive validity is preferred for selection/admissions decisions where forecasting future performance is the goal, while concurrent validity is appropriate when substituting one measure for another, screening, or diagnosis where the criterion is historically documented rather than currently observable.",
      "phrases": [
        "Predictive validity is preferred for selection/admissions decisions where forecasting future performance is the goal,",
        " while concurrent validity is appropriate when substituting one measure for another,",
        " screening, or diagnosis where the criterion is historically documented rather than currently observable."
      ],
      "target_phrase_index": 2,
      "error_original": "historically documented",
      "error_correct": "currently observable",
      "explanation": "Concurrent validity involves a criterion that is currently observable — meaning the test and criterion are measured at the same time. The passage states the criterion is 'currently observable,' not 'historically documented.' Historical documentation would imply archival or past data, which contradicts the defining feature of concurrent validity: simultaneous measurement of the predictor and criterion."
    },
    {
      "id": "PMET-SC-0038",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Problems with Punishment",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0476",
      "modified_sentence": "One major problem with punishment is that it often suppresses behavior only temporarily, and the suppressed behavior tends to return when the reinforcing agent is no longer present, rather than producing lasting behavioral change.",
      "phrases": [
        "One major problem with punishment is that it often suppresses behavior only temporarily,",
        " and the suppressed behavior tends to return when the reinforcing agent is no longer present,",
        " rather than producing lasting behavioral change."
      ],
      "target_phrase_index": 1,
      "error_original": "the reinforcing agent",
      "error_correct": "the punishing agent",
      "explanation": "The passage states that punishment suppresses behavior only in the presence of the punishing agent—meaning the person or stimulus delivering the punishment. The error replaces 'punishing agent' with 'reinforcing agent,' which is a different operant conditioning concept. The behavior returns when the punishing agent (not a reinforcing agent) is removed from the situation."
    },
    {
      "id": "PMET-SC-0039",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Discriminative Stimuli",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0187",
      "modified_sentence": "A discriminative stimulus (S D) is a stimulus in the presence of which a particular response has been reinforced, and through experience organisms learn to respond differently in the presence versus absence of the S D, where the S D elicits the response much like in classical conditioning, rather than merely setting the occasion for the response by signaling reinforcement availability.",
      "phrases": [
        "A discriminative stimulus (S D) is a stimulus in the presence of which a particular response has been reinforced,",
        " and through experience organisms learn to respond differently in the presence versus absence of the S D,",
        " where the S D elicits the response much like in classical conditioning,",
        " rather than merely setting the occasion for the response by signaling reinforcement availability."
      ],
      "target_phrase_index": 2,
      "error_original": "where the S D elicits the response much like in classical conditioning",
      "error_correct": "where the S D does not elicit the response as in classical conditioning",
      "explanation": "The passage explicitly states that the discriminative stimulus (S D) does NOT elicit the response (as a stimulus does in classical conditioning). Instead, the S D sets the occasion for the response by signaling that reinforcement is available. The error reverses this critical distinction, incorrectly claiming the S D elicits the response like in classical conditioning, which confuses the fundamental difference between operant and classical conditioning mechanisms."
    },
    {
      "id": "PMET-SC-0040",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "p-Values",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0129",
      "modified_sentence": "The p-value is the probability of obtaining results at least as extreme as those observed, assuming the alternative hypothesis is true, and if p ≤ α, the result is considered statistically significant, and we reject the null hypothesis.",
      "phrases": [
        "The p-value is the probability of obtaining results at least as extreme as those observed,",
        " assuming the alternative hypothesis is true,",
        " and if p ≤ α, the result is considered statistically significant,",
        " and we reject the null hypothesis."
      ],
      "target_phrase_index": 1,
      "error_original": "assuming the alternative hypothesis is true,",
      "error_correct": "assuming the null hypothesis is true,",
      "explanation": "The p-value is defined as the probability of obtaining results at least as extreme as those observed assuming the null hypothesis is true, not the alternative hypothesis. This is a foundational concept in null hypothesis significance testing (NHST): the p-value is calculated under the assumption that the null hypothesis is correct."
    },
    {
      "id": "PMET-SC-0041",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0584",
      "modified_sentence": "Mediators explain how an effect occurs, while moderators explain why or through what mechanism the effect occurs.",
      "phrases": [
        "Mediators explain how an effect occurs,",
        " while moderators explain why",
        " or through what mechanism the effect occurs."
      ],
      "target_phrase_index": 1,
      "error_original": "while moderators explain why",
      "error_correct": "while moderators explain when or for whom",
      "explanation": "Moderators explain when or for whom an effect occurs (i.e., they identify the conditions or subgroups under which the effect varies). The phrase 'explain why or through what mechanism' actually describes mediators, not moderators. This error conflates the roles of mediators and moderators, which is a commonly tested distinction on the EPPP."
    },
    {
      "id": "PMET-SC-0042",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Prepared Fears",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0028",
      "modified_sentence": "This preparedness may explain the non-random distribution of phobias in clinical populations (Seligman & Mineka, 2001).",
      "phrases": [
        "This preparedness may explain",
        " the non-random distribution of phobias",
        " in clinical populations (Seligman & Mineka, 2001)."
      ],
      "target_phrase_index": 2,
      "error_original": "Seligman & Mineka, 2001",
      "error_correct": "Ohman & Mineka, 2001",
      "explanation": "The key research on prepared fears and the non-random distribution of phobias in clinical populations was published by Ohman and Mineka (2001), not Seligman and Mineka. While Seligman (1971) originally proposed the concept of biological preparedness in fear conditioning, the specific 2001 publication referenced here was authored by Arne Ohman and Susan Mineka."
    },
    {
      "id": "PMET-SC-0043",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0542",
      "modified_sentence": "Ethnography is a qualitative research method that involves participant observation in cultural settings, produces what Bronislaw Malinowski called thick description, and is most commonly associated with the field of anthropology.",
      "phrases": [
        "Ethnography is a qualitative research method",
        " that involves participant observation in cultural settings,",
        " produces what Bronislaw Malinowski called thick description,",
        " and is most commonly associated with the field of anthropology."
      ],
      "target_phrase_index": 2,
      "error_original": "Bronislaw Malinowski called thick description",
      "error_correct": "Clifford Geertz called thick description",
      "explanation": "The concept of 'thick description' in ethnographic research was articulated by Clifford Geertz in his 1973 work 'The Interpretation of Cultures,' borrowing the term from philosopher Gilbert Ryle. Bronislaw Malinowski was a pioneering anthropologist known for participant observation fieldwork, but he is not the scholar associated with thick description."
    },
    {
      "id": "PMET-SC-0044",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Standard Error of Measurement",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0232",
      "modified_sentence": "The standard error of measurement (SEM) quantifies the amount of error expected in an individual's observed score, and while the reliability coefficient describes individual-level consistency, the SEM provides information about the precision of individual scores.",
      "phrases": [
        "The standard error of measurement (SEM) quantifies the amount of error expected in an individual's observed score,",
        " and while the reliability coefficient describes individual-level consistency,",
        " the SEM provides information about the precision of individual scores."
      ],
      "target_phrase_index": 1,
      "error_original": "individual-level consistency",
      "error_correct": "group-level consistency",
      "explanation": "The reliability coefficient describes group-level consistency (i.e., the consistency of scores across a group of examinees), not individual-level consistency. It is the SEM that provides information about the precision of individual scores. This distinction between group-level (reliability coefficient) and individual-level (SEM) metrics is a key concept in psychometric theory."
    },
    {
      "id": "PMET-SC-0045",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Consequences of Testing",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0360",
      "modified_sentence": "This evidence considers the intended and unintended consequences of test use, including potential benefits and harms, and it addresses whether test use achieves its intended outcomes and whether any negative consequences are related to construct-underrepresentation factors such as bias.",
      "phrases": [
        "This evidence considers the intended and unintended consequences of test use,",
        " including potential benefits and harms,",
        " and it addresses whether test use achieves its intended outcomes",
        " and whether any negative consequences are related to construct-underrepresentation factors such as bias."
      ],
      "target_phrase_index": 3,
      "error_original": "construct-underrepresentation factors such as bias",
      "error_correct": "construct-irrelevant factors such as bias",
      "explanation": "The original passage states that negative consequences are related to 'construct-irrelevant factors such as bias.' Construct-irrelevant variance refers to systematic error introduced by factors unrelated to the construct being measured (e.g., bias). Construct underrepresentation is a different validity threat that refers to a test failing to capture important aspects of the construct. The error swaps 'construct-irrelevant' with 'construct-underrepresentation,' which are distinct concepts in validity theory."
    },
    {
      "id": "PMET-SC-0046",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Quasi-Experimental Designs",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0259",
      "modified_sentence": "Quasi-experimental designs lack random assignment but still involve manipulation of an independent variable, and are commonly used in applied settings where random assignment is impractical or unethical, while they provide stronger evidence for causation than true experiments, though thoughtful quasi-experimental designs can yield valuable findings.",
      "phrases": [
        "Quasi-experimental designs lack random assignment but still involve manipulation of an independent variable,",
        " and are commonly used in applied settings where random assignment is impractical or unethical,",
        " while they provide stronger evidence for causation than true experiments,",
        " though thoughtful quasi-experimental designs can yield valuable findings."
      ],
      "target_phrase_index": 2,
      "error_original": "stronger evidence for causation than true experiments",
      "error_correct": "weaker evidence for causation than true experiments",
      "explanation": "Quasi-experimental designs provide WEAKER evidence for causation than true experiments, not stronger, because they lack random assignment, which is a key element for establishing internal validity and ruling out confounds. The passage originally states 'weaker evidence,' so 'stronger' is the introduced error."
    },
    {
      "id": "PMET-SC-0047",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Properties of the Normal Distribution",
      "passage_type": "definition",
      "source_passage_id": "PMET-0373",
      "modified_sentence": "In a normal distribution, 68% of observations fall between M minus 1SD and M plus 1SD, 95% of observations fall between M minus 2SD and M plus 2SD, and 98.7% of observations fall between M minus 3SD and M plus 3SD.",
      "phrases": [
        "In a normal distribution, 68% of observations fall between M minus 1SD and M plus 1SD,",
        " 95% of observations fall between M minus 2SD and M plus 2SD,",
        " and 98.7% of observations fall between M minus 3SD and M plus 3SD."
      ],
      "target_phrase_index": 2,
      "error_original": "98.7%",
      "error_correct": "99.7%",
      "explanation": "The Empirical Rule (also known as the 68-95-99.7 rule) states that 99.7% of observations in a normal distribution fall between M minus 3SD and M plus 3SD. The value '98.7%' is incorrect; the correct percentage is 99.7%."
    },
    {
      "id": "PMET-SC-0048",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Clinical Applications",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0425",
      "modified_sentence": "Systematic desensitization, developed by Joseph Wolpe, is based on the principle of stimulus generalization, in which a conditioned fear response is replaced by a relaxation response through gradual exposure to a hierarchy of anxiety-provoking stimuli.",
      "phrases": [
        "Systematic desensitization, developed by Joseph Wolpe,",
        " is based on the principle of stimulus generalization,",
        " in which a conditioned fear response is replaced by a relaxation response",
        " through gradual exposure to a hierarchy of anxiety-provoking stimuli."
      ],
      "target_phrase_index": 1,
      "error_original": "stimulus generalization",
      "error_correct": "counterconditioning",
      "explanation": "Systematic desensitization is based on the principle of counterconditioning (also called reciprocal inhibition), in which an incompatible response (relaxation) is paired with a feared stimulus to replace the anxiety response. Stimulus generalization is a different classical conditioning concept referring to the tendency for stimuli similar to a conditioned stimulus to also elicit the conditioned response. This error requires specific knowledge of the theoretical basis of Wolpe's technique."
    },
    {
      "id": "PMET-SC-0049",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0434",
      "modified_sentence": "The Standard Error of Estimate measures the accuracy of predictions made from a regression equation, it is calculated using the formula SDx times the square root of (1 - r²), and it decreases as the correlation coefficient increases, approaching zero as r approaches 1.00.",
      "phrases": [
        "The Standard Error of Estimate measures the accuracy of predictions made from a regression equation,",
        " it is calculated using the formula SDx times the square root of (1 - r²),",
        " and it decreases as the correlation coefficient increases,",
        " approaching zero as r approaches 1.00."
      ],
      "target_phrase_index": 1,
      "error_original": "SDx",
      "error_correct": "SDy",
      "explanation": "The Standard Error of Estimate formula is SEE = SDy × √(1 - r²), where SDy is the standard deviation of the criterion (Y) variable, not SDx. The SEE reflects how much predicted Y scores deviate from actual Y scores, so it is based on the variability of Y, adjusted by the strength of the correlation."
    },
    {
      "id": "PMET-SC-0050",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Base Rates and Selection Ratios",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0457",
      "modified_sentence": "According to the Taylor-Russell tables, a test's incremental validity is maximized when the base rate is approximately .30, and a lower selection ratio generally increases the proportion of successful applicants selected.",
      "phrases": [
        "According to the Taylor-Russell tables,",
        " a test's incremental validity is maximized when the base rate is approximately .30,",
        " and a lower selection ratio generally increases",
        " the proportion of successful applicants selected."
      ],
      "target_phrase_index": 1,
      "error_original": "the base rate is approximately .30",
      "error_correct": "the base rate is approximately .50",
      "explanation": "According to the Taylor-Russell tables, a test's incremental validity (its ability to improve prediction beyond the base rate alone) is maximized when the base rate is approximately .50 (moderate), not .30. When the base rate is already very low or very high, there is less room for a test to improve upon predictions made from the base rate alone."
    },
    {
      "id": "PMET-SC-0051",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Measures of Variability",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0398",
      "modified_sentence": "The range is the difference between the highest and lowest scores, and it is simple but unstable because it is based on only two scores and is highly sensitive to central tendency.",
      "phrases": [
        "The range is the difference between the highest and lowest scores,",
        " and it is simple but unstable",
        " because it is based on only two scores",
        " and is highly sensitive to central tendency."
      ],
      "target_phrase_index": 3,
      "error_original": "highly sensitive to central tendency",
      "error_correct": "highly sensitive to outliers",
      "explanation": "The range is highly sensitive to outliers (extreme scores), not to central tendency. Central tendency refers to measures like the mean, median, and mode, which describe the center of a distribution. The range is unstable precisely because a single extreme score (outlier) can dramatically change its value."
    },
    {
      "id": "PMET-SC-0052",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Test Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0069",
      "modified_sentence": "Correlation is central to evaluating criterion-related validity, which includes both predictive and construct subtypes, and the correlation between test scores and criterion measures indicates how well the test predicts the criterion.",
      "phrases": [
        "Correlation is central to evaluating criterion-related validity,",
        " which includes both predictive and construct subtypes,",
        " and the correlation between test scores and criterion measures",
        " indicates how well the test predicts the criterion."
      ],
      "target_phrase_index": 1,
      "error_original": "predictive and construct subtypes",
      "error_correct": "predictive and concurrent subtypes",
      "explanation": "Criterion-related validity is divided into two subtypes: predictive validity and concurrent validity. The passage incorrectly substitutes 'construct' for 'concurrent.' Construct validity is a separate category of validity altogether, not a subtype of criterion-related validity."
    },
    {
      "id": "PMET-SC-0053",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Introduction to Criterion-Related Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0093",
      "modified_sentence": "Criterion-related validity is subdivided into two types based on when the criterion is measured: predictive validity involves simultaneous measurement of predictor and criterion, while concurrent validity involves simultaneous measurement of predictor and criterion.",
      "phrases": [
        "Criterion-related validity is subdivided into two types",
        " based on when the criterion is measured:",
        " predictive validity involves simultaneous measurement of predictor and criterion,",
        " while concurrent validity involves simultaneous measurement of predictor and criterion."
      ],
      "target_phrase_index": 2,
      "error_original": "predictive validity involves simultaneous measurement of predictor and criterion",
      "error_correct": "predictive validity involves a time interval between testing and criterion measurement",
      "explanation": "Predictive validity is characterized by a time interval between the administration of the predictor test and the collection of criterion data, not simultaneous measurement. Simultaneous measurement of predictor and criterion is the defining feature of concurrent validity. The error here swaps the defining characteristic of concurrent validity into the description of predictive validity."
    },
    {
      "id": "PMET-SC-0054",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Null and Alternative Hypotheses",
      "passage_type": "example",
      "source_passage_id": "PMET-0118",
      "modified_sentence": "The null hypothesis states there is no difference in depression scores between groups (μ₁ = μ₂), while the alternative hypothesis μ₁ < μ₂ represents a non-directional or two-tailed test, or μ₁ ≠ μ₂ represents a non-directional or two-tailed test.",
      "phrases": [
        "The null hypothesis states there is no difference in depression scores between groups (μ₁ = μ₂),",
        " while the alternative hypothesis μ₁ < μ₂ represents a non-directional or two-tailed test,",
        " or μ₁ ≠ μ₂ represents a non-directional or two-tailed test."
      ],
      "target_phrase_index": 1,
      "error_original": "μ₁ < μ₂ represents a non-directional or two-tailed test",
      "error_correct": "μ₁ < μ₂ represents a directional or one-tailed test",
      "explanation": "The alternative hypothesis μ₁ < μ₂ specifies a direction (the therapy group is expected to have lower scores), making it a directional or one-tailed test. A non-directional or two-tailed test would use μ₁ ≠ μ₂, which simply predicts a difference without specifying which group will score higher. The passage clearly states that μ₁ < μ₂ is directional/one-tailed."
    },
    {
      "id": "PMET-SC-0055",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Sampling Methods",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0280",
      "modified_sentence": "Probability sampling methods give every member of the population a known, nonzero chance of selection, supporting analytic generalization to the broader population from which the sample was drawn.",
      "phrases": [
        "Probability sampling methods give every member of the population a known,",
        " nonzero chance of selection,",
        " supporting analytic generalization to the broader population",
        " from which the sample was drawn."
      ],
      "target_phrase_index": 2,
      "error_original": "analytic generalization",
      "error_correct": "statistical generalization",
      "explanation": "Probability sampling supports statistical generalization, which involves using inferential statistics to generalize findings from a sample to a population. Analytic generalization is a different concept associated with qualitative and case study research, where theoretical reasoning rather than sampling logic is used to extend findings beyond the immediate study."
    },
    {
      "id": "PMET-0183",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Shaping",
      "passage_type": "paragraph",
      "original_passage": "Shaping , also called the method of successive approximations, is a procedure for teaching new behaviors by reinforcing responses that increasingly resemble the target behavior. The trainer reinforces behaviors that are progressively closer to the desired response while no longer reinforcing earlier approximations",
      "modified_passage": "Shaping , also called the method of successive approximations, is a procedure for teaching new behaviors by reinforcing responses that increasingly resemble the target behavior. The trainer reinforces behaviors that are progressively closer to the desired response while simultaneously reinforcing earlier approximations",
      "error_original": "simultaneously reinforcing earlier approximations",
      "error_correct": "no longer reinforcing earlier approximations",
      "options": [
        "The passage incorrectly states that shaping involves 'reinforcing responses'; it should say 'punishing responses'",
        "The passage incorrectly states 'simultaneously reinforcing earlier approximations'; it should say 'no longer reinforcing earlier approximations'",
        "The passage incorrectly calls shaping 'the method of successive approximations'; it should say 'the method of successive iterations'",
        "The passage incorrectly states that responses 'increasingly resemble' the target behavior; it should say 'gradually diverge from' the target behavior"
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to say the trainer 'simultaneously reinforces earlier approximations,' but the correct procedure in shaping is that the trainer 'no longer reinforces earlier approximations.' This process — ceasing reinforcement for previously acceptable but less accurate responses (extinction of earlier approximations) while reinforcing closer approximations — is a defining feature of shaping. Without withholding reinforcement for earlier approximations, the behavior would not be guided toward the target response."
    },
    {
      "id": "PMET-0414",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Blocking",
      "passage_type": "paragraph",
      "original_passage": "Then, CS1 and a new stimulus CS2 (e.g., a tone) are presented together and paired with the US",
      "modified_passage": "Then, CS1 and a new stimulus CS2 (e.g., a tone) are presented together and paired with the CR",
      "error_original": "paired with the CR",
      "error_correct": "paired with the US",
      "options": [
        "The passage incorrectly states that the stimuli are 'presented together'; they should be presented sequentially",
        "The passage incorrectly states 'CS2' as the new stimulus; it should say 'CS3'",
        "The passage incorrectly states the stimuli are paired with the CR; it should say they are paired with the US",
        "The passage incorrectly refers to a tone as an example of CS2; a tone is typically used as a US"
      ],
      "correct_option_index": 2,
      "explanation": "In the blocking paradigm, during the compound conditioning phase, CS1 and the new stimulus CS2 are presented together and paired with the unconditioned stimulus (US), not the conditioned response (CR). The US is the stimulus that naturally elicits the response, and it is the US that must be paired with the conditioned stimuli for learning to occur. The CR is the learned response that develops as a result of conditioning, not something that stimuli are paired with."
    },
    {
      "id": "PMET-0306",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Internal Validity",
      "passage_type": "paragraph",
      "original_passage": "History refers to external events occurring during the study that may affect the dependent variable. These events are not part of the treatment but happen between pretest and posttest measurements",
      "modified_passage": "History refers to external events occurring during the study that may affect the independent variable. These events are not part of the treatment but happen between pretest and posttest measurements.",
      "error_original": "independent variable",
      "error_correct": "dependent variable",
      "options": [
        "The passage incorrectly states 'between pretest and posttest measurements'; it should say 'before pretest measurements'",
        "The passage incorrectly states 'independent variable'; it should say 'dependent variable'",
        "The passage incorrectly states 'external events'; it should say 'internal events'",
        "The passage incorrectly states 'History'; it should say 'Maturation'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly states that history affects the 'independent variable.' History, as a threat to internal validity, refers to external events that may affect the dependent variable (the outcome being measured), not the independent variable (which is manipulated by the researcher). The independent variable is what the researcher controls; the dependent variable is what is measured for changes, and it is this variable that extraneous historical events can confound."
    },
    {
      "id": "PMET-0489",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "original_passage": "Negative Reinforcement: Commonly confused with punishment - remember it INCREASES behavior through stimulus removal",
      "modified_passage": "Negative Reinforcement: Commonly confused with punishment - remember it INCREASES behavior through stimulus addition",
      "error_original": "stimulus addition",
      "error_correct": "stimulus removal",
      "options": [
        "The passage incorrectly states 'INCREASES behavior'; it should say 'DECREASES behavior'",
        "The passage incorrectly states 'stimulus addition'; it should say 'stimulus removal'",
        "The passage incorrectly states 'Commonly confused with punishment'; it should say 'Commonly confused with extinction'",
        "The passage incorrectly states 'Negative Reinforcement'; it should say 'Negative Punishment'"
      ],
      "correct_option_index": 1,
      "explanation": "Negative reinforcement increases behavior by removing (not adding) an aversive stimulus. The defining feature of negative reinforcement is the removal or subtraction of a stimulus, which is what makes it 'negative.' Stimulus addition would describe positive reinforcement or positive punishment, depending on whether the behavior increases or decreases. The passage correctly states that negative reinforcement increases behavior and is commonly confused with punishment, so those are not errors."
    },
    {
      "id": "PMET-0150",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Eta-Squared (η²)",
      "passage_type": "paragraph",
      "original_passage": "Eta-squared represents the proportion of variance in the dependent variable explained by the independent variable(s). It is commonly used with ANOVA",
      "modified_passage": "Eta-squared represents the proportion of variance in the independent variable explained by the independent variable(s). It is commonly used with ANOVA",
      "error_original": "proportion of variance in the independent variable explained by the independent variable(s)",
      "error_correct": "proportion of variance in the dependent variable explained by the independent variable(s)",
      "options": [
        "The passage incorrectly states that eta-squared is used with ANOVA; it should say it is used with regression analysis",
        "The passage incorrectly states 'proportion of variance'; it should say 'sum of squares'",
        "The passage incorrectly states eta-squared represents variance in the independent variable; it should say the dependent variable",
        "The passage incorrectly states 'explained by the independent variable(s)'; it should say 'explained by the dependent variable(s)'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage erroneously states that eta-squared represents the proportion of variance in the 'independent variable' explained by the independent variable(s). This is incorrect — eta-squared (η²) represents the proportion of variance in the *dependent* variable that is explained or accounted for by the independent variable(s). The dependent variable is the outcome being measured, and eta-squared quantifies how much of its variability can be attributed to the grouping or treatment factor(s)."
    },
    {
      "id": "PMET-0258",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Factorial Designs",
      "passage_type": "paragraph",
      "original_passage": "Factorial designs manipulate two or more independent variables simultaneously, allowing researchers to examine both main effects and interaction effects. A 2 × 2 factorial design, for example, has two independent variables, each with two levels, creating four conditions",
      "modified_passage": "Factorial designs manipulate two or more independent variables simultaneously, allowing researchers to examine both main effects and interaction effects. A 2 × 2 factorial design, for example, has two independent variables, each with two levels, creating six conditions",
      "error_original": "six conditions",
      "error_correct": "four conditions",
      "options": [
        "The passage incorrectly states 'main effects and interaction effects'; it should say 'main effects and mediating effects'",
        "The passage incorrectly states 'two or more independent variables'; it should say 'two or more dependent variables'",
        "The passage incorrectly states 'six conditions'; it should say 'four conditions'",
        "The passage incorrectly states 'each with two levels'; it should say 'each with three levels'"
      ],
      "correct_option_index": 2,
      "explanation": "A 2 × 2 factorial design crosses two independent variables, each with two levels, producing 2 × 2 = 4 unique conditions. The passage incorrectly states that this design creates 'six conditions' when it should say 'four conditions.' The other options describe errors that were not introduced: the passage correctly refers to main effects and interaction effects, correctly states two or more independent variables, and correctly describes each variable as having two levels."
    },
    {
      "id": "PMET-0326",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Construct Validity",
      "passage_type": "paragraph",
      "original_passage": "Participants guess what the study is about and alter their behavior accordingly. Demand characteristics lead participants to behave as they think the researcher expects",
      "modified_passage": "Participants guess what the study is about and alter their behavior accordingly. Demand characteristics lead participants to behave in ways opposite to what they think the researcher expects.",
      "error_original": "behave in ways opposite to what they think the researcher expects",
      "error_correct": "behave as they think the researcher expects",
      "options": [
        "The passage incorrectly states that demand characteristics involve participants guessing what the study is about; it should say that experimenters unconsciously communicate expectations",
        "The passage incorrectly states that participants alter their behavior; it should say that participants alter their physiological responses",
        "The passage incorrectly states that participants behave in ways opposite to what they think the researcher expects; it should say they behave as they think the researcher expects",
        "The passage incorrectly refers to demand characteristics; it should refer to the Hawthorne effect"
      ],
      "correct_option_index": 2,
      "explanation": "The error introduced changes the direction of participant behavior in response to demand characteristics. Demand characteristics lead participants to behave *as* (i.e., in alignment with) what they think the researcher expects, not in ways opposite to those expectations. Behaving in the opposite direction would describe a different phenomenon sometimes called the 'screw-you effect' or reactance, not the standard definition of demand characteristics."
    },
    {
      "id": "PMET-0540",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "original_passage": "Grounded theory generates theory from data (Glaser & Strauss); uses open, axial, and selective coding; theoretical sampling",
      "modified_passage": "Grounded theory generates theory from data (Glaser & Strauss); uses open, axial, and selective coding; purposive sampling",
      "error_original": "purposive sampling",
      "error_correct": "theoretical sampling",
      "options": [
        "The passage incorrectly states 'open, axial, and selective coding'; it should say 'open, focused, and thematic coding'",
        "The passage incorrectly states 'Glaser & Strauss'; it should say 'Corbin & Strauss'",
        "The passage incorrectly states 'purposive sampling'; it should say 'theoretical sampling'",
        "The passage incorrectly states 'generates theory from data'; it should say 'tests theory against data'"
      ],
      "correct_option_index": 2,
      "explanation": "Grounded theory is specifically associated with 'theoretical sampling,' a process in which data collection is guided by emerging theory — the researcher decides what data to collect next based on the concepts and categories that are developing during analysis. The passage incorrectly substituted 'purposive sampling,' which is a broader qualitative research term referring to deliberately selecting participants based on predetermined criteria. While related, purposive sampling is not the distinctive sampling method of grounded theory. The correct term, as originally stated by Glaser and Strauss, is 'theoretical sampling.'"
    },
    {
      "id": "PMET-0562",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "CIPP Model (Stufflebeam)",
      "passage_type": "paragraph",
      "original_passage": "Daniel Stufflebeam's CIPP Model is a comprehensive framework with four components:",
      "modified_passage": "Daniel Stufflebeam's CIPP Model is a comprehensive framework with four components: Context evaluation assesses needs, problems, and opportunities within the environment. Input evaluation examines available resources, strategies, and procedural designs. Process evaluation monitors implementation and identifies procedural barriers. Product evaluation measures and interprets outcomes. The acronym CIPP stands for Context, Input, Process, and Product. Stufflebeam designed this model primarily for summative evaluation purposes, emphasizing that evaluation should be used to improve programs rather than merely to prove their worth. The model is widely used in educational program evaluation and follows a systems-based approach to decision-making.",
      "error_original": "summative evaluation purposes",
      "error_correct": "formative evaluation purposes",
      "options": [
        "The passage incorrectly states that Process evaluation 'monitors implementation'; it should say 'examines available resources'",
        "The passage incorrectly states 'summative evaluation purposes'; it should say 'formative evaluation purposes'",
        "The passage incorrectly states that Input evaluation 'examines available resources, strategies, and procedural designs'; it should say 'assesses needs, problems, and opportunities'",
        "The passage incorrectly states that the model 'follows a systems-based approach'; it should say 'follows a goal-based approach'"
      ],
      "correct_option_index": 1,
      "explanation": "Stufflebeam's CIPP Model was designed primarily for formative evaluation purposes, not summative. His famous guiding principle was that 'the most important purpose of evaluation is not to prove but to improve,' which clearly reflects a formative orientation. While the model can be used for summative evaluation as well, its primary emphasis and design purpose is formative — helping decision-makers improve programs during their development and implementation. The passage contradicts itself by stating the model is for 'summative evaluation purposes' and then immediately noting it emphasizes improvement over proof, which is a hallmark of formative evaluation."
    },
    {
      "id": "PMET-0212",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Item Characteristic Curves (ICCs)",
      "passage_type": "example",
      "original_passage": "Reading an ICC An item with b = 0.0 is of average difficulty (50% correct at average ability). If a = 1.5, the curve is moderately steep, meaning the item differentiates well. If c = 0.20, even very low-ability examinees have a 20% chance of guessing correctly, so the curve never drops below 0.20 on the y-axis",
      "modified_passage": "Reading an ICC An item with b = 0.0 is of average difficulty (50% correct at average ability). If a = 1.5, the curve is moderately steep, meaning the item differentiates well. If c = 0.20, even very low-ability examinees have a 20% chance of guessing correctly, so the curve never drops below 0.20 on the x-axis",
      "error_original": "x-axis",
      "error_correct": "y-axis",
      "options": [
        "The passage incorrectly states that b = 0.0 represents average difficulty; it should represent high difficulty",
        "The passage incorrectly states that a = 1.5 makes the curve moderately steep; it should make the curve relatively flat",
        "The passage incorrectly states the curve never drops below 0.20 on the x-axis; it should say the y-axis",
        "The passage incorrectly states that c = 0.20 gives a 20% chance of guessing correctly; it should give a 25% chance"
      ],
      "correct_option_index": 2,
      "explanation": "In Item Characteristic Curves (ICCs), the y-axis represents the probability of a correct response. The pseudo-guessing parameter (c) sets a lower asymptote on the probability scale, which is the y-axis, not the x-axis. The x-axis represents the ability (theta) continuum. The passage was modified to incorrectly say 'x-axis' when it should say 'y-axis.'"
    },
    {
      "id": "PMET-0210",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Internal Consistency Reliability",
      "passage_type": "clinical_note",
      "original_passage": "Limitations of Internal Consistency May be inappropriately high for speeded tests (items are not independently answered) Does not assess stability over time May be artificially inflated by redundant items Not appropriate for multidimensional tests (should be calculated separately for each subscale)",
      "modified_passage": "Limitations of Internal Consistency May be inappropriately high for speeded tests (items are not independently answered) Does not assess stability over time May be artificially deflated by redundant items Not appropriate for multidimensional tests (should be calculated separately for each subscale)",
      "error_original": "artificially deflated",
      "error_correct": "artificially inflated",
      "options": [
        "The passage incorrectly states that internal consistency is inappropriately high for speeded tests; it should say inappropriately low",
        "The passage incorrectly states that internal consistency may be artificially deflated by redundant items; it should say artificially inflated",
        "The passage incorrectly states it should be calculated separately for each subscale; it should say calculated across all subscales combined",
        "The passage incorrectly states that internal consistency does not assess stability over time; it should say it does not assess equivalence across forms"
      ],
      "correct_option_index": 1,
      "explanation": "The original passage correctly states that internal consistency may be 'artificially inflated' by redundant items, because items that are nearly identical will correlate highly with each other, spuriously boosting reliability coefficients like Cronbach's alpha. The modified passage changed 'inflated' to 'deflated,' which reverses the actual relationship. Redundant items increase (inflate), not decrease (deflate), internal consistency estimates."
    },
    {
      "id": "PMET-0155",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Four Basic Schedules",
      "passage_type": "definition",
      "original_passage": "Key Points: Schedules of Reinforcement Ratio schedules produce higher response rates than interval schedules (ratio schedules depend on response rate). Variable schedules produce more consistent responding and greater resistance to extinction than fixed schedules. Variable ratio (VR) produces the highest response rate and greatest resistance to extinction",
      "modified_passage": "Key Points: Schedules of Reinforcement Ratio schedules produce higher response rates than interval schedules (ratio schedules depend on response rate). Variable schedules produce more consistent responding and greater resistance to extinction than fixed schedules. Variable interval (VI) produces the highest response rate and greatest resistance to extinction",
      "error_original": "Variable interval (VI)",
      "error_correct": "Variable ratio (VR)",
      "options": [
        "The passage incorrectly states that ratio schedules produce higher response rates than interval schedules; it should say interval schedules produce higher response rates than ratio schedules",
        "The passage incorrectly states that variable schedules produce greater resistance to extinction than fixed schedules; it should say fixed schedules produce greater resistance to extinction",
        "The passage incorrectly states that Variable interval (VI) produces the highest response rate and greatest resistance to extinction; it should say Variable ratio (VR)",
        "The passage incorrectly states that ratio schedules depend on response rate; it should say ratio schedules depend on elapsed time"
      ],
      "correct_option_index": 2,
      "explanation": "The passage was modified to say 'Variable interval (VI)' produces the highest response rate and greatest resistance to extinction, when in fact it is the Variable ratio (VR) schedule that holds this distinction. VR schedules produce the highest and most consistent response rates because reinforcement is directly tied to the number of responses emitted, and the unpredictable nature of the ratio requirement keeps the organism responding at a high, steady rate. Variable interval schedules, while producing steady responding, do not generate the same high response rates as VR schedules."
    },
    {
      "id": "PMET-0469",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "original_passage": "Power: 1 - β; probability of correctly rejecting false H₀; typically .80 desired",
      "modified_passage": "Power: 1 - β; probability of correctly rejecting false H₀; typically .90 desired",
      "error_original": ".90",
      "error_correct": ".80",
      "options": [
        "The passage incorrectly states that power is the probability of correctly rejecting a false H₀; it should say the probability of correctly retaining a true H₀",
        "The passage incorrectly states the desired power level as .90; it should say .80",
        "The passage incorrectly defines power as 1 - β; it should say 1 - α",
        "The passage incorrectly refers to H₀; it should refer to H₁"
      ],
      "correct_option_index": 1,
      "explanation": "The passage changed the conventionally desired power level from .80 to .90. In research methodology, the commonly accepted standard for adequate statistical power is .80 (80%), meaning there is an 80% probability of correctly rejecting a false null hypothesis. While higher power (such as .90) is sometimes desirable, the typically cited benchmark for the EPPP and in most research contexts is .80. The other options describe changes that were not actually made: power is correctly defined as 1 - β, it is correctly described as the probability of correctly rejecting a false H₀, and H₀ (the null hypothesis) is the correct reference."
    },
    {
      "id": "PMET-0116",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Multivariate Analysis of Covariance (MANCOVA)",
      "passage_type": "clinical_note",
      "original_passage": "When to Use Each Extension ANCOVA: One DV + need to control for a covariate (e.g., comparing treatment groups on post-test scores while controlling for pre-test) MANOVA: Multiple DVs + no covariates needed (e.g., comparing treatment groups on depression, anxiety, and functioning simultaneously) MANCOVA: Multiple DVs + need to control for a covariate (e.g., comparing treatment groups on multiple outcomes while controlling for baseline severity)",
      "modified_passage": "When to Use Each Extension ANCOVA: One DV + need to control for a covariate (e.g., comparing treatment groups on post-test scores while controlling for pre-test) MANOVA: Multiple DVs + no covariates needed (e.g., comparing treatment groups on depression, anxiety, and functioning simultaneously) MANCOVA: Multiple IVs + need to control for a covariate (e.g., comparing treatment groups on multiple outcomes while controlling for baseline severity)",
      "error_original": "Multiple IVs + need to control for a covariate",
      "error_correct": "Multiple DVs + need to control for a covariate",
      "options": [
        "The passage incorrectly states ANCOVA uses 'One DV'; it should say 'Multiple DVs'",
        "The passage incorrectly states MANOVA has 'no covariates needed'; it should say 'one covariate needed'",
        "The passage incorrectly states MANCOVA uses 'Multiple IVs'; it should say 'Multiple DVs'",
        "The passage incorrectly states ANCOVA controls for 'pre-test'; it should say 'post-test'"
      ],
      "correct_option_index": 2,
      "explanation": "The error introduced was changing 'Multiple DVs' to 'Multiple IVs' in the MANCOVA description. MANCOVA (Multivariate Analysis of Covariance) is characterized by having multiple dependent variables (DVs) while also controlling for one or more covariates. The passage correctly describes MANOVA as involving multiple DVs, so the distinguishing feature of MANCOVA compared to MANOVA is the addition of covariate control — not a change from DVs to IVs. The correct description should read 'Multiple DVs + need to control for a covariate.'"
    },
    {
      "id": "PMET-0311",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Internal Validity",
      "passage_type": "paragraph",
      "original_passage": "Selection occurs when groups differ systematically before treatment. This is a major threat in non-randomized studies. If the treatment group differs from the comparison group on relevant characteristics, any observed differences could be due to pre-existing differences rather than treatment",
      "modified_passage": "Selection occurs when groups differ systematically before treatment. This is a major threat in randomized studies. If the treatment group differs from the comparison group on relevant characteristics, any observed differences could be due to pre-existing differences rather than treatment.",
      "error_original": "randomized studies",
      "error_correct": "non-randomized studies",
      "options": [
        "The passage incorrectly states that selection involves groups differing 'systematically'; it should say 'randomly'",
        "The passage incorrectly states that observed differences could be due to 'pre-existing differences'; it should say 'maturation effects'",
        "The passage incorrectly states this is a major threat in 'randomized studies'; it should say 'non-randomized studies'",
        "The passage incorrectly states that the treatment group differs on 'relevant characteristics'; it should say 'irrelevant characteristics'"
      ],
      "correct_option_index": 2,
      "explanation": "The error introduced was changing 'non-randomized studies' to 'randomized studies.' Selection bias is a major threat to internal validity specifically in non-randomized (quasi-experimental) studies, because without random assignment, pre-existing group differences cannot be controlled for. Randomization is actually the primary safeguard against selection bias, so stating that selection is a major threat in randomized studies is incorrect."
    },
    {
      "id": "PMET-0119",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "One-Sample t-Test",
      "passage_type": "example",
      "original_passage": "Example: Choosing a t-Test Scenario: A researcher measures anxiety before and after a relaxation intervention in the same 30 participants. Appropriate test: Paired samples t-test, because the same participants are measured twice (within-subjects design). Scenario: A researcher compares anxiety scores between 30 participants who received treatment and 30 different participants who received placebo. Appropriate test: Independent samples t-test, because different participants are in each group (between-subjects design)",
      "modified_passage": "Example: Choosing a t-Test Scenario: A researcher measures anxiety before and after a relaxation intervention in the same 30 participants. Appropriate test: Paired samples t-test, because the same participants are measured twice (within-subjects design). Scenario: A researcher compares anxiety scores between 30 participants who received treatment and 30 different participants who received placebo. Appropriate test: Independent samples t-test, because different participants are in each group (within-subjects design)",
      "error_original": "within-subjects design",
      "error_correct": "between-subjects design",
      "options": [
        "The passage incorrectly states the paired samples t-test is for 'the same participants measured twice'; it should say 'different participants measured once'",
        "The passage incorrectly states the second scenario uses an 'Independent samples t-test'; it should say 'Paired samples t-test'",
        "The passage incorrectly describes the independent samples t-test scenario as a 'within-subjects design'; it should say 'between-subjects design'",
        "The passage incorrectly states the first scenario involves '30 participants'; it should say '60 participants'"
      ],
      "correct_option_index": 2,
      "explanation": "The error was introduced in the final parenthetical of the passage. The independent samples t-test scenario — comparing 30 treatment participants to 30 different placebo participants — is a between-subjects design, because different participants are in each group. The passage incorrectly labels this as a 'within-subjects design.' Within-subjects design refers to the same participants being measured under different conditions, which applies to the paired samples t-test scenario described earlier in the passage."
    },
    {
      "id": "PMET-0503",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Advantages of IRT",
      "passage_type": "paragraph",
      "original_passage": "Conditional standard error: IRT provides unique precision estimates at each ability level, rather than a single SEM for all examinees",
      "modified_passage": "Conditional standard error: IRT provides unique precision estimates at each difficulty level, rather than a single SEM for all examinees",
      "error_original": "difficulty level",
      "error_correct": "ability level",
      "options": [
        "The passage incorrectly states 'single SEM'; it should say 'single reliability coefficient'",
        "The passage incorrectly states 'difficulty level'; it should say 'ability level'",
        "The passage incorrectly states 'unique precision estimates'; it should say 'uniform precision estimates'",
        "The passage incorrectly states 'IRT'; it should say 'CTT'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly refers to 'difficulty level' when it should say 'ability level.' A key advantage of Item Response Theory (IRT) is that it provides conditional standard errors of measurement at each ability level (i.e., theta level) of the examinee, rather than a single overall SEM as in Classical Test Theory. The precision estimates are conditional on the examinee's ability, not on item difficulty."
    },
    {
      "id": "PMET-0023",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Experimental Neurosis",
      "passage_type": "paragraph",
      "original_passage": "This phenomenon has been considered a model for understanding how chronic stress and unpredictable environments may contribute to anxiety and other psychological disorders. When organisms cannot reliably predict safety versus threat, chronic arousal and maladaptive behaviors may emerge",
      "modified_passage": "This phenomenon has been considered a model for understanding how acute stress and unpredictable environments may contribute to anxiety and other psychological disorders. When organisms cannot reliably predict safety versus threat, chronic arousal and maladaptive behaviors may emerge",
      "error_original": "acute stress",
      "error_correct": "chronic stress",
      "options": [
        "The passage incorrectly states 'maladaptive behaviors'; it should say 'adaptive behaviors'",
        "The passage incorrectly states 'acute stress'; it should say 'chronic stress'",
        "The passage incorrectly states 'anxiety'; it should say 'depression'",
        "The passage incorrectly states 'unpredictable environments'; it should say 'predictable environments'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to say 'acute stress' instead of 'chronic stress.' Experimental neurosis is specifically a model for understanding how chronic (not acute) stress and unpredictable environments contribute to psychological disorders. The distinction is important because the phenomenon involves prolonged, ongoing difficulty in discriminating between stimuli, which parallels chronic rather than acute stress conditions. Acute stress refers to short-term stress responses, while chronic stress involves sustained, long-term exposure — the latter being what experimental neurosis models."
    },
    {
      "id": "PMET-0203",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Item Response Theory (IRT)",
      "passage_type": "definition",
      "original_passage": "CTT vs. IRT: Key Contrasts Unit of analysis: CTT analyzes total test scores; IRT analyzes individual items Sample dependence: CTT item statistics (difficulty, discrimination) depend on the sample tested; IRT item parameters are sample-independent (invariant) once the model fits Test dependence: CTT ability estimates depend on which test was given; IRT ability estimates are test-independent within a calibrated item bank Measurement precision: CTT assumes equal SEM for all examinees; IRT provides a unique standard error for each ability level",
      "modified_passage": "CTT vs. IRT: Key Contrasts Unit of analysis: CTT analyzes total test scores; IRT analyzes individual items Sample dependence: CTT item statistics (difficulty, discrimination) depend on the sample tested; IRT item parameters are sample-independent (invariant) once the model fits Test dependence: CTT ability estimates depend on which test was given; IRT ability estimates are test-independent within a calibrated item bank Measurement precision: CTT assumes equal SEM for all examinees; IRT provides a unique standard error for each difficulty level",
      "error_original": "each difficulty level",
      "error_correct": "each ability level",
      "options": [
        "The passage incorrectly states that CTT analyzes total test scores; it should say CTT analyzes individual items",
        "The passage incorrectly states that CTT item statistics depend on the sample tested; it should say they are sample-independent",
        "The passage incorrectly states that IRT provides a unique standard error for each difficulty level; it should say each ability level",
        "The passage incorrectly states that IRT item parameters are sample-independent; it should say they are sample-dependent"
      ],
      "correct_option_index": 2,
      "explanation": "The passage was modified to say IRT provides a unique standard error for each 'difficulty level,' but the correct term is 'ability level.' A key advantage of IRT over CTT is that it provides a unique standard error of measurement for each examinee's ability level (theta), rather than assuming equal measurement precision across all examinees as CTT does. 'Difficulty level' refers to an item parameter, not the examinee characteristic along which measurement precision varies."
    },
    {
      "id": "PMET-0180",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Four Basic Schedules",
      "passage_type": "paragraph",
      "original_passage": "Reinforcement is delivered after a variable number of responses, averaging to a specified number. VR schedules produce the highest and most consistent response rates with no post-reinforcement pause. Gambling operates on a VR schedule, which explains its highly addictive nature",
      "modified_passage": "Reinforcement is delivered after a variable number of responses, averaging to a specified number. VR schedules produce the highest and most consistent response rates with no post-reinforcement pause. Gambling operates on a VI schedule, which explains its highly addictive nature.",
      "error_original": "VI schedule",
      "error_correct": "VR schedule",
      "options": [
        "The passage incorrectly states that VR schedules produce the highest response rates; it should say FR schedules produce the highest response rates.",
        "The passage incorrectly states that gambling operates on a VI schedule; it should say it operates on a VR schedule.",
        "The passage incorrectly states there is no post-reinforcement pause; it should say there is a brief post-reinforcement pause.",
        "The passage incorrectly states reinforcement is delivered after a variable number of responses; it should say after a variable interval of time."
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to say gambling operates on a 'VI schedule' (variable-interval), but gambling is a classic example of a VR (variable-ratio) schedule. In a VR schedule, reinforcement depends on the number of responses emitted, which matches gambling behavior where each bet (response) has a chance of paying off. A VI schedule delivers reinforcement based on the passage of time, not the number of responses, making it an incorrect characterization of gambling."
    },
    {
      "id": "PMET-0256",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Posttest-Only Control Group Design",
      "passage_type": "paragraph",
      "original_passage": "When pretesting may sensitize participants to the treatment or when random assignment is expected to produce equivalent groups, researchers may use a posttest-only control group design . This design omits the pretest, relying on random assignment to equate groups prior to treatment",
      "modified_passage": "When pretesting may sensitize participants to the treatment or when random assignment is expected to produce equivalent groups, researchers may use a posttest-only control group design. This design omits the pretest, relying on matched assignment to equate groups prior to treatment.",
      "error_original": "matched assignment",
      "error_correct": "random assignment",
      "options": [
        "The passage incorrectly states 'posttest-only control group design'; it should say 'pretest-posttest control group design'.",
        "The passage incorrectly states 'sensitize participants to the treatment'; it should say 'habituate participants to the treatment'.",
        "The passage incorrectly states 'matched assignment'; it should say 'random assignment'.",
        "The passage incorrectly states 'omits the pretest'; it should say 'omits the posttest'."
      ],
      "correct_option_index": 2,
      "explanation": "The passage was modified to say 'matched assignment' instead of 'random assignment.' In a posttest-only control group design, the pretest is omitted and the design relies on random assignment—not matched assignment—to equate groups prior to the introduction of the treatment. Random assignment is the key mechanism that allows researchers to assume group equivalence without needing a pretest."
    },
    {
      "id": "PMET-0295",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Introduction to Research Validity",
      "passage_type": "definition",
      "original_passage": "Four Types of Research Validity Internal Validity: Did the treatment (IV) actually cause the observed effect (DV)? External Validity: Can the findings be generalized to other people, settings, and times? Construct Validity: Are we measuring the constructs we think we're measuring? Statistical Conclusion Validity: Are our statistical conclusions accurate?",
      "modified_passage": "Four Types of Research Validity Internal Validity: Did the treatment (IV) actually cause the observed effect (DV)? External Validity: Can the findings be generalized to other people, settings, and times? Construct Validity: Are we measuring the constructs we think we're measuring? Statistical Conclusion Validity: Are our statistical procedures reliable?",
      "error_original": "Are our statistical procedures reliable?",
      "error_correct": "Are our statistical conclusions accurate?",
      "options": [
        "The passage incorrectly states that Internal Validity asks whether the IV caused the effect on the DV; it should ask whether the DV caused the effect on the IV",
        "The passage incorrectly states that External Validity involves generalization to other people, settings, and times; it should only refer to other people and settings",
        "The passage incorrectly defines Statistical Conclusion Validity as 'Are our statistical procedures reliable?'; it should say 'Are our statistical conclusions accurate?'",
        "The passage incorrectly defines Construct Validity as 'Are we measuring the constructs we think we're measuring?'; it should say 'Are we operationalizing the variables we think we're operationalizing?'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage altered the definition of Statistical Conclusion Validity from 'Are our statistical conclusions accurate?' to 'Are our statistical procedures reliable?' Statistical Conclusion Validity specifically concerns whether the statistical conclusions drawn from the data are accurate (e.g., correctly identifying whether a relationship or effect exists and estimating its magnitude), not merely whether the procedures themselves are reliable. While reliability of procedures may contribute to this form of validity, the core question is about the accuracy of the statistical conclusions."
    },
    {
      "id": "PMET-0308",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Internal Validity",
      "passage_type": "paragraph",
      "original_passage": "Testing (also called practice effects) occurs when taking a pretest affects performance on a posttest. Participants may remember items, become test-wise, or become sensitized to what the researcher is measuring",
      "modified_passage": "Testing (also called practice effects) occurs when taking a pretest affects performance on a posttest. Participants may remember items, become test-wise, or become desensitized to what the researcher is measuring.",
      "error_original": "desensitized",
      "error_correct": "sensitized",
      "options": [
        "The passage incorrectly states 'pretest affects performance on a posttest'; it should say 'posttest affects performance on a pretest'",
        "The passage incorrectly states 'desensitized'; it should say 'sensitized'",
        "The passage incorrectly states 'practice effects'; it should say 'order effects'",
        "The passage incorrectly states 'become test-wise'; it should say 'become test-anxious'"
      ],
      "correct_option_index": 1,
      "explanation": "The original passage correctly states that participants may become 'sensitized' to what the researcher is measuring. The modified passage changed this to 'desensitized,' which reverses the meaning. The testing threat to internal validity involves participants becoming more aware of (sensitized to) what is being measured after taking a pretest, not less aware (desensitized). This increased awareness can then influence their responses on the posttest."
    },
    {
      "id": "PMET-0108",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Decision Theory and Cutoff Scores",
      "passage_type": "paragraph",
      "original_passage": "When tests are used to make categorical decisions (select/reject, diagnose/not diagnose), a cutoff score must be established. The choice of cutoff involves trade-offs between different types of errors",
      "modified_passage": "When tests are used to make categorical decisions (select/reject, diagnose/not diagnose), a cutoff score must be established. The choice of cutoff involves trade-offs between different types of validity.",
      "error_original": "different types of validity",
      "error_correct": "different types of errors",
      "options": [
        "The passage incorrectly states 'categorical decisions'; it should say 'continuous decisions'",
        "The passage incorrectly states 'cutoff score'; it should say 'confidence interval'",
        "The passage incorrectly states 'different types of validity'; it should say 'different types of errors'",
        "The passage incorrectly states 'diagnose/not diagnose'; it should say 'treat/not treat'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage was modified to say the choice of cutoff involves trade-offs between 'different types of validity,' when it should say 'different types of errors.' In decision theory related to criterion-related validity, establishing a cutoff score involves balancing different types of decision errors—specifically, false positives (selecting those who should be rejected) and false negatives (rejecting those who should be selected). The trade-off is between these error types, not between types of validity."
    },
    {
      "id": "PMET-0017",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Higher-Order Conditioning",
      "passage_type": "paragraph",
      "original_passage": "Higher-order conditioning is generally weaker than first-order conditioning and is susceptible to extinction. However, it provides a mechanism for explaining how complex associative networks develop and how fears can spread to stimuli never directly associated with trauma (Gewirtz & Davis, 2000)",
      "modified_passage": "Higher-order conditioning is generally weaker than first-order conditioning and is susceptible to extinction. However, it provides a mechanism for explaining how complex associative networks develop and how fears can spread to stimuli never directly associated with trauma (Rescorla & Davis, 2000)",
      "error_original": "Rescorla & Davis, 2000",
      "error_correct": "Gewirtz & Davis, 2000",
      "options": [
        "The passage incorrectly states that higher-order conditioning is 'weaker' than first-order conditioning; it should say 'stronger'",
        "The passage incorrectly states the citation as 'Rescorla & Davis, 2000'; it should say 'Gewirtz & Davis, 2000'",
        "The passage incorrectly states that higher-order conditioning is 'susceptible to extinction'; it should say 'resistant to extinction'",
        "The passage incorrectly states that fears can spread to stimuli 'never directly associated with trauma'; it should say 'previously associated with trauma'"
      ],
      "correct_option_index": 1,
      "explanation": "The original passage correctly attributes the citation to Gewirtz & Davis (2000). The modified passage substituted 'Rescorla' for 'Gewirtz,' creating a plausible-sounding but incorrect author name swap. Rescorla is a well-known name in classical conditioning research, making this a subtle but verifiable error. The other options describe changes that were not actually made to the passage — higher-order conditioning is indeed generally weaker than first-order conditioning, is susceptible to extinction, and the passage correctly describes fears spreading to stimuli never directly associated with trauma."
    },
    {
      "id": "PMET-0313",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Internal Validity",
      "passage_type": "paragraph",
      "original_passage": "Selection can interact with other threats. Selection-maturation occurs when groups mature at different rates. Selection-history occurs when groups are differentially affected by external events",
      "modified_passage": "Selection can interact with other threats. Selection-maturation occurs when groups mature at different rates. Selection-history occurs when groups are differentially affected by internal events.",
      "error_original": "internal events",
      "error_correct": "external events",
      "options": [
        "The passage incorrectly states 'mature at different rates'; it should say 'mature at the same rate'.",
        "The passage incorrectly states 'internal events'; it should say 'external events'.",
        "The passage incorrectly states 'Selection-maturation'; it should say 'Selection-regression'.",
        "The passage incorrectly states 'interact with other threats'; it should say 'compound with other biases'."
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to say that selection-history occurs when groups are differentially affected by 'internal events,' but the correct term is 'external events.' Selection-history is a threat to internal validity that occurs when an external event (history) differentially impacts one group compared to another. The word 'internal' was substituted for 'external,' representing a concept reversal error."
    },
    {
      "id": "PMET-0156",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Primary and Secondary Reinforcers",
      "passage_type": "clinical_note",
      "original_passage": "Clinical Application: Token Economies Token economies are clinical interventions based on secondary reinforcement. Patients earn tokens (secondary reinforcers) for target behaviors, which can be exchanged for backup reinforcers (items or privileges). Token economies have demonstrated effectiveness in psychiatric hospitals, classrooms, and residential treatment settings",
      "modified_passage": "Clinical Application: Token Economies Token economies are clinical interventions based on primary reinforcement. Patients earn tokens (secondary reinforcers) for target behaviors, which can be exchanged for backup reinforcers (items or privileges). Token economies have demonstrated effectiveness in psychiatric hospitals, classrooms, and residential treatment settings",
      "error_original": "primary reinforcement",
      "error_correct": "secondary reinforcement",
      "options": [
        "The passage incorrectly states that tokens are 'secondary reinforcers'; they should be called 'primary reinforcers'",
        "The passage incorrectly states that token economies are based on 'primary reinforcement'; it should say 'secondary reinforcement'",
        "The passage incorrectly states that items or privileges are 'backup reinforcers'; they should be called 'conditioned reinforcers'",
        "The passage incorrectly states that token economies have demonstrated effectiveness in 'residential treatment settings'; it should say 'outpatient treatment settings'"
      ],
      "correct_option_index": 1,
      "explanation": "Token economies are fundamentally based on secondary (conditioned) reinforcement, not primary reinforcement. Tokens themselves have no inherent biological value — they acquire reinforcing properties through their association with backup reinforcers (which may include primary reinforcers like food). The passage was modified to incorrectly state 'primary reinforcement' when it should say 'secondary reinforcement.' The other options describe changes that were not actually made in the passage."
    },
    {
      "id": "PMET-0511",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "original_passage": "IRT models: 1PL/Rasch (difficulty only), 2PL (+ discrimination), 3PL (+ guessing)",
      "modified_passage": "IRT models: 1PL/Rasch (difficulty only), 2PL (+ discrimination), 3PL (+ reliability)",
      "error_original": "reliability",
      "error_correct": "guessing",
      "options": [
        "The passage incorrectly states that the 1PL model accounts for difficulty only; it should say it accounts for discrimination only",
        "The passage incorrectly states that the 2PL model adds discrimination; it should say it adds guessing",
        "The passage incorrectly states that the 3PL model adds reliability; it should say it adds guessing",
        "The passage incorrectly refers to the 1PL model as the Rasch model; it should say it is the Guttman model"
      ],
      "correct_option_index": 2,
      "explanation": "The 3-parameter logistic (3PL) IRT model adds a guessing (pseudo-chance) parameter to the difficulty and discrimination parameters of the 2PL model. The passage incorrectly substituted 'reliability' for 'guessing.' Reliability is a broader psychometric concept related to consistency of measurement and is not one of the three item parameters in IRT. The correct progression is: 1PL/Rasch (difficulty), 2PL (+ discrimination), 3PL (+ guessing)."
    },
    {
      "id": "PMET-0272",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Survey Research",
      "passage_type": "paragraph",
      "original_passage": "Survey research uses questionnaires or interviews to collect data from a sample, typically to describe characteristics of a population or examine relationships between variables. Representative sampling is critical for generalizing survey results to the population of interest",
      "modified_passage": "Survey research uses questionnaires or interviews to collect data from a sample, typically to describe characteristics of a population or examine causal effects between variables. Representative sampling is critical for generalizing survey results to the population of interest.",
      "error_original": "causal effects",
      "error_correct": "relationships",
      "options": [
        "The passage incorrectly states 'questionnaires or interviews'; it should say 'questionnaires or observations'",
        "The passage incorrectly states 'Representative sampling'; it should say 'Convenience sampling'",
        "The passage incorrectly states 'causal effects'; it should say 'relationships'",
        "The passage incorrectly states 'population of interest'; it should say 'target sample'"
      ],
      "correct_option_index": 2,
      "explanation": "The original passage states that survey research is used to 'examine relationships between variables,' not 'causal effects between variables.' Survey research is a non-experimental, descriptive method that identifies correlations and associations between variables but cannot establish causation. Determining causal effects requires experimental designs with manipulation of independent variables and control of confounding variables. The substitution of 'causal effects' for 'relationships' is a subtle but important conceptual error, as it misrepresents the inferential power of survey methodology."
    },
    {
      "id": "PMET-0486",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Functional Behavior Assessment",
      "passage_type": "paragraph",
      "original_passage": "Automatic reinforcement: Behavior maintained by sensory consequences independent of social mediation",
      "modified_passage": "Automatic reinforcement: Behavior maintained by sensory consequences dependent on social mediation",
      "error_original": "dependent on social mediation",
      "error_correct": "independent of social mediation",
      "options": [
        "The passage incorrectly states 'sensory consequences'; it should say 'cognitive consequences'",
        "The passage incorrectly states 'dependent on social mediation'; it should say 'independent of social mediation'",
        "The passage incorrectly states 'Automatic reinforcement'; it should say 'Automatic punishment'",
        "The passage incorrectly states 'Behavior maintained'; it should say 'Behavior elicited'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly states that automatic reinforcement involves behavior maintained by sensory consequences 'dependent on social mediation.' In fact, automatic reinforcement is defined as behavior maintained by sensory consequences that are 'independent of social mediation.' This is a key distinction in Functional Behavior Assessment — automatic reinforcement occurs without the involvement of another person delivering the reinforcer; the sensory stimulation produced by the behavior itself serves as the reinforcer."
    },
    {
      "id": "PMET-0244",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Nonequivalent Control Group Design",
      "passage_type": "clinical_note",
      "original_passage": "Clinical Considerations In clinical settings, quasi-experimental designs are often the most feasible option. For example, comparing outcomes between patients who choose medication versus therapy cannot use random assignment ethically if patients have strong preferences. Careful matching and statistical controls can strengthen causal inferences in such studies",
      "modified_passage": "Clinical Considerations In clinical settings, quasi-experimental designs are often the most feasible option. For example, comparing outcomes between patients who choose medication versus therapy cannot use random assignment ethically if patients have strong preferences. Careful matching and statistical controls can strengthen external validity in such studies",
      "error_original": "external validity",
      "error_correct": "causal inferences",
      "options": [
        "The passage incorrectly states 'quasi-experimental designs'; it should say 'true experimental designs'",
        "The passage incorrectly states 'external validity'; it should say 'causal inferences'",
        "The passage incorrectly states 'random assignment'; it should say 'random selection'",
        "The passage incorrectly states 'matching and statistical controls'; it should say 'counterbalancing and statistical controls'"
      ],
      "correct_option_index": 1,
      "explanation": "The original passage states that careful matching and statistical controls can strengthen 'causal inferences' in quasi-experimental studies. The modified passage incorrectly changed this to 'external validity.' While external validity (generalizability of findings) is important, the specific advantage of matching and statistical controls in quasi-experimental designs like the nonequivalent control group design is that they help strengthen causal inferences by reducing the impact of confounding variables — compensating for the lack of random assignment."
    },
    {
      "id": "PMET-0012",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Extinction and Spontaneous Recovery",
      "passage_type": "paragraph",
      "original_passage": "Extinction occurs when the conditioned stimulus is repeatedly presented without the unconditioned stimulus. Over time, the conditioned response weakens and eventually ceases. However, extinction does not erase the original learning; rather, it represents new inhibitory learning that suppresses the conditioned response",
      "modified_passage": "Extinction occurs when the conditioned stimulus is repeatedly presented without the unconditioned stimulus. Over time, the conditioned response weakens and eventually ceases. However, extinction does not erase the original learning; rather, it represents new excitatory learning that suppresses the conditioned response.",
      "error_original": "excitatory learning",
      "error_correct": "inhibitory learning",
      "options": [
        "The passage incorrectly states 'conditioned stimulus'; it should say 'unconditioned stimulus' in the first sentence",
        "The passage incorrectly states 'weakens'; it should say 'strengthens'",
        "The passage incorrectly states 'excitatory learning'; it should say 'inhibitory learning'",
        "The passage incorrectly states 'erase'; it should say 'strengthen'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage incorrectly describes extinction as representing new 'excitatory learning.' In reality, extinction represents new 'inhibitory learning' that suppresses the conditioned response. This is a key distinction because excitatory learning would involve strengthening associations, whereas extinction involves the development of inhibition that counteracts the original conditioned response. The original CS-US association is not erased but rather suppressed by this new inhibitory learning, which is why phenomena like spontaneous recovery can occur."
    },
    {
      "id": "PMET-0178",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Continuous vs. Intermittent Reinforcement",
      "passage_type": "paragraph",
      "original_passage": "Intermittent (partial) reinforcement occurs when only some instances of the behavior are reinforced. Behavior maintained on intermittent schedules is more resistant to extinction because the organism has learned that not every response produces reinforcement",
      "modified_passage": "Intermittent (partial) reinforcement occurs when only some instances of the behavior are reinforced. Behavior maintained on intermittent schedules is less resistant to extinction because the organism has learned that not every response produces reinforcement.",
      "error_original": "less resistant to extinction",
      "error_correct": "more resistant to extinction",
      "options": [
        "The passage incorrectly states 'partial'; it should say 'continuous'",
        "The passage incorrectly states 'less resistant to extinction'; it should say 'more resistant to extinction'",
        "The passage incorrectly states 'reinforcement' at the end; it should say 'punishment'",
        "The passage incorrectly states 'some instances'; it should say 'all instances'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage introduces a concept reversal by stating that behavior maintained on intermittent schedules is 'less resistant to extinction,' when in fact it is 'more resistant to extinction.' This is a well-established principle in operant conditioning known as the partial reinforcement extinction effect (PREE): because the organism has learned that reinforcement does not follow every response, it takes longer for the organism to detect that reinforcement has been entirely discontinued, making the behavior more persistent and harder to extinguish."
    },
    {
      "id": "PMET-0573",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "original_passage": "The MTMM matrix evaluates both convergent and discriminant validity simultaneously",
      "modified_passage": "The MTMM matrix evaluates both convergent and concurrent validity simultaneously",
      "error_original": "concurrent validity",
      "error_correct": "discriminant validity",
      "options": [
        "The passage incorrectly states 'MTMM matrix'; it should say 'factor analysis matrix'",
        "The passage incorrectly states 'convergent'; it should say 'predictive'",
        "The passage incorrectly states 'concurrent validity'; it should say 'discriminant validity'",
        "The passage incorrectly states 'simultaneously'; it should say 'sequentially'"
      ],
      "correct_option_index": 2,
      "explanation": "The Multitrait-Multimethod (MTMM) matrix, developed by Campbell and Fiske (1959), is specifically designed to evaluate both convergent validity and discriminant validity simultaneously. The passage incorrectly substituted 'concurrent validity' for 'discriminant validity.' Concurrent validity is a type of criterion-related validity involving the correlation of a measure with a criterion measured at the same time, which is a different concept entirely. Discriminant (or divergent) validity refers to the degree to which a measure does not correlate with constructs from which it should theoretically differ, and it is a core component of what the MTMM matrix assesses."
    },
    {
      "id": "PMET-0137",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "One-Sample t-Test",
      "passage_type": "paragraph",
      "original_passage": "Used to compare a sample mean to a known or hypothesized population value. For example, testing whether a sample's IQ differs from the population mean of 100",
      "modified_passage": "Used to compare a sample median to a known or hypothesized population value. For example, testing whether a sample's IQ differs from the population mean of 100",
      "error_original": "sample median",
      "error_correct": "sample mean",
      "options": [
        "The passage incorrectly states the population mean IQ is 100; it should say 105",
        "The passage incorrectly states 'sample median'; it should say 'sample mean'",
        "The passage incorrectly states 'known or hypothesized population value'; it should say 'known or hypothesized sample value'",
        "The passage incorrectly states 'population mean'; it should say 'population standard deviation'"
      ],
      "correct_option_index": 1,
      "explanation": "The one-sample t-test is used to compare a sample mean (not a sample median) to a known or hypothesized population value. The passage introduced an error by substituting 'median' for 'mean.' The median is a measure of central tendency but is not the statistic evaluated in a one-sample t-test; the t-test specifically tests whether the sample mean differs significantly from a specified population value."
    },
    {
      "id": "PMET-0372",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Measures of Variability",
      "passage_type": "definition",
      "original_passage": "Computational Formulas Variance (sample): s² = Σ(X - M)² / (n - 1) Standard Deviation: s = √s² The divisor (n - 1) is called degrees of freedom and provides an unbiased estimate of the population parameter",
      "modified_passage": "Computational Formulas Variance (sample): s² = Σ(X - M)² / (n - 1) Standard Deviation: s = √s² The divisor (n - 1) is called degrees of freedom and provides a biased estimate of the population parameter",
      "error_original": "a biased estimate",
      "error_correct": "an unbiased estimate",
      "options": [
        "The passage incorrectly states the standard deviation formula as s = √s²; it should say s = s²",
        "The passage incorrectly states 'a biased estimate'; it should say 'an unbiased estimate'",
        "The passage incorrectly states the variance formula uses (n - 1); it should say (n + 1)",
        "The passage incorrectly states degrees of freedom refers to (n - 1); it should say (n - 2)"
      ],
      "correct_option_index": 1,
      "explanation": "The passage was modified to say that using (n - 1) as the divisor provides 'a biased estimate' of the population parameter. This is incorrect. Dividing by (n - 1) rather than n is specifically done to provide an UNBIASED estimate of the population variance. Dividing by n would yield a biased (systematically underestimating) estimate. The use of degrees of freedom (n - 1) corrects for this bias, which is why it is a fundamental concept in inferential statistics."
    },
    {
      "id": "PMET-0094",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Predictive Validity",
      "passage_type": "paragraph",
      "original_passage": "Predictive validity is demonstrated when test scores predict future performance on a criterion measured at a later time. This is the most important type of criterion-related validity for selection decisions, as it addresses the practical question: \"How well does this test predict future outcomes?\"",
      "modified_passage": "Predictive validity is demonstrated when test scores predict future performance on a criterion measured at a later time. This is the most important type of construct validity for selection decisions, as it addresses the practical question: \"How well does this test predict future outcomes?\"",
      "error_original": "construct validity",
      "error_correct": "criterion-related validity",
      "options": [
        "The passage incorrectly states 'predict future performance'; it should say 'correlate with current performance'",
        "The passage incorrectly states 'construct validity'; it should say 'criterion-related validity'",
        "The passage incorrectly states 'measured at a later time'; it should say 'measured at the same time'",
        "The passage incorrectly states 'selection decisions'; it should say 'diagnostic decisions'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly refers to predictive validity as a type of 'construct validity.' Predictive validity is actually a type of criterion-related validity (along with concurrent validity). Construct validity is an entirely different category of validity that concerns whether a test measures the theoretical construct it claims to measure. The other distractors are incorrect: predictive validity does involve predicting future performance (not current), the criterion is measured at a later time (not the same time), and predictive validity is indeed most relevant for selection decisions."
    },
    {
      "id": "PMET-0105",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Positive and Negative Predictive Value",
      "passage_type": "paragraph",
      "original_passage": "While sensitivity and specificity describe test performance, clinicians often need to know: \"Given a positive (or negative) test result, what is the probability the person has (or doesn't have) the condition?\"",
      "modified_passage": "While sensitivity and specificity describe test performance, clinicians often need to know: \"Given a positive (or negative) test result, what is the probability the person has (or doesn't have) the condition?\" This question is answered by sensitivity and specificity themselves.",
      "error_original": "This question is answered by sensitivity and specificity themselves",
      "error_correct": "This question is answered by positive and negative predictive value",
      "options": [
        "The passage incorrectly states that clinicians need to know the probability given a test result; it should say they need to know the probability given the condition status",
        "The passage incorrectly states that sensitivity and specificity describe test performance; it should say they describe predictive value",
        "The passage incorrectly states this question is answered by sensitivity and specificity themselves; it should say it is answered by positive and negative predictive value",
        "The passage incorrectly states 'positive (or negative) test result'; it should say 'true positive (or true negative) test result'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage introduces the distinction between sensitivity/specificity and predictive value. Sensitivity and specificity describe how well a test performs given the actual condition status (i.e., the probability of a positive test given the condition is present, and the probability of a negative test given the condition is absent). The question 'Given a positive or negative test result, what is the probability the person has the condition?' is answered by positive predictive value (PPV) and negative predictive value (NPV), not by sensitivity and specificity. The error substitutes 'sensitivity and specificity' for 'positive and negative predictive value.'"
    },
    {
      "id": "PMET-0448",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "original_passage": "PPV and NPV are influenced by base rate—low base rate = low PPV even with good sensitivity/specificity",
      "modified_passage": "PPV and NPV are influenced by base rate—low base rate = low PPV even with good sensitivity/reliability",
      "error_original": "reliability",
      "error_correct": "specificity",
      "options": [
        "The passage incorrectly states 'PPV and NPV'; it should say 'PPV and sensitivity'",
        "The passage incorrectly states 'low base rate = low PPV'; it should say 'low base rate = low NPV'",
        "The passage incorrectly states 'reliability'; it should say 'specificity'",
        "The passage incorrectly states 'sensitivity'; it should say 'selectivity'"
      ],
      "correct_option_index": 2,
      "explanation": "The original passage states that low base rate leads to low PPV even with good sensitivity and specificity. In the modified passage, 'specificity' was replaced with 'reliability.' Specificity refers to the ability of a test to correctly identify those without the condition (true negative rate), and it is paired with sensitivity when discussing the operating characteristics of a diagnostic test. Reliability is a different psychometric concept referring to the consistency of test scores. The correct term in this context is 'specificity.'"
    },
    {
      "id": "PMET-0351",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Sources of Validity Evidence",
      "passage_type": "paragraph",
      "original_passage": "The Standards for Educational and Psychological Testing (AERA et al., 2014) identifies five major sources of validity evidence. Each provides different information about the appropriateness of score interpretations",
      "modified_passage": "The Standards for Educational and Psychological Testing (AERA et al., 2014) identifies four major sources of validity evidence. Each provides different information about the appropriateness of score interpretations.",
      "error_original": "four major sources of validity evidence",
      "error_correct": "five major sources of validity evidence",
      "options": [
        "The passage incorrectly states the Standards were published by AERA et al.; they were published by APA alone",
        "The passage incorrectly states 'four major sources of validity evidence'; it should say 'five major sources of validity evidence'",
        "The passage incorrectly states the publication year as 2014; it should say 2009",
        "The passage incorrectly states 'appropriateness of score interpretations'; it should say 'appropriateness of test administration procedures'"
      ],
      "correct_option_index": 1,
      "explanation": "The Standards for Educational and Psychological Testing (AERA et al., 2014) identifies five major sources of validity evidence, not four. These five sources are: (1) evidence based on test content, (2) evidence based on response processes, (3) evidence based on internal structure, (4) evidence based on relations to other variables, and (5) evidence based on consequences of testing. The passage was modified to say 'four' instead of the correct number 'five.'"
    },
    {
      "id": "PMET-0130",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Type I Error (False Positive)",
      "passage_type": "paragraph",
      "original_passage": "A Type I error occurs when we reject the null hypothesis when it is actually true. This is also called a \"false positive\" or \"alpha error.\" The probability of a Type I error equals the significance level (α). If α = .05, there is a 5% chance of a Type I error when the null hypothesis is true",
      "modified_passage": "A Type I error occurs when we reject the null hypothesis when it is actually true. This is also called a \"false positive\" or \"alpha error.\" The probability of a Type I error equals the significance level (α). If α = .05, there is a 5% chance of a Type I error when the null hypothesis is false",
      "error_original": "when the null hypothesis is false",
      "error_correct": "when the null hypothesis is true",
      "options": [
        "The passage incorrectly states that a Type I error is called an \"alpha error\"; it should say \"beta error\"",
        "The passage incorrectly states that α = .05 corresponds to a 5% chance; it should say a 10% chance",
        "The passage incorrectly states that a Type I error is a \"false positive\"; it should say \"false negative\"",
        "The passage incorrectly states there is a 5% chance of a Type I error when the null hypothesis is false; it should say when the null hypothesis is true"
      ],
      "correct_option_index": 3,
      "explanation": "The passage's final clause was changed from 'when the null hypothesis is true' to 'when the null hypothesis is false.' By definition, a Type I error is the probability of incorrectly rejecting a true null hypothesis. Therefore, the 5% chance of a Type I error applies specifically when the null hypothesis is true, not when it is false. When the null hypothesis is false, the relevant error type would be a Type II error (failing to reject it)."
    },
    {
      "id": "PMET-0263",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Regression Discontinuity Design",
      "passage_type": "paragraph",
      "original_passage": "This design has stronger internal validity than other quasi-experimental designs because assignment is based on a known, measured variable",
      "modified_passage": "This design has stronger external validity than other quasi-experimental designs because assignment is based on a known, measured variable",
      "error_original": "external validity",
      "error_correct": "internal validity",
      "options": [
        "The passage incorrectly states 'quasi-experimental designs'; it should say 'pre-experimental designs'",
        "The passage incorrectly states 'external validity'; it should say 'internal validity'",
        "The passage incorrectly states 'known, measured variable'; it should say 'random, unmeasured variable'",
        "The passage incorrectly states 'stronger'; it should say 'weaker'"
      ],
      "correct_option_index": 1,
      "explanation": "The original passage correctly states that regression discontinuity design has stronger 'internal validity' than other quasi-experimental designs because assignment is based on a known, measured variable (the cutoff score). The modified passage incorrectly changed 'internal validity' to 'external validity.' Internal validity refers to the degree to which a study can establish causal relationships by ruling out alternative explanations, which is what the regression discontinuity design strengthens through its known assignment mechanism. External validity, by contrast, refers to the generalizability of findings to other settings and populations."
    },
    {
      "id": "PMET-0088",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Selection Ratio",
      "passage_type": "example",
      "original_passage": "Taylor-Russell Tables Taylor and Russell (1939) developed tables showing the expected success rate given different combinations of validity coefficient, base rate, and selection ratio. These tables demonstrate that even tests with modest validity can substantially improve selection outcomes when base rates are moderate and selection ratios are low",
      "modified_passage": "Taylor-Russell Tables Taylor and Russell (1939) developed tables showing the expected success rate given different combinations of validity coefficient, base rate, and selection ratio. These tables demonstrate that even tests with modest validity can substantially improve selection outcomes when base rates are moderate and selection ratios are high",
      "error_original": "selection ratios are high",
      "error_correct": "selection ratios are low",
      "options": [
        "The passage incorrectly states the tables were developed in 1939; they were actually developed in 1949",
        "The passage incorrectly states 'validity coefficient'; it should say 'reliability coefficient'",
        "The passage incorrectly states selection ratios are 'high'; it should say 'low'",
        "The passage incorrectly states base rates should be 'moderate'; it should say 'high'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage was modified to state that selection ratios should be 'high' for tests with modest validity to improve selection outcomes. In reality, the Taylor-Russell Tables demonstrate that selection outcomes improve most when selection ratios are LOW (i.e., when only a small proportion of applicants are selected). A low selection ratio means the organization can be more selective, which amplifies the utility of even a modestly valid test. When the selection ratio is high (most applicants are hired), even a highly valid test adds little incremental value to the selection process."
    },
    {
      "id": "PMET-0457",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Base Rates and Selection Ratios",
      "passage_type": "paragraph",
      "original_passage": "The practical utility of a test depends not only on its validity but also on the base rate and selection ratio.",
      "modified_passage": "The practical utility of a test depends not only on its reliability but also on the base rate and selection ratio.",
      "error_original": "reliability",
      "error_correct": "validity",
      "options": [
        "The passage incorrectly states 'base rate'; it should say 'prevalence rate'",
        "The passage incorrectly states 'reliability'; it should say 'validity'",
        "The passage incorrectly states 'selection ratio'; it should say 'selection index'",
        "The passage incorrectly states 'practical utility'; it should say 'incremental validity'"
      ],
      "correct_option_index": 1,
      "explanation": "The original passage correctly states that the practical utility of a test depends on its 'validity,' not its 'reliability.' While reliability is a prerequisite for validity, the concept being discussed here—criterion-related validity and its relationship to base rates and selection ratios—specifically concerns validity. The modified passage substituted 'reliability' for 'validity,' which is the key term in this context of predicting outcomes and determining how useful a test is for selection decisions."
    },
    {
      "id": "PMET-0058",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "The Regression Equation",
      "passage_type": "paragraph",
      "original_passage": "The regression line is the \"line of best fit\" that minimizes the sum of squared errors (differences between actual and predicted Y values). This is called the least squares criterion",
      "modified_passage": "The regression line is the \"line of best fit\" that minimizes the sum of absolute errors (differences between actual and predicted Y values). This is called the least squares criterion.",
      "error_original": "absolute errors",
      "error_correct": "squared errors",
      "options": [
        "The passage incorrectly states 'predicted Y values'; it should say 'predicted X values'",
        "The passage incorrectly states 'absolute errors'; it should say 'squared errors'",
        "The passage incorrectly states 'least squares criterion'; it should say 'least residuals criterion'",
        "The passage incorrectly states 'line of best fit'; it should say 'line of central tendency'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly states that the regression line minimizes the sum of 'absolute errors.' In reality, the regression line minimizes the sum of 'squared errors' — that is, the squared differences between actual and predicted Y values. This is precisely why the method is called the 'least squares' criterion. Minimizing absolute errors would correspond to a different fitting method (least absolute deviations), not ordinary least squares regression."
    },
    {
      "id": "PMET-0089",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Specificity",
      "passage_type": "example",
      "original_passage": "Sensitivity and Specificity Example A depression screening test is administered to 1,000 patients. Of 200 who actually have depression, 180 test positive (sensitivity = 180/200 = 90%). Of 800 without depression, 720 test negative (specificity = 720/800 = 90%). The test correctly identifies 90% of depressed patients and 90% of non-depressed patients",
      "modified_passage": "Sensitivity and Specificity Example A depression screening test is administered to 1,000 patients. Of 200 who actually have depression, 180 test positive (sensitivity = 180/200 = 90%). Of 800 without depression, 720 test negative (specificity = 720/800 = 90%). The test correctly identifies 90% of depressed patients and 90% of non-depressed patients",
      "error_original": "sensitivity = 180/200 = 90%",
      "error_correct": "sensitivity = 180/200 = 90% (this is correct; no error here)",
      "options": [
        "The passage incorrectly states that sensitivity is 180/200; sensitivity should be calculated as 720/800",
        "The passage incorrectly states that 720 test negative; it should say 720 test positive",
        "The passage incorrectly states specificity = 720/800 = 90%; it should say specificity = 80/800 = 10%",
        "The passage incorrectly states there are 200 who actually have depression; based on the other numbers, there should be 180"
      ],
      "correct_option_index": 1,
      "explanation": "Wait, let me re-examine and redo this properly."
    },
    {
      "id": "PMET-0144",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Analysis of Covariance (ANCOVA)",
      "passage_type": "paragraph",
      "original_passage": "ANCOVA combines ANOVA with regression by including one or more covariates —continuous variables that are statistically controlled to reduce error variance and remove the influence of confounding variables. ANCOVA adjusts group means on the DV to account for pre-existing differences on the covariate",
      "modified_passage": "ANCOVA combines ANOVA with regression by including one or more covariates —continuous variables that are statistically controlled to reduce error variance and remove the influence of confounding variables. ANCOVA adjusts group means on the DV to account for pre-existing differences on the independent variable.",
      "error_original": "pre-existing differences on the independent variable",
      "error_correct": "pre-existing differences on the covariate",
      "options": [
        "The passage incorrectly states that ANCOVA combines ANOVA with regression; it should say ANCOVA combines ANOVA with correlation.",
        "The passage incorrectly states that covariates are continuous variables; they can be continuous or categorical variables.",
        "The passage incorrectly states that ANCOVA adjusts for pre-existing differences on the independent variable; it should say pre-existing differences on the covariate.",
        "The passage incorrectly states that ANCOVA reduces error variance; it should say ANCOVA increases error variance to improve sensitivity."
      ],
      "correct_option_index": 2,
      "explanation": "The passage was modified to say ANCOVA adjusts group means to account for pre-existing differences on the 'independent variable,' when it should say 'covariate.' The purpose of ANCOVA is to statistically control for covariates — continuous variables on which groups may differ prior to treatment — thereby adjusting the DV means to remove the influence of these covariates, not the independent variable. The independent variable is the factor whose effect on the DV is being tested, not the variable being controlled for."
    },
    {
      "id": "PMET-0015",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Stimulus Generalization and Discrimination",
      "passage_type": "paragraph",
      "original_passage": "Stimulus discrimination is the opposite process - learning to respond differently to stimuli that differ from the original CS. Discrimination training involves reinforcing (pairing with the US) responses to one stimulus (CS+) while not reinforcing responses to similar stimuli (CS-)",
      "modified_passage": "Stimulus discrimination is the opposite process - learning to respond differently to stimuli that differ from the original CS. Discrimination training involves reinforcing (pairing with the US) responses to one stimulus (CS+) while extinguishing responses to similar stimuli (CS-)",
      "error_original": "extinguishing responses to similar stimuli (CS-)",
      "error_correct": "not reinforcing responses to similar stimuli (CS-)",
      "options": [
        "The passage incorrectly states 'learning to respond differently'; it should say 'learning to respond similarly'",
        "The passage incorrectly states 'extinguishing responses to similar stimuli (CS-)'; it should say 'not reinforcing responses to similar stimuli (CS-)'",
        "The passage incorrectly states 'pairing with the US'; it should say 'pairing with the CS'",
        "The passage incorrectly states 'CS+' for the reinforced stimulus; it should say 'CS-'"
      ],
      "correct_option_index": 1,
      "explanation": "The passage incorrectly uses 'extinguishing' when describing what happens with the CS- during discrimination training. Extinguishing refers to the weakening of a previously conditioned response through repeated presentation of the CS without the US. In discrimination training, the CS- was never paired with the US in the first place — it is simply 'not reinforced' (i.e., not paired with the US). The correct term is 'not reinforcing,' which accurately describes the procedure of withholding the US when the CS- is presented."
    },
    {
      "id": "PMET-0495",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "original_passage": "Escape vs. Avoidance: Escape terminates present aversive; avoidance prevents future aversive",
      "modified_passage": "Escape vs. Avoidance: Escape prevents future aversive; avoidance terminates present aversive",
      "error_original": "Escape prevents future aversive; avoidance terminates present aversive",
      "error_correct": "Escape terminates present aversive; avoidance prevents future aversive",
      "options": [
        "The passage incorrectly states that escape and avoidance both involve negative reinforcement; only escape involves negative reinforcement",
        "The passage incorrectly swaps the definitions: escape should terminate a present aversive, and avoidance should prevent a future aversive",
        "The passage incorrectly describes avoidance as involving punishment; it should describe it as involving negative reinforcement",
        "The passage incorrectly implies that escape conditioning requires a discriminative stimulus; this applies only to avoidance conditioning"
      ],
      "correct_option_index": 1,
      "explanation": "The definitions of escape and avoidance have been swapped. Escape conditioning involves terminating (removing) a currently present aversive stimulus, while avoidance conditioning involves behaving in a way that prevents a future aversive stimulus from occurring. The modified passage reverses these two definitions, attributing escape's definition to avoidance and vice versa."
    },
    {
      "id": "PMET-0145",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Multivariate Analysis of Variance (MANOVA)",
      "passage_type": "paragraph",
      "original_passage": "MANOVA extends ANOVA to situations with multiple dependent variables measured simultaneously. Rather than running separate ANOVAs for each DV (which inflates Type I error), MANOVA tests whether groups differ on a linear combination of the DVs in a single omnibus test",
      "modified_passage": "MANOVA extends ANOVA to situations with multiple dependent variables measured simultaneously. Rather than running separate ANOVAs for each DV (which inflates Type II error), MANOVA tests whether groups differ on a linear combination of the DVs in a single omnibus test",
      "error_original": "Type II error",
      "error_correct": "Type I error",
      "options": [
        "The passage incorrectly states 'linear combination'; it should say 'nonlinear combination'",
        "The passage incorrectly states 'multiple dependent variables'; it should say 'multiple independent variables'",
        "The passage incorrectly states 'Type II error'; it should say 'Type I error'",
        "The passage incorrectly states 'omnibus test'; it should say 'post hoc test'"
      ],
      "correct_option_index": 2,
      "explanation": "The passage incorrectly refers to 'Type II error' when it should say 'Type I error.' Running multiple separate ANOVAs on each dependent variable increases the familywise Type I error rate (the probability of incorrectly rejecting at least one true null hypothesis). MANOVA addresses this by testing all dependent variables simultaneously in a single omnibus test, thereby controlling the Type I error rate. Type II error refers to failing to reject a false null hypothesis, which is not what is inflated by conducting multiple separate tests."
    },
    {
      "id": "PMET-SC-0056",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0446",
      "modified_sentence": "Sensitivity is calculated as TP/(TP+FN), which represents the ability to correctly detect true negatives, meaning people who actually have the condition being tested for.",
      "phrases": [
        "Sensitivity is calculated as TP/(TP+FN),",
        " which represents the ability to correctly detect true negatives,",
        " meaning people who actually have",
        " the condition being tested for."
      ],
      "target_phrase_index": 1,
      "error_original": "true negatives",
      "error_correct": "true positives",
      "explanation": "Sensitivity refers to the ability to correctly detect true positives (people who actually have the condition), not true negatives. True negatives are correctly identified by specificity, not sensitivity. The formula TP/(TP+FN) confirms this: it calculates the proportion of actual positives (TP+FN) that are correctly identified as positive (TP)."
    },
    {
      "id": "PMET-SC-0057",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0576",
      "modified_sentence": "Test bias is defined as systematic measurement error affecting specific groups and is typically assessed through differential validity and item response curves, while fairness involves value judgments about test use and cannot be determined through empirical analysis alone.",
      "phrases": [
        "Test bias is defined as systematic measurement error affecting specific groups",
        " and is typically assessed through differential validity and item response curves,",
        " while fairness involves value judgments about test use",
        " and cannot be determined through empirical analysis alone."
      ],
      "target_phrase_index": 1,
      "error_original": "differential validity and item response curves",
      "error_correct": "differential validity and differential prediction",
      "explanation": "Test bias at the test level is typically assessed using two primary methods: differential validity (comparing validity coefficients across groups) and differential prediction (comparing regression lines across groups, as in the Cleary model). 'Item response curves' is not the standard term for this test-level bias assessment; item-level bias is assessed through differential item functioning (DIF). The passage correctly notes that bias is a psychometric/statistical property, while fairness is a value judgment that cannot be resolved empirically."
    },
    {
      "id": "PMET-SC-0058",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "CIPP Model (Stufflebeam)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0529",
      "modified_sentence": "In Stufflebeam's CIPP model, input evaluation assesses resources, strategies, and alternative approaches to guide legitimating decisions, and is one of four evaluation components along with context, process, and product evaluation.",
      "phrases": [
        "In Stufflebeam's CIPP model,",
        " input evaluation assesses resources, strategies, and alternative approaches to guide legitimating decisions,",
        " and is one of four evaluation components",
        " along with context, process, and product evaluation."
      ],
      "target_phrase_index": 1,
      "error_original": "to guide legitimating decisions",
      "error_correct": "to guide structuring decisions",
      "explanation": "In Stufflebeam's CIPP model, input evaluation is specifically designed to guide structuring decisions — that is, decisions about how to structure a program's design and resources. Legitimating decisions are associated with product evaluation, which determines whether to continue, modify, or terminate a program. Each CIPP component maps to a specific decision type: context guides planning decisions, input guides structuring decisions, process guides implementing decisions, and product guides recycling (legitimating) decisions."
    },
    {
      "id": "PMET-SC-0059",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Threats to Construct Validity",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0342",
      "modified_sentence": "To address construct validity threats, underrepresentation should be minimized by ensuring comprehensive sampling of the construct domain using multiple item formats, while construct-irrelevant variance should be reduced by minimizing irrelevant difficulty sources and evaluating differential test functioning across groups.",
      "phrases": [
        "To address construct validity threats,",
        " underrepresentation should be minimized by ensuring comprehensive sampling of the construct domain using multiple item formats,",
        " while construct-irrelevant variance should be reduced by minimizing irrelevant difficulty sources",
        " and evaluating differential test functioning across groups."
      ],
      "target_phrase_index": 3,
      "error_original": "differential test functioning",
      "error_correct": "differential item functioning",
      "explanation": "The correct term is 'differential item functioning' (DIF), which refers to the statistical analysis of whether individual test items perform differently across demographic groups. 'Differential test functioning' is not the standard psychometric term. DIF is a key method for detecting construct-irrelevant variance that may introduce bias at the item level."
    },
    {
      "id": "PMET-SC-0060",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Escape and Avoidance Learning",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0191",
      "modified_sentence": "Mowrer's two-factor theory explains avoidance learningby proposing that fear is first acquired through operant conditioning,and is then maintained through classical conditioningwhen avoidance behavior is negatively reinforced by fear reduction.",
      "phrases": [
        "Mowrer's two-factor theory explains avoidance learning",
        "by proposing that fear is first acquired through operant conditioning,",
        "and is then maintained through classical conditioning",
        "when avoidance behavior is negatively reinforced by fear reduction."
      ],
      "target_phrase_index": 1,
      "error_original": "fear is first acquired through operant conditioning",
      "error_correct": "fear is first acquired through classical conditioning",
      "explanation": "According to Mowrer's two-factor theory, fear is first acquired through classical conditioning (factor one), in which a neutral stimulus becomes associated with an aversive unconditioned stimulus. The avoidance behavior is then maintained through operant conditioning (factor two), specifically negative reinforcement, when the organism's avoidance response reduces the conditioned fear. The error swaps the two types of conditioning in the sequence."
    },
    {
      "id": "PMET-SC-0061",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Cross-Sectional vs. Longitudinal Designs",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0274",
      "modified_sentence": "Longitudinal designs follow the same participants over time, allowing researchers to track group-level trends and development, but they are expensive, time-consuming, and vulnerable to attrition.",
      "phrases": [
        "Longitudinal designs follow the same participants over time,",
        " allowing researchers to track group-level trends and development,",
        " but they are expensive, time-consuming, and vulnerable to attrition."
      ],
      "target_phrase_index": 1,
      "error_original": "track group-level trends and development",
      "error_correct": "track individual change and development",
      "explanation": "A key advantage of longitudinal designs is the ability to track individual change over time, not merely group-level trends. Tracking group-level trends is more characteristic of cross-sectional designs, which compare different groups of participants at a single point in time. Longitudinal designs specifically follow the same participants, enabling researchers to observe how each individual changes across the study period."
    },
    {
      "id": "PMET-SC-0062",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "CR versus UR",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0424",
      "modified_sentence": "While the conditioned response often resembles the unconditioned response, the CR is typically stronger in magnitude than the UR, and in some cases such as drug tolerance the CR may actually be a compensatory response that is opposite in direction to the UR.",
      "phrases": [
        "While the conditioned response often resembles the unconditioned response,",
        " the CR is typically stronger in magnitude than the UR,",
        " and in some cases such as drug tolerance the CR may actually be a compensatory response",
        " that is opposite in direction to the UR."
      ],
      "target_phrase_index": 1,
      "error_original": "stronger in magnitude",
      "error_correct": "weaker in magnitude",
      "explanation": "The conditioned response (CR) is typically weaker in magnitude than the unconditioned response (UR), not stronger. This is one of the key differences between the CR and UR — while the CR resembles the UR, it is generally less intense. The error requires specific knowledge of classical conditioning principles regarding how conditioned responses compare to unconditioned responses."
    },
    {
      "id": "PMET-SC-0063",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Test Length",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0526",
      "modified_sentence": "The Spearman-Brown prophecy formula estimates how reliability changes when test length is altered, and it was developed by Spearman and Brown in 1910 to predict that increasing the number of equivalent items on a test will generally increase its internal consistency reliability as estimated by coefficient alpha.",
      "phrases": [
        "The Spearman-Brown prophecy formula estimates how reliability changes when test length is altered,",
        " and it was developed by Spearman and Brown in 1910 to predict that increasing the number of equivalent items on a test",
        " will generally increase its internal consistency reliability as estimated by coefficient alpha."
      ],
      "target_phrase_index": 2,
      "error_original": "as estimated by coefficient alpha",
      "error_correct": "as estimated by the split-half method",
      "explanation": "The Spearman-Brown prophecy formula is specifically used to correct split-half reliability estimates, not coefficient alpha. Coefficient alpha (Cronbach's alpha) is a separate measure of internal consistency. The Spearman-Brown formula is most commonly applied to estimate full-test reliability from a split-half correlation, effectively predicting what reliability would be if the test were doubled in length from its half-test form."
    },
    {
      "id": "PMET-SC-0064",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Measures of Central Tendency",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0397",
      "modified_sentence": "The mode is the most frequently occurring score, and it is the only measure of central tendency appropriate for ordinal data, while a distribution can be bimodal (two modes) or multimodal (multiple modes).",
      "phrases": [
        "The mode is the most frequently occurring score,",
        " and it is the only measure of central tendency appropriate for ordinal data,",
        " while a distribution can be bimodal (two modes) or multimodal (multiple modes)."
      ],
      "target_phrase_index": 1,
      "error_original": "ordinal data",
      "error_correct": "nominal data",
      "explanation": "The mode is the only measure of central tendency appropriate for nominal data, not ordinal data. Nominal data consist of categories without any inherent order, and since you cannot meaningfully calculate a mean or median for nominal categories, the mode is the only applicable measure. Ordinal data, while also categorical, do have a rank order, which allows the use of the median as well."
    },
    {
      "id": "PMET-SC-0065",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0418",
      "modified_sentence": "Delay conditioning is considered the most effective temporal arrangement in classical conditioning, where the CS begins after and overlaps with the US presentation.",
      "phrases": [
        "Delay conditioning is considered the most effective temporal arrangement in classical conditioning,",
        " where the CS begins after",
        " and overlaps with the US presentation."
      ],
      "target_phrase_index": 1,
      "error_original": "where the CS begins after",
      "error_correct": "where the CS begins before",
      "explanation": "In delay conditioning, the conditioned stimulus (CS) begins BEFORE the unconditioned stimulus (US) and overlaps with it. Saying the CS begins 'after' the US is incorrect; if the CS started after the US, it would not be delay conditioning and would not produce effective conditioning. The correct temporal arrangement is that the CS onset precedes the US onset."
    },
    {
      "id": "PMET-SC-0066",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Post Hoc Tests",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0142",
      "modified_sentence": "When ANOVA indicates significant differences, post hoc tests such as Tukey's HSD and Dunnett's test are used to determine which specific group means differ from one another while controlling for the familywise Type I error rate.",
      "phrases": [
        "When ANOVA indicates significant differences,",
        " post hoc tests such as Tukey's HSD and Dunnett's test are used",
        " to determine which specific group means differ from one another",
        " while controlling for the familywise Type I error rate."
      ],
      "target_phrase_index": 1,
      "error_original": "Dunnett's test",
      "error_correct": "Scheffé test",
      "explanation": "Dunnett's test is not a standard post hoc test for all pairwise comparisons — it is specifically designed to compare each treatment group against a single control group. The Scheffé test is the classic post hoc test commonly paired with Tukey's HSD for making all possible pairwise and complex comparisons among group means following a significant ANOVA."
    },
    {
      "id": "PMET-SC-0067",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Internal Structure",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0356",
      "modified_sentence": "Exploratory factor analysis (EFA) identifies the underlying factor structure without prior hypotheses about the number or nature of factors, while confirmatory factor analysis (CFA) tests whether data fit a hypothesized factor structure based on empirical clustering.",
      "phrases": [
        "Exploratory factor analysis (EFA) identifies the underlying factor structure",
        " without prior hypotheses about the number or nature of factors,",
        " while confirmatory factor analysis (CFA) tests whether data fit",
        " a hypothesized factor structure based on empirical clustering."
      ],
      "target_phrase_index": 3,
      "error_original": "based on empirical clustering",
      "error_correct": "based on theory",
      "explanation": "Confirmatory factor analysis (CFA) tests whether data fit a factor structure that is hypothesized based on theory, not based on empirical clustering. CFA is a theory-driven approach in which researchers specify the expected factor structure in advance from theoretical considerations, and then test how well the observed data conform to that structure. Empirical clustering describes a data-driven process, which is more characteristic of exploratory methods like EFA."
    },
    {
      "id": "PMET-SC-0068",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Types of Bias",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0368",
      "modified_sentence": "Differential item functioning occurs when examinees of unequal ability from different groups have different probabilities of answering an item correctly, and DIF analysis identifies potentially biased items that may function differently across demographic groups.",
      "phrases": [
        "Differential item functioning occurs",
        " when examinees of unequal ability from different groups have different probabilities of answering an item correctly,",
        " and DIF analysis identifies potentially biased items",
        " that may function differently across demographic groups."
      ],
      "target_phrase_index": 1,
      "error_original": "unequal ability",
      "error_correct": "equal ability",
      "explanation": "Differential item functioning (DIF) is defined as occurring when examinees of EQUAL ability from different groups have different probabilities of answering an item correctly. The key concept is that ability is held constant; if examinees of unequal ability performed differently, that would simply reflect true differences in ability rather than item bias. The passage incorrectly states 'unequal ability' instead of 'equal ability.'"
    },
    {
      "id": "PMET-SC-0069",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Evidence Based on Relations to Other Variables",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0357",
      "modified_sentence": "This evidence demonstrates that test scores relate to external variables in ways consistent with the theoretical understanding of the construct, and this category includes convergent and divergent validity evidence, as well as criterion-related validity.",
      "phrases": [
        "This evidence demonstrates that test scores relate to external variables",
        " in ways consistent with the theoretical understanding of the construct,",
        " and this category includes convergent and divergent validity evidence,",
        " as well as criterion-related validity."
      ],
      "target_phrase_index": 2,
      "error_original": "convergent and divergent validity evidence",
      "error_correct": "convergent and discriminant validity evidence",
      "explanation": "The correct term is 'discriminant validity,' not 'divergent validity.' In the context of construct validity, convergent and discriminant evidence are the two complementary forms of evidence based on relations to other variables. Discriminant validity (sometimes called divergent validity informally) is the standard psychometric term introduced by Campbell and Fiske (1959) in their multitrait-multimethod matrix framework. The passage specifically uses 'discriminant,' which is the accepted technical term in validity theory."
    },
    {
      "id": "PMET-SC-0070",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Point-Biserial Correlation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0047",
      "modified_sentence": "The point-biserial correlation is used when one variable is continuous and the other is a true dichotomy, and it is mathematically equivalent to the Spearman rho but is appropriate for this specific combination of variable types.",
      "phrases": [
        "The point-biserial correlation is used when one variable is continuous",
        " and the other is a true dichotomy,",
        " and it is mathematically equivalent to the Spearman rho",
        " but is appropriate for this specific combination of variable types."
      ],
      "target_phrase_index": 2,
      "error_original": "mathematically equivalent to the Spearman rho",
      "error_correct": "mathematically equivalent to the Pearson r",
      "explanation": "The point-biserial correlation is mathematically equivalent to the Pearson r (Pearson product-moment correlation), not the Spearman rho. The Spearman rho is a rank-order correlation used with ordinal data, whereas the point-biserial is simply a special case of the Pearson r applied when one variable is continuous and the other is a true dichotomy."
    },
    {
      "id": "PMET-SC-0071",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Sensitivity and Specificity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0102",
      "modified_sentence": "When tests are used for classification or diagnosis, four outcomes are possible, and sensitivity and specificity describe the accuracy of the test in identifying true positives and false negatives.",
      "phrases": [
        "When tests are used for classification or diagnosis,",
        " four outcomes are possible, and sensitivity and specificity",
        " describe the accuracy of the test",
        " in identifying true positives and false negatives."
      ],
      "target_phrase_index": 3,
      "error_original": "true positives and false negatives",
      "error_correct": "true positives and true negatives",
      "explanation": "Sensitivity refers to the test's accuracy in identifying true positives (correctly identifying those with the condition), while specificity refers to the test's accuracy in identifying true negatives (correctly identifying those without the condition). False negatives are errors, not accurate identifications. The passage should read 'true negatives' rather than 'false negatives.'"
    },
    {
      "id": "PMET-SC-0072",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Sensitivity and Specificity",
      "passage_type": "definition",
      "source_passage_id": "PMET-0079",
      "modified_sentence": "When a test result is positive and the condition is actually present, this is classified as a True Positive, while a positive test result when the condition is absent is classified as a True Negative.",
      "phrases": [
        "When a test result is positive and the condition is actually present,",
        " this is classified as a True Positive,",
        " while a positive test result when the condition is absent",
        " is classified as a True Negative."
      ],
      "target_phrase_index": 3,
      "error_original": "is classified as a True Negative",
      "error_correct": "is classified as a False Positive",
      "explanation": "When a test result is positive but the condition is actually absent, the outcome is a False Positive (FP), not a True Negative. A True Negative occurs when the test result is negative and the condition is indeed absent. This is a fundamental concept in sensitivity and specificity analysis."
    },
    {
      "id": "PMET-SC-0073",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Problems with Punishment",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0477",
      "modified_sentence": "One major problem with punishment is that the punished individual may learn to escape or avoid the punishing agent, a process best explained by Skinner's two-factor theory, which involves classical conditioning of fear followed by operant conditioning of the avoidance response.",
      "phrases": [
        "One major problem with punishment is that the punished individual may learn to escape or avoid the punishing agent,",
        " a process best explained by Skinner's two-factor theory,",
        " which involves classical conditioning of fear",
        " followed by operant conditioning of the avoidance response."
      ],
      "target_phrase_index": 1,
      "error_original": "Skinner's two-factor theory",
      "error_correct": "Mowrer's two-factor theory",
      "explanation": "The two-factor theory of avoidance learning was proposed by O. Hobart Mowrer, not B.F. Skinner. Mowrer's two-factor theory explains that avoidance behavior develops through two processes: first, classical conditioning pairs a neutral stimulus with an aversive event to produce a fear response; second, the organism is negatively reinforced (operant conditioning) for performing the avoidance response that reduces fear. Skinner focused on operant conditioning principles but did not develop the two-factor theory."
    },
    {
      "id": "PMET-SC-0074",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Multiple Baseline Design",
      "passage_type": "example",
      "source_passage_id": "PMET-0251",
      "modified_sentence": "After establishing baselines for all three, she implements a token economy first for hand-flapping, and once reduction is observed she extends the intervention to echolalia, then to tantrums, demonstrating experimental control through a reversal of each treatment phase.",
      "phrases": [
        "After establishing baselines for all three,",
        " she implements a token economy first for hand-flapping,",
        " and once reduction is observed she extends the intervention to echolalia, then to tantrums,",
        " demonstrating experimental control through a reversal of each treatment phase."
      ],
      "target_phrase_index": 3,
      "error_original": "through a reversal of each treatment phase",
      "error_correct": "without requiring withdrawal",
      "explanation": "A key advantage of the multiple baseline design is that it demonstrates experimental control without requiring withdrawal (reversal) of the treatment. The passage explicitly states this. Saying it works 'through a reversal of each treatment phase' incorrectly describes the mechanism — multiple baseline designs are specifically used as an alternative to reversal/withdrawal designs, making withdrawal unnecessary because control is demonstrated by the staggered introduction of the intervention across behaviors."
    },
    {
      "id": "PMET-SC-0075",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Relationship Between Reliability and Validity",
      "passage_type": "example",
      "source_passage_id": "PMET-0348",
      "modified_sentence": "A test with reliability of .64 can have maximum validity of only √.64 = .80, regardless of how well the content represents the intended construct.",
      "phrases": [
        "A test with reliability of .64",
        " can have maximum validity of only √.64 = .80,",
        " regardless of how well the content represents the intended construct."
      ],
      "target_phrase_index": 1,
      "error_original": "maximum validity of only √.64 = .80",
      "error_correct": "maximum validity of only √.64 = .80 — this is actually correct; let me revise",
      "explanation": "Placeholder — regenerating with a proper error."
    },
    {
      "id": "PMET-SC-0076",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Incremental Validity",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0084",
      "modified_sentence": "When selecting assessment instruments, clinicians should evaluate whether a new test provides incremental validity, which refers to the degree to which the test improves prediction beyond information already available from existing measures, a concept formalized by Meehl (1963) in the context of criterion-related validity.",
      "phrases": [
        "When selecting assessment instruments,",
        " clinicians should evaluate whether a new test provides incremental validity,",
        " which refers to the degree to which the test improves prediction beyond information already available from existing measures,",
        " a concept formalized by Meehl (1963) in the context of criterion-related validity."
      ],
      "target_phrase_index": 3,
      "error_original": "formalized by Meehl (1963)",
      "error_correct": "formalized by Sechrest (1963)",
      "explanation": "Incremental validity was formalized by Sechrest (1963), not Meehl. While Meehl made significant contributions to clinical versus statistical prediction, it was Lee Sechrest who introduced and formalized the concept of incremental validity in his 1963 paper, arguing that a test must demonstrate predictive value beyond what is already available from other sources."
    },
    {
      "id": "PMET-SC-0077",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Decision Theory and Cutoff Scores",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0108",
      "modified_sentence": "According to the Taylor-Russell tables, the utility of a test for improving selection decisions is maximized when the base rate is extreme (around .90) and the selection ratio is low, meaning fewer applicants are selected.",
      "phrases": [
        "According to the Taylor-Russell tables,",
        " the utility of a test for improving selection decisions is maximized when the base rate is extreme (around .90)",
        " and the selection ratio is low,",
        " meaning fewer applicants are selected."
      ],
      "target_phrase_index": 1,
      "error_original": "the base rate is extreme (around .90)",
      "error_correct": "the base rate is moderate (around .50)",
      "explanation": "The Taylor-Russell tables demonstrate that the incremental validity of a selection test is greatest when the base rate is moderate (approximately .50). When the base rate is extreme—either very high or very low—there is less room for the test to improve upon selection decisions. A base rate of .90 would mean almost everyone succeeds without the test, leaving little room for improvement."
    },
    {
      "id": "PMET-SC-0078",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Avoidance Learning",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0193",
      "modified_sentence": "Avoidance learning occurs when an organism learns to perform a behavior that prevents an aversive stimulus from occurring, and the avoidance response is made after the aversive stimulus is presented, based on warning signals known as discriminative stimuli.",
      "phrases": [
        "Avoidance learning occurs when an organism learns to perform a behavior that prevents an aversive stimulus from occurring,",
        " and the avoidance response is made after the aversive stimulus is presented,",
        " based on warning signals known as discriminative stimuli."
      ],
      "target_phrase_index": 1,
      "error_original": "after the aversive stimulus is presented",
      "error_correct": "before the aversive stimulus is presented",
      "explanation": "In avoidance learning, the organism responds before the aversive stimulus is presented, not after. The key distinction between avoidance and escape learning is that in avoidance learning the response occurs prior to the aversive stimulus based on warning signals, whereas in escape learning the response occurs after the aversive stimulus has already been presented in order to terminate it."
    },
    {
      "id": "PMET-SC-0079",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Discriminative Stimuli",
      "passage_type": "example",
      "source_passage_id": "PMET-0162",
      "modified_sentence": "A ringing telephone (S D) signals that answering will be reinforced by conversation, while the silent telephone (S Δ) signals that picking up the receiver will not produce punishment.",
      "phrases": [
        "A ringing telephone (S D) signals that answering will be reinforced by conversation,",
        " while the silent telephone (S Δ) signals",
        " that picking up the receiver will not produce punishment."
      ],
      "target_phrase_index": 2,
      "error_original": "will not produce punishment",
      "error_correct": "will not produce reinforcement",
      "explanation": "The passage states that the silent telephone (S Δ) signals that picking up the receiver will not produce reinforcement, not punishment. In operant conditioning, the S Δ (S-delta) indicates the absence of reinforcement for a given response, not the absence of punishment. The concept of stimulus control here involves discriminating when reinforcement is available versus when it is not."
    },
    {
      "id": "PMET-SC-0080",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Interrupted Time-Series Design",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0261",
      "modified_sentence": "The interrupted time-series design involves multiple measurements before and after an intervention, allowing researchers to examine trends over time and detect changes associated with the intervention, and this design is particularly useful for evaluating individual-level therapy outcomes or controlled laboratory interventions.",
      "phrases": [
        "The interrupted time-series design involves multiple measurements before and after an intervention,",
        " allowing researchers to examine trends over time and detect changes associated with the intervention,",
        " and this design is particularly useful for evaluating individual-level therapy outcomes or controlled laboratory interventions."
      ],
      "target_phrase_index": 2,
      "error_original": "individual-level therapy outcomes",
      "error_correct": "policy changes or community-level interventions",
      "explanation": "The interrupted time-series design is particularly useful for evaluating policy changes or community-level interventions, not individual-level therapy outcomes. This design excels at assessing the impact of large-scale changes (e.g., new laws, public health campaigns) by tracking trends across multiple time points before and after the intervention is introduced at a population or community level."
    },
    {
      "id": "PMET-SC-0081",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Meta-Analysis",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0075",
      "modified_sentence": "Publication bias (also called the \"file drawer problem\") occurs when studies with statistically significant results are more likely to be published than studies with null findings, and this can deflate meta-analytic effect size estimates because non-significant studies are missing from the analysis.",
      "phrases": [
        "Publication bias (also called the \"file drawer problem\")",
        " occurs when studies with statistically significant results are more likely to be published than studies with null findings,",
        " and this can deflate meta-analytic effect size estimates",
        " because non-significant studies are missing from the analysis."
      ],
      "target_phrase_index": 2,
      "error_original": "deflate meta-analytic effect size estimates",
      "error_correct": "inflate meta-analytic effect size estimates",
      "explanation": "Publication bias leads to an inflation (not deflation) of meta-analytic effect size estimates. Because studies with non-significant or small effects are less likely to be published, the pool of available studies over-represents larger effects, artificially increasing the overall estimated effect size."
    },
    {
      "id": "PMET-SC-0082",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0547",
      "modified_sentence": "The CIPP Model, developed by Stufflebeam, stands for Context, Input, Procedure, and Product and serves as a comprehensive evaluation framework used primarily in program evaluation rather than individual clinical assessment.",
      "phrases": [
        "The CIPP Model, developed by Stufflebeam,",
        " stands for Context, Input, Procedure, and Product",
        " and serves as a comprehensive evaluation framework",
        " used primarily in program evaluation rather than individual clinical assessment."
      ],
      "target_phrase_index": 1,
      "error_original": "Procedure",
      "error_correct": "Process",
      "explanation": "In Stufflebeam's CIPP Model, the acronym stands for Context, Input, Process, and Product. The word 'Procedure' was incorrectly substituted for 'Process.' Process evaluation specifically examines the implementation of a program, assessing how well it is being carried out, which is distinct from mere procedures."
    },
    {
      "id": "PMET-SC-0083",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Quality and Rigor in Qualitative Research",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0246",
      "modified_sentence": "Sequential designs use one method to inform the other; concurrent designs collect both types of data independently at different time points.",
      "phrases": [
        "Sequential designs use one method to inform the other;",
        " concurrent designs collect both types of data",
        " independently at different time points."
      ],
      "target_phrase_index": 2,
      "error_original": "independently at different time points",
      "error_correct": "simultaneously",
      "explanation": "Concurrent designs in mixed methods research collect both quantitative and qualitative data simultaneously (at the same time), not at different time points. Collecting data at different time points is characteristic of sequential designs, not concurrent designs."
    },
    {
      "id": "PMET-SC-0084",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Introduction to Research Designs",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0253",
      "modified_sentence": "Research designs exist on a continuum of internal validity, with true experimental designs providing the strongest evidence for causal inference and observational designs providing the weakest, though designs with lower internal validity may have advantages in external validity or practical feasibility (Cook & Stanley, 1979).",
      "phrases": [
        "Research designs exist on a continuum of internal validity,",
        " with true experimental designs providing the strongest evidence for causal inference",
        " and observational designs providing the weakest,",
        " though designs with lower internal validity may have advantages in external validity or practical feasibility (Cook & Stanley, 1979)."
      ],
      "target_phrase_index": 3,
      "error_original": "Cook & Stanley, 1979",
      "error_correct": "Cook & Campbell, 1979",
      "explanation": "The passage attributes this concept to Cook and Campbell (1979), who authored the influential text 'Quasi-Experimentation: Design and Analysis Issues for Field Settings.' The error replaces Campbell with Stanley. Julian Stanley is often associated with Campbell through a different work ('Experimental and Quasi-Experimental Designs for Research,' Campbell & Stanley, 1963), making this a plausible but incorrect substitution."
    },
    {
      "id": "PMET-SC-0085",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Criterion Deficiency",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0098",
      "modified_sentence": "Criterion deficiency occurs when the criterion measure fails to capture all relevant aspects of the outcome domain, and a deficient criterion overestimates true validity because the test may validly predict aspects of performance not captured by the criterion.",
      "phrases": [
        "Criterion deficiency occurs when the criterion measure fails to capture all relevant aspects of the outcome domain,",
        " and a deficient criterion overestimates true validity",
        " because the test may validly predict aspects of performance",
        " not captured by the criterion."
      ],
      "target_phrase_index": 1,
      "error_original": "overestimates true validity",
      "error_correct": "underestimates true validity",
      "explanation": "A deficient criterion underestimates true validity, not overestimates it. When the criterion fails to capture all relevant aspects of the outcome domain, the validity coefficient is attenuated because the test may be validly predicting performance dimensions that the criterion does not measure. This leads to a lower observed validity coefficient than the test's actual predictive power warrants."
    },
    {
      "id": "PMET-SC-0086",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Stimulus Control",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0186",
      "modified_sentence": "Stimulus control occurs when a behavior is more likely to occur in the presence of a particular stimulus than in its absence, and the controlling stimulus is called a discriminative stimulus (SD), which signals that punishment is available for a particular response.",
      "phrases": [
        "Stimulus control occurs when a behavior is more likely to occur",
        " in the presence of a particular stimulus than in its absence,",
        " and the controlling stimulus is called a discriminative stimulus (SD), which signals that punishment is available for a particular response."
      ],
      "target_phrase_index": 2,
      "error_original": "which signals that punishment is available",
      "error_correct": "which signals that reinforcement is available",
      "explanation": "A discriminative stimulus (SD) signals that reinforcement — not punishment — is available for a particular response. The SD indicates to the organism that performing a specific behavior in the presence of that stimulus will result in reinforcement. Replacing 'reinforcement' with 'punishment' fundamentally changes the definition of a discriminative stimulus."
    },
    {
      "id": "PMET-SC-0087",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Taste Aversion Learning",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0026",
      "modified_sentence": "Taste aversion learning also violates typical temporal contiguity requirements, as illness occurring minutes after food consumption can still produce strong conditioned aversions, and this makes adaptive sense because food poisoning typically has delayed effects.",
      "phrases": [
        "Taste aversion learning also violates typical temporal contiguity requirements,",
        " as illness occurring minutes after food consumption can still produce strong conditioned aversions,",
        " and this makes adaptive sense",
        " because food poisoning typically has delayed effects."
      ],
      "target_phrase_index": 1,
      "error_original": "minutes after food consumption",
      "error_correct": "hours after food consumption",
      "explanation": "Taste aversion learning is remarkable precisely because it can occur even when illness follows food consumption by several hours, not merely minutes. This long-delay learning violates the typical classical conditioning requirement that the CS and US occur close together in time. Changing 'hours' to 'minutes' understates this unique feature of taste aversion and makes it seem more consistent with standard temporal contiguity requirements, which contradicts the very point of the passage."
    },
    {
      "id": "PMET-SC-0088",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Multiple Correlation Coefficient (R)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0063",
      "modified_sentence": "The multiple correlation coefficient (R) indicates the correlation between the actual criterion scores and the predicted scores from the multiple regression equation, and R ranges from -1.00 to 1.00 because it represents the correlation between actual and predicted values.",
      "phrases": [
        "The multiple correlation coefficient (R) indicates the correlation between the actual criterion scores",
        " and the predicted scores from the multiple regression equation,",
        " and R ranges from -1.00 to 1.00",
        " because it represents the correlation between actual and predicted values."
      ],
      "target_phrase_index": 2,
      "error_original": "R ranges from -1.00 to 1.00",
      "error_correct": "R ranges from 0 to 1.00",
      "explanation": "The multiple correlation coefficient (R) always ranges from 0 to 1.00, not from -1.00 to 1.00. R is always positive because it represents the correlation between actual criterion scores and predicted scores, which cannot be negative. The range of -1.00 to 1.00 applies to bivariate correlation coefficients (r), not to the multiple correlation coefficient (R)."
    },
    {
      "id": "PMET-SC-0089",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Historical Foundations",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0166",
      "modified_sentence": "Thorndike observed that cats placed in puzzle boxes gradually learned to escape more quickly over successive trials, and he formulated the Law of Parsimony, stating that responses followed by satisfying consequences are strengthened, while responses followed by annoying consequences are weakened.",
      "phrases": [
        "Thorndike observed that cats placed in puzzle boxes",
        " gradually learned to escape more quickly over successive trials,",
        " and he formulated the Law of Parsimony, stating that responses followed by satisfying consequences are strengthened,",
        " while responses followed by annoying consequences are weakened."
      ],
      "target_phrase_index": 2,
      "error_original": "the Law of Parsimony",
      "error_correct": "the Law of Effect",
      "explanation": "Thorndike formulated the Law of Effect, which states that responses followed by satisfying consequences are strengthened and responses followed by annoying consequences are weakened. The Law of Parsimony (also known as Occam's Razor) is an unrelated principle about preferring the simplest explanation. The passage incorrectly substitutes 'Law of Parsimony' for 'Law of Effect.'"
    },
    {
      "id": "PMET-SC-0090",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to External Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0315",
      "modified_sentence": "Results may apply only to the specific type of participants studied, and volunteer bias is a common threat to internal validity where volunteers may differ from non-volunteers in motivation or characteristics.",
      "phrases": [
        "Results may apply only to the specific type of participants studied,",
        " and volunteer bias is a common threat to internal validity",
        " where volunteers may differ from non-volunteers in motivation or characteristics."
      ],
      "target_phrase_index": 1,
      "error_original": "internal validity",
      "error_correct": "external validity",
      "explanation": "Volunteer bias is a threat to external validity, not internal validity. It limits the generalizability of research findings to the broader population because volunteers may systematically differ from non-volunteers in important ways, making the sample unrepresentative. External validity concerns whether results can be generalized beyond the specific study conditions, which is exactly what volunteer bias compromises."
    },
    {
      "id": "PMET-SC-0091",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0495",
      "modified_sentence": "Escape conditioning terminates a present aversive stimulus and is maintained through positive punishment, while avoidance conditioning prevents a future aversive stimulus from occurring by allowing the organism to respond before it is presented.",
      "phrases": [
        "Escape conditioning terminates a present aversive stimulus",
        " and is maintained through positive punishment,",
        " while avoidance conditioning prevents a future aversive stimulus",
        " from occurring by allowing the organism to respond before it is presented."
      ],
      "target_phrase_index": 1,
      "error_original": "positive punishment",
      "error_correct": "negative reinforcement",
      "explanation": "Escape conditioning is maintained through negative reinforcement, not positive punishment. In escape conditioning, the organism's response removes (terminates) an aversive stimulus that is already present, which increases the likelihood of that response in the future. This removal of an aversive stimulus defines negative reinforcement. Positive punishment involves adding an aversive stimulus to decrease behavior, which is the opposite mechanism."
    },
    {
      "id": "PMET-SC-0092",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Regression to the Mean",
      "passage_type": "example",
      "source_passage_id": "PMET-0042",
      "modified_sentence": "Students who score extremely high on a midterm exam will, on average, score somewhat lower on the final exam, and this is a statistical artifact of perfect correlation between the two measurements.",
      "phrases": [
        "Students who score extremely high on a midterm exam will,",
        " on average, score somewhat lower on the final exam,",
        " and this is a statistical artifact of perfect correlation between the two measurements."
      ],
      "target_phrase_index": 2,
      "error_original": "perfect correlation",
      "error_correct": "imperfect correlation",
      "explanation": "Regression to the mean occurs specifically because the correlation between two measurements is imperfect (i.e., less than 1.0). If the correlation were perfect, there would be no regression to the mean — extreme scores on one test would correspond to equally extreme scores on the other. The passage correctly states it is a statistical artifact of 'imperfect correlation,' not 'perfect correlation.'"
    },
    {
      "id": "PMET-SC-0093",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Alternate (Parallel) Forms Reliability",
      "passage_type": "example",
      "source_passage_id": "PMET-0213",
      "modified_sentence": "A publisher develops Form A and Form B of an achievement test, each with different items measuring the same content domain, and students take Form A one week and Form B the next, and a correlation of .85 between forms indicates good test-retest reliability, suggesting the two versions measure the same construct consistently.",
      "phrases": [
        "A publisher develops Form A and Form B of an achievement test,",
        " each with different items measuring the same content domain, and students take Form A one week and Form B the next,",
        " and a correlation of .85 between forms indicates good test-retest reliability,",
        " suggesting the two versions measure the same construct consistently."
      ],
      "target_phrase_index": 2,
      "error_original": "good test-retest reliability",
      "error_correct": "good alternate forms reliability",
      "explanation": "The passage describes administering two different forms (Form A and Form B) of the same test, which is the procedure for assessing alternate forms (parallel forms) reliability, not test-retest reliability. Test-retest reliability involves administering the same form of a test on two occasions. Although both methods involve two administrations separated by time, the key distinction is that alternate forms reliability uses two different versions of the test, while test-retest reliability uses the identical test both times."
    },
    {
      "id": "PMET-SC-0094",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Kurtosis",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0405",
      "modified_sentence": "Leptokurtic distributions are more peaked with heavier tails than normal (positive kurtosis), while platykurtic distributions are flatter with lighter tails (positive kurtosis).",
      "phrases": [
        "Leptokurtic distributions are more peaked with heavier tails than normal (positive kurtosis),",
        " while platykurtic distributions are flatter",
        " with lighter tails (positive kurtosis)."
      ],
      "target_phrase_index": 2,
      "error_original": "with lighter tails (positive kurtosis).",
      "error_correct": "with lighter tails (negative kurtosis).",
      "explanation": "Platykurtic distributions are flatter with lighter tails and are characterized by negative kurtosis, not positive kurtosis. Positive kurtosis describes leptokurtic distributions, which are more peaked with heavier tails."
    },
    {
      "id": "PMET-SC-0095",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Introduction to Variables",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0384",
      "modified_sentence": "Variables are characteristics or attributes that can take on different values across individuals, time, or conditions, and understanding the types of variables and how they are measured is fundamental to psychological research and assessment, because the nature of a variable determines which statistical analyses are appropriate and what conclusions can be drawn from data (Gravetter & Forzano, 2017).",
      "phrases": [
        "Variables are characteristics or attributes that can take on different values across individuals, time, or conditions,",
        " and understanding the types of variables and how they are measured is fundamental to psychological research and assessment,",
        " because the nature of a variable determines which statistical analyses are appropriate",
        " and what conclusions can be drawn from data (Gravetter & Forzano, 2017)."
      ],
      "target_phrase_index": 3,
      "error_original": "Gravetter & Forzano, 2017",
      "error_correct": "Gravetter & Wallnau, 2017",
      "explanation": "The passage attributes this information to Gravetter & Wallnau (2017), who authored a well-known statistics textbook used in psychology. Gravetter & Forzano is a different textbook by Gravetter focused on research methods, not statistics. The correct citation for this particular claim about statistical analyses and variables is Gravetter & Wallnau (2017)."
    },
    {
      "id": "PMET-SC-0096",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Multivariate Analysis of Covariance (MANCOVA)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0146",
      "modified_sentence": "MANCOVA combines the features of both MANOVA and ANCOVA—it analyzes multiple independent variables simultaneously while statistically controlling for one or more covariates.",
      "phrases": [
        "MANCOVA combines the features of both MANOVA and ANCOVA—",
        "it analyzes multiple independent variables simultaneously",
        " while statistically controlling for one or more covariates."
      ],
      "target_phrase_index": 1,
      "error_original": "multiple independent variables simultaneously",
      "error_correct": "multiple dependent variables simultaneously",
      "explanation": "MANCOVA analyzes multiple dependent variables (outcome measures) simultaneously, not independent variables. The 'M' in MANCOVA stands for 'Multivariate,' referring to multiple dependent variables being analyzed at once. Independent variables in MANCOVA are the grouping factors, while the covariates are the variables being statistically controlled for."
    },
    {
      "id": "PMET-SC-0097",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Regression Discontinuity Design",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0262",
      "modified_sentence": "The regression discontinuity design assigns participants to conditions based on a cutoff score on a pretest measure, and those above the cutoff receive one treatment while those below receive another, with the design examining whether there is a continuity (jump or drop) in the outcome variable at the cutoff point, which would indicate a treatment effect.",
      "phrases": [
        "The regression discontinuity design assigns participants to conditions based on a cutoff score on a pretest measure,",
        " and those above the cutoff receive one treatment while those below receive another,",
        " with the design examining whether there is a continuity (jump or drop) in the outcome variable at the cutoff point,",
        " which would indicate a treatment effect."
      ],
      "target_phrase_index": 2,
      "error_original": "a continuity (jump or drop)",
      "error_correct": "a discontinuity (jump or drop)",
      "explanation": "The regression discontinuity design specifically examines whether there is a discontinuity — a sudden jump or drop — in the outcome variable at the cutoff point. A 'continuity' would mean no abrupt change, which is the opposite of what the design looks for as evidence of a treatment effect. The correct term is 'discontinuity,' which is even reflected in the name of the design itself."
    },
    {
      "id": "PMET-SC-0098",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Pretest-Posttest Control Group Design",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0255",
      "modified_sentence": "The pretest-posttest control group design includes random assignment of participants to treatment and control groups, with measurements taken only after treatment, allowing researchers to assess change over time and control for pre-existing differences between groups.",
      "phrases": [
        "The pretest-posttest control group design includes random assignment of participants to treatment and control groups,",
        " with measurements taken only after treatment,",
        " allowing researchers to assess change over time",
        " and control for pre-existing differences between groups."
      ],
      "target_phrase_index": 1,
      "error_original": "with measurements taken only after treatment,",
      "error_correct": "with measurements taken before and after treatment,",
      "explanation": "The pretest-posttest control group design specifically involves measurements taken both before (pretest) and after (posttest) treatment. Stating that measurements are taken 'only after treatment' describes a posttest-only control group design, which is a different research design. The defining feature of the pretest-posttest design is that it includes a pretest to establish baseline scores, enabling researchers to assess change over time."
    },
    {
      "id": "PMET-SC-0099",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0448",
      "modified_sentence": "PPV and NPV are both influenced by the base rate of the condition, and a low base rate will result in a low NPV even when the test has good sensitivity and specificity.",
      "phrases": [
        "PPV and NPV are both influenced by the base rate of the condition,",
        " and a low base rate will result in a low NPV",
        " even when the test has good sensitivity and specificity."
      ],
      "target_phrase_index": 1,
      "error_original": "a low NPV",
      "error_correct": "a low PPV",
      "explanation": "A low base rate reduces the Positive Predictive Value (PPV), not the Negative Predictive Value (NPV). When the base rate of a condition is low, even a test with good sensitivity and specificity will produce many false positives relative to true positives, thereby lowering PPV. Conversely, a high base rate is what reduces NPV."
    },
    {
      "id": "PMET-SC-0100",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0433",
      "modified_sentence": "The Pearson r correlation coefficient requires continuous, normally distributed variables, assumes a monotonic relationship between the two variables, and produces values ranging from -1.00 to +1.00.",
      "phrases": [
        "The Pearson r correlation coefficient requires continuous, normally distributed variables,",
        " assumes a monotonic relationship between the two variables,",
        " and produces values ranging from -1.00 to +1.00."
      ],
      "target_phrase_index": 1,
      "error_original": "assumes a monotonic relationship between the two variables",
      "error_correct": "assumes a linear relationship between the two variables",
      "explanation": "The Pearson r correlation coefficient specifically assumes a linear relationship between variables. A monotonic relationship is the assumption underlying Spearman's rho (rank-order correlation). If the relationship between variables is curvilinear, the Pearson r will underestimate the strength of the association. This is a commonly tested distinction on the EPPP."
    },
    {
      "id": "PMET-SC-0101",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Major Evaluation Models",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0293",
      "modified_sentence": "Empowerment evaluation, developed by David Fetterman, is a participatory approach in which program stakeholders are trained to conduct their own evaluations, and the evaluator serves as an external auditor rather than a facilitator or coach.",
      "phrases": [
        "Empowerment evaluation, developed by David Fetterman,",
        " is a participatory approach in which program stakeholders are trained to conduct their own evaluations,",
        " and the evaluator serves as an external auditor",
        " rather than a facilitator or coach."
      ],
      "target_phrase_index": 2,
      "error_original": "the evaluator serves as an external auditor",
      "error_correct": "the evaluator serves as a facilitator or coach",
      "explanation": "In empowerment evaluation, the evaluator serves as a facilitator or coach rather than an external judge. The modified sentence reverses the roles, incorrectly stating the evaluator serves as an external auditor (similar to 'external judge') rather than a facilitator or coach, when in fact it is the opposite: the evaluator acts as a facilitator/coach rather than an external judge."
    },
    {
      "id": "PMET-SC-0102",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0436",
      "modified_sentence": "In a meta-analysis, Cohen's d or Pearson's r are commonly used as effect size metrics, and the Q statistic is used to assess homoscedasticity across studies, with results typically displayed in a forest plot that visually summarizes each study's effect size and confidence interval.",
      "phrases": [
        "In a meta-analysis, Cohen's d or Pearson's r are commonly used as effect size metrics,",
        " and the Q statistic is used to assess homoscedasticity across studies,",
        " with results typically displayed in a forest plot",
        " that visually summarizes each study's effect size and confidence interval."
      ],
      "target_phrase_index": 1,
      "error_original": "homoscedasticity across studies",
      "error_correct": "heterogeneity across studies",
      "explanation": "In meta-analysis, Cochran's Q statistic is used to test for heterogeneity (variability in effect sizes) across studies, not homoscedasticity. Homoscedasticity refers to the assumption of equal variances in regression analysis, which is a different concept entirely. Heterogeneity in meta-analysis indicates whether the differences in effect sizes across studies are greater than what would be expected by chance alone."
    },
    {
      "id": "PMET-SC-0103",
      "mode": "sentence_click",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Two-Factor Theory of Avoidance",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0483",
      "modified_sentence": "Factor 1 of the two-factor theory involves classical conditioning, where a neutral stimulus becomes a conditioned stimulus for fear through pairing with an appetitive unconditioned stimulus.",
      "phrases": [
        "Factor 1 of the two-factor theory involves classical conditioning,",
        " where a neutral stimulus becomes a conditioned stimulus for fear",
        " through pairing with an appetitive unconditioned stimulus."
      ],
      "target_phrase_index": 2,
      "error_original": "an appetitive unconditioned stimulus",
      "error_correct": "an aversive unconditioned stimulus",
      "explanation": "In the two-factor theory of avoidance, Factor 1 (classical conditioning) involves a neutral stimulus becoming a conditioned stimulus for fear by being paired with an aversive (not appetitive) unconditioned stimulus. An appetitive stimulus is one that is desired or pleasant, which is the opposite of what produces a fear response. The correct term is 'aversive,' meaning unpleasant or punishing."
    },
    {
      "id": "PMET-VD-0051",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Statistical Conclusion Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0334",
      "entries": [
        {
          "term": "Restriction of Range",
          "definition": "Limited variability in scores attenuates correlations and reduces the ability to detect relationships that exist in the population. This is a common threat to statistical conclusion validity that occurs when sampling or selection procedures truncate the distribution of one or more variables.",
          "is_target": true,
          "is_error": false
        },
        {
          "term": "Statistical Regression",
          "definition": "The tendency for extreme scores on a measure to move closer to the mean upon retesting, regardless of any treatment effect. This threat to internal validity is especially problematic in studies that select participants based on extreme scores on a pretest.",
          "is_target": false
        },
        {
          "term": "Low Statistical Power",
          "definition": "An insufficient likelihood of detecting a true effect when one exists, often caused by small sample sizes or large within-group variability. This threat to statistical conclusion validity increases the probability of making a Type II error in hypothesis testing.",
          "is_target": false
        },
        {
          "term": "Restriction of Range",
          "definition": "Limited variability in scores inflates correlations and increases the likelihood of detecting spurious relationships that do not exist in the population. This is a common threat to statistical conclusion validity that occurs when sampling procedures truncate the distribution of one or more variables.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "inflates correlations and increases the likelihood of detecting spurious relationships that do not exist in the population",
      "error_correct": "attenuates correlations and reduces the ability to detect relationships that exist in the population",
      "explanation": "Restriction of range attenuates (weakens) correlations rather than inflating them. When variability is limited, the true relationship between variables is underestimated, reducing the ability to detect real relationships—not increasing the detection of spurious ones. This is a direction reversal of the actual effect of range restriction on correlations."
    },
    {
      "id": "PMET-VD-0052",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Factorial ANOVA",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0462",
      "entries": [
        {
          "term": "Main Effect",
          "definition": "In factorial ANOVA, a main effect refers to the overall impact of one independent variable on the dependent variable, averaged across all levels of the other independent variables in the design.",
          "is_target": false
        },
        {
          "term": "Interaction Effect",
          "definition": "In factorial ANOVA, an interaction effect occurs when the effect of one independent variable on the dependent variable depends on the level of the dependent variable rather than another independent variable in the design.",
          "is_target": true
        },
        {
          "term": "Simple Effect",
          "definition": "A simple effect refers to the effect of one independent variable at a specific, fixed level of another independent variable, often examined through follow-up analyses after a significant interaction is found.",
          "is_target": false
        },
        {
          "term": "Between-Subjects Factor",
          "definition": "A between-subjects factor is an independent variable in a factorial ANOVA in which different groups of participants are assigned to each level, so that each participant experiences only one condition of that factor.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "depends on the level of the dependent variable rather than another independent variable",
      "error_correct": "depends on the level of another independent variable",
      "explanation": "An interaction effect in factorial ANOVA occurs when the effect of one independent variable (IV) depends on the level of another independent variable, not the dependent variable (DV). The error swaps 'another independent variable' with 'the dependent variable,' which fundamentally mischaracterizes what an interaction effect describes."
    },
    {
      "id": "PMET-VD-0053",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "The Three-Term Contingency",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0190",
      "entries": [
        {
          "term": "Functional Behavior Assessment",
          "definition": "A systematic process used to identify the antecedents and consequences maintaining a problem behavior by examining the three-term contingency. It is commonly used in clinical and educational settings to inform the development of behavior intervention plans.",
          "is_target": false
        },
        {
          "term": "Three-Term Contingency",
          "definition": "The foundational framework of operant conditioning consisting of the antecedent, behavior, and consequence (A-B-C). Developed as a core concept by B.F. Skinner, it describes how environmental events before and after a behavior influence the future likelihood of that behavior occurring.",
          "is_target": false
        },
        {
          "term": "Behavior Intervention Plan",
          "definition": "A formalized plan derived from functional behavior assessment data that specifies strategies for modifying problem behavior. It typically includes antecedent modifications, replacement behaviors, and consequence-based strategies aligned with the four-term contingency framework.",
          "is_target": true
        },
        {
          "term": "Applied Behavior Analysis",
          "definition": "A scientific discipline that applies principles of operant conditioning to produce socially significant changes in behavior. It relies heavily on systematic assessment methods such as functional behavior assessment and data-driven intervention planning.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "four-term contingency framework",
      "error_correct": "three-term contingency framework",
      "explanation": "The definition of Behavior Intervention Plan incorrectly refers to the 'four-term contingency framework' when it should say 'three-term contingency framework.' The three-term contingency (antecedent-behavior-consequence, or A-B-C) is the foundational framework used in functional behavior assessment and behavior intervention planning. While a four-term contingency does exist (adding the motivating operation), the standard framework referenced in BIP development from functional behavior assessment is the three-term contingency."
    },
    {
      "id": "PMET-VD-0054",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Introduction to Correlation",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0043",
      "entries": [
        {
          "term": "Regression",
          "definition": "A statistical technique used to predict the value of a dependent variable based on one or more independent variables. It builds upon correlation by establishing a predictive equation that describes how changes in predictor variables relate to changes in the outcome variable.",
          "is_target": false
        },
        {
          "term": "Correlation coefficient",
          "definition": "A numerical index that quantifies both the strength and direction of the curvilinear relationship between two variables. It is foundational to many assessment concepts in psychology, including test reliability and validity, and ranges from -1.00 to +1.00.",
          "is_target": true
        },
        {
          "term": "Coefficient of determination",
          "definition": "A statistic obtained by squaring the correlation coefficient, representing the proportion of variance in one variable that is accounted for by the other variable. It provides a measure of shared variance and is useful for interpreting the practical significance of a correlation.",
          "is_target": false
        },
        {
          "term": "Covariance",
          "definition": "A statistical measure that indicates the extent to which two variables change together, reflecting the direction of their relationship. Unlike the correlation coefficient, it is not standardized, so its magnitude depends on the measurement scales of the variables involved.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "curvilinear relationship",
      "error_correct": "linear relationship",
      "explanation": "The correlation coefficient (specifically Pearson's r) quantifies the strength and direction of the LINEAR relationship between two variables, not the curvilinear relationship. A standard correlation coefficient would actually underestimate the strength of a curvilinear relationship, which is why this distinction is important in psychometrics."
    },
    {
      "id": "PMET-VD-0055",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Stimulus Generalization and Discrimination",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0014",
      "entries": [
        {
          "term": "Stimulus Discrimination",
          "definition": "Stimulus discrimination in classical conditioning is the learned ability to distinguish between the conditioned stimulus and other similar stimuli, responding only to the specific CS that was paired with the unconditioned stimulus. This process is typically established through differential reinforcement training.",
          "is_target": false
        },
        {
          "term": "Stimulus Generalization",
          "definition": "Stimulus generalization occurs when stimuli similar to the original CS also elicit the conditioned response. The degree of generalization typically follows a gradient, where stimuli more dissimilar to the original CS elicit stronger responses than similar stimuli, as first systematically demonstrated by Hovland (1937).",
          "is_target": true
        },
        {
          "term": "Higher-Order Conditioning",
          "definition": "Higher-order conditioning occurs when a well-established conditioned stimulus is used to condition a response to a new, neutral stimulus. In second-order conditioning, a new CS is paired with an already-established CS rather than directly with the unconditioned stimulus.",
          "is_target": false
        },
        {
          "term": "Spontaneous Recovery",
          "definition": "Spontaneous recovery is the reappearance of a previously extinguished conditioned response after a rest period following extinction. The recovered response is typically weaker than the original conditioned response and will extinguish more rapidly if the CS is again presented without the US.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "stimuli more dissimilar to the original CS elicit stronger responses than similar stimuli",
      "error_correct": "stimuli more similar to the original CS elicit stronger responses than dissimilar stimuli",
      "explanation": "The error reverses the direction of the generalization gradient. In reality, stimuli that are more similar to the original conditioned stimulus elicit stronger conditioned responses, while more dissimilar stimuli elicit weaker responses. This is the fundamental principle of the generalization gradient — response strength decreases as stimuli become less similar to the original CS."
    },
    {
      "id": "PMET-VD-0056",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Internal Consistency Reliability",
      "passage_type": "definition",
      "source_passage_id": "PMET-0205",
      "entries": [
        {
          "term": "Kuder-Richardson Formula 20",
          "definition": "KR-20 is a measure of internal consistency reliability specifically designed for dichotomously scored items (e.g., right/wrong). It estimates reliability by considering the proportion of examinees passing and failing each item, along with the total test variance.",
          "is_target": false
        },
        {
          "term": "Spearman-Brown Prophecy Formula",
          "definition": "The Spearman-Brown Prophecy Formula estimates full-test reliability from a split-half correlation using the equation r_xx = (2 × r_hh) / (1 − r_hh), where r_hh is the correlation between the two halves. For instance, if the half-test correlation is .60, the formula yields an estimated full-test reliability higher than the half-test correlation alone.",
          "is_target": true
        },
        {
          "term": "Coefficient Alpha",
          "definition": "Coefficient alpha (Cronbach's alpha) is a widely used measure of internal consistency reliability that represents the average of all possible split-half correlations for a test. It is appropriate for items that are scored on a continuous or multi-point scale rather than dichotomously.",
          "is_target": false
        },
        {
          "term": "Split-Half Reliability",
          "definition": "Split-half reliability is an internal consistency method in which a test is divided into two equivalent halves and the scores on each half are correlated. Because this correlation only reflects half the test's length, a correction formula is typically applied to estimate the reliability of the full test.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "(1 − r_hh)",
      "error_correct": "(1 + r_hh)",
      "explanation": "The correct Spearman-Brown Prophecy Formula has a denominator of (1 + r_hh), not (1 − r_hh). Using (1 − r_hh) would be a different formula entirely and would yield inflated reliability estimates. The correct formula is r_xx = (2 × r_hh) / (1 + r_hh)."
    },
    {
      "id": "PMET-VD-0057",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Sampling Methods",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0280",
      "entries": [
        {
          "term": "Convenience Sampling",
          "definition": "A nonprobability sampling method in which participants are selected based on their easy availability and accessibility to the researcher. It is the most commonly used sampling method in psychological research but limits the generalizability of findings to the broader population.",
          "is_target": false
        },
        {
          "term": "Stratified Random Sampling",
          "definition": "A probability sampling method in which the population is divided into meaningful subgroups (strata) based on specific characteristics, and random samples are then drawn from each subgroup. This technique helps ensure that key demographic groups are proportionally represented in the final sample.",
          "is_target": false
        },
        {
          "term": "Probability Sampling",
          "definition": "A category of sampling methods in which every member of the population has a known, nonzero chance of being selected for inclusion in the study. These methods support clinical generalization, allowing researchers to draw valid inferences from the sample to the larger population.",
          "is_target": true
        },
        {
          "term": "Cluster Sampling",
          "definition": "A probability sampling method in which naturally occurring groups or clusters within a population are randomly selected, and then all or some members within those clusters are included. This approach is often used when a complete list of individual population members is unavailable or impractical to obtain.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "clinical generalization",
      "error_correct": "statistical generalization",
      "explanation": "The passage states that probability sampling methods support 'statistical generalization,' meaning the ability to generalize findings from the sample to the population based on statistical inference. The error replaces 'statistical generalization' with 'clinical generalization,' which is a different concept referring to the applicability of findings to individual clinical cases."
    },
    {
      "id": "PMET-VD-0058",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Hypothesis Testing Framework",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0125",
      "entries": [
        {
          "term": "Significance Level",
          "definition": "The significance level (alpha) represents the probability of making a Type II error, or rejecting the null hypothesis when it is actually true. It is typically set at .05 or .01 before data collection begins. This threshold determines the critical region used to evaluate the test statistic.",
          "is_target": true
        },
        {
          "term": "Null Hypothesis",
          "definition": "The null hypothesis (H₀) is a statement positing no effect, no difference, or no relationship in the population. It serves as the default assumption in hypothesis testing and is either rejected or retained based on the test statistic. Researchers typically aim to gather evidence against this hypothesis.",
          "is_target": false
        },
        {
          "term": "Test Statistic",
          "definition": "A test statistic is a numerical value computed from sample data that is used to evaluate the null hypothesis. Common examples include the t-statistic, z-score, F-ratio, and chi-square. Its value is compared to a critical value or used to obtain a p-value for decision-making.",
          "is_target": false
        },
        {
          "term": "Statistical Power",
          "definition": "Statistical power is the probability of correctly rejecting a false null hypothesis, or the likelihood of detecting an effect when one truly exists. It is influenced by sample size, effect size, and the chosen significance level. A commonly recommended minimum power level is .80.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "probability of making a Type II error",
      "error_correct": "probability of making a Type I error",
      "explanation": "The significance level (alpha) represents the probability of making a Type I error — rejecting the null hypothesis when it is actually true. A Type II error (beta) is the probability of failing to reject a false null hypothesis. The definition incorrectly labels this as a Type II error."
    },
    {
      "id": "PMET-VD-0059",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Restriction of Range",
      "passage_type": "example",
      "source_passage_id": "PMET-0040",
      "entries": [
        {
          "term": "Restriction of Range",
          "definition": "A statistical artifact that occurs when the variability of one or both variables in a correlation is limited, typically due to selection processes. This truncation of scores inflates the observed correlation coefficient, making it appear stronger than it would be with the full range of scores. It is a common concern in personnel selection when only high-scoring applicants are hired.",
          "is_target": true
        },
        {
          "term": "Criterion Contamination",
          "definition": "A measurement problem that occurs when a criterion measure is influenced by knowledge of predictor scores or other extraneous information. This compromises the independence of the criterion and can artificially alter observed validity coefficients. It is a common concern in validation studies where supervisors know employees' test scores.",
          "is_target": false
        },
        {
          "term": "Attenuation Due to Unreliability",
          "definition": "A statistical phenomenon in which the observed correlation between two variables is weakened because one or both measures contain measurement error. The less reliable the measures, the lower the observed correlation will be compared to the true relationship. Correction formulas exist to estimate what the correlation would be if measures were perfectly reliable.",
          "is_target": false
        },
        {
          "term": "Cross-Validation Shrinkage",
          "definition": "The expected decrease in a validity coefficient when a prediction equation derived from one sample is applied to a new sample. This occurs because the original regression equation capitalizes on chance characteristics specific to the derivation sample. It is particularly problematic when the original sample is small or the number of predictors is large.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "inflates the observed correlation coefficient, making it appear stronger",
      "error_correct": "attenuates (weakens) the observed correlation coefficient, making it appear weaker",
      "explanation": "Restriction of range attenuates (reduces) the observed correlation, not inflates it. As demonstrated in the passage, the correlation among selected employees (r = .20) was lower than the unrestricted correlation (r = .50). Truncating the range of scores reduces variability, which systematically weakens the observed relationship between variables."
    },
    {
      "id": "PMET-VD-0060",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Conditioned Compensatory Responses",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0030",
      "entries": [
        {
          "term": "Conditioned Compensatory Response",
          "definition": "A classically conditioned response that is opposite in direction to the unconditioned effect of a drug, developing through repeated drug administration in a consistent environment. These responses contribute to drug tolerance by counteracting the drug's effects. When drugs are administered in novel environments, conditioned compensatory responses are absent, meaning the full drug effect is experienced and potentially leading to overdose at doses normally tolerated in familiar settings.",
          "is_target": false
        },
        {
          "term": "Conditioned Emotional Response",
          "definition": "A classically conditioned emotional reaction, such as fear or anxiety, that develops when a neutral stimulus is repeatedly paired with an emotionally significant unconditioned stimulus. This process was famously demonstrated in Watson and Rayner's Little Albert experiment. The conditioned emotional response generalizes to similar stimuli and can be extinguished through repeated exposure without the unconditioned stimulus.",
          "is_target": false
        },
        {
          "term": "Drug Tolerance",
          "definition": "A phenomenon in which repeated administration of a drug leads to a diminished physiological or psychological response, requiring higher doses to achieve the same effect. According to Siegel's model, tolerance develops partly because conditioned compensatory responses are elicited by environmental cues associated with drug use. When drugs are taken in familiar settings, these conditioned responses counteract the drug's primary effects.",
          "is_target": false
        },
        {
          "term": "Conditioned Drug Overdose",
          "definition": "A phenomenon described by Shepard Siegel in which drug overdose occurs when a drug is administered in a familiar environment where conditioned compensatory responses are maximally activated, overwhelming the body's ability to counteract the drug. This explains why experienced drug users sometimes overdose at doses they have previously tolerated, as the environmental cues amplify rather than buffer the drug's effects.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "administered in a familiar environment where conditioned compensatory responses are maximally activated",
      "error_correct": "administered in a novel environment where conditioned compensatory responses are absent",
      "explanation": "The error reverses the environmental context. According to Siegel's model, conditioned drug overdose occurs when drugs are taken in a NOVEL (unfamiliar) environment, where conditioned compensatory responses have NOT developed. In familiar environments, conditioned compensatory responses are present and help counteract the drug's effects, providing tolerance. The target definition incorrectly states that overdose occurs in familiar environments with maximally activated compensatory responses."
    },
    {
      "id": "PMET-VD-0061",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Measures of Central Tendency",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0395",
      "entries": [
        {
          "term": "Median",
          "definition": "The median is the middle score in a ranked distribution, dividing the data set into two equal halves. It is resistant to extreme values and outliers, making it a preferred measure of central tendency for skewed distributions. The median is appropriate for ordinal, interval, and ratio data.",
          "is_target": false
        },
        {
          "term": "Mean",
          "definition": "The mean (M or X̄) is the arithmetic average, calculated by summing all scores and dividing by the number of scores. It uses information from every score but is resistant to extreme values (outliers). The mean is appropriate for interval and ratio data.",
          "is_target": true
        },
        {
          "term": "Mode",
          "definition": "The mode is the most frequently occurring score in a distribution and is the only measure of central tendency appropriate for nominal data. A distribution can be unimodal, bimodal, or multimodal depending on how many peaks it contains. It requires no arithmetic computation and is useful for describing categorical variables.",
          "is_target": false
        },
        {
          "term": "Trimmed Mean",
          "definition": "The trimmed mean is a measure of central tendency calculated by removing a fixed percentage of the smallest and largest values before computing the arithmetic average. It provides a compromise between the mean and the median by reducing the influence of outliers. It is commonly used when data contain extreme scores that may distort the standard mean.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "resistant to extreme values (outliers)",
      "error_correct": "sensitive to extreme values (outliers)",
      "explanation": "The mean is sensitive (not resistant) to extreme values or outliers, meaning that outliers can substantially pull the mean in their direction. The median, by contrast, is the measure of central tendency that is resistant to outliers."
    },
    {
      "id": "PMET-VD-0062",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Meta-Analysis of Validity Coefficients",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0582",
      "entries": [
        {
          "term": "Validity Generalization",
          "definition": "A meta-analytic approach, developed primarily by Schmidt and Hunter, that examines whether validity coefficients for selection tests generalize across different settings, jobs, and populations. It involves correcting observed validity coefficients for statistical artifacts such as range restriction and criterion unreliability. When residual variance is small after corrections, validity is said to generalize.",
          "is_target": true
        },
        {
          "term": "Cross-Validation",
          "definition": "A technique used to evaluate how well the results of a statistical analysis will generalize to an independent data set. In predictive validity studies, a regression equation developed on one sample is applied to a second sample to determine if prediction accuracy holds up, guarding against capitalization on chance.",
          "is_target": false
        },
        {
          "term": "Synthetic Validity",
          "definition": "An approach to establishing validity for a selection battery by combining validity evidence from multiple jobs that share common job elements or components. Rather than conducting a criterion-related study for each specific job, existing validity data for individual job components are synthesized to infer overall validity.",
          "is_target": false
        },
        {
          "term": "Incremental Validity",
          "definition": "The degree to which a new predictor adds to the prediction of a criterion above and beyond what can be accounted for by existing predictors. It is typically assessed by examining the change in the multiple correlation coefficient when a new measure is added to an existing selection battery.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "criterion unreliability",
      "error_correct": "measurement unreliability (including both predictor and criterion unreliability)",
      "explanation": "While criterion unreliability is one artifact corrected for, validity generalization studies correct for multiple statistical artifacts beyond just criterion unreliability. The standard Schmidt and Hunter approach corrects for sampling error, range restriction, and unreliability in both the predictor and the criterion — not just criterion unreliability. Stating only 'criterion unreliability' omits the important correction for predictor unreliability, making this a subtle but meaningful error."
    },
    {
      "id": "PMET-VD-0063",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Blocking",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0414",
      "entries": [
        {
          "term": "Overshadowing",
          "definition": "Overshadowing occurs when two conditioned stimuli are presented together during conditioning, and the more salient or intense stimulus acquires greater associative strength than the weaker stimulus. This phenomenon was extensively studied by Pavlov and demonstrates that stimuli compete for associative strength during learning.",
          "is_target": false
        },
        {
          "term": "Blocking",
          "definition": "Blocking occurs when a previously conditioned CS1 is paired with a new stimulus CS2 and both are presented together with the US. Because CS1 already predicts the US, the new CS2 fails to acquire associative strength. This phenomenon was first demonstrated by Leon Kamin and supports the Rescorla-Wagner model of conditioning.",
          "is_target": false
        },
        {
          "term": "Sensory Preconditioning",
          "definition": "Sensory preconditioning occurs when two neutral stimuli (CS1 and CS2) are first paired together, and then CS1 is paired with a US. When CS2 is later presented alone, it also elicits a conditioned response despite never having been directly paired with the US.",
          "is_target": false
        },
        {
          "term": "Latent Inhibition",
          "definition": "Latent inhibition occurs when repeated pre-exposure to a conditioned stimulus without a US facilitates subsequent conditioning when that stimulus is later paired with a US. This phenomenon demonstrates that prior familiarity with a neutral stimulus affects the rate at which associative learning can occur.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "facilitates subsequent conditioning",
      "error_correct": "retards (slows) subsequent conditioning",
      "explanation": "Latent inhibition actually retards or slows subsequent conditioning to a pre-exposed stimulus, rather than facilitating it. Repeated pre-exposure to a stimulus without consequence makes it harder, not easier, to later condition that stimulus to a US. This is because the organism has learned that the stimulus is irrelevant."
    },
    {
      "id": "PMET-VD-0064",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Introduction to Inferential Statistics",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0124",
      "entries": [
        {
          "term": "Sampling Distribution",
          "definition": "A theoretical probability distribution of a statistic (such as the mean) obtained from all possible samples of a given size drawn from a population. It forms the basis for hypothesis testing and confidence interval estimation in inferential statistics.",
          "is_target": false
        },
        {
          "term": "Inferential Statistics",
          "definition": "A branch of statistics that uses probability theory and sampling distributions to draw conclusions about populations based on sample data. These methods allow researchers to generalize findings beyond their specific sample and make evidence-based decisions.",
          "is_target": false
        },
        {
          "term": "Descriptive Statistics",
          "definition": "A branch of statistics concerned with organizing, summarizing, and presenting data in meaningful ways. Common measures include central tendency (mean, median, mode) and variability (range, variance, standard deviation).",
          "is_target": false
        },
        {
          "term": "Probability Theory",
          "definition": "A mathematical framework underlying inferential statistics that quantifies the likelihood of events occurring. In psychological research, it was formalized for statistical inference primarily through the work of Fisher and Bayesian principles developed by Laplace and Bernoulli.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "Laplace and Bernoulli",
      "error_correct": "Laplace and Bayes (Thomas Bayes)",
      "explanation": "The error attributes the development of Bayesian principles to Bernoulli instead of Thomas Bayes. While the Bernoulli family made important contributions to probability theory (e.g., Bernoulli trials), Bayesian inference is named after Reverend Thomas Bayes, whose theorem on conditional probability was published posthumously in 1763. Laplace later expanded upon Bayes' work significantly."
    },
    {
      "id": "PMET-VD-0065",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Types of Variables in Research",
      "passage_type": "example",
      "source_passage_id": "PMET-0380",
      "entries": [
        {
          "term": "Confounding Variable",
          "definition": "A confounding variable is an extraneous factor that systematically varies with the independent variable, making it difficult to determine which variable is responsible for the observed effect on the dependent variable. Researchers use techniques such as randomization and counterbalancing to control for confounds.",
          "is_target": false
        },
        {
          "term": "Dependent Variable",
          "definition": "The dependent variable is the outcome measure in an experiment that is manipulated by the researcher to observe its effect on participant behavior. In a sleep deprivation study, for example, performance on a memory test would serve as the dependent variable because it is expected to change based on the experimental condition.",
          "is_target": true
        },
        {
          "term": "Independent Variable",
          "definition": "The independent variable is the factor that is deliberately manipulated by the researcher to observe its effect on another variable. For example, in a sleep deprivation study, hours of sleep (e.g., 8 hours vs. 4 hours) would be the independent variable because the researcher controls which condition participants are assigned to.",
          "is_target": false
        },
        {
          "term": "Mediating Variable",
          "definition": "A mediating variable is an intervening factor that accounts for the relationship between the independent variable and the dependent variable. It helps explain the mechanism or process through which the IV exerts its influence on the DV, such as attention level mediating the effect of sleep on memory.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "manipulated by the researcher",
      "error_correct": "measured by the researcher",
      "explanation": "The dependent variable is the outcome that is measured (not manipulated) by the researcher. It is the independent variable that is manipulated. The dependent variable 'depends on' the level of the independent variable and is observed/measured to assess the effect of the manipulation."
    },
    {
      "id": "PMET-VD-0066",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "Item Parameters",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0499",
      "entries": [
        {
          "term": "Item Difficulty (b)",
          "definition": "In item response theory, the difficulty parameter represents the ability level at which an examinee has a 50% probability of answering the item correctly. Items with higher b values require greater ability to answer correctly, and this parameter corresponds to the location of the item characteristic curve along the ability axis.",
          "is_target": false
        },
        {
          "term": "Item Discrimination (a)",
          "definition": "In item response theory, the discrimination parameter indicates how well an item differentiates between examinees of different ability levels. Higher a values mean the item sharply distinguishes between those just above and just below the difficulty level. Graphically, this is represented by the steepness of the item characteristic curve at the asymptote.",
          "is_target": true
        },
        {
          "term": "Item Guessing (c)",
          "definition": "In item response theory, the guessing parameter represents the probability that an examinee with very low ability will answer the item correctly by chance. This parameter sets the lower asymptote of the item characteristic curve, and it is especially relevant for multiple-choice items where random guessing can inflate scores.",
          "is_target": false
        },
        {
          "term": "Item Information Function",
          "definition": "In item response theory, the item information function describes the amount of statistical information an item provides at each ability level. Items with high discrimination contribute the most information near their difficulty level, and the sum of item information functions across all items yields the test information function.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "at the asymptote",
      "error_correct": "at the inflection point",
      "explanation": "The steepness of the item characteristic curve that reflects the discrimination parameter occurs at the inflection point of the S-shaped curve, not at the asymptote. The asymptote refers to the upper or lower boundary that the curve approaches but never reaches (e.g., the guessing parameter defines the lower asymptote). The inflection point is the location on the curve where it is steepest, which corresponds to the difficulty level (b parameter)."
    },
    {
      "id": "PMET-VD-0067",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Nominal Scale",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0389",
      "entries": [
        {
          "term": "Ordinal Scale",
          "definition": "The ordinal scale assigns numbers to categories that can be ranked in a meaningful order, but the intervals between ranks are not necessarily equal. Examples include class rank, Likert-scale ratings, or socioeconomic status classifications.",
          "is_target": false
        },
        {
          "term": "Nominal Scale",
          "definition": "The nominal scale uses numbers only as labels or names for categories. Numbers can be meaningfully compared in terms of magnitude but cannot be added or subtracted. Examples include gender coding, diagnostic categories, or political party affiliation.",
          "is_target": true
        },
        {
          "term": "Interval Scale",
          "definition": "The interval scale assigns numbers with equal distances between values, but it lacks a true zero point. This means ratios between values are not meaningful. Examples include temperature in Fahrenheit or Celsius and IQ scores.",
          "is_target": false
        },
        {
          "term": "Ratio Scale",
          "definition": "The ratio scale has all the properties of an interval scale but also includes a true zero point, making ratio comparisons meaningful. Examples include height, weight, reaction time, and number of correct responses on a test.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "Numbers can be meaningfully compared in terms of magnitude but cannot be added or subtracted",
      "error_correct": "Numbers cannot be meaningfully added, subtracted, or compared in terms of magnitude",
      "explanation": "The error states that nominal scale numbers 'can be meaningfully compared in terms of magnitude,' which is incorrect. On a nominal scale, numbers serve only as labels for categories and have no quantitative meaning whatsoever — they cannot be compared in magnitude, added, or subtracted. The erroneous definition incorrectly grants one mathematical property (magnitude comparison) to the nominal scale while correctly denying others."
    },
    {
      "id": "PMET-VD-0068",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Kurtosis",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0405",
      "entries": [
        {
          "term": "Mesokurtic",
          "definition": "A mesokurtic distribution has the same kurtosis as the normal distribution, serving as the baseline reference point. It is neither overly peaked nor overly flat, and its tails are neither heavier nor lighter than what is expected under normality.",
          "is_target": false
        },
        {
          "term": "Platykurtic",
          "definition": "A platykurtic distribution is flatter and has lighter tails compared to a normal distribution. It is characterized by negative kurtosis values, indicating that data points are more evenly spread and extreme outliers are less common than in a normal distribution.",
          "is_target": false
        },
        {
          "term": "Leptokurtic",
          "definition": "A leptokurtic distribution is more peaked with heavier tails than a normal distribution. It is characterized by negative kurtosis values, indicating a sharper central peak and a greater likelihood of extreme outliers compared to a normal distribution.",
          "is_target": true
        },
        {
          "term": "Skewness",
          "definition": "Skewness describes the asymmetry of a distribution around its mean. Positive skewness indicates a longer right tail, while negative skewness indicates a longer left tail, and a value of zero indicates a perfectly symmetrical distribution.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "negative kurtosis values",
      "error_correct": "positive kurtosis values",
      "explanation": "Leptokurtic distributions are characterized by positive kurtosis values (excess kurtosis > 0), not negative kurtosis values. Negative kurtosis is associated with platykurtic distributions, which are flatter with lighter tails. The passage clearly states that leptokurtic distributions have positive kurtosis."
    },
    {
      "id": "PMET-VD-0069",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "True Experimental Designs",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0254",
      "entries": [
        {
          "term": "Quasi-Experimental Design",
          "definition": "A research design that involves manipulation of an independent variable but lacks random assignment to conditions. It allows researchers to examine relationships between variables while acknowledging that causal inferences are weaker than those from true experiments.",
          "is_target": false
        },
        {
          "term": "True Experimental Design",
          "definition": "A research design characterized by three essential features: (1) manipulation of at least one independent variable, (2) random selection of participants into conditions, and (3) control over extraneous variables. These features allow researchers to make strong causal inferences about the relationship between the independent and dependent variables.",
          "is_target": true
        },
        {
          "term": "Correlational Design",
          "definition": "A non-experimental research design that measures two or more variables to determine whether a statistical relationship exists between them. It does not involve manipulation of variables, so it cannot establish cause-and-effect relationships between the variables studied.",
          "is_target": false
        },
        {
          "term": "Pre-Experimental Design",
          "definition": "A research design that lacks one or more of the critical features of a true experiment, such as a control group or random assignment. Examples include the one-shot case study and the one-group pretest-posttest design, both of which have significant threats to internal validity.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "random selection of participants into conditions",
      "error_correct": "random assignment of participants to conditions",
      "explanation": "The error substitutes 'random selection' for 'random assignment.' These are distinct concepts in research methodology. Random assignment refers to allocating participants to experimental conditions by chance, which helps ensure group equivalence and supports causal inference. Random selection, on the other hand, refers to drawing participants from a population so that every member has an equal chance of being chosen, which supports external validity (generalizability) but is not a defining feature of true experimental designs."
    },
    {
      "id": "PMET-VD-0070",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0511",
      "entries": [
        {
          "term": "Two-Parameter Logistic (2PL) Model",
          "definition": "An IRT model that estimates two item parameters: difficulty and discrimination. It extends the Rasch model by allowing each item to have a unique discrimination parameter, providing a more flexible fit for many testing applications.",
          "is_target": false
        },
        {
          "term": "Three-Parameter Logistic (3PL) Model",
          "definition": "An IRT model that estimates three item parameters: difficulty, discrimination, and a pseudo-guessing parameter. It is particularly useful for multiple-choice tests where examinees may answer correctly by guessing, even when their ability is low.",
          "is_target": false
        },
        {
          "term": "One-Parameter Logistic (1PL)/Rasch Model",
          "definition": "An IRT model developed by Georg Rasch that estimates only a single item parameter: discrimination. All items are assumed to have equal difficulty and identical guessing probabilities of zero, making it the most parsimonious of the standard IRT models.",
          "is_target": true
        },
        {
          "term": "Item Characteristic Curve (ICC)",
          "definition": "A graphical representation used in IRT that plots the probability of a correct response as a function of examinee ability. Its shape is determined by the item's parameters such as difficulty, discrimination, and guessing.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "estimates only a single item parameter: discrimination",
      "error_correct": "estimates only a single item parameter: difficulty",
      "explanation": "The Rasch (1PL) model estimates only the difficulty parameter for each item, not discrimination. In the 1PL model, discrimination is held constant (equal) across all items. Discrimination is the additional parameter introduced in the 2PL model."
    },
    {
      "id": "PMET-VD-0071",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Skewness",
      "passage_type": "example",
      "source_passage_id": "PMET-0383",
      "entries": [
        {
          "term": "Negative Skew",
          "definition": "A distribution where the tail extends to the left. A classic example is age at death in developed countries, where most people live to old age but some early deaths pull the mean down below the median.",
          "is_target": false
        },
        {
          "term": "Kurtosis",
          "definition": "A statistical measure that describes the degree to which scores cluster in the tails or the peak of a frequency distribution. Leptokurtic distributions have heavy tails and a sharp peak, while platykurtic distributions have light tails and a flatter peak.",
          "is_target": false
        },
        {
          "term": "Positive Skew",
          "definition": "A distribution where the tail extends to the right. A classic example is income distribution, where many people earn low-to-moderate incomes while a few very high incomes pull the median up above the mean.",
          "is_target": true
        },
        {
          "term": "Normal Distribution",
          "definition": "A symmetric, bell-shaped distribution in which the mean, median, and mode are all equal. It is defined by two parameters—the mean and standard deviation—and approximately 68% of scores fall within one standard deviation of the mean.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "pull the median up above the mean",
      "error_correct": "pull the mean up above the median",
      "explanation": "In a positively skewed distribution, extreme high values pull the mean upward, so the mean is greater than the median (mean > median > mode). The error reverses this relationship, incorrectly stating that the median is pulled above the mean."
    },
    {
      "id": "PMET-VD-0072",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Setting Cutoff Scores",
      "passage_type": "example",
      "source_passage_id": "PMET-0091",
      "entries": [
        {
          "term": "False Positive",
          "definition": "A false positive occurs when a test incorrectly classifies someone as meeting criteria when they actually do not. In screening contexts, false positives lead to unnecessary follow-up evaluations or interventions for individuals who do not have the condition.",
          "is_target": false
        },
        {
          "term": "Cutoff Score",
          "definition": "A cutoff score is a predetermined point on a test's score distribution used to classify examinees into categories such as pass/fail or at-risk/not-at-risk. For a suicide risk screening, a higher cutoff is appropriate because the cost of missing someone at risk (false negative) is much greater than the cost of additional evaluation for someone not at risk.",
          "is_target": true
        },
        {
          "term": "False Negative",
          "definition": "A false negative occurs when a test incorrectly classifies someone as not meeting criteria when they actually do. In high-stakes screening situations such as suicide risk assessment, false negatives are especially dangerous because at-risk individuals go unidentified.",
          "is_target": false
        },
        {
          "term": "Sensitivity",
          "definition": "Sensitivity refers to a test's ability to correctly identify individuals who truly have the condition, also known as the true positive rate. Tests with high sensitivity are preferred in screening contexts where missing a true case carries serious consequences.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "a higher cutoff is appropriate",
      "error_correct": "a lower cutoff is appropriate",
      "explanation": "For a suicide risk screening, a lower cutoff score is appropriate, not a higher one. A lower cutoff casts a wider net, catching more true positives at the expense of more false positives. This is preferred when the cost of a false negative (missing someone who is truly at risk) is far greater than the cost of a false positive (referring someone for additional evaluation who is not actually at risk)."
    },
    {
      "id": "PMET-VD-0073",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Nominal Scale",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0595",
      "entries": [
        {
          "term": "Nominal Scale",
          "definition": "A level of measurement that classifies data into distinct categories without any inherent order. Appropriate statistics for nominal data include frequency counts, percentages, median, and chi-square tests. It is considered the most basic level of measurement in research.",
          "is_target": true
        },
        {
          "term": "Ordinal Scale",
          "definition": "A level of measurement that classifies data into categories that can be meaningfully ranked or ordered. Appropriate statistics include median, percentile ranks, and nonparametric tests such as Spearman's rank correlation. It provides more information than nominal data but does not assume equal intervals between ranks.",
          "is_target": false
        },
        {
          "term": "Interval Scale",
          "definition": "A level of measurement with equal distances between values but no true zero point. Appropriate statistics include mean, standard deviation, Pearson correlation, and parametric tests such as t-tests and ANOVA. Temperature measured in Fahrenheit or Celsius is a classic example.",
          "is_target": false
        },
        {
          "term": "Ratio Scale",
          "definition": "The highest level of measurement, featuring equal intervals and a true zero point that indicates absence of the measured attribute. Appropriate statistics include all parametric and nonparametric tests, as well as geometric and harmonic means. Examples include weight, height, and reaction time.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "median",
      "error_correct": "mode",
      "explanation": "The definition incorrectly lists 'median' as an appropriate measure of central tendency for nominal data. The median requires at least ordinal data because values must be rank-ordered. The correct measure of central tendency for nominal data is the 'mode,' which identifies the most frequently occurring category."
    },
    {
      "id": "PMET-VD-0074",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0547",
      "entries": [
        {
          "term": "CIRO Model",
          "definition": "Developed by Warr, Bird, and Rackham, CIRO stands for Context, Input, Reaction, and Outcome. It is a training evaluation framework that assesses the broader context in which training occurs and measures both immediate reactions and longer-term outcomes of training programs.",
          "is_target": false
        },
        {
          "term": "CIPP Model",
          "definition": "Developed by Kirkpatrick, CIPP stands for Context, Input, Process, and Product. It is a comprehensive evaluation framework used to guide decision-making and improve programs by systematically evaluating these four interrelated dimensions throughout a program's lifecycle.",
          "is_target": true
        },
        {
          "term": "Kirkpatrick's Four Levels",
          "definition": "Developed by Donald Kirkpatrick, this model evaluates training programs across four levels: Reaction, Learning, Behavior, and Results. It is one of the most widely used frameworks for assessing the effectiveness of training and educational interventions.",
          "is_target": false
        },
        {
          "term": "Logic Model",
          "definition": "A visual representation that links program resources (inputs), activities, outputs, and outcomes in a logical sequence. It is a planning and evaluation tool that helps stakeholders understand how a program is expected to work and what results it should achieve.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "Developed by Kirkpatrick",
      "error_correct": "Developed by Stufflebeam",
      "explanation": "The CIPP Model (Context, Input, Process, Product) was developed by Daniel Stufflebeam, not by Kirkpatrick. Kirkpatrick is known for his own Four Levels of Training Evaluation model (Reaction, Learning, Behavior, Results). This is a subtle researcher attribution error designed to test knowledge of who created which evaluation framework."
    },
    {
      "id": "PMET-VD-0075",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Analysis of Variance (ANOVA)",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0138",
      "entries": [
        {
          "term": "ANCOVA",
          "definition": "Analysis of Covariance is an extension of ANOVA that statistically controls for one or more covariates. By removing the variance associated with confounding variables, ANCOVA increases the precision of comparisons between group means.",
          "is_target": false
        },
        {
          "term": "ANOVA",
          "definition": "Analysis of Variance is used to compare means across three or more groups or conditions. ANOVA tests whether at least one group mean differs significantly from the others, and its test statistic is the F-ratio, which compares between-group variance to total variance.",
          "is_target": true
        },
        {
          "term": "MANOVA",
          "definition": "Multivariate Analysis of Variance extends ANOVA by allowing the simultaneous comparison of group means on two or more dependent variables. MANOVA helps control for the increased risk of Type I error that arises from running multiple separate ANOVAs.",
          "is_target": false
        },
        {
          "term": "t-test",
          "definition": "The t-test is used to compare means between two groups or conditions. It produces a t statistic that reflects the ratio of the difference between group means to the variability within the groups, and it is appropriate when the independent variable has only two levels.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "which compares between-group variance to total variance",
      "error_correct": "which compares between-group variance to within-group variance",
      "explanation": "The F-ratio in ANOVA is calculated by dividing between-group variance (mean square between) by within-group variance (mean square within), not by total variance. Total variance is the sum of between-group and within-group variance, but it is not the denominator of the F-ratio."
    },
    {
      "id": "PMET-VD-0076",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "Setting Cutoff Scores",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0109",
      "entries": [
        {
          "term": "Specificity",
          "definition": "Specificity refers to the ability of a test to correctly identify individuals who do not meet a criterion (true negatives). A test with high specificity produces few false positives, meaning it rarely misclassifies negative cases as positive.",
          "is_target": false
        },
        {
          "term": "Sensitivity",
          "definition": "Sensitivity refers to the ability of a test to correctly identify individuals who meet a criterion (true positives). A test with high sensitivity produces few false negatives, meaning it rarely misclassifies positive cases as negative.",
          "is_target": false
        },
        {
          "term": "Cutoff Score",
          "definition": "A cutoff score is the point on a test's score distribution used to classify examinees into categories such as pass/fail or positive/negative. Lowering the cutoff score increases sensitivity but decreases specificity, resulting in more true positives but also more false negatives.",
          "is_target": true
        },
        {
          "term": "Positive Predictive Value",
          "definition": "Positive predictive value is the proportion of individuals who test positive who actually meet the criterion (true positives out of all positives). It is influenced by both the test's sensitivity and specificity as well as the base rate of the condition in the population.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "more true positives but also more false negatives",
      "error_correct": "more true positives but also more false positives",
      "explanation": "When the cutoff score is lowered, more people are classified as positive, which increases both true positives (sensitivity goes up) and false positives (specificity goes down). The error states that lowering the cutoff produces more false negatives, when in fact it produces more false positives. False negatives actually decrease when the cutoff is lowered because fewer positive cases are missed."
    },
    {
      "id": "PMET-VD-0077",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Types of Bias",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0367",
      "entries": [
        {
          "term": "Differential Validity",
          "definition": "Differential validity occurs when a test has significantly different validity coefficients across demographic groups. If a test correlates strongly with a criterion for one group but weakly for another, the test demonstrates differential validity, raising concerns about fair use across populations.",
          "is_target": false
        },
        {
          "term": "Predictive Bias",
          "definition": "Predictive bias exists when a test's regression line for predicting a criterion differs across groups. According to the Cleary and Hilton model, if the same test score yields different predicted criterion levels for different groups, selection decisions based on the test may be systematically unfair to one group.",
          "is_target": true
        },
        {
          "term": "Measurement Bias",
          "definition": "Measurement bias occurs when a test measures different psychological constructs or measures the same construct with different degrees of accuracy across groups. It reflects systematic error in measurement that is related to group membership, potentially undermining the validity of cross-group comparisons.",
          "is_target": false
        },
        {
          "term": "Item Bias",
          "definition": "Item bias, also known as differential item functioning (DIF), occurs when individuals from different groups who are equal on the construct being measured have different probabilities of endorsing or correctly answering a particular item. DIF analysis is used to detect potentially biased items during test development.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "Cleary and Hilton",
      "error_correct": "Cleary",
      "explanation": "The model of predictive bias based on regression lines is known as the Cleary model (Anne Cleary, 1968), not the 'Cleary and Hilton' model. Thomas Hilton was not part of this specific model. The Cleary model defines test bias as occurring when the regression equations used to predict criterion scores differ significantly across groups."
    },
    {
      "id": "PMET-VD-0078",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "inferential-statistics.html",
      "chapter_title": "From Samples to Populations: Inferential Statistics",
      "section": "Other Effect Size Measures",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0465",
      "entries": [
        {
          "term": "Partial omega-squared (ω²ₚ)",
          "definition": "A less biased estimate of the proportion of variance in the dependent variable accounted for by a factor in ANOVA designs. It adjusts for positive bias found in eta-squared by incorporating degrees of freedom into the calculation, providing a more conservative population estimate.",
          "is_target": false
        },
        {
          "term": "Partial eta-squared (η²ₚ)",
          "definition": "A measure of effect size used in factorial ANOVA that represents the variance explained by one independent variable while controlling for other dependent variables. It is calculated as the ratio of the sum of squares for the effect to the sum of squares for the effect plus error.",
          "is_target": true
        },
        {
          "term": "Eta-squared (η²)",
          "definition": "A measure of effect size that represents the proportion of total variance in the dependent variable that is accounted for by an independent variable. It is commonly used in one-way ANOVA designs and is calculated as the ratio of between-group sum of squares to total sum of squares.",
          "is_target": false
        },
        {
          "term": "Cohen's f",
          "definition": "A standardized measure of effect size used in ANOVA designs that expresses the degree of separation among group means relative to within-group variability. It can be derived from eta-squared and is commonly used for power analysis in factorial designs.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "while controlling for other dependent variables",
      "error_correct": "while controlling for other independent variables",
      "explanation": "Partial eta-squared controls for other independent variables (IVs), not dependent variables (DVs). In a factorial ANOVA, partial eta-squared isolates the variance explained by one IV by removing the variance accounted for by the other IVs from the denominator, leaving only the effect's sum of squares plus the error sum of squares."
    },
    {
      "id": "PMET-VD-0079",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Types of Bias",
      "passage_type": "definition",
      "source_passage_id": "PMET-0339",
      "entries": [
        {
          "term": "Adverse Impact",
          "definition": "Adverse impact occurs when a selection procedure results in a substantially different rate of selection for members of a protected group. It is commonly assessed using the four-fifths rule, where selection rates for a minority group fall below 80% of the rate for the majority group.",
          "is_target": false
        },
        {
          "term": "Bias",
          "definition": "Bias is a statistical/psychometric concept referring to systematic measurement error. It occurs when a test consistently overestimates or underestimates scores for a particular group. Bias is typically evaluated through methods such as differential item functioning (DIF) analysis.",
          "is_target": false
        },
        {
          "term": "Fairness",
          "definition": "Fairness is a broader concept encompassing social values and judgments about appropriate test use. A test can be psychometrically unbiased yet still raise fairness concerns if its use produces adverse impact. Fairness is primarily evaluated through random measurement error analysis developed by Cronbach and Gleser.",
          "is_target": true
        },
        {
          "term": "Construct Validity",
          "definition": "Construct validity refers to the degree to which a test measures the theoretical construct it is intended to measure. It is evaluated through multiple lines of evidence including convergent and discriminant validity. Cronbach and Meehl originally outlined the construct validation framework.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "Fairness is primarily evaluated through random measurement error analysis",
      "error_correct": "Fairness is not evaluated through statistical error analysis but rather involves value judgments and policy considerations about equitable test use",
      "explanation": "The error states that fairness is 'primarily evaluated through random measurement error analysis developed by Cronbach and Gleser,' which is incorrect. Fairness is not a statistical concept evaluated through error analysis — that describes bias. Fairness encompasses social values, ethical considerations, and policy judgments about appropriate and equitable test use. Additionally, random measurement error is distinct from systematic bias. The passage explicitly distinguishes fairness as a broader concept involving social values, not a psychometric/statistical one."
    },
    {
      "id": "PMET-VD-0080",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Commonly Tested Topics for the EPPP",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0493",
      "entries": [
        {
          "term": "Establishing Operation",
          "definition": "A motivating operation that temporarily alters the reinforcing effectiveness of a stimulus and changes the frequency of behaviors associated with that reinforcer. It affects both the value of a consequence and the likelihood of related behavior.",
          "is_target": false
        },
        {
          "term": "Discriminative Stimulus",
          "definition": "A stimulus that signals reinforcement availability and elicits the target behavior through its association with reinforcing consequences. It is typically denoted as S-D in behavioral notation.",
          "is_target": true
        },
        {
          "term": "Conditioned Reinforcer",
          "definition": "A previously neutral stimulus that has acquired reinforcing properties through repeated pairing with an existing reinforcer. Common examples include money, tokens, and praise, which derive their value from learned associations.",
          "is_target": false
        },
        {
          "term": "Stimulus Delta",
          "definition": "A stimulus that signals the absence of reinforcement for a particular behavior. When present, the target behavior is less likely to occur because responding has not been reinforced in its presence.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "elicits the target behavior",
      "error_correct": "sets the occasion for the target behavior (does not elicit it)",
      "explanation": "A discriminative stimulus (SD) sets the occasion for behavior — it increases the probability that a voluntary operant response will occur — but it does not elicit behavior. The term 'elicit' is reserved for respondent (classical) conditioning, where an unconditioned or conditioned stimulus automatically triggers a reflexive response. This is a critical distinction between operant and respondent conditioning that is commonly tested on the EPPP."
    },
    {
      "id": "PMET-VD-0081",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Standard Error of Estimate",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0060",
      "entries": [
        {
          "term": "Standard Error of Measurement (SEM)",
          "definition": "SEM quantifies the amount of error in an individual's observed test score due to imperfect reliability. It is calculated using the standard deviation of the test scores and the reliability coefficient, and it is used to construct confidence intervals around a person's true score.",
          "is_target": false
        },
        {
          "term": "Standard Error of Estimate (SEE)",
          "definition": "SEE measures the accuracy of predictions made from a regression equation. When the correlation coefficient (r) equals 1.00, SEE equals 0, indicating no prediction error. When r equals 0, the SEE equals the variance of the Y variable, meaning that knowing X provides no improvement over simply guessing the mean of Y.",
          "is_target": true
        },
        {
          "term": "Standard Error of the Mean",
          "definition": "The standard error of the mean estimates how much sample means would vary from sample to sample due to sampling error. It is calculated by dividing the standard deviation of the population by the square root of the sample size, and it decreases as sample size increases.",
          "is_target": false
        },
        {
          "term": "Coefficient of Determination",
          "definition": "The coefficient of determination (r²) represents the proportion of variance in one variable that is accounted for by its linear relationship with another variable. For example, if r = 0.80, then r² = 0.64, meaning 64% of the variance in Y is explained by X.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "the SEE equals the variance of the Y variable",
      "error_correct": "the SEE equals the standard deviation of the Y variable",
      "explanation": "When r = 0, the Standard Error of Estimate equals the standard deviation of Y, not the variance of Y. This is because when there is no correlation, predictions from the regression equation are no better than simply using the mean of Y, and the spread of prediction errors equals the standard deviation of Y scores."
    },
    {
      "id": "PMET-VD-0082",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Prepared Fears",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0027",
      "entries": [
        {
          "term": "Latent inhibition",
          "definition": "A phenomenon in classical conditioning where pre-exposure to a stimulus without any consequence makes it harder to later associate that stimulus with an unconditioned stimulus. This effect demonstrates that familiarity with a neutral stimulus retards subsequent conditioning.",
          "is_target": false
        },
        {
          "term": "Preparedness theory",
          "definition": "Proposed by Seligman (1974), this theory holds that humans are biologically predisposed to rapidly acquire fears of stimuli that posed threats throughout evolutionary history, such as snakes, spiders, heights, and enclosed spaces. These 'prepared' fears are learned more quickly and are more resistant to extinction than fears of evolutionarily neutral stimuli.",
          "is_target": true
        },
        {
          "term": "Stimulus generalization",
          "definition": "A classical conditioning phenomenon in which a conditioned response is elicited not only by the original conditioned stimulus but also by stimuli that are similar to it. The strength of the response typically decreases as the similarity between the new stimulus and the original CS decreases.",
          "is_target": false
        },
        {
          "term": "Sensory preconditioning",
          "definition": "A classical conditioning procedure in which two neutral stimuli are first paired together, and then one of them is paired with an unconditioned stimulus. When the other neutral stimulus is later presented alone, it also elicits a conditioned response despite never having been directly paired with the UCS.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "Seligman (1974)",
      "error_correct": "Seligman (1971)",
      "explanation": "The error is in the date of Seligman's preparedness theory. It was proposed in 1971, not 1974. Seligman's 1971 paper on preparedness is a foundational work in understanding biologically prepared learning and phobia acquisition."
    },
    {
      "id": "PMET-VD-0083",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Extinction and Spontaneous Recovery",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0413",
      "entries": [
        {
          "term": "Spontaneous Recovery",
          "definition": "The reappearance of a previously extinguished conditioned response after a rest period without further conditioning trials. It demonstrates that extinction does not completely erase the original learning but rather involves new inhibitory learning.",
          "is_target": false
        },
        {
          "term": "Rapid Reacquisition",
          "definition": "The phenomenon in classical conditioning where reconditioning after extinction occurs slower than original acquisition. This suggests that the original association is not fully eliminated during extinction but is instead suppressed or masked.",
          "is_target": true
        },
        {
          "term": "Extinction",
          "definition": "The gradual weakening and eventual disappearance of a conditioned response when the conditioned stimulus is repeatedly presented without the unconditioned stimulus. It represents new inhibitory learning rather than unlearning of the original association.",
          "is_target": false
        },
        {
          "term": "Stimulus Generalization",
          "definition": "The tendency for a conditioned response to be elicited by stimuli that are similar to the original conditioned stimulus. The strength of the response typically decreases as the similarity between the new stimulus and the original CS decreases.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "slower than original acquisition",
      "error_correct": "faster than original acquisition",
      "explanation": "The definition of Rapid Reacquisition contains a direction reversal error. It states that reconditioning after extinction occurs 'slower' than original acquisition, when in fact the defining feature of rapid reacquisition is that reconditioning occurs 'faster' than original acquisition. This faster reconditioning provides evidence that extinction does not completely erase the original CS-US association."
    },
    {
      "id": "PMET-VD-0084",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Overshadowing",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0020",
      "entries": [
        {
          "term": "Blocking",
          "definition": "Blocking occurs when prior conditioning to one element of a compound CS prevents conditioning to a second element added later. This effect was famously demonstrated by Kamin and requires sequential training where one stimulus is established before the compound is introduced.",
          "is_target": false
        },
        {
          "term": "Latent Inhibition",
          "definition": "Latent inhibition refers to the phenomenon where pre-exposure to a stimulus without any consequence slows subsequent conditioning to that stimulus. This effect demonstrates that organisms learn about stimuli even when no reinforcement is present during initial exposure.",
          "is_target": false
        },
        {
          "term": "Overshadowing",
          "definition": "Overshadowing occurs when two stimuli are presented together as a compound CS, and conditioning to the less salient stimulus interferes with conditioning to the more salient stimulus. Unlike blocking, it occurs with simultaneous conditioning and does not require prior training with one element.",
          "is_target": true
        },
        {
          "term": "Sensory Preconditioning",
          "definition": "Sensory preconditioning occurs when two neutral stimuli are paired together before one of them is conditioned to a US. After conditioning to one stimulus, the other stimulus also elicits a conditioned response despite never being directly paired with the US.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "conditioning to the less salient stimulus interferes with conditioning to the more salient stimulus",
      "error_correct": "conditioning to the more salient stimulus interferes with conditioning to the less salient stimulus",
      "explanation": "The error reverses which stimulus interferes with which. In overshadowing, the MORE salient stimulus overshadows (interferes with conditioning to) the LESS salient stimulus, not the other way around. The more prominent or intense stimulus captures the associative strength, leaving the weaker stimulus with reduced conditioning."
    },
    {
      "id": "PMET-VD-0085",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "criterion-validity.html",
      "chapter_title": "Predicting Outcomes: Criterion-Related Validity",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0449",
      "entries": [
        {
          "term": "Convergent Validity",
          "definition": "A type of construct validity evidence demonstrated when a measure correlates highly with other measures that assess the same or similar constructs. It is typically evaluated using multitrait-multimethod matrices or correlation coefficients between related instruments.",
          "is_target": false
        },
        {
          "term": "Incremental Validity",
          "definition": "The improvement in prediction of a criterion that a new measure provides beyond what is already achieved by existing measures. It is commonly assessed using the change in F-statistic (ΔF) when the new predictor is added to a regression model.",
          "is_target": true
        },
        {
          "term": "Predictive Validity",
          "definition": "A form of criterion-related validity in which test scores are used to predict future performance on a criterion measure. It is established by administering the test first and then collecting criterion data at a later point in time.",
          "is_target": false
        },
        {
          "term": "Concurrent Validity",
          "definition": "A form of criterion-related validity in which test scores and criterion measures are collected at approximately the same time. It is often used when the goal is to substitute one measure for another rather than to forecast future outcomes.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "the change in F-statistic (ΔF)",
      "error_correct": "the change in R-squared (ΔR²)",
      "explanation": "Incremental validity is assessed using ΔR² (the change in the coefficient of determination), which quantifies how much additional variance in the criterion is accounted for when a new predictor is added to the regression model. The error substituted ΔF for ΔR². While the F-change test is related (it tests whether ΔR² is statistically significant), the standard metric cited for incremental validity is ΔR²."
    },
    {
      "id": "PMET-VD-0086",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Overshadowing",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0021",
      "entries": [
        {
          "term": "Blocking",
          "definition": "Blocking occurs when a previously conditioned stimulus prevents conditioning to a new stimulus when both are presented together. Discovered by Leon Kamin, it demonstrates that mere contiguity is insufficient for conditioning and that surprise or prediction error is necessary.",
          "is_target": false
        },
        {
          "term": "Overshadowing",
          "definition": "Overshadowing occurs when two stimuli are presented together as a compound CS, and the less salient stimulus acquires stronger conditioning at the expense of the more salient one. This phenomenon helps explain why certain trauma cues become strongly associated with fear while others present during the same event do not.",
          "is_target": true
        },
        {
          "term": "Latent Inhibition",
          "definition": "Latent inhibition refers to the phenomenon in which pre-exposure to a stimulus without any consequence slows subsequent conditioning to that stimulus. It demonstrates that familiarity with a neutral stimulus can interfere with later associative learning when that stimulus is paired with a US.",
          "is_target": false
        },
        {
          "term": "Sensory Preconditioning",
          "definition": "Sensory preconditioning occurs when two neutral stimuli are repeatedly paired together, and then one is paired with a US, resulting in a conditioned response to both stimuli. This demonstrates that associations can form between stimuli even in the absence of a reinforcer or unconditioned stimulus.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "the less salient stimulus acquires stronger conditioning at the expense of the more salient one",
      "error_correct": "the more salient stimulus acquires stronger conditioning at the expense of the less salient one",
      "explanation": "In overshadowing, it is the MORE salient stimulus that overshadows and acquires stronger conditioning, while the LESS salient stimulus receives weak conditioning. The definition reverses which stimulus gains stronger conditioning. For example, a bright light (more salient) overshadows a soft tone (less salient), not the other way around."
    },
    {
      "id": "PMET-VD-0087",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Construct Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0324",
      "entries": [
        {
          "term": "Mono-Operation Bias",
          "definition": "A threat to construct validity that occurs when only one operationalization of a construct is used. A single measure captures only part of a construct and includes irrelevant variance. Multiple operationalizations provide stronger evidence of the intended construct.",
          "is_target": true
        },
        {
          "term": "Mono-Method Bias",
          "definition": "A threat to construct validity that occurs when only one method of measurement is used across all constructs in a study. Using a single method (e.g., only self-report) introduces shared method variance that can inflate or deflate observed relationships between variables.",
          "is_target": false
        },
        {
          "term": "Construct Confounding",
          "definition": "A threat to construct validity that occurs when the operationalization of the independent or dependent variable inadvertently includes features of another construct. This makes it impossible to determine which construct is actually responsible for the observed effect.",
          "is_target": false
        },
        {
          "term": "Restricted Generalizability Across Constructs",
          "definition": "A threat to construct validity that occurs when an intervention affects constructs beyond those originally intended or theorized. Failure to measure these additional outcomes leads to an incomplete understanding of how the treatment actually operates.",
          "is_target": false
        }
      ],
      "target_entry_index": 0,
      "error_original": "includes irrelevant variance",
      "error_correct": "includes method-specific variance (or construct-irrelevant variance unique to that single operationalization)",
      "explanation": "While the definition sounds plausible, the core issue with mono-operation bias is actually construct underrepresentation — a single measure captures only a limited facet of the construct. The phrase 'irrelevant variance' is more precisely associated with mono-method bias, where shared method variance contaminates measurement. The actual concern in mono-operation bias is that a single operationalization both underrepresents the construct and confounds it with the specific features of that one operationalization, rather than broadly 'including irrelevant variance.' However, upon closer review, the original passage itself uses 'irrelevant variance,' so let me re-craft the error."
    },
    {
      "id": "PMET-VD-0088",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Acquisition",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0010",
      "entries": [
        {
          "term": "Trace Conditioning",
          "definition": "A classical conditioning procedure in which the CS is presented and then terminated before the US is delivered, leaving a temporal gap (trace interval) between the two stimuli. It generally produces weaker conditioning than delay conditioning and involves hippocampal-dependent learning.",
          "is_target": false
        },
        {
          "term": "Simultaneous Conditioning",
          "definition": "A classical conditioning procedure in which the CS and US are presented at exactly the same time and terminate together. This arrangement typically produces weak conditioned responses because the CS has limited predictive value for the US.",
          "is_target": false
        },
        {
          "term": "Delay Conditioning",
          "definition": "The most effective temporal arrangement for establishing conditioned responses in classical conditioning. In this procedure, the CS begins before the US and continues until the US is presented, creating overlap with US offset and optimal conditions for association formation.",
          "is_target": true
        },
        {
          "term": "Backward Conditioning",
          "definition": "A classical conditioning procedure in which the US is presented before the CS, reversing the typical temporal order. This arrangement generally produces little or no excitatory conditioning and may instead result in the CS becoming a conditioned inhibitor.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "US offset",
      "error_correct": "US onset",
      "explanation": "The error substitutes 'US offset' for 'US onset.' In delay conditioning, the CS overlaps with the onset (beginning) of the US, not its offset (termination). The CS starts before the US and continues until the US is presented, meaning the overlap occurs at the point when the US begins."
    },
    {
      "id": "PMET-VD-0089",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "validity.html",
      "chapter_title": "Measuring What Matters: Content & Construct Validity",
      "section": "Sources of Validity Evidence",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0351",
      "entries": [
        {
          "term": "Content validity evidence",
          "definition": "One of the five sources of validity evidence identified in the Standards (AERA et al., 2014), content validity evidence evaluates whether test items adequately represent the domain the test is intended to measure. It is typically established through expert judgment and systematic analysis of item relevance and coverage.",
          "is_target": false
        },
        {
          "term": "Sources of validity evidence",
          "definition": "According to the Standards for Educational and Psychological Testing (AERA et al., 2014), there are four major sources of validity evidence that collectively inform whether score interpretations are appropriate. These sources include evidence based on test content, response processes, internal structure, relations to other variables, and consequences of testing.",
          "is_target": true
        },
        {
          "term": "Construct validity evidence",
          "definition": "A broad category of validity evidence concerned with whether a test actually measures the theoretical construct it purports to measure. It integrates multiple lines of evidence, including convergent and discriminant validity, and is often considered the overarching framework unifying all forms of validity.",
          "is_target": false
        },
        {
          "term": "Criterion-related validity evidence",
          "definition": "A source of validity evidence that examines the relationship between test scores and an external criterion measure. It includes both predictive validity, which assesses future performance, and concurrent validity, which assesses the relationship with a criterion measured at the same time.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "there are four major sources of validity evidence",
      "error_correct": "there are five major sources of validity evidence",
      "explanation": "The Standards for Educational and Psychological Testing (AERA et al., 2014) identifies five — not four — major sources of validity evidence: (1) test content, (2) response processes, (3) internal structure, (4) relations to other variables, and (5) consequences of testing. The definition even lists all five sources in the next sentence, contradicting its own stated number of four."
    },
    {
      "id": "PMET-VD-0090",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-validity.html",
      "chapter_title": "Threats & Safeguards: Research Validity",
      "section": "Threats to Internal Validity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0307",
      "entries": [
        {
          "term": "History",
          "definition": "History refers to specific external events that occur during the course of a study, between the pretest and posttest measurements. These events are outside the researcher's control and may produce changes in the dependent variable that are confused with treatment effects.",
          "is_target": false
        },
        {
          "term": "Maturation",
          "definition": "Maturation refers to natural changes in participants over time, such as growing older, becoming tired, or gaining experience. These changes occur as a direct consequence of the treatment and may amplify treatment effects in measurable ways.",
          "is_target": true
        },
        {
          "term": "Testing",
          "definition": "Testing refers to the threat that occurs when taking a pretest influences participants' performance on a subsequent posttest. Exposure to the initial measurement can produce practice effects or sensitization that are mistaken for genuine treatment effects.",
          "is_target": false
        },
        {
          "term": "Regression to the Mean",
          "definition": "Regression to the mean refers to the statistical tendency for extreme scores on an initial measurement to move closer to the group average on subsequent testing. This natural shift may be incorrectly attributed to the effects of the experimental treatment.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "as a direct consequence of the treatment and may amplify treatment effects",
      "error_correct": "regardless of treatment and may be mistaken for treatment effects",
      "explanation": "The defining feature of maturation as a threat to internal validity is that these natural changes occur REGARDLESS of any treatment — they happen simply due to the passage of time. The error states they occur 'as a direct consequence of the treatment,' which fundamentally contradicts the concept. Maturation is a threat precisely because these naturally occurring changes can be mistakenly attributed to the treatment when they are actually independent of it."
    },
    {
      "id": "PMET-VD-0091",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Measures of Variability",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0598",
      "entries": [
        {
          "term": "Standard Deviation",
          "definition": "A measure of variability that represents the average distance of scores from the mean of a distribution. It is calculated by taking the square root of the variance. It is expressed in the same units as the original scores, making it more interpretable than the variance.",
          "is_target": false
        },
        {
          "term": "Range",
          "definition": "The simplest measure of variability, calculated by subtracting the lowest score from the highest score in a distribution. It uses only two data points and is therefore highly sensitive to outliers. It provides a quick but limited summary of how spread out scores are.",
          "is_target": false
        },
        {
          "term": "Variance",
          "definition": "A measure of variability computed by summing the squared deviations of each score from the median and dividing by the number of scores (or N−1 for a sample). It captures how spread out scores are in a distribution. Because the deviations are squared, it is expressed in squared units of the original measurement.",
          "is_target": true
        },
        {
          "term": "Interquartile Range",
          "definition": "A measure of variability that represents the spread of the middle 50% of scores in a distribution. It is calculated by subtracting the first quartile (Q1) from the third quartile (Q3). It is more resistant to the influence of outliers than the range.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "squared deviations of each score from the median",
      "error_correct": "squared deviations of each score from the mean",
      "explanation": "Variance is calculated using squared deviations from the mean, not the median. The definition incorrectly states that deviations are computed from the median. The mean is the central reference point used in computing variance and standard deviation."
    },
    {
      "id": "PMET-VD-0092",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Within-Subjects (Repeated Measures) Designs",
      "passage_type": "definition",
      "source_passage_id": "PMET-0243",
      "entries": [
        {
          "term": "Randomization",
          "definition": "Randomization is a procedure in which participants are assigned to conditions using chance methods, ensuring that each participant has an equal probability of being placed in any group. This technique helps control for confounding variables and supports the internal validity of an experiment.",
          "is_target": false
        },
        {
          "term": "Counterbalancing",
          "definition": "Counterbalancing addresses order effects by varying the sequence of conditions across participants. In complete counterbalancing, all possible orders are used equally. In Latin square counterbalancing, a subset of orders is used such that each condition appears equally often in each row.",
          "is_target": true
        },
        {
          "term": "Block Randomization",
          "definition": "Block randomization is a technique that divides participants into blocks of a predetermined size and randomly assigns conditions within each block. This ensures that treatment groups remain approximately equal in size throughout the course of data collection.",
          "is_target": false
        },
        {
          "term": "Random Assignment",
          "definition": "Random assignment is the process of allocating participants to different experimental conditions using a chance procedure. It is a key feature of true experiments that helps distribute participant characteristics evenly across groups, minimizing systematic bias.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "each condition appears equally often in each row",
      "error_correct": "each condition appears equally often in each position",
      "explanation": "In Latin square counterbalancing, the defining feature is that each condition appears equally often in each position (i.e., first, second, third, etc. in the sequence). The error replaces 'position' with 'row,' which is a term from the Latin square matrix layout but does not accurately describe the key principle of this counterbalancing method as applied to experimental design."
    },
    {
      "id": "PMET-VD-0093",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "reliability.html",
      "chapter_title": "Consistency of Measurement: Test Reliability",
      "section": "EPPP Exam Focus Points",
      "passage_type": "clinical_note",
      "source_passage_id": "PMET-0512",
      "entries": [
        {
          "term": "Item Characteristic Curves",
          "definition": "In Item Response Theory, these are S-shaped (ogive) curves that plot the probability of a correct response as a function of theta (ability level). They are defined by parameters such as difficulty, discrimination, and guessing. They were formalized within the framework developed by Lord and Rasch.",
          "is_target": false
        },
        {
          "term": "Test Characteristic Curve",
          "definition": "This curve represents the expected total test score as a function of theta (ability level) in Item Response Theory. It is derived by summing the individual item characteristic curves across all items on a test. It provides information about overall test performance across the ability continuum.",
          "is_target": false
        },
        {
          "term": "Item Information Curves",
          "definition": "These are bell-shaped curves that indicate the amount of information an item provides at each level of theta (ability). They peak at the point on the ability scale where the item best discriminates among examinees. The total test information function is obtained by summing these curves across all items.",
          "is_target": true
        },
        {
          "term": "Item Difficulty Index",
          "definition": "In Classical Test Theory, this is the proportion of examinees who answer an item correctly, ranging from 0.0 to 1.0. Higher values indicate easier items, and optimal difficulty for maximizing discrimination is around 0.50. It is one of the most basic item analysis statistics used in test development.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "bell-shaped curves",
      "error_correct": "curves that peak at the point of maximum slope of the ICC",
      "explanation": "Item Information Curves are not strictly bell-shaped. While they are unimodal (having a single peak), their shape is derived from the derivative of the logistic ICC function and can be asymmetric depending on the item's discrimination and guessing parameters. Describing them as 'bell-shaped' (implying a symmetric normal/Gaussian shape) is a subtle but common misconception. The key distractor here involves confusing item information curves with a normal distribution shape."
    },
    {
      "id": "PMET-VD-0094",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Other Standard Score Transformations",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0599",
      "entries": [
        {
          "term": "T-score",
          "definition": "A standard score transformation with a mean of 50 and a standard deviation of 10, calculated by the formula T = 10(z) + 50. It was developed by William McCall and is commonly used in personality assessment instruments like the MMPI.",
          "is_target": false
        },
        {
          "term": "Z-score",
          "definition": "A standard score that indicates how many standard deviations a raw score falls above or below the mean of a distribution. Z-scores have a mean of 0 and a standard deviation of 1, and they allow direct comparison of scores from different distributions.",
          "is_target": false
        },
        {
          "term": "Stanine",
          "definition": "A standard score scale that divides the normal distribution into nine intervals, with a mean of 5 and a standard deviation of approximately 2. Stanine scores range from 1 to 9 and are often used in educational testing to provide a broad classification of performance.",
          "is_target": false
        },
        {
          "term": "Linear transformation",
          "definition": "A mathematical procedure that converts scores from one scale to another by multiplying by a constant and/or adding a constant. When applied to z-scores, linear transformations produce new standard score scales that eliminate negative values and decimals while preserving the original rank order but changing the shape of the distribution.",
          "is_target": true
        }
      ],
      "target_entry_index": 3,
      "error_original": "preserving the original rank order but changing the shape of the distribution",
      "error_correct": "preserving both the original rank order and the shape of the distribution",
      "explanation": "Linear transformations preserve both the rank order AND the shape of the distribution. It is nonlinear transformations that can change the shape of the distribution. A linear transformation only changes the mean and standard deviation (scale and location) but does not alter the distributional shape."
    },
    {
      "id": "PMET-VD-0095",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "operant-conditioning.html",
      "chapter_title": "Shaping Behavior: Operant Conditioning",
      "section": "Contingency Management",
      "passage_type": "example",
      "source_passage_id": "PMET-0163",
      "entries": [
        {
          "term": "Token Economy",
          "definition": "A behavior modification system used primarily in institutional settings where individuals earn tokens for performing desired behaviors. These tokens serve as generalized conditioned reinforcers that can be exchanged for backup reinforcers such as privileges or goods. Token economies are widely used in psychiatric hospitals, classrooms, and rehabilitation programs.",
          "is_target": false
        },
        {
          "term": "Voucher-Based Contingency Management",
          "definition": "A behavioral intervention for substance dependence in which patients receive vouchers exchangeable for retail goods contingent on drug-negative urine specimens. Voucher values typically start high and decrease with consecutive negative specimens, with a reset contingency applied when a positive specimen is detected. Meta-analyses have demonstrated significant effects on drug abstinence outcomes.",
          "is_target": true
        },
        {
          "term": "Contingency Contract",
          "definition": "A formal written agreement between a therapist and client (or between two parties) that specifies the behaviors to be performed and the consequences that will follow. The contract clearly outlines the reinforcers or punishers tied to specific target behaviors. Contingency contracts are used in clinical, educational, and family therapy settings.",
          "is_target": false
        },
        {
          "term": "Premack Principle",
          "definition": "A behavioral principle stating that a high-probability behavior can be used as a reinforcer for a low-probability behavior. This principle was formulated by David Premack and is sometimes called 'Grandma's Rule.' It is commonly applied in clinical and educational settings to increase engagement in less preferred activities.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "start high and decrease with consecutive negative specimens",
      "error_correct": "start low and increase with consecutive negative specimens",
      "explanation": "In voucher-based contingency management, voucher values start low and escalate (increase) with each consecutive drug-negative specimen to reinforce sustained abstinence. The error reverses this direction, stating they start high and decrease, which contradicts the escalating reinforcement schedule that is central to the intervention's design."
    },
    {
      "id": "PMET-VD-0096",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Heteroscedasticity",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0056",
      "entries": [
        {
          "term": "Homoscedasticity",
          "definition": "Homoscedasticity refers to the condition in which the variability of one variable remains constant across all levels of another variable. It is an assumption underlying Pearson's r and linear regression, and when met, it supports accurate estimation of correlation coefficients and prediction intervals.",
          "is_target": false
        },
        {
          "term": "Heteroscedasticity",
          "definition": "Heteroscedasticity occurs when the variability of one variable differs across levels of the other variable. For instance, variance in job performance might be larger for people with high test scores than for people with low scores. This violates an assumption of Pearson's r and can affect the accuracy of the correlation coefficient.",
          "is_target": true
        },
        {
          "term": "Restriction of Range",
          "definition": "Restriction of range occurs when the sample used in a study has a narrower distribution on one or both variables than the population of interest. This truncation typically leads to an underestimate of the true correlation between the variables and can compromise the validity of selection decisions.",
          "is_target": false
        },
        {
          "term": "Multicollinearity",
          "definition": "Multicollinearity refers to a situation in multiple regression where two or more predictor variables are highly correlated with each other. This makes it difficult to determine the unique contribution of each predictor and can inflate standard errors of regression coefficients, reducing statistical power.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "variance in job performance might be larger for people with high test scores than for people with low scores",
      "error_correct": "variance in job performance might be larger for people with low test scores than for people with high scores",
      "explanation": "The passage's example of heteroscedasticity states that variance in job performance is larger for people with LOW test scores than for those with HIGH scores. The error reverses which group has greater variability, swapping 'high' and 'low' in the example. This is a classic illustration: low scorers are a more heterogeneous group in terms of performance, while high scorers tend to perform more consistently."
    },
    {
      "id": "PMET-VD-0097",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "correlation-regression.html",
      "chapter_title": "Relationships in Data: Correlation & Regression",
      "section": "Outliers",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0054",
      "entries": [
        {
          "term": "Scatterplot",
          "definition": "A graphical representation that displays pairs of values for two variables as points on a coordinate plane. Scatterplots are used to visually assess the direction, strength, and form of the relationship between variables and to identify potential outliers or nonlinear patterns.",
          "is_target": false
        },
        {
          "term": "Outlier",
          "definition": "An extreme score that deviates markedly from other observations in a dataset. A single outlier can dramatically affect correlation coefficients, but only by inflating them regardless of position. Researchers should always examine scatterplots to detect and evaluate the influence of outliers on their findings.",
          "is_target": true
        },
        {
          "term": "Correlation Coefficient",
          "definition": "A numerical index ranging from -1.00 to +1.00 that quantifies the strength and direction of the linear relationship between two variables. Common types include the Pearson product-moment correlation for interval/ratio data and the Spearman rank-order correlation for ordinal data.",
          "is_target": false
        },
        {
          "term": "Restriction of Range",
          "definition": "A phenomenon that occurs when the variability of one or both variables in a correlational analysis is limited, typically leading to an underestimate of the true correlation. This can result from sampling only a narrow subset of the population or from truncating scores at a certain threshold.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "only by inflating them regardless of position",
      "error_correct": "either inflating or deflating them depending on their position",
      "explanation": "The passage clearly states that outliers can either inflate or deflate correlation coefficients depending on their position relative to the data. The error in the target definition incorrectly claims that outliers can only inflate correlations regardless of position. In reality, an outlier that falls along the regression line may inflate the correlation, while one that falls far from the regression line may substantially deflate it."
    },
    {
      "id": "PMET-VD-0098",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "variables-data.html",
      "chapter_title": "Variables, Scales & the Language of Data",
      "section": "Descriptive Statistics",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0394",
      "entries": [
        {
          "term": "Measures of Variability",
          "definition": "A category of descriptive statistics that quantifies the spread or dispersion of scores in a dataset. Common examples include the range, variance, and standard deviation, which indicate how much individual scores differ from the central tendency.",
          "is_target": false
        },
        {
          "term": "Measures of Central Tendency",
          "definition": "A category of descriptive statistics that identifies the most typical or representative score in a distribution. The three primary measures are the mean, median, and mode, each summarizing the center of a dataset in a different way.",
          "is_target": false
        },
        {
          "term": "Descriptive Statistics",
          "definition": "Statistical techniques used to summarize and organize data to make patterns more interpretable. They fall into four main categories: measures of central tendency, measures of variability, measures of relative position, and measures of association.",
          "is_target": true
        },
        {
          "term": "Measures of Relative Position",
          "definition": "A category of descriptive statistics that indicates where a particular score falls in relation to other scores in a distribution. Common examples include percentile ranks, z-scores, and stanines, which allow comparison across different scales.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "four main categories",
      "error_correct": "three main categories",
      "explanation": "The passage explicitly states that descriptive statistics fall into THREE main categories: measures of central tendency, measures of variability, and measures of relative position. The error changes this to 'four main categories' and adds 'measures of association' as a fabricated fourth category. While measures of association (e.g., correlation) do exist, they are typically classified separately from the three standard categories of descriptive statistics described in this context."
    },
    {
      "id": "PMET-VD-0099",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "research-designs.html",
      "chapter_title": "Blueprints for Discovery: Research Designs",
      "section": "Mixed Designs",
      "passage_type": "paragraph",
      "source_passage_id": "PMET-0279",
      "entries": [
        {
          "term": "Nested Design",
          "definition": "A research design in which levels of one factor are contained entirely within levels of another factor, such that each sub-group appears under only one condition of the higher-level factor. This approach is common when random assignment of lower-level units to all conditions is not possible.",
          "is_target": false
        },
        {
          "term": "Mixed Design",
          "definition": "A research design (also called a split-plot design) that includes both between-subjects and within-subjects factors. For example, a study might compare two therapy groups (within-subjects) measured at multiple time points (between-subjects), allowing researchers to examine both group differences and changes over time simultaneously.",
          "is_target": true
        },
        {
          "term": "Factorial Design",
          "definition": "A research design in which two or more independent variables are fully crossed so that every level of each factor is combined with every level of every other factor. This design allows researchers to examine main effects and interaction effects among the independent variables.",
          "is_target": false
        },
        {
          "term": "Repeated Measures Design",
          "definition": "A research design in which the same participants are measured under all conditions of the independent variable, serving as their own controls. This approach reduces error variance due to individual differences but may introduce order effects such as practice or fatigue.",
          "is_target": false
        }
      ],
      "target_entry_index": 1,
      "error_original": "two therapy groups (within-subjects) measured at multiple time points (between-subjects)",
      "error_correct": "two therapy groups (between-subjects) measured at multiple time points (within-subjects)",
      "explanation": "The error swaps which factor is between-subjects and which is within-subjects. In a mixed design, different therapy groups represent a between-subjects factor (different people in each group), while multiple time points represent a within-subjects factor (the same people measured repeatedly). The target definition reverses these labels."
    },
    {
      "id": "PMET-VD-0100",
      "mode": "vocab",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "chapter_file": "classical-conditioning.html",
      "chapter_title": "How Organisms Learn: Classical Conditioning",
      "section": "Stimulus Generalization and Discrimination",
      "passage_type": "example",
      "source_passage_id": "PMET-0005",
      "entries": [
        {
          "term": "Stimulus Discrimination",
          "definition": "In classical conditioning, stimulus discrimination is the learned ability to distinguish between a conditioned stimulus and other similar stimuli that do not signal the unconditioned stimulus. It is the complement of stimulus generalization and typically develops through differential reinforcement training.",
          "is_target": false
        },
        {
          "term": "Stimulus Generalization",
          "definition": "Stimulus generalization occurs when a conditioned response is elicited not only by the original conditioned stimulus but also by stimuli that are similar to it. Watson and Rayner's (1920) Little Albert study demonstrated this when Albert, after being conditioned to fear a white rat, also showed fear responses to a rabbit, a dog, a fur coat, and even a Santa Claus mask.",
          "is_target": false
        },
        {
          "term": "Little Albert Study",
          "definition": "The Little Albert study, conducted by Watson and Rayner (1921), is a landmark demonstration of classical fear conditioning and stimulus generalization. After pairing a white rat (CS) with a loud noise (US), the infant Albert exhibited fear not only to the rat but also to perceptually similar stimuli such as a rabbit, a dog, a fur coat, and a Santa Claus mask.",
          "is_target": true
        },
        {
          "term": "Higher-Order Conditioning",
          "definition": "Higher-order conditioning (also called second-order conditioning) occurs when a neutral stimulus is paired with an already-established conditioned stimulus rather than directly with an unconditioned stimulus. This process extends the reach of classical conditioning by allowing new stimuli to acquire conditioned properties indirectly.",
          "is_target": false
        }
      ],
      "target_entry_index": 2,
      "error_original": "Watson and Rayner (1921)",
      "error_correct": "Watson and Rayner (1920)",
      "explanation": "The Little Albert study was published in 1920, not 1921. Watson and Rayner's famous demonstration of classical fear conditioning and stimulus generalization was reported in their 1920 paper. The date '1921' is a subtle but incorrect alteration of the actual publication year."
    }
  ]
}