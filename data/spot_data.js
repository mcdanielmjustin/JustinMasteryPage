window.__SPOT_DATA = {
  "PMET": {"domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "total_questions": 141, "questions": [{"id": "PMET-0001", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Introduction to Classical Conditioning", "passage_type": "definition", "original_passage": "Key Terminology Unconditioned Stimulus (US): A stimulus that naturally and automatically triggers a response without prior learning (e.g., food causing salivation). Unconditioned Response (UR): The unlearned, naturally occurring response to the unconditioned stimulus (e.g., salivation to food). Conditioned Stimulus (CS): A previously neutral stimulus that, after association with the US, comes to trigger a conditioned response (e.g., bell after pairing with food). Conditioned Response (CR): The learned response to the conditioned stimulus (e.g., salivation to the bell)", "modified_passage": "Key Terminology Unconditioned Stimulus (US): A stimulus that naturally and automatically triggers a response without prior learning (e.g., food causing salivation). Unconditioned Response (UR): The unlearned, naturally occurring response to the unconditioned stimulus (e.g., salivation to food). Conditioned Stimulus (CS): A previously reflexive stimulus that, after association with the US, comes to trigger a conditioned response (e.g., bell after pairing with food). Conditioned Response (CR): The learned response to the conditioned stimulus (e.g., salivation to the bell)", "error_original": "reflexive stimulus", "error_correct": "neutral stimulus", "options": ["The passage incorrectly states 'Unconditioned Response (UR)'; it should say 'Conditioned Response (CR)'.", "The passage incorrectly states 'after association with the US'; it should say 'after association with the UR'.", "The passage incorrectly states 'reflexive stimulus'; it should say 'neutral stimulus'.", "The passage incorrectly states 'learned response to the conditioned stimulus'; it should say 'unlearned response to the conditioned stimulus'."], "correct_option_index": 2, "explanation": "The Conditioned Stimulus (CS) is defined as a previously neutral stimulus — one that did not naturally produce the conditioned response before training. Calling it a 'reflexive stimulus' would imply it already triggered an automatic response, which instead describes an unconditioned stimulus (US). The defining feature of the CS is that it starts out neutral and only acquires response-eliciting power through repeated pairing with the US."}, {"id": "PMET-0002", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Extinction and Spontaneous Recovery", "passage_type": "clinical_note", "original_passage": "Clinical Implications The context-dependence of extinction has important implications for exposure therapy. Fear extinguished in a therapist's office may return when the client encounters the feared stimulus in different contexts. Effective treatment should include exposure in multiple contexts to promote generalization of extinction learning", "modified_passage": "Clinical Implications The context-dependence of extinction has important implications for exposure therapy. Fear extinguished in a therapist's office may return when the client encounters the feared stimulus in different contexts. Effective treatment should include exposure in a single context to promote generalization of extinction learning", "error_original": "a single context", "error_correct": "multiple contexts", "options": ["The passage incorrectly states 'therapist's office'; it should say 'a laboratory setting'.", "The passage incorrectly states 'a single context'; it should say 'multiple contexts'.", "The passage incorrectly states 'context-dependence of extinction'; it should say 'context-independence of extinction'.", "The passage incorrectly states 'generalization of extinction learning'; it should say 'inhibition of extinction learning'."], "correct_option_index": 1, "explanation": "Effective exposure therapy requires practice across multiple, varied contexts. Because extinction is highly context-dependent, fear eliminated in the therapist's office may reemerge when the client encounters the feared stimulus in a different environment (e.g., a grocery store, public transit). Exposing the client in only a single context fails to promote generalization and is the opposite of best practice."}, {"id": "PMET-0003", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Conditioned Compensatory Responses", "passage_type": "clinical_note", "original_passage": "Clinical Application: Understanding Drug Overdose Siegel's research explains why many drug overdoses occur when users take their typical dose in unfamiliar environments. The novel context lacks the conditioned cues that normally trigger compensatory responses, resulting in an unexpectedly powerful drug effect. This underscores the importance of contextual factors in substance use treatment", "modified_passage": "Clinical Application: Understanding Drug Overdose Watson's research explains why many drug overdoses occur when users take their typical dose in unfamiliar environments. The novel context lacks the conditioned cues that normally trigger compensatory responses, resulting in an unexpectedly powerful drug effect. This underscores the importance of contextual factors in substance use treatment", "error_original": "Watson's", "error_correct": "Siegel's", "options": ["The passage incorrectly states 'unfamiliar environments'; it should say 'familiar environments'.", "The passage incorrectly states 'compensatory responses'; it should say 'conditioned responses'.", "The passage incorrectly states 'unexpectedly powerful drug effect'; it should say 'unexpectedly weak drug effect'.", "The passage incorrectly states 'Watson's research'; it should say 'Siegel's research'."], "correct_option_index": 3, "explanation": "The context-dependent drug tolerance model — explaining overdoses in unfamiliar settings — is the work of psychologist Shepard Siegel, not John B. Watson. Watson is primarily known for founding behaviorism and the Little Albert fear-conditioning experiment. Siegel demonstrated that drug tolerance is a conditioned compensatory response, and that novel environments lacking those conditioned cues can cause a lethal drug effect at the user's usual dose."}, {"id": "PMET-0004", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Acquisition", "passage_type": "example", "original_passage": "Clinical Example: Fear Acquisition A child is bitten by a dog (US) while at a park. The pain from the bite naturally produces fear (UR). The visual appearance of dogs (CS) was present just before and during the bite. After this single pairing (one-trial learning), the child now experiences fear (CR) when seeing any dog. The temporal relationship - seeing the dog immediately before and during the painful bite - exemplifies delay conditioning producing rapid fear acquisition", "modified_passage": "Clinical Example: Fear Acquisition A child is bitten by a dog (US) while at a park. The pain from the bite naturally produces fear (UR). The visual appearance of dogs (CS) was present just before and during the bite. After this single pairing (multi-trial learning), the child now experiences fear (CR) when seeing any dog. The temporal relationship - seeing the dog immediately before and during the painful bite - exemplifies delay conditioning producing rapid fear acquisition", "error_original": "multi-trial learning", "error_correct": "one-trial learning", "options": ["The passage incorrectly states 'multi-trial learning'; it should say 'one-trial learning'.", "The passage incorrectly states 'dog (US)'; it should say 'dog (CS)'.", "The passage incorrectly states 'delay conditioning'; it should say 'trace conditioning'.", "The passage incorrectly states 'fear (UR)'; it should say 'fear (CR)'."], "correct_option_index": 0, "explanation": "Fear conditioning following a single, highly aversive event is called one-trial learning (or single-trial learning). The passage explicitly states 'After this single pairing' — confirming only one CS–US pairing occurred. Multi-trial learning refers to the typical gradual acquisition over repeated pairings, which is the opposite of what is described here. Highly traumatic or aversive events are classically associated with one-trial fear conditioning."}, {"id": "PMET-0005", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Stimulus Generalization and Discrimination", "passage_type": "example", "original_passage": "The Little Albert Study Watson and Rayner's (1920) famous demonstration with \"Little Albert\" illustrated both fear conditioning and stimulus generalization. After conditioning fear to a white rat (CS) by pairing it with a loud noise (US), Albert showed fear responses not only to the rat but also to a rabbit, a dog, a fur coat, and even a Santa Claus mask - stimuli that shared the white, furry characteristics of the original CS. This generalization occurred despite these stimuli never being directly paired with the aversive noise", "modified_passage": "The Little Albert Study Watson and Rayner's (1920) famous demonstration with \"Little Albert\" illustrated both fear conditioning and stimulus discrimination. After conditioning fear to a white rat (CS) by pairing it with a loud noise (US), Albert showed fear responses not only to the rat but also to a rabbit, a dog, a fur coat, and even a Santa Claus mask - stimuli that shared the white, furry characteristics of the original CS. This generalization occurred despite these stimuli never being directly paired with the aversive noise", "error_original": "stimulus discrimination", "error_correct": "stimulus generalization", "options": ["The passage incorrectly states 'white rat (CS)'; it should say 'white rat (US)'.", "The passage incorrectly states 'loud noise (US)'; it should say 'loud noise (CS)'.", "The passage incorrectly states 'stimulus discrimination'; it should say 'stimulus generalization'.", "The passage incorrectly states 'Watson and Rayner's (1920)'; it should say 'Watson and Skinner's (1920)'."], "correct_option_index": 2, "explanation": "The Little Albert study demonstrated stimulus generalization — the spread of a conditioned fear response to stimuli that resemble the original CS (the white rat). Albert's fear extending to a rabbit, fur coat, and Santa Claus mask is a textbook example of generalization. Stimulus discrimination is the opposite process, where an organism learns to respond only to the specific CS and not to similar stimuli. The word 'generalization' even appears later in the passage, further confirming the error."}, {"id": "PMET-PC-0001", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Avoidance Learning", "passage_type": "paragraph", "source_passage_id": "PMET-0194", "sentences": ["Mowrer's (1947) two-factor theory explains avoidance learning through a combination of classical and operant conditioning.", "In the first factor, a neutral stimulus becomes associated with an aversive event through classical conditioning, eliciting a conditioned fear response.", "In the second factor, the organism learns to perform a behavior that removes the conditioned fear stimulus, and this behavior is maintained through positive reinforcement.", "The reduction of fear serves as the reinforcing consequence that strengthens the avoidance response.", "One challenge for two-factor theory is explaining why avoidance behaviors are often highly resistant to extinction.", "Critics have noted that if the conditioned fear extinguishes over trials, the motivation for the avoidance response should also diminish.", "Despite these criticisms, Mowrer's two-factor theory remains one of the most influential accounts of avoidance learning in behavioral psychology."], "target_sentence_index": 2, "original_sentence": "In the second factor, the organism learns to perform a behavior that removes the conditioned fear stimulus, and this behavior is maintained through negative reinforcement.", "error_original": "positive reinforcement", "error_correct": "negative reinforcement", "explanation": "The second factor in Mowrer's two-factor theory involves negative reinforcement, not positive reinforcement. The avoidance behavior is strengthened because it removes or reduces an aversive stimulus (the conditioned fear), which is the defining feature of negative reinforcement. Positive reinforcement involves the addition of a pleasant stimulus, which is not what occurs in avoidance learning."}, {"id": "PMET-PC-0002", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Test-Retest Reliability", "passage_type": "paragraph", "source_passage_id": "PMET-0224", "sentences": ["Test-retest reliability (also called temporal stability) assesses the consistency of scores across time.", "The same test is administered to the same individuals on two occasions, and the correlation between scores indicates internal consistency."], "target_sentence_index": 1, "original_sentence": "The same test is administered to the same individuals on two occasions, and the correlation between scores indicates stability.", "error_original": "internal consistency", "error_correct": "stability", "explanation": "The correlation obtained from test-retest reliability indicates 'stability' (i.e., temporal stability of scores over time), not 'internal consistency.' Internal consistency is a different type of reliability that assesses the degree to which items within a single test measure the same construct, typically estimated by methods such as Cronbach's alpha or split-half reliability. Test-retest reliability specifically evaluates whether scores remain stable across different time points."}, {"id": "PMET-PC-0003", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Meta-Analysis of Validity Coefficients", "passage_type": "example", "source_passage_id": "PMET-0347", "sentences": ["Meta-analyses of cognitive ability tests show that their validity for predicting job performance generalizes across jobs and settings, with corrected validity coefficients typically ranging from .50 to .60 for overall job performance.", "This suggests that cognitive ability is a valid predictor across many employment contexts."], "target_sentence_index": 0, "original_sentence": "Meta-analyses of cognitive ability tests show that their validity for predicting job performance generalizes across jobs and settings, with corrected validity coefficients typically ranging from .50 to .60 for overall job performance.", "error_original": "typically ranging from .50 to .60", "error_correct": "typically ranging from .30 to .50", "explanation": "The corrected validity coefficients for cognitive ability tests predicting overall job performance, as reported in validity generalization research (e.g., Schmidt & Hunter, 1998), typically range from approximately .30 to .50 (with values around .50-.55 at the high end for complex jobs). The passage inflated the range to .50 to .60, which overstates the typical findings in the validity generalization literature."}, {"id": "PMET-PC-0004", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Test Length", "passage_type": "example", "source_passage_id": "PMET-0215", "sentences": ["Spearman-Brown Example: If a 20-item test has reliability of .70, doubling its length (k = 2) would yield an estimated reliability using the Spearman-Brown prophecy formula.", "The calculation is: (2 × .70) / [1 + (2 - 1) × .70].", "The numerator equals 1.40.", "The denominator equals 1.70.", "This gives an estimated reliability of .76."], "target_sentence_index": 4, "original_sentence": "This gives an estimated reliability of .82.", "error_original": ".76", "error_correct": ".82", "explanation": "The correct result of dividing 1.40 by 1.70 is approximately .82, not .76. This is a number/value change error. The Spearman-Brown formula predicts that doubling the length of a test with reliability .70 yields an estimated reliability of .82."}, {"id": "PMET-PC-0005", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "The Four Basic Schedules", "passage_type": "paragraph", "source_passage_id": "PMET-0181", "sentences": ["Reinforcement is delivered for the first response after a fixed time period has elapsed.", "FI schedules produce a characteristic \"scalloped\" pattern - responding is rapid immediately after reinforcement and decelerates as the interval ends.", "Checking the mailbox for an expected delivery is an example."], "target_sentence_index": 1, "original_sentence": "FI schedules produce a characteristic \"scalloped\" pattern - responding is slow immediately after reinforcement and accelerates as the interval ends.", "error_original": "responding is rapid immediately after reinforcement and decelerates as the interval ends", "error_correct": "responding is slow immediately after reinforcement and accelerates as the interval ends", "explanation": "In a Fixed-Interval (FI) schedule, the characteristic 'scalloped' pattern occurs because the organism pauses or responds slowly right after reinforcement is delivered (a post-reinforcement pause), then gradually increases its response rate as the end of the interval approaches. The error reverses this pattern, incorrectly stating that responding is rapid after reinforcement and decelerates toward the end of the interval."}, {"id": "PMET-PC-0006", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Evidence Based on Test Content", "passage_type": "example", "source_passage_id": "PMET-0344", "sentences": ["A licensing exam for clinical psychologists should sample knowledge across relevant domains: assessment, diagnosis, treatment, ethics, and research methods.", "If the exam heavily emphasizes one area while neglecting others, it would have poor construct validity for measuring overall competence in clinical psychology."], "target_sentence_index": 1, "original_sentence": "If the exam heavily emphasizes one area while neglecting others, it would have poor content validity for measuring overall competence in clinical psychology.", "error_original": "poor construct validity", "error_correct": "poor content validity", "explanation": "The passage is specifically discussing content validity, which concerns whether a test adequately samples the full domain of content it is intended to measure. The error substitutes 'construct validity' for 'content validity.' Construct validity refers to whether a test measures the theoretical construct it claims to measure, which is a related but distinct concept. In this example, the issue is that the exam fails to adequately sample all relevant content areas, which is a content validity problem."}, {"id": "PMET-PC-0007", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Internal Consistency Reliability", "passage_type": "paragraph", "source_passage_id": "PMET-0230", "sentences": ["The Kuder-Richardson formulas are special cases of Cronbach's alpha for dichotomously scored items (correct/incorrect).", "KR-20 is used when item difficulties are equal, while KR-21 assumes varying item difficulties (Kuder & Richardson, 1937)."], "target_sentence_index": 1, "original_sentence": "KR-20 is used when item difficulties vary, while KR-21 assumes equal item difficulties (Kuder & Richardson, 1937).", "error_original": "KR-20 is used when item difficulties are equal, while KR-21 assumes varying item difficulties", "error_correct": "KR-20 is used when item difficulties vary, while KR-21 assumes equal item difficulties", "explanation": "The descriptions of KR-20 and KR-21 have been swapped. In reality, KR-20 is the more general formula used when item difficulties vary across items, while KR-21 is a simplified version that assumes all items have equal difficulty levels. KR-21 is computationally easier but less accurate when item difficulties differ."}, {"id": "PMET-PC-0008", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Restriction of Range", "passage_type": "paragraph", "source_passage_id": "PMET-0053", "sentences": ["Restriction of range occurs when the variability in one or both variables is limited compared to the full population.", "This artificial restriction inflates (increases) the observed correlation coefficient.", "When range is restricted, correlations appear weaker than they would be in the unrestricted population (Sackett & Yang, 2000)."], "target_sentence_index": 1, "original_sentence": "This artificial restriction attenuates (reduces) the observed correlation coefficient.", "error_original": "inflates (increases)", "error_correct": "attenuates (reduces)", "explanation": "Restriction of range attenuates (reduces) the observed correlation coefficient, making it appear weaker than it truly is. The error reverses this effect by stating that restriction of range 'inflates (increases)' the correlation, which contradicts the well-established psychometric principle that limited variability in scores leads to an underestimation of the true correlation."}, {"id": "PMET-PC-0009", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0445", "sentences": ["Selection ratio refers to the proportion of applicants who are selected.", "A higher selection ratio allows an organization to be more selective by choosing only top scorers.", "This increased selectivity typically improves the predictive validity of the selection procedure."], "target_sentence_index": 1, "original_sentence": "A lower selection ratio allows an organization to be more selective by choosing only top scorers.", "error_original": "higher selection ratio", "error_correct": "lower selection ratio", "explanation": "A lower selection ratio (e.g., selecting 5 out of 100 applicants = 0.05) means fewer people are selected, allowing the organization to choose only the top scorers. A higher selection ratio means more applicants are selected, which reduces selectivity. This is a concept reversal error."}, {"id": "PMET-PC-0010", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Survey Research", "passage_type": "paragraph", "source_passage_id": "PMET-0272", "sentences": ["Survey research uses questionnaires or interviews to collect data from a sample, typically to describe characteristics of a population or examine relationships between variables.", "Representative sampling is critical for generalizing survey results to the sample of interest."], "target_sentence_index": 1, "original_sentence": "Representative sampling is critical for generalizing survey results to the population of interest.", "error_original": "to the sample of interest", "error_correct": "to the population of interest", "explanation": "The error substitutes 'sample' for 'population.' The purpose of representative sampling is to allow generalization from the sample to the broader population, not back to the sample itself. The sample is what is directly studied; the population is the larger group to which findings are generalized."}, {"id": "PMET-PC-0011", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "inferential-statistics.html", "chapter_title": "From Samples to Populations: Inferential Statistics", "section": "Cohen's d", "passage_type": "paragraph", "source_passage_id": "PMET-0149", "sentences": ["Cohen's d measures the standardized difference between two means, expressed in standard deviation units.", "It is commonly used with t-tests to quantify effect size.", "The formula divides the difference between the two group means by the pooled standard error.", "According to Cohen's guidelines, a d of 0.2 is considered small, 0.5 is medium, and 0.8 is large.", "Unlike statistical significance, Cohen's d provides information about the practical magnitude of an effect.", "A larger Cohen's d indicates greater separation between the two group distributions."], "target_sentence_index": 2, "original_sentence": "The formula divides the difference between the two group means by the pooled standard deviation.", "error_original": "pooled standard error", "error_correct": "pooled standard deviation", "explanation": "Cohen's d is calculated by dividing the difference between two group means by the pooled standard deviation, not the pooled standard error. The standard error reflects variability of a sampling distribution and is used in computing t-statistics, whereas Cohen's d specifically uses the pooled standard deviation to express the mean difference in standard deviation units."}, {"id": "PMET-PC-0012", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Classical Test Theory", "passage_type": "paragraph", "source_passage_id": "PMET-0219", "sentences": ["Classical Test Theory provides the theoretical foundation for understanding reliability.", "The core assumption is that any observed score (X) consists of three components: a true score (T) and error (E)."], "target_sentence_index": 1, "original_sentence": "The core assumption is that any observed score (X) consists of two components: a true score (T) and error (E).", "error_original": "three components", "error_correct": "two components", "explanation": "In Classical Test Theory, the observed score (X) is composed of exactly two components: the true score (T) and measurement error (E), expressed as X = T + E. The modified sentence incorrectly states 'three components' instead of 'two components.'"}, {"id": "PMET-PC-0013", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0453", "sentences": ["Meehl, P. E., & Rosen, A. (1955).", "Antecedent probability and the efficiency of psychometric signs, patterns, or cutting scores.", "Psychological Review, 52(3), 194-216."], "target_sentence_index": 2, "original_sentence": "Psychological Bulletin, 52(3), 194-216.", "error_original": "Psychological Review", "error_correct": "Psychological Bulletin", "explanation": "The seminal Meehl and Rosen (1955) article on antecedent probability and the efficiency of psychometric signs was published in the Psychological Bulletin, not the Psychological Review. These are two distinct APA journals, and misattributing the publication source is a common error."}, {"id": "PMET-PC-0014", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "Unreliability", "passage_type": "paragraph", "source_passage_id": "PMET-0099", "sentences": ["Unreliability in either the predictor or criterion inflates the validity coefficient.", "The correction for attenuation formula estimates what validity would be if both measures were perfectly reliable."], "target_sentence_index": 0, "original_sentence": "Unreliability in either the predictor or criterion attenuates the validity coefficient.", "error_original": "inflates", "error_correct": "attenuates", "explanation": "Unreliability in the predictor or criterion attenuates (reduces) the validity coefficient, not inflates it. This is a concept reversal error. The correction for attenuation formula exists precisely because unreliability causes an underestimate of the true relationship between predictor and criterion."}, {"id": "PMET-PC-0015", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Evidence Based on Response Processes", "passage_type": "clinical_note", "source_passage_id": "PMET-0341", "sentences": ["Response Process Investigation: If a reading comprehension test is designed to measure inference-making, validity evidence should demonstrate that examinees actually make inferences to answer questions.", "This evidence should confirm that examinees are not relying on prior knowledge or test-taking strategies that bypass the intended processes.", "Methods such as think-aloud protocols and eye-tracking studies are commonly used to gather evidence based on response processes.", "This type of evidence is most closely associated with criterion validity in the Standards for Educational and Psychological Testing framework.", "Without response process evidence, there is a risk that test scores reflect construct-irrelevant variance rather than the targeted cognitive skill."], "target_sentence_index": 3, "original_sentence": "This type of evidence is most closely associated with construct validity in the Standards for Educational and Psychological Testing framework.", "error_original": "criterion validity", "error_correct": "construct validity", "explanation": "Evidence based on response processes is a source of construct validity evidence, not criterion validity evidence. According to the Standards for Educational and Psychological Testing, response process evidence helps establish that examinees are engaging in the cognitive processes the test is intended to measure, which is fundamentally a construct validity concern. Criterion validity, by contrast, involves the relationship between test scores and an external criterion measure."}, {"id": "PMET-PC-0016", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Major Qualitative Traditions", "passage_type": "paragraph", "source_passage_id": "PMET-0286", "sentences": ["The case study approach involves in-depth examination of a single case or a small number of cases.", "A \"case\" can be an individual, a program, an organization, or an event.", "Case studies use multiple data sources (interviews, documents, observations) and are valuable for exploring complex phenomena in real-world contexts.", "They provide rich detail but have limited internal validity."], "target_sentence_index": 3, "original_sentence": "They provide rich detail but have limited generalizability.", "error_original": "limited internal validity", "error_correct": "limited generalizability", "explanation": "The original passage correctly states that case studies have limited generalizability, which is the well-known trade-off of this approach — the rich, in-depth detail comes at the cost of being difficult to generalize to other cases or populations. The error substitutes 'internal validity' for 'generalizability' (which relates to external validity). Case studies are actually often noted for their strength in exploring real-world contexts with depth, but their primary limitation is generalizability, not internal validity."}, {"id": "PMET-PC-0017", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Standard Error of Measurement", "passage_type": "definition", "source_passage_id": "PMET-0207", "sentences": ["The Standard Error of Measurement (SEM) is calculated using a specific formula.", "SEM = SD × √(1 - rxx).", "In this formula, SD represents the standard deviation of test scores.", "The term rxx refers to the validity coefficient."], "target_sentence_index": 3, "original_sentence": "The term rxx refers to the reliability coefficient.", "error_original": "validity coefficient", "error_correct": "reliability coefficient", "explanation": "The passage incorrectly refers to rxx as the 'validity coefficient.' In the SEM formula, rxx represents the reliability coefficient of the test, not the validity coefficient. Reliability refers to the consistency of measurement, whereas validity refers to how well a test measures what it is intended to measure. These are distinct psychometric properties."}, {"id": "PMET-PC-0018", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0540", "sentences": ["Grounded theory generates theory from data, as originally described by Glaser and Strauss.", "The approach uses three main coding stages: open, axial, and selective coding.", "Data collection in grounded theory is guided by theoretical sampling, where subsequent participants are selected based on emerging categories.", "The process continues until theoretical saturation is reached, meaning no new themes or categories emerge from additional data.", "A key feature of grounded theory is that the researcher begins with a pre-established hypothesis and then collects data to confirm or disconfirm it."], "target_sentence_index": 4, "original_sentence": "A key feature of grounded theory is that the researcher begins without a pre-established hypothesis and allows theory to emerge from the data.", "error_original": "begins with a pre-established hypothesis and then collects data to confirm or disconfirm it", "error_correct": "begins without a pre-established hypothesis and allows theory to emerge from the data", "explanation": "Grounded theory is an inductive methodology in which theory emerges from the data rather than starting with a pre-established hypothesis. The error reverses this core concept, incorrectly stating that the researcher begins with a hypothesis—a deductive approach that is the opposite of what grounded theory entails."}, {"id": "PMET-PC-0019", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Discriminative Stimuli", "passage_type": "paragraph", "source_passage_id": "PMET-0187", "sentences": ["A discriminative stimulus (S D) is a stimulus in the presence of which a particular response has been reinforced.", "Through experience, organisms learn to respond similarly in the presence versus absence of the S D.", "The S D does not elicit the response (as in classical conditioning) but rather sets the occasion for the response by signaling reinforcement availability."], "target_sentence_index": 1, "original_sentence": "Through experience, organisms learn to respond differently in the presence versus absence of the S D.", "error_original": "respond similarly", "error_correct": "respond differently", "explanation": "The concept of a discriminative stimulus is that organisms learn to respond DIFFERENTLY depending on whether the S D is present or absent. The error reverses this core concept by stating they respond 'similarly,' which contradicts the fundamental principle of stimulus discrimination in operant conditioning."}, {"id": "PMET-PC-0020", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "inferential-statistics.html", "chapter_title": "From Samples to Populations: Inferential Statistics", "section": "One-Sample t-Test", "passage_type": "paragraph", "source_passage_id": "PMET-0137", "sentences": ["The one-sample t-test is used to compare a sample mean to a known or hypothesized population value.", "For example, it can be used to test whether a sample's IQ differs from the population median of 100."], "target_sentence_index": 1, "original_sentence": "For example, it can be used to test whether a sample's IQ differs from the population mean of 100.", "error_original": "population median of 100", "error_correct": "population mean of 100", "explanation": "The one-sample t-test compares a sample mean to a known or hypothesized population mean, not the population median. The population mean for IQ is 100. A test that compares to a median would be a different nonparametric procedure (e.g., the sign test or Wilcoxon signed-rank test)."}, {"id": "PMET-PC-0021", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Evidence Based on Relations to Other Variables", "passage_type": "paragraph", "source_passage_id": "PMET-0358", "sentences": ["Convergent validity is demonstrated when a test correlates strongly with other measures of the same or similar constructs.", "High correlations with conceptually unrelated measures support the interpretation that the test measures the intended construct."], "target_sentence_index": 1, "original_sentence": "High correlations with conceptually related measures support the interpretation that the test measures the intended construct.", "error_original": "conceptually unrelated measures", "error_correct": "conceptually related measures", "explanation": "The passage incorrectly states 'conceptually unrelated measures' when it should say 'conceptually related measures.' Convergent validity is supported when a test correlates highly with measures of related or similar constructs. High correlations with conceptually unrelated measures would actually be evidence against construct validity (and would undermine discriminant validity). The defining feature of convergent validity is strong associations with theoretically related measures."}, {"id": "PMET-PC-0022", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "Decision Theory and Cutoff Scores", "passage_type": "paragraph", "source_passage_id": "PMET-0108", "sentences": ["When tests are used to make categorical decisions (select/reject, diagnose/not diagnose), a cutoff score must be established.", "The choice of cutoff involves trade-offs between different types of errors.", "Raising the cutoff score on a selection test will decrease the number of false positives but increase the number of false negatives.", "Lowering the cutoff score will have the opposite effect, increasing false positives while decreasing false negatives.", "The base rate, which refers to the proportion of applicants who would succeed on the criterion without any selection, influences the utility of the test.", "When the base rate is extremely high or extremely low, the test adds the most incremental validity to the selection process.", "The Taylor-Russell tables demonstrate how the selection ratio, base rate, and test validity interact to determine the proportion of correct decisions."], "target_sentence_index": 5, "original_sentence": "When the base rate is extremely high or extremely low, the test adds the least incremental validity to the selection process.", "error_original": "the test adds the most incremental validity", "error_correct": "the test adds the least incremental validity", "explanation": "When the base rate is extremely high or extremely low, a selection test adds the least incremental validity because decisions could already be made with reasonable accuracy without the test. A test is most useful when the base rate is moderate (around .50), as there is the most room for the test to improve upon chance-level prediction. The error reverses 'least' to 'most,' representing a concept reversal."}, {"id": "PMET-PC-0023", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Types of Bias", "passage_type": "definition", "source_passage_id": "PMET-0339", "sentences": ["Bias is a statistical/psychometric concept referring to systematic measurement error.", "Fairness is a broader concept encompassing social values and judgments about appropriate test use.", "A test can be psychometrically unbiased yet still raise fairness concerns if its use produces random impact on protected groups."], "target_sentence_index": 2, "original_sentence": "A test can be psychometrically unbiased yet still raise fairness concerns if its use produces adverse impact on protected groups.", "error_original": "random impact", "error_correct": "adverse impact", "explanation": "The original passage correctly uses the term 'adverse impact,' which is the established legal and psychometric term referring to a substantially different selection rate for members of a protected group. 'Random impact' is not a recognized concept in this context. Adverse impact is specifically defined (e.g., by the EEOC's four-fifths rule) and is the key concern when a psychometrically unbiased test still raises fairness issues."}, {"id": "PMET-PC-0024", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Evidence Based on Relations to Other Variables", "passage_type": "definition", "source_passage_id": "PMET-0338", "sentences": ["Campbell and Fiske (1959) introduced the multitrait-multimethod (MTMM) matrix to evaluate both convergent and discriminant validity simultaneously.", "The matrix examines correlations among multiple traits measured by multiple methods.", "Convergent validity is indicated by high correlations between the same trait measured by different methods, known as monotrait-heteromethod correlations.", "Discriminant validity is indicated by lower correlations between different traits, even when measured by different methods, known as heterotrait-monomethod correlations."], "target_sentence_index": 3, "original_sentence": "Discriminant validity is indicated by lower correlations between different traits, even when measured by the same method, known as heterotrait-monomethod correlations.", "error_original": "even when measured by different methods", "error_correct": "even when measured by the same method", "explanation": "Discriminant validity in the MTMM matrix is demonstrated by low correlations between different traits measured by the SAME method (heterotrait-monomethod block). The error changes 'same method' to 'different methods,' which misrepresents the concept. The key insight of discriminant validity is that correlations should remain low between different traits even when method variance is shared (i.e., the same method is used), showing that the traits are truly distinct."}, {"id": "PMET-PC-0025", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "The Three-Term Contingency", "passage_type": "paragraph", "source_passage_id": "PMET-0189", "sentences": ["The relationship between discriminative stimuli, behavior, and consequences is captured in the three-term contingency.", "This framework is also commonly referred to as the ABC model, where A stands for Antecedent, B stands for Behavior, and C stands for Consequence.", "The antecedent is the discriminative stimulus that signals the availability of reinforcement for a particular response.", "The behavior is the operant response emitted by the organism in the presence of that stimulus.", "The consequence is what follows the behavior, such as reinforcement or punishment, and it decreases the future probability of that behavior occurring under similar antecedent conditions.", "This three-term contingency forms the foundational unit of analysis in operant conditioning as described by B.F. Skinner."], "target_sentence_index": 4, "original_sentence": "The consequence is what follows the behavior, such as reinforcement or punishment, and it affects the future probability of that behavior occurring under similar antecedent conditions.", "error_original": "decreases the future probability", "error_correct": "affects the future probability", "explanation": "The consequence in the three-term contingency can either increase or decrease the future probability of behavior depending on whether it involves reinforcement or punishment. Saying it 'decreases' the probability is incorrect because reinforcement increases the probability while punishment decreases it. The correct phrasing should be 'affects' (or 'influences') the future probability, since the direction of change depends on the type of consequence."}, {"id": "PMET-PC-0026", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-validity.html", "chapter_title": "Threats & Safeguards: Research Validity", "section": "Construct Validity in Research", "passage_type": "paragraph", "source_passage_id": "PMET-0322", "sentences": ["Construct validity in research refers to the degree to which the independent and dependent variables represent the theoretical constructs of interest.", "This is distinct from test validity (covered in later chapters) and addresses whether the operational definitions used in research adequately capture the unintended constructs."], "target_sentence_index": 1, "original_sentence": "This is distinct from test validity (covered in later chapters) and addresses whether the operational definitions used in research adequately capture the intended constructs.", "error_original": "unintended constructs", "error_correct": "intended constructs", "explanation": "The passage was modified to say 'unintended constructs' instead of 'intended constructs.' Construct validity in research concerns whether the operational definitions adequately capture the constructs the researcher intends to study, not unintended ones. This is a concept reversal error that changes the meaning of the sentence to the opposite of what is correct."}, {"id": "PMET-PC-0027", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "Clinical Utility", "passage_type": "paragraph", "source_passage_id": "PMET-0107", "sentences": ["Clinical utility refers to the practical value of a test in clinical settings—whether using the test improves patient outcomes or clinical decision-making.", "A test may have adequate reliability evidence but limited clinical utility if it doesn't lead to better decisions or outcomes."], "target_sentence_index": 1, "original_sentence": "A test may have adequate validity evidence but limited clinical utility if it doesn't lead to better decisions or outcomes.", "error_original": "reliability evidence", "error_correct": "validity evidence", "explanation": "The original passage states that a test may have adequate 'validity evidence' but limited clinical utility. The error substitutes 'reliability evidence' for 'validity evidence.' While reliability is a necessary condition for validity, the passage specifically discusses criterion-related validity and the distinction between having sufficient validity evidence and actually being useful in clinical practice. The correct term here is 'validity evidence.'"}, {"id": "PMET-PC-0028", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Introduction to Research Designs", "passage_type": "definition", "source_passage_id": "PMET-0239", "sentences": ["True Experiment: A design with random assignment, manipulation of the independent variable, and control of extraneous variables.", "Quasi-Experiment: A design lacking random assignment but including manipulation of the independent variable.", "Random Assignment: Procedure ensuring each participant has an equal chance of being assigned to any condition.", "Control Group: A comparison condition that does not receive the experimental treatment.", "Between-Subjects Design: Same participants tested in all conditions.", "Within-Subjects Design: Same participants tested in all conditions."], "target_sentence_index": 4, "original_sentence": "Between-Subjects Design: Different participants in each condition.", "error_original": "Same participants tested in all conditions", "error_correct": "Different participants in each condition", "explanation": "The definition given for Between-Subjects Design is actually the definition of a Within-Subjects Design. In a between-subjects design, different participants are assigned to each condition, whereas in a within-subjects design, the same participants are tested across all conditions."}, {"id": "PMET-PC-0029", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "ABAB (Withdrawal/Reversal) Design", "passage_type": "paragraph", "source_passage_id": "PMET-0266", "sentences": ["The ABAB design strengthens causal inference by adding withdrawal and reintroduction phases.", "The sequence is: baseline (A) → treatment (B) → withdrawal (A) → treatment (B).", "If behavior improves during treatment phases and deteriorates during withdrawal phases, this provides compelling evidence that the treatment is correlated with the change."], "target_sentence_index": 2, "original_sentence": "If behavior improves during treatment phases and deteriorates during withdrawal phases, this provides compelling evidence that the treatment caused the change.", "error_original": "correlated with the change", "error_correct": "caused the change", "explanation": "The ABAB design is specifically used to strengthen causal inference, not merely correlational evidence. The repeated withdrawal and reintroduction of treatment allows researchers to demonstrate a causal relationship between the treatment and the observed behavior change. Saying the evidence supports 'correlation' rather than 'causation' undermines the fundamental purpose and strength of this experimental single-subject design."}, {"id": "PMET-PC-0030", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Item Parameters", "passage_type": "paragraph", "source_passage_id": "PMET-0522", "sentences": ["IRT models characterize items using up to three parameters.", "The a-parameter represents item difficulty, indicating the point on the latent trait continuum where the probability of a correct response is 0.50.", "The b-parameter represents item discrimination, reflecting how well an item differentiates between examinees at different ability levels.", "Higher discrimination values produce steeper item characteristic curves.", "The c-parameter, sometimes called the pseudo-guessing parameter, represents the lower asymptote of the item characteristic curve.", "In the three-parameter logistic model, all three parameters are estimated, while the one-parameter Rasch model constrains discrimination to be equal across items and fixes the c-parameter at zero."], "target_sentence_index": 1, "original_sentence": "The b-parameter represents item difficulty, indicating the point on the latent trait continuum where the probability of a correct response is 0.50.", "error_original": "The a-parameter represents item difficulty", "error_correct": "The b-parameter represents item difficulty", "explanation": "In IRT, the b-parameter (not the a-parameter) represents item difficulty. The a-parameter represents item discrimination. The passage swapped the labels of the a- and b-parameters in sentences 1 and 2, assigning difficulty to 'a' and discrimination to 'b' when it should be the reverse."}, {"id": "PMET-PC-0031", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0557", "sentences": ["Kline, R. B. (2015).", "Principles and practice of structural equation modeling (3rd ed.).", "Guilford Press."], "target_sentence_index": 1, "original_sentence": "Principles and practice of structural equation modeling (4th ed.).", "error_original": "3rd ed.", "error_correct": "4th ed.", "explanation": "The 2015 publication of Kline's 'Principles and Practice of Structural Equation Modeling' is the 4th edition, not the 3rd edition. The 3rd edition was published in 2011. This is a number/value change error."}, {"id": "PMET-PC-0032", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "variables-data.html", "chapter_title": "Variables, Scales & the Language of Data", "section": "Percentiles and Percentile Ranks", "passage_type": "definition", "source_passage_id": "PMET-0375", "sentences": ["Special percentiles are known as quartiles, which divide a distribution into four equal parts.", "Q1, the 25th percentile, is the first quartile, meaning 25% of scores fall below this point.", "Q2, the 50th percentile, is the second quartile and corresponds to the mode of the distribution.", "Q3, the 75th percentile, is the third quartile, meaning 75% of scores fall below this point.", "The Interquartile Range (IQR) is calculated as Q3 minus Q1 and contains the middle 50% of scores."], "target_sentence_index": 2, "original_sentence": "Q2, the 50th percentile, is the second quartile and corresponds to the median of the distribution.", "error_original": "the mode", "error_correct": "the median", "explanation": "Q2, the 50th percentile, corresponds to the median (the middle value of a distribution), not the mode (the most frequently occurring value). The passage originally correctly identifies Q2 as the median."}, {"id": "PMET-PC-0033", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Introduction to Correlation", "passage_type": "paragraph", "source_passage_id": "PMET-0043", "sentences": ["Correlation is a statistical technique used to describe and measure the relationship between two variables.", "Understanding correlation is essential for psychological research and practice, as it underlies many assessment concepts including test reliability and validity.", "The correlation coefficient quantifies both the strength and direction of the curvilinear relationship between two variables."], "target_sentence_index": 2, "original_sentence": "The correlation coefficient quantifies both the strength and direction of the linear relationship between two variables.", "error_original": "curvilinear relationship", "error_correct": "linear relationship", "explanation": "The correlation coefficient (specifically Pearson's r) measures the strength and direction of the LINEAR relationship between two variables, not the curvilinear relationship. A standard correlation coefficient can actually miss or underestimate curvilinear relationships, which is why it is specifically described as a measure of linear association."}, {"id": "PMET-PC-0034", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0560", "sentences": ["Solomon, R. L. (1949).", "An extension of control group design.", "Psychological Review, 46(2), 137-150."], "target_sentence_index": 2, "original_sentence": "Psychological Bulletin, 46(2), 137-150.", "error_original": "Psychological Review", "error_correct": "Psychological Bulletin", "explanation": "Solomon's classic 1949 paper on the Solomon Four-Group Design was published in the Psychological Bulletin, not the Psychological Review. This is a journal name substitution error, swapping one well-known APA journal for another."}, {"id": "PMET-PC-0035", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "Incremental Validity", "passage_type": "example", "source_passage_id": "PMET-0090", "sentences": ["A clinician wants to know if a new personality measure adds predictive value for therapy outcome beyond established predictors (severity, chronicity, social support).", "Existing predictors account for 25% of outcome variance.", "Adding the personality measure increases this to 32%.", "The 7% increase (ΔR² = .07) represents the construct validity of the new measure."], "target_sentence_index": 3, "original_sentence": "The 7% increase (ΔR² = .07) represents the incremental validity of the new measure.", "error_original": "construct validity", "error_correct": "incremental validity", "explanation": "The passage is specifically about incremental validity, which refers to the degree to which a new measure improves prediction beyond existing predictors. The error substituted 'construct validity' (which refers to whether a test measures the theoretical construct it is intended to measure) for 'incremental validity.' The 7% increase in variance accounted for (ΔR² = .07) is the classic definition of incremental validity, not construct validity."}, {"id": "PMET-PC-0036", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Strength of Correlation", "passage_type": "paragraph", "source_passage_id": "PMET-0050", "sentences": ["However, these guidelines should be interpreted in context.", "In some research areas (e.g., personality psychology), correlations of .30 are considered substantial, while in others (e.g., test-retest reliability), correlations below .60 might be considered inadequate."], "target_sentence_index": 1, "original_sentence": "In some research areas (e.g., personality psychology), correlations of .30 are considered substantial, while in others (e.g., test-retest reliability), correlations below .80 might be considered inadequate.", "error_original": "correlations below .60 might be considered inadequate", "error_correct": "correlations below .80 might be considered inadequate", "explanation": "The passage incorrectly states that correlations below .60 might be considered inadequate for test-retest reliability. The correct threshold is .80. In psychometrics, test-retest reliability is expected to be quite high, and correlations below .80 are generally considered inadequate for this purpose."}, {"id": "PMET-PC-0037", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Commonly Tested Topics for the EPPP", "passage_type": "clinical_note", "source_passage_id": "PMET-0420", "sentences": ["Preparedness refers to biological constraints on learning.", "It explains why evolutionary-relevant fears are acquired faster than other associations.", "These biologically prepared fears also extinguish more quickly than non-prepared associations."], "target_sentence_index": 2, "original_sentence": "These biologically prepared fears also extinguish more slowly than non-prepared associations.", "error_original": "extinguish more quickly", "error_correct": "extinguish more slowly", "explanation": "According to the concept of preparedness, evolutionary-relevant fears not only are acquired faster but also extinguish more slowly (not more quickly) than non-prepared associations. This is a concept reversal error — prepared fears are notably resistant to extinction, which is a key feature of the preparedness theory proposed by Seligman."}, {"id": "PMET-PC-0038", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Nonprobability Sampling", "passage_type": "paragraph", "source_passage_id": "PMET-0281", "sentences": ["Nonprobability sampling methods do not ensure every member has a chance of selection.", "These include convenience sampling (readily available participants), purposive sampling (participants selected for specific characteristics), and snowball sampling (participants are randomly assigned by other participants)."], "target_sentence_index": 1, "original_sentence": "These include convenience sampling (readily available participants), purposive sampling (participants selected for specific characteristics), and snowball sampling (participants recruit other participants).", "error_original": "participants are randomly assigned by other participants", "error_correct": "participants recruit other participants", "explanation": "Snowball sampling involves existing participants recruiting additional participants from among their acquaintances, not randomly assigning them. The error substitutes 'randomly assigned by' for 'recruit,' which incorrectly introduces a concept from experimental design (random assignment) into a description of a nonprobability sampling technique."}, {"id": "PMET-PC-0039", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "CR versus UR", "passage_type": "paragraph", "source_passage_id": "PMET-0424", "sentences": ["While the conditioned response often resembles the unconditioned response, they are not identical.", "The CR is typically weaker in magnitude compared to the UR.", "The CR usually has a longer latency, meaning it takes more time to appear after the stimulus is presented.", "Additionally, the CR may sometimes be opposite in direction to the UR, a phenomenon known as compensatory conditioning.", "For example, if a drug (US) causes a decrease in pain sensitivity, the CR to cues associated with the drug may be an increase in pain sensitivity.", "This compensatory response mechanism was first extensively studied by Robert Rescorla as part of his opponent-process theory of conditioning."], "target_sentence_index": 5, "original_sentence": "This compensatory response mechanism was first extensively studied by Shepard Siegel as part of his opponent-process theory of conditioning.", "error_original": "Robert Rescorla", "error_correct": "Shepard Siegel", "explanation": "The compensatory response model of drug tolerance through classical conditioning was extensively studied by Shepard Siegel, not Robert Rescorla. Rescorla is best known for the Rescorla-Wagner model of associative learning, while Siegel's work focused on how conditioned compensatory responses explain drug tolerance and overdose."}, {"id": "PMET-PC-0040", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "inferential-statistics.html", "chapter_title": "From Samples to Populations: Inferential Statistics", "section": "Null and Alternative Hypotheses", "passage_type": "example", "source_passage_id": "PMET-0118", "sentences": ["A researcher wants to test whether a new therapy reduces depression scores compared to a control condition.", "H₀: There is no difference in depression scores between the therapy and control groups (μ₁ = μ₂).", "H₁: The therapy group has lower depression scores than the control group (μ₁ < μ₂) — this is a directional/one-tailed hypothesis.", "Alternatively, H₁: There is a difference in depression scores between groups (μ₁ ≠ μ₂) — this is a directional/two-tailed hypothesis."], "target_sentence_index": 3, "original_sentence": "Alternatively, H₁: There is a difference in depression scores between groups (μ₁ ≠ μ₂) — this is a non-directional/two-tailed hypothesis.", "error_original": "directional/two-tailed", "error_correct": "non-directional/two-tailed", "explanation": "A two-tailed hypothesis that simply states there is a difference between groups (μ₁ ≠ μ₂) without specifying the direction of the difference is classified as non-directional, not directional. The directional hypothesis is the one-tailed version (μ₁ < μ₂), which specifies which group is expected to score higher or lower."}, {"id": "PMET-PC-0041", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Alternate (Parallel) Forms Reliability", "passage_type": "example", "source_passage_id": "PMET-0213", "sentences": ["A publisher develops Form A and Form B of an achievement test, each with different items measuring the same content domain.", "Students take Form A one week and Form B the next.", "A correlation of .85 between forms indicates good alternate forms reliability, suggesting the two versions measure different constructs consistently."], "target_sentence_index": 2, "original_sentence": "A correlation of .85 between forms indicates good alternate forms reliability, suggesting the two versions measure the same construct consistently.", "error_original": "different constructs", "error_correct": "the same construct", "explanation": "The error introduces a concept reversal. A high correlation between alternate forms (.85) indicates that the two versions measure the same construct consistently, not different constructs. The entire purpose of alternate forms reliability is to demonstrate that both forms are equivalent measures of the same underlying construct."}, {"id": "PMET-PC-0042", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0512", "sentences": ["Item Characteristic Curves are J-shaped curves showing the probability of a correct response as a function of theta (ability).", "In Item Response Theory, theta represents the latent trait or ability level of the examinee.", "As theta increases, the probability of answering an item correctly also increases, producing the characteristic curve shape.", "The three main parameters that can define an ICC are item difficulty, item discrimination, and a guessing parameter."], "target_sentence_index": 0, "original_sentence": "Item Characteristic Curves are S-shaped curves showing the probability of a correct response as a function of theta (ability).", "error_original": "J-shaped curves", "error_correct": "S-shaped curves", "explanation": "Item Characteristic Curves (ICCs) in Item Response Theory are S-shaped (sigmoid/ogive) curves, not J-shaped curves. The S-shape reflects the logistic function where the probability of a correct response starts near zero (or the guessing parameter) at low ability levels, increases steeply around the item's difficulty level, and asymptotically approaches 1.0 at high ability levels."}, {"id": "PMET-PC-0043", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0576", "sentences": ["Test bias is random measurement error for specific groups.", "Fairness involves value judgments about test use."], "target_sentence_index": 0, "original_sentence": "Test bias is systematic measurement error for specific groups.", "error_original": "random measurement error", "error_correct": "systematic measurement error", "explanation": "Test bias refers to systematic (not random) measurement error that consistently affects specific groups. Random error is unsystematic and affects all examinees equally, whereas bias is a consistent, directional distortion in scores for a particular group. This is a concept reversal error substituting 'random' for 'systematic.'"}, {"id": "PMET-PC-0044", "mode": "passage_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Blocking", "passage_type": "paragraph", "source_passage_id": "PMET-0018", "sentences": ["The blocking phenomenon, discovered by Leon Kamin (1969), demonstrated that mere temporal contiguity between CS and US is insufficient for conditioning.", "In a blocking experiment, a CS1 is first paired with a US until a strong conditioned response is established.", "In the second phase, CS1 is presented together with a novel CS2, and this compound stimulus is paired with the same US.", "Despite repeated pairings of CS2 with the US, subsequent testing reveals that CS2 elicits little or no conditioned response.", "The prior conditioning of CS1 effectively facilitates learning about CS2 during the compound phase.", "This finding was critical in supporting the Rescorla-Wagner model, which holds that conditioning depends on the degree to which the US is surprising or unexpected."], "target_sentence_index": 4, "original_sentence": "The prior conditioning of CS1 effectively blocks learning about CS2 during the compound phase.", "error_original": "facilitates", "error_correct": "blocks", "explanation": "The error involves a concept reversal. In the blocking effect, prior conditioning of CS1 blocks (prevents) learning about CS2, because the US is already fully predicted by CS1. The modified sentence incorrectly states that prior conditioning 'facilitates' learning about CS2, which is the opposite of what actually occurs in the blocking phenomenon."}, {"id": "PMET-VD-0001", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "Setting Cutoff Scores", "passage_type": "definition", "source_passage_id": "PMET-0081", "entries": [{"term": "Sensitivity", "definition": "Sensitivity refers to the ability of a test to correctly identify individuals who truly have the condition (true positives). A test with high sensitivity will have a low rate of false negatives, meaning fewer cases are missed. Sensitivity is particularly important when the cost of missing a true case is high.", "is_target": false}, {"term": "Specificity", "definition": "Specificity refers to the ability of a test to correctly identify individuals who truly do not have the condition (true negatives). A test with high specificity will have a low rate of false positives, meaning fewer false alarms occur. Specificity is prioritized when the consequences of incorrectly labeling someone as positive are severe.", "is_target": false}, {"term": "Cutoff Score Selection", "definition": "Cutoff score selection involves choosing a threshold on a test that balances sensitivity and specificity based on the purpose of the assessment. A higher cutoff score increases sensitivity by capturing more true positives while decreasing specificity through more false positives. The optimal cutoff depends on whether missing true cases or generating false alarms carries a greater cost.", "is_target": true}, {"term": "Positive Predictive Value", "definition": "Positive predictive value (PPV) is the proportion of individuals who test positive who actually have the condition. PPV depends not only on the test's sensitivity and specificity but also on the base rate of the condition in the population. When base rates are low, even highly specific tests can produce a relatively low PPV.", "is_target": false}], "target_entry_index": 2, "error_original": "A higher cutoff score increases sensitivity by capturing more true positives while decreasing specificity through more false positives", "error_correct": "A lower cutoff score increases sensitivity by capturing more true positives while decreasing specificity through more false positives", "explanation": "The definition incorrectly states that a higher cutoff score increases sensitivity. In reality, a LOWER cutoff score increases sensitivity (more true positives) at the expense of specificity (more false positives). A HIGHER cutoff score does the opposite — it increases specificity (more true negatives) while decreasing sensitivity (more false negatives)."}, {"id": "PMET-VD-0002", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Coefficient of Determination (r²)", "passage_type": "paragraph", "source_passage_id": "PMET-0051", "entries": [{"term": "Standard Error of Estimate", "definition": "The standard error of estimate is a measure of the accuracy of predictions made with a regression equation. It quantifies the average distance that observed values fall from the regression line, with smaller values indicating more accurate predictions.", "is_target": false}, {"term": "Coefficient of Determination (r²)", "definition": "The coefficient of determination (r²) is obtained by taking the square root of the correlation coefficient. It represents the proportion of variance in one variable that is explained by or shared with the other variable. This value is also referred to as shared variance or common variance.", "is_target": true}, {"term": "Correlation Coefficient (r)", "definition": "The correlation coefficient (r) is a statistical measure that quantifies the strength and direction of the linear relationship between two variables. Its values range from -1.00 to +1.00, where values closer to the extremes indicate stronger relationships.", "is_target": false}, {"term": "Coefficient of Alienation (k²)", "definition": "The coefficient of alienation (k²) represents the proportion of variance in one variable that is not explained by the other variable. It is calculated by subtracting the coefficient of determination from 1.00 and reflects unexplained or error variance.", "is_target": false}], "target_entry_index": 1, "error_original": "obtained by taking the square root of the correlation coefficient", "error_correct": "obtained by squaring the correlation coefficient", "explanation": "The coefficient of determination (r²) is obtained by squaring the correlation coefficient (r), not by taking its square root. For example, if r = .80, then r² = .64, meaning 64% of the variance in one variable is explained by the other. Taking the square root would be the reverse operation—used to derive r from r²."}, {"id": "PMET-VD-0003", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "inferential-statistics.html", "chapter_title": "From Samples to Populations: Inferential Statistics", "section": "Analysis of Covariance (ANCOVA)", "passage_type": "definition", "source_passage_id": "PMET-0114", "entries": [{"term": "MANCOVA", "definition": "An extension of ANCOVA used when there are two or more dependent variables. It statistically controls for one or more covariates while simultaneously analyzing multiple DVs, helping to reduce the risk of Type I error from running multiple separate ANCOVAs.", "is_target": false}, {"term": "ANCOVA", "definition": "A statistical technique used to control for confounding variables that cannot be controlled experimentally by including them as covariates. It reduces between-group error variance, thereby increasing statistical power. A key assumption is homogeneity of regression slopes, meaning the relationship between the covariate and the DV must be similar across groups.", "is_target": true}, {"term": "ANOVA", "definition": "A statistical technique used to compare means across two or more groups by partitioning total variance into between-group and within-group components. It tests whether group means differ significantly and assumes homogeneity of variance, independence of observations, and normality of distributions.", "is_target": false}, {"term": "Repeated Measures ANOVA", "definition": "A variant of ANOVA used when the same participants are measured under multiple conditions or at multiple time points. It accounts for the correlation between repeated observations on the same subjects, thereby reducing error variance associated with individual differences.", "is_target": false}], "target_entry_index": 1, "error_original": "reduces between-group error variance", "error_correct": "reduces within-group error variance", "explanation": "ANCOVA works by statistically removing variability associated with the covariate, which reduces within-group (not between-group) error variance. By reducing within-group error variance, the F-ratio becomes larger, thereby increasing statistical power to detect true group differences."}, {"id": "PMET-VD-0004", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Standard Error of the Difference", "passage_type": "paragraph", "source_passage_id": "PMET-0233", "entries": [{"term": "Standard Error of Measurement (SEM)", "definition": "An estimate of the amount of error in an individual's observed test score. It is derived from the test's standard deviation and its reliability coefficient, and is used to construct confidence intervals around an observed score.", "is_target": false}, {"term": "Standard Error of the Difference (SEdiff)", "definition": "A statistic used when comparing two scores (e.g., Verbal IQ vs. Performance IQ, or pretest vs. posttest) to determine whether the difference between them is clinically meaningful. It is calculated by taking the square root of the product of the two standard errors of measurement.", "is_target": true}, {"term": "Standard Error of the Mean (SEMean)", "definition": "An estimate of how much a sample mean is expected to vary from the true population mean. It is calculated by dividing the sample standard deviation by the square root of the sample size, and decreases as sample size increases.", "is_target": false}, {"term": "Standard Error of Estimate (SEest)", "definition": "A measure of the accuracy of predictions made using a regression equation. It reflects the average distance that observed values fall from the regression line and is derived from the correlation coefficient and the standard deviation of the criterion variable.", "is_target": false}], "target_entry_index": 1, "error_original": "the square root of the product of the two standard errors of measurement", "error_correct": "the square root of the sum of the two squared standard errors of measurement", "explanation": "The SEdiff is calculated by taking the square root of the SUM of the two squared SEMs (i.e., SEdiff = √(SEM₁² + SEM₂²)), not the square root of their product. This formula is based on the principle that variances of independent errors are additive. The error subtly swaps 'sum of the squared' for 'product of the,' which would yield incorrect values and represents a wrong computational mechanism."}, {"id": "PMET-VD-0005", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Commonly Tested Topics for the EPPP", "passage_type": "clinical_note", "source_passage_id": "PMET-0434", "entries": [{"term": "Standard Error of Measurement", "definition": "A statistic that estimates the amount of error in an individual's observed test score. It is derived from the test's reliability coefficient and the standard deviation of the test scores, and it is used to construct confidence intervals around observed scores.", "is_target": false}, {"term": "Standard Error of Estimate", "definition": "A measure of prediction accuracy in regression analysis that indicates the average amount by which predicted scores deviate from actual scores. It increases as the correlation coefficient (r) increases, reflecting less variability around the regression line.", "is_target": true}, {"term": "Standard Error of the Mean", "definition": "A statistic that estimates how much a sample mean is likely to differ from the population mean. It is calculated by dividing the standard deviation of the sample by the square root of the sample size, and it decreases as sample size increases.", "is_target": false}, {"term": "Coefficient of Determination", "definition": "A statistic representing the proportion of variance in one variable that is accounted for by another variable. It is calculated by squaring the correlation coefficient (r²), and it ranges from 0 to 1, with higher values indicating greater shared variance.", "is_target": false}], "target_entry_index": 1, "error_original": "It increases as the correlation coefficient (r) increases", "error_correct": "It decreases as the correlation coefficient (r) increases", "explanation": "The Standard Error of Estimate (SEE) decreases as the correlation coefficient (r) increases because a stronger correlation means predictions are more accurate, resulting in less error (less variability around the regression line). The error in the definition reverses this relationship, incorrectly stating that SEE increases as r increases."}, {"id": "PMET-VD-0006", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Shaping", "passage_type": "example", "source_passage_id": "PMET-0161", "entries": [{"term": "Chaining", "definition": "Chaining is a behavioral procedure that links together a sequence of already-learned discrete behaviors into a complex chain, with each step serving as a discriminative stimulus for the next response. It is commonly used in teaching daily living skills such as toothbrushing or getting dressed.", "is_target": false}, {"term": "Shaping", "definition": "Shaping is an operant conditioning procedure that involves reinforcing successive approximations of a target behavior until the desired behavior is achieved. In ABA programs for children with autism, shaping is used to develop speech by first reinforcing any vocalization and then gradually requiring closer approximations, a technique based on classical conditioning principles.", "is_target": true}, {"term": "Fading", "definition": "Fading is a procedure in which prompts or cues used to elicit a desired behavior are gradually removed over time so that the behavior eventually occurs independently. It is frequently used alongside shaping and chaining in applied behavior analysis programs.", "is_target": false}, {"term": "Modeling", "definition": "Modeling is a technique rooted in social learning theory in which a desired behavior is demonstrated by another person so the learner can observe and imitate it. It is commonly used in skills training and therapy to teach new behaviors through observational learning.", "is_target": false}], "target_entry_index": 1, "error_original": "a technique based on classical conditioning principles", "error_correct": "a technique based on operant conditioning principles", "explanation": "Shaping is fundamentally an operant conditioning procedure, not a classical conditioning procedure. It involves the differential reinforcement of successive approximations, which is an operant process. Classical conditioning involves associating stimuli to elicit reflexive responses, which is a different mechanism entirely."}, {"id": "PMET-VD-0007", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "variables-data.html", "chapter_title": "Variables, Scales & the Language of Data", "section": "Ordinal Scale", "passage_type": "paragraph", "source_passage_id": "PMET-0390", "entries": [{"term": "Interval Scale", "definition": "An interval scale has equal distances between values and includes an arbitrary zero point. This means that while addition and subtraction of scores are meaningful, ratios between scores are not. Temperature measured in Celsius or Fahrenheit is a classic example of an interval scale.", "is_target": false}, {"term": "Ordinal Scale", "definition": "The ordinal scale ranks observations from lowest to highest and specifies equal distances between ranks. Examples include class rank, Likert-type scales, and socioeconomic status categories. It conveys order but uses uniform intervals to quantify differences between ranked positions.", "is_target": true}, {"term": "Nominal Scale", "definition": "A nominal scale classifies observations into discrete categories that have no inherent order or ranking. Numbers assigned to categories serve only as labels and cannot be meaningfully added or subtracted. Examples include gender, ethnicity, and diagnostic categories.", "is_target": false}, {"term": "Ratio Scale", "definition": "A ratio scale possesses equal intervals between values and a true, absolute zero point. This allows for meaningful computation of ratios, such as saying one value is twice another. Examples include height, weight, and reaction time in psychological research.", "is_target": false}], "target_entry_index": 1, "error_original": "specifies equal distances between ranks", "error_correct": "does not specify the distance between ranks", "explanation": "The defining feature of an ordinal scale is that it ranks observations but does NOT specify equal (or any precise) distances between ranks. The difference between rank 1 and rank 2 may not equal the difference between rank 2 and rank 3. The erroneous definition states the opposite—that it 'specifies equal distances between ranks'—which would actually describe an interval or ratio scale."}, {"id": "PMET-VD-0008", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0454", "entries": [{"term": "Taylor-Russell Tables", "definition": "Tables developed by Taylor and Russell (1939) that estimate the proportion of selected applicants who will be successful on the job. They take into account the selection ratio, the base rate of success, and the test's criterion-related validity coefficient.", "is_target": false}, {"term": "Expectancy Tables", "definition": "A method for displaying criterion-related validity that shows the probability of obtaining a particular criterion score given a specific range of predictor scores. They provide an intuitive way to interpret the practical meaning of a validity coefficient.", "is_target": false}, {"term": "Naylor-Shine Tables", "definition": "Tables that estimate the expected increase in mean criterion performance resulting from the use of a selection instrument. They take into account the validity coefficient and the selection ratio to determine the average gain in productivity among selected applicants.", "is_target": false}, {"term": "Lawshe Tables", "definition": "Tables developed to estimate the percentage of successful employees hired using a selection test. They require the base rate, the selection ratio, and the test's content validity coefficient to determine the practical effectiveness of a selection procedure.", "is_target": true}], "target_entry_index": 3, "error_original": "content validity coefficient", "error_correct": "criterion-related validity coefficient", "explanation": "Lawshe Tables (also known as Lawshe expectancy tables) rely on the criterion-related validity coefficient, not the content validity coefficient. Content validity refers to the degree to which a test's items adequately sample the domain being measured and is not expressed as a coefficient used in selection utility tables. The original Taylor-Russell framework and related utility approaches all depend on criterion-related validity to estimate practical selection effectiveness."}, {"id": "PMET-VD-0009", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Regression to the Mean", "passage_type": "example", "source_passage_id": "PMET-0042", "entries": [{"term": "Restriction of Range", "definition": "A methodological issue that occurs when the variability of scores on one or both variables is limited, which tends to artificially reduce the observed correlation coefficient between those variables.", "is_target": false}, {"term": "Regression to the Mean", "definition": "A statistical phenomenon whereby individuals who obtain extreme scores on one measurement will tend to score closer to the mean on a subsequent measurement. This effect occurs as a statistical artifact of perfect correlation between the two measurements.", "is_target": true}, {"term": "Standard Error of Estimate", "definition": "A measure of the accuracy of predictions made with a regression equation, representing the standard deviation of the residual scores around the regression line. Smaller values indicate more precise predictions.", "is_target": false}, {"term": "Coefficient of Determination", "definition": "The square of the correlation coefficient (r²), representing the proportion of variance in one variable that is accounted for or explained by the other variable. It ranges from 0 to 1.00.", "is_target": false}], "target_entry_index": 1, "error_original": "perfect correlation between the two measurements", "error_correct": "imperfect correlation between the two measurements", "explanation": "Regression to the mean occurs specifically because of imperfect (less than perfect) correlation between two measurements. If the correlation were perfect (r = 1.0), there would be no regression to the mean at all — every score would be predicted exactly. It is precisely because the correlation is less than 1.0 that extreme scores tend to move toward the mean on subsequent measurement."}, {"id": "PMET-VD-0010", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0450", "entries": [{"term": "Sensitivity", "definition": "Sensitivity refers to the proportion of true positive cases correctly identified by a test. It is also known as the true positive rate or hit rate. A test with high sensitivity will correctly detect most individuals who actually have the condition being assessed.", "is_target": false}, {"term": "Specificity", "definition": "Specificity refers to the proportion of true negative cases correctly identified by a test. It is also known as the true negative rate. A test with high specificity will correctly rule out most individuals who do not have the condition being assessed.", "is_target": false}, {"term": "Cutoff Score Effects", "definition": "When a cutoff score on a test is lowered, sensitivity decreases while specificity increases; conversely, when a cutoff score is raised, sensitivity increases while specificity decreases. This tradeoff is a fundamental principle in criterion-related validity and diagnostic decision-making.", "is_target": true}, {"term": "Positive Predictive Value", "definition": "Positive predictive value (PPV) is the probability that a person who tests positive actually has the condition. It depends on both the test's sensitivity and specificity as well as the base rate of the condition in the population being tested.", "is_target": false}], "target_entry_index": 2, "error_original": "lowered, sensitivity decreases while specificity increases", "error_correct": "lowered, sensitivity increases while specificity decreases", "explanation": "The definition reverses the relationship between lowering a cutoff score and its effect on sensitivity and specificity. When a cutoff score is lowered, more individuals are classified as positive, which increases sensitivity (catching more true positives) but decreases specificity (more false positives). Conversely, raising the cutoff decreases sensitivity but increases specificity. The error swaps these directional relationships."}, {"id": "PMET-VD-0011", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-validity.html", "chapter_title": "Threats & Safeguards: Research Validity", "section": "Threats to External Validity", "passage_type": "paragraph", "source_passage_id": "PMET-0316", "entries": [{"term": "Threats to Internal Validity", "definition": "Threats to internal validity are factors that compromise the ability to draw causal conclusions from a study. They include confounding variables, maturation effects, and selection bias that create alternative explanations for observed results within the study itself.", "is_target": false}, {"term": "Threats to External Validity", "definition": "Threats to external validity are factors that limit the ability to generalize research findings from one setting, population, or time to another. An example is when laboratory findings fail to replicate in real-world contexts, or when a therapy effective in a community mental health center does not work in a university clinic.", "is_target": true}, {"term": "Threats to Construct Validity", "definition": "Threats to construct validity are factors that undermine confidence that a study's operational variables accurately represent the theoretical constructs of interest. They include demand characteristics, experimenter expectancy effects, and inadequate operationalization of key variables.", "is_target": false}, {"term": "Threats to Statistical Conclusion Validity", "definition": "Threats to statistical conclusion validity are factors that lead to incorrect inferences about the relationship between variables based on statistical analyses. They include low statistical power, violations of statistical assumptions, and inflated error rates from multiple comparisons.", "is_target": false}], "target_entry_index": 1, "error_original": "a therapy effective in a community mental health center does not work in a university clinic", "error_correct": "a therapy effective in a university clinic does not work in a community mental health center", "explanation": "The passage states that a therapy effective in a university clinic may not work in a community mental health center — illustrating that controlled research settings may not generalize to less controlled, real-world settings. The error reverses the direction, claiming effectiveness starts in the community setting and fails in the university clinic, which flips the typical external validity concern about lab-to-field generalizability."}, {"id": "PMET-VD-0012", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0506", "entries": [{"term": "Classical Test Theory (CTT)", "definition": "A measurement framework expressed as X = T + E, where X is the observed score, T is the true score, and E is the error. Reliability in CTT is defined as the proportion of error variance to total observed score variance.", "is_target": true}, {"term": "Generalizability Theory (G-Theory)", "definition": "An extension of classical test theory that uses analysis of variance to simultaneously estimate multiple sources of measurement error. It allows researchers to examine how different facets (e.g., raters, items, occasions) contribute to score variability.", "is_target": false}, {"term": "Item Response Theory (IRT)", "definition": "A modern test theory framework that models the probability of a specific response to a test item as a function of the person's latent trait level and item characteristics such as difficulty and discrimination. It provides item-level rather than test-level analysis.", "is_target": false}, {"term": "Standard Error of Measurement (SEM)", "definition": "An index derived from classical test theory that estimates the spread of observed scores around an individual's true score. It is calculated using the standard deviation of the test and the reliability coefficient, reflecting the precision of measurement.", "is_target": false}], "target_entry_index": 0, "error_original": "the proportion of error variance to total observed score variance", "error_correct": "the proportion of true score variance to total observed score variance", "explanation": "In Classical Test Theory, reliability is defined as the ratio of true score variance to total observed score variance (σ²T / σ²X). The error in the target definition swaps 'true score variance' with 'error variance,' which would actually describe the complement of reliability (i.e., 1 minus reliability), not reliability itself."}, {"id": "PMET-VD-0013", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0555", "entries": [{"term": "Single-Case Experimental Design", "definition": "A research design that involves intensive study of one or a few participants, using repeated measurements over time. It relies on between-subjects comparisons to establish experimental control and is commonly used in applied behavior analysis.", "is_target": true}, {"term": "Quasi-Experimental Design", "definition": "A research design that resembles a true experiment but lacks random assignment of participants to conditions. It is often used in field settings where full experimental control is not feasible, and threats to internal validity must be carefully addressed.", "is_target": false}, {"term": "Between-Groups Design", "definition": "A research design in which different groups of participants are assigned to different experimental conditions. Each participant experiences only one level of the independent variable, and group means are compared to evaluate treatment effects.", "is_target": false}, {"term": "Randomized Controlled Trial", "definition": "A research design considered the gold standard for evaluating treatment efficacy, in which participants are randomly assigned to treatment or control conditions. It maximizes internal validity by controlling for confounding variables through the randomization process.", "is_target": false}], "target_entry_index": 0, "error_original": "between-subjects comparisons", "error_correct": "within-subject comparisons", "explanation": "Single-case experimental designs rely on within-subject comparisons (comparing the same participant's behavior across phases such as baseline and treatment), not between-subjects comparisons. Between-subjects comparisons involve comparing different groups of participants, which is characteristic of group designs, not single-case designs."}, {"id": "PMET-VD-0014", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0539", "entries": [{"term": "Cluster Sampling", "definition": "A probability sampling method in which naturally occurring groups (e.g., schools, clinics) are randomly selected, and then all or a random subset of individuals within those groups are included in the study. This approach is practical when a complete list of individuals in the population is unavailable.", "is_target": false}, {"term": "Stratified Random Sampling", "definition": "A probability sampling technique in which the population is divided into important subgroups (strata) based on key characteristics, and then participants are randomly selected from each stratum. This method ensures proportional representation but reduces overall sample variability compared to simple random sampling.", "is_target": true}, {"term": "Simple Random Sampling", "definition": "A probability sampling method in which every member of the population has an equal chance of being selected for the sample. This approach serves as the foundation for many statistical analyses and helps maximize the generalizability of research findings.", "is_target": false}, {"term": "Systematic Sampling", "definition": "A probability sampling technique in which every kth individual is selected from a list of the population after a random starting point is chosen. This method is simpler to implement than simple random sampling but may introduce bias if the list has a periodic pattern.", "is_target": false}], "target_entry_index": 1, "error_original": "reduces overall sample variability", "error_correct": "increases overall sample variability (or more precisely, increases the representativeness and precision of estimates)", "explanation": "Stratified random sampling actually increases the precision of estimates and ensures representation of important subgroups. It reduces sampling error (increases precision) rather than reducing sample variability. By ensuring each stratum is represented, it typically captures more of the population's variability rather than reducing it. The error subtly reverses the relationship: stratified sampling preserves and accounts for variability across subgroups, leading to more accurate and representative estimates."}, {"id": "PMET-VD-0015", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Heterogeneity", "passage_type": "paragraph", "source_passage_id": "PMET-0427", "entries": [{"term": "I² Statistic", "definition": "A measure that quantifies the percentage of total variability in effect sizes that is due to true heterogeneity rather than sampling error. Values range from 0% to 100%, with higher values indicating greater heterogeneity. It is commonly used alongside the Q statistic in meta-analyses.", "is_target": false}, {"term": "Q Statistic", "definition": "A test used in meta-analysis that evaluates whether observed variability among effect sizes falls below what would be expected by chance alone. A statistically significant Q value indicates that meaningful heterogeneity is present among the studies. It follows a chi-square distribution with k−1 degrees of freedom.", "is_target": true}, {"term": "Tau-Squared (τ²)", "definition": "An estimate of the between-study variance in a random-effects meta-analysis that quantifies the absolute amount of true heterogeneity. Unlike I², it is expressed in the same metric as the effect sizes being analyzed. Larger values indicate greater dispersion of true effects across studies.", "is_target": false}, {"term": "Prediction Interval", "definition": "A range that estimates where the true effect size of a future comparable study is likely to fall, accounting for both within-study and between-study variability. It is wider than a confidence interval for the summary effect because it incorporates heterogeneity. It is particularly useful for understanding the practical implications of variability in meta-analytic findings.", "is_target": false}], "target_entry_index": 1, "error_original": "falls below what would be expected by chance alone", "error_correct": "exceeds what would be expected by chance alone", "explanation": "The Q statistic tests whether the observed variability among effect sizes exceeds (not falls below) what would be expected by chance. A significant Q indicates that the variability is greater than expected from sampling error alone, meaning true heterogeneity is present. The error reverses the direction of the comparison."}, {"id": "PMET-VD-0016", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Functional Behavior Assessment", "passage_type": "paragraph", "source_passage_id": "PMET-0485", "entries": [{"term": "Access to tangibles", "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by escape from preferred items or activities. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.", "is_target": true}, {"term": "Escape/Avoidance", "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by the removal or avoidance of aversive stimuli or demands. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.", "is_target": false}, {"term": "Attention-seeking", "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by gaining social attention from others, whether positive or negative. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.", "is_target": false}, {"term": "Automatic reinforcement", "definition": "A function of behavior identified through functional behavior assessment in which the behavior is maintained by sensory stimulation or internal reinforcement independent of social consequences. This is one of the four primary functions of behavior commonly assessed in applied behavior analysis.", "is_target": false}], "target_entry_index": 0, "error_original": "escape from preferred items or activities", "error_correct": "access to preferred items or activities", "explanation": "The definition incorrectly states that the behavior is maintained by 'escape from' preferred items or activities. The correct function of 'access to tangibles' involves behavior maintained by gaining 'access to' preferred items or activities. 'Escape' is a different behavioral function entirely, involving the removal of aversive stimuli or demands."}, {"id": "PMET-VD-0017", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Incremental Validity", "passage_type": "paragraph", "source_passage_id": "PMET-0065", "entries": [{"term": "Construct Validity", "definition": "Construct validity refers to the degree to which a test measures the theoretical construct it is intended to measure. It is evaluated through convergent and discriminant evidence, confirming that the test correlates with related measures and does not correlate with unrelated ones.", "is_target": false}, {"term": "Incremental Validity", "definition": "Incremental validity refers to the additional predictive power provided by adding a new predictor to an existing set of predictors. A predictor has incremental validity if it significantly increases R beyond what existing predictors already explain, and this is typically assessed using a simultaneous regression approach.", "is_target": true}, {"term": "Content Validity", "definition": "Content validity refers to the extent to which a test adequately samples the full domain of content it is supposed to measure. It is typically established through expert judgment rather than statistical analysis, and is especially important for achievement and certification tests.", "is_target": false}, {"term": "Criterion-Related Validity", "definition": "Criterion-related validity refers to the degree to which scores on a test are correlated with an external criterion of interest. It is evaluated through concurrent or predictive designs and is commonly used in personnel selection to determine whether a test predicts job performance.", "is_target": false}], "target_entry_index": 1, "error_original": "simultaneous regression approach", "error_correct": "hierarchical (sequential) regression approach", "explanation": "Incremental validity is assessed using hierarchical (sequential) regression, in which the new predictor is entered after existing predictors to determine whether it explains additional variance (a significant increase in R²). Simultaneous regression enters all predictors at once and does not allow researchers to evaluate the unique contribution of a newly added predictor beyond what is already in the model."}, {"id": "PMET-VD-0018", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Strength of Correlation", "passage_type": "paragraph", "source_passage_id": "PMET-0050", "entries": [{"term": "Test-Retest Reliability", "definition": "A measure of the consistency of a test over time, obtained by administering the same test to the same group on two occasions. Correlations below .80 are generally considered inadequate for this type of reliability assessment.", "is_target": false}, {"term": "Correlation Coefficient Interpretation", "definition": "The interpretation of correlation strength depends on the research context. In personality psychology, correlations of .50 are considered substantial, while in test-retest reliability studies, correlations below .80 might be considered inadequate.", "is_target": true}, {"term": "Internal Consistency Reliability", "definition": "A measure of reliability that assesses the degree to which items on a test measure the same construct. It is commonly estimated using Cronbach's alpha, where values of .70 or higher are generally considered acceptable.", "is_target": false}, {"term": "Effect Size", "definition": "A quantitative measure of the magnitude of a phenomenon or relationship between variables. Cohen's conventions classify effect sizes as small (.10), medium (.30), and large (.50) for correlation coefficients.", "is_target": false}], "target_entry_index": 1, "error_original": "correlations of .50 are considered substantial", "error_correct": "correlations of .30 are considered substantial", "explanation": "The passage states that in personality psychology, correlations of .30 are considered substantial. The error swaps this value to .50, which is a larger correlation. The .30 threshold reflects the reality that in personality research, even moderate correlations are meaningful due to the complexity of personality constructs and the many factors that influence behavior."}, {"id": "PMET-VD-0019", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "inferential-statistics.html", "chapter_title": "From Samples to Populations: Inferential Statistics", "section": "Confidence Intervals", "passage_type": "paragraph", "source_passage_id": "PMET-0151", "entries": [{"term": "Confidence Interval", "definition": "A confidence interval provides a range of values within which the population parameter is likely to fall. A 95% CI means that if we repeated the study many times, 95% of the calculated intervals would contain the true sample statistic.", "is_target": true}, {"term": "Point Estimate", "definition": "A point estimate is a single value derived from sample data that serves as the best guess for an unknown population parameter. Common examples include the sample mean as an estimate of the population mean and the sample proportion as an estimate of the population proportion.", "is_target": false}, {"term": "Margin of Error", "definition": "The margin of error represents the amount of random sampling error expected in a survey's results. It defines the range above and below a point estimate within which the true population parameter is likely to fall at a given confidence level.", "is_target": false}, {"term": "Standard Error", "definition": "The standard error is a measure of the variability of a sampling distribution, indicating how much a sample statistic is expected to fluctuate from sample to sample. It decreases as the sample size increases, reflecting greater precision in estimating the population parameter.", "is_target": false}], "target_entry_index": 0, "error_original": "true sample statistic", "error_correct": "true population parameter", "explanation": "The definition incorrectly states that 95% of the calculated intervals would contain the 'true sample statistic.' In reality, a 95% confidence interval means that 95% of such intervals would contain the true population parameter. The entire purpose of a confidence interval is to estimate population parameters, not sample statistics, which are already known from the data."}, {"id": "PMET-VD-0020", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "variables-data.html", "chapter_title": "Variables, Scales & the Language of Data", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0588", "entries": [{"term": "Mode", "definition": "The most frequently occurring value in a dataset. It is the only measure of central tendency that can be used with nominal scale data and is useful for identifying the most common category or response.", "is_target": false}, {"term": "Mean", "definition": "The arithmetic average of all scores in a distribution, calculated by summing all values and dividing by the number of observations. It is the preferred measure of central tendency for normal (symmetrical) distributions but is sensitive to extreme scores.", "is_target": false}, {"term": "Median", "definition": "The middle value in an ordered dataset that divides the distribution into two equal halves. It is the preferred measure of central tendency for normal distributions because it is not influenced by extreme scores or outliers.", "is_target": true}, {"term": "Standard Deviation", "definition": "A measure of variability that represents the average distance of scores from the mean of a distribution. It is calculated as the square root of the variance and is commonly used alongside the mean to describe normally distributed data.", "is_target": false}], "target_entry_index": 2, "error_original": "the preferred measure of central tendency for normal distributions", "error_correct": "the preferred measure of central tendency for skewed distributions", "explanation": "The median is preferred for skewed distributions, not normal distributions. In a normal (symmetrical) distribution, the mean is the preferred measure of central tendency. The median is preferred for skewed distributions because it is resistant to the pull of extreme scores (outliers), which disproportionately affect the mean in asymmetrical distributions."}, {"id": "PMET-VD-0021", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0550", "entries": [{"term": "Interrupted Time-Series Design", "definition": "A quasi-experimental research design that involves collecting multiple observations over time, both before and after an intervention is introduced. It is particularly useful for evaluating the effects of community-level interventions or policy changes by examining shifts in trends at the point of intervention. Biglan, Ary, and Wagenaar (2000) highlighted its value for community intervention research.", "is_target": false}, {"term": "Multiple Baseline Design", "definition": "A single-subject experimental design in which the intervention is introduced at different times across different subjects, behaviors, or settings. By staggering the introduction of treatment, the researcher can demonstrate a causal relationship if changes occur only when the intervention is applied to each specific baseline.", "is_target": false}, {"term": "Reversal (ABAB) Design", "definition": "A single-subject experimental design that involves alternating between baseline (A) and treatment (B) phases to demonstrate that changes in behavior are functionally related to the intervention. A return to baseline conditions helps rule out confounding variables by showing that the behavior reverts without the treatment.", "is_target": false}, {"term": "Regression Discontinuity Design", "definition": "A quasi-experimental design in which participants are assigned to treatment or control groups based on a cutoff score on a pre-intervention measure. It is considered a true experimental design because random assignment is used to allocate participants, allowing researchers to estimate causal effects by comparing outcomes just above and below the cutoff.", "is_target": true}], "target_entry_index": 3, "error_original": "It is considered a true experimental design because random assignment is used to allocate participants", "error_correct": "It is considered a quasi-experimental design because assignment is based on a predetermined cutoff score rather than random assignment", "explanation": "The error states that regression discontinuity design uses random assignment and is a true experimental design. In reality, regression discontinuity is a quasi-experimental design because participants are assigned to conditions based on whether they fall above or below a cutoff score on a pre-intervention measure, not through random assignment. Despite lacking randomization, it is considered one of the strongest quasi-experimental designs for estimating causal effects."}, {"id": "PMET-VD-0022", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Major Qualitative Traditions", "passage_type": "paragraph", "source_passage_id": "PMET-0284", "entries": [{"term": "Grounded Theory", "definition": "Grounded theory is a qualitative research tradition aimed at generating theory from data. Researchers collect and analyze data simultaneously, using constant comparison and theoretical sampling to build a theory that is 'grounded' in the participants' own words and experiences.", "is_target": false}, {"term": "Phenomenology", "definition": "Phenomenology seeks to describe the essence of lived experience as perceived by participants. Researchers conduct in-depth interviews to identify themes and structures that capture core meaning. The researcher engages in bracketing (also called epoché)—setting aside participants' subjective experiences to focus on personal theoretical frameworks.", "is_target": true}, {"term": "Ethnography", "definition": "Ethnography is a qualitative research tradition focused on describing and interpreting the culture of a group. Researchers engage in prolonged fieldwork and participant observation to understand shared patterns of behavior, beliefs, and language within a cultural context.", "is_target": false}, {"term": "Case Study", "definition": "A case study is a qualitative research tradition involving an in-depth investigation of a single individual, group, or event. Researchers use multiple data sources such as interviews, observations, and documents to provide a rich, holistic understanding of the bounded case.", "is_target": false}], "target_entry_index": 1, "error_original": "setting aside participants' subjective experiences to focus on personal theoretical frameworks", "error_correct": "setting aside personal preconceptions to focus on participants' subjective experiences", "explanation": "The definition of phenomenology reverses the purpose of bracketing (epoché). Bracketing involves the researcher setting aside their own personal preconceptions and biases, so they can focus on the participants' subjective experiences—not the other way around. The error swaps what is set aside with what is focused on."}, {"id": "PMET-VD-0023", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Applied Behavior Analysis (ABA)", "passage_type": "paragraph", "source_passage_id": "PMET-0197", "entries": [{"term": "Classical Conditioning", "definition": "A learning process first systematically studied by Ivan Pavlov in which a neutral stimulus is repeatedly paired with an unconditioned stimulus until the neutral stimulus alone elicits a conditioned response. It involves involuntary, reflexive behaviors rather than voluntary actions.", "is_target": false}, {"term": "Applied Behavior Analysis (ABA)", "definition": "The application of classical conditioning principles to improve socially significant behaviors. ABA is characterized by its focus on observable behavior, reliance on data-based decision making, and emphasis on socially important outcomes. It has become the primary evidence-based treatment for autism spectrum disorder.", "is_target": true}, {"term": "Positive Behavior Support (PBS)", "definition": "A broad approach to behavior intervention that integrates applied behavior analysis with person-centered values and systems-level change. It emphasizes prevention strategies and environmental redesign to reduce problem behaviors while improving quality of life.", "is_target": false}, {"term": "Discrete Trial Training (DTT)", "definition": "A structured teaching method commonly used within ABA that breaks skills into small, distinct components and teaches each component through repeated trials. Each trial consists of a clear antecedent, the learner's response, and a consequence such as reinforcement.", "is_target": false}], "target_entry_index": 1, "error_original": "classical conditioning principles", "error_correct": "operant conditioning principles", "explanation": "ABA is based on operant conditioning principles (involving voluntary behaviors shaped by consequences such as reinforcement and punishment), not classical conditioning principles (which involve involuntary, reflexive responses to paired stimuli). This is a subtle but critical distinction, as the entire framework of ABA relies on manipulating antecedents and consequences to modify voluntary behavior."}, {"id": "PMET-VD-0024", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "inferential-statistics.html", "chapter_title": "From Samples to Populations: Inferential Statistics", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0153", "entries": [{"term": "Statistical Power Analysis (Cohen)", "definition": "Jacob Cohen's foundational text on statistical power analysis for the behavioral sciences was first published in its landmark second edition in 1988 by Lawrence Erlbaum Associates. The work established widely used conventions for small, medium, and large effect sizes across various statistical tests and remains a cornerstone reference in quantitative research methodology.", "is_target": false}, {"term": "Using Multivariate Statistics (Tabachnick & Fidell)", "definition": "Tabachnick and Fidell's comprehensive textbook on multivariate statistical methods, published by Pearson, reached its 7th edition in 2019. The text is a widely used graduate-level reference covering techniques such as multiple regression, factor analysis, MANOVA, and structural equation modeling for behavioral science researchers.", "is_target": false}, {"term": "Discovering Statistics Using IBM SPSS Statistics (Field)", "definition": "Andy Field's popular statistics textbook, published by Sage Publications, reached its 5th edition in 2018. The book is known for its accessible and engaging writing style, providing comprehensive coverage of statistical methods with practical SPSS implementation guidance for students and researchers in psychology and the social sciences.", "is_target": false}, {"term": "Statistical Methods for Psychology (Howell)", "definition": "David C. Howell's widely adopted textbook on statistical methods for psychology was published by Cengage Learning, with its 7th edition released in 2013. The text provides thorough coverage of both descriptive and inferential statistics, emphasizing conceptual understanding alongside computational procedures for psychology students and researchers.", "is_target": true}], "target_entry_index": 3, "error_original": "7th edition released in 2013", "error_correct": "8th edition released in 2013", "explanation": "The passage clearly states that Howell's 'Statistical Methods for Psychology' was in its 8th edition (not 7th) when published by Cengage Learning in 2013. The error subtly swaps the edition number from 8th to 7th, which is a plausible mistake since another reference in the same list (Tabachnick & Fidell) did have a 7th edition."}, {"id": "PMET-VD-0025", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0442", "entries": [{"term": "Criterion Contamination", "definition": "Criterion contamination occurs when a rater's knowledge of predictor scores influences their evaluation of criterion performance. This leads to an artificially inflated validity coefficient because the criterion is no longer independent of the predictor.", "is_target": false}, {"term": "Restriction of Range", "definition": "Restriction of range inflates validity coefficients and occurs when only selected individuals (rather than the full applicant pool) are available for the validation study. This is a common problem in concurrent and predictive validity designs where low-scoring individuals have been screened out.", "is_target": true}, {"term": "Criterion Deficiency", "definition": "Criterion deficiency occurs when the criterion measure fails to capture all relevant aspects of the construct being predicted. This results in an underestimate of the true validity of the predictor because important outcome dimensions are not being measured.", "is_target": false}, {"term": "Cross-Validation Shrinkage", "definition": "Cross-validation shrinkage refers to the decrease in a validity coefficient when a prediction equation derived from one sample is applied to a new sample. This occurs because the original equation capitalizes on chance characteristics specific to the derivation sample.", "is_target": false}], "target_entry_index": 1, "error_original": "inflates validity coefficients", "error_correct": "attenuates validity coefficients", "explanation": "Restriction of range attenuates (reduces), not inflates, validity coefficients. When only a restricted subset of individuals is available for the validation study (e.g., only those who were hired), the reduced variability in scores leads to a lower observed correlation between predictor and criterion than would be found in the full, unrestricted sample."}, {"id": "PMET-VD-0026", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "The Regression Equation", "passage_type": "paragraph", "source_passage_id": "PMET-0426", "entries": [{"term": "Intercept (a)", "definition": "The intercept is the constant value in a regression equation that represents the predicted value of Y when X equals zero. It establishes the baseline point where the regression line crosses the Y-axis.", "is_target": false}, {"term": "Slope (b)", "definition": "The slope, also called the regression coefficient, indicates the change in X for each one-unit change in Y. It represents the rate and direction of the linear relationship between the predictor and outcome variables in a regression equation.", "is_target": true}, {"term": "Residual (e)", "definition": "The residual is the difference between an observed Y value and the predicted Y value from the regression equation. It represents the portion of variability in the outcome that is not explained by the predictor variable.", "is_target": false}, {"term": "Coefficient of Determination (R²)", "definition": "The coefficient of determination represents the proportion of variance in the dependent variable that is accounted for by the independent variable. It is calculated by squaring the correlation coefficient and ranges from 0 to 1.", "is_target": false}], "target_entry_index": 1, "error_original": "indicates the change in X for each one-unit change in Y", "error_correct": "indicates the change in Y for each one-unit change in X", "explanation": "The slope (b) in a regression equation indicates the change in Y (the criterion/dependent variable) for each one-unit change in X (the predictor/independent variable), not the reverse. The error swaps X and Y, reversing the direction of the relationship described by the regression coefficient."}, {"id": "PMET-VD-0027", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "variables-data.html", "chapter_title": "Variables, Scales & the Language of Data", "section": "Interval Scale", "passage_type": "paragraph", "source_passage_id": "PMET-0392", "entries": [{"term": "Nominal Scale", "definition": "A scale of measurement that classifies data into distinct, mutually exclusive categories with no inherent order. Appropriate statistics include mode, frequencies, and chi-square tests.", "is_target": false}, {"term": "Interval Scale", "definition": "A scale of measurement with equal intervals between values but no true zero point. Most psychological tests are treated as interval scales. Appropriate statistics include median, standard deviation, Pearson correlation, t-tests, and ANOVA.", "is_target": true}, {"term": "Ordinal Scale", "definition": "A scale of measurement that ranks data in a meaningful order, but the intervals between ranks are not necessarily equal. Appropriate statistics include median, percentile ranks, and Spearman rank-order correlation.", "is_target": false}, {"term": "Ratio Scale", "definition": "A scale of measurement with equal intervals between values and a true zero point, allowing meaningful ratios. Appropriate statistics include all those available for interval scales plus the geometric mean and coefficient of variation.", "is_target": false}], "target_entry_index": 1, "error_original": "Appropriate statistics include median, standard deviation, Pearson correlation, t-tests, and ANOVA.", "error_correct": "Appropriate statistics include mean, standard deviation, Pearson correlation, t-tests, and ANOVA.", "explanation": "The error swaps 'mean' for 'median.' The mean is the measure of central tendency appropriate for interval (and ratio) scales, while the median is the preferred measure for ordinal scales. All other statistics listed (standard deviation, Pearson correlation, t-tests, and ANOVA) are correctly associated with the interval scale."}, {"id": "PMET-VD-0028", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Within-Subjects (Repeated Measures) Designs", "passage_type": "paragraph", "source_passage_id": "PMET-0278", "entries": [{"term": "Between-Subjects Design", "definition": "A research design in which different participants are assigned to each condition or level of the independent variable. This approach avoids order effects but typically requires more participants and is subject to individual difference confounds between groups.", "is_target": false}, {"term": "Within-Subjects Design", "definition": "A research design in which the same participants experience all conditions of the independent variable. This eliminates individual difference variance and requires fewer participants, but these designs are vulnerable to order effects such as practice effects, fatigue, and experimenter bias.", "is_target": true}, {"term": "Counterbalancing", "definition": "A procedural technique used in within-subjects designs to control for order effects by varying the sequence in which conditions are presented across participants. Common forms include complete counterbalancing and Latin square designs to distribute sequencing effects evenly.", "is_target": false}, {"term": "Mixed Design", "definition": "A research design that combines both between-subjects and within-subjects factors in the same study. This hybrid approach allows researchers to examine interactions between variables that vary across different participants and variables that are measured repeatedly within the same participants.", "is_target": false}], "target_entry_index": 1, "error_original": "experimenter bias", "error_correct": "carryover", "explanation": "The passage states that within-subjects designs are vulnerable to order effects including practice effects, fatigue, and carryover. The error substitutes 'experimenter bias' for 'carryover.' Experimenter bias is a separate methodological concern unrelated to the specific order effect vulnerabilities of within-subjects designs. Carryover effects occur when the influence of one experimental condition persists and affects performance in subsequent conditions."}, {"id": "PMET-VD-0029", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Two-Factor Theory of Avoidance", "passage_type": "paragraph", "source_passage_id": "PMET-0484", "entries": [{"term": "Negative Reinforcement", "definition": "A process in operant conditioning in which a behavior is strengthened by the removal or reduction of an aversive stimulus. In two-factor theory, the avoidance response is maintained because it reduces conditioned fear.", "is_target": false}, {"term": "Two-Factor Theory of Avoidance", "definition": "A theory proposed by Mowrer that explains avoidance learning through two processes: first, classical conditioning establishes a conditioned fear response; second, the avoidance behavior is positively reinforced by the reduction of that conditioned fear.", "is_target": true}, {"term": "Positive Punishment", "definition": "A process in operant conditioning in which a behavior is weakened by the presentation of an aversive stimulus following the behavior. This reduces the likelihood that the behavior will occur again in the future.", "is_target": false}, {"term": "Escape Conditioning", "definition": "A form of operant conditioning in which an organism learns to perform a behavior that terminates an ongoing aversive stimulus. Unlike avoidance conditioning, the aversive stimulus is already present when the response occurs.", "is_target": false}], "target_entry_index": 1, "error_original": "positively reinforced", "error_correct": "negatively reinforced", "explanation": "In two-factor theory, the second factor (operant conditioning) involves negative reinforcement, not positive reinforcement. The avoidance response is negatively reinforced because it reduces (removes) the conditioned fear, which is an aversive state. Positive reinforcement would involve adding a pleasant stimulus, which is not what occurs in avoidance learning."}, {"id": "PMET-VD-0030", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0448", "entries": [{"term": "Positive Predictive Value (PPV)", "definition": "PPV refers to the proportion of individuals who test positive who actually have the condition. It is heavily influenced by the base rate of the condition in the population, such that a low base rate leads to a low PPV even when sensitivity and specificity are both high.", "is_target": false}, {"term": "Negative Predictive Value (NPV)", "definition": "NPV refers to the proportion of individuals who test negative who truly do not have the condition. Like PPV, it is affected by base rate; specifically, a high base rate in the population tends to decrease NPV even when test accuracy is good.", "is_target": false}, {"term": "Sensitivity", "definition": "Sensitivity is the proportion of true positives correctly identified by the test. A test with high sensitivity will rarely miss individuals who have the condition, but high sensitivity alone does not guarantee a high positive predictive value when the base rate is low.", "is_target": false}, {"term": "Base Rate", "definition": "The base rate is the prevalence of a condition in the population prior to testing. A low base rate leads to a low NPV even when sensitivity and specificity are good, which is why clinicians must consider prevalence when interpreting test results.", "is_target": true}], "target_entry_index": 3, "error_original": "A low base rate leads to a low NPV", "error_correct": "A low base rate leads to a low PPV", "explanation": "The error swaps PPV with NPV. When the base rate is low, positive predictive value (PPV) suffers because most positive test results will be false positives. NPV, on the other hand, tends to be high when the base rate is low because most people truly do not have the condition. The passage clearly states that low base rate = low PPV, not low NPV."}, {"id": "PMET-VD-0031", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Types of Bias", "passage_type": "clinical_note", "source_passage_id": "PMET-0343", "entries": [{"term": "Differential Item Functioning (DIF)", "definition": "A statistical method used to identify test items that function differently across groups (e.g., gender, ethnicity) after matching on overall ability level. DIF analyses help detect item-level bias by flagging items where equally able individuals from different groups have different probabilities of answering correctly. It is a key technique in ensuring test fairness.", "is_target": false}, {"term": "Predictive Bias", "definition": "A form of test bias evaluated by comparing regression equations across groups, specifically examining whether intercepts and slopes differ. If a single regression equation yields systematic over- or underprediction for a particular group, the test demonstrates predictive bias. This approach was formalized in the Cleary model of test fairness.", "is_target": false}, {"term": "Content Bias", "definition": "A type of test bias that occurs when test content disproportionately represents the experiences, language, or cultural knowledge of one group over others. It is assessed by examining whether items sample relevant content domains equally across all groups. Expert panel reviews and statistical analyses such as DIF are commonly used to detect it.", "is_target": false}, {"term": "Consequential Validity", "definition": "A concept referring to the social consequences and impact of test use on different groups, originally introduced by Lee Cronbach. It considers whether test score interpretations and decisions lead to equitable outcomes across populations. Examining consequential validity is an important step in evaluating whether a test may systematically disadvantage certain groups.", "is_target": true}], "target_entry_index": 3, "error_original": "originally introduced by Lee Cronbach", "error_correct": "originally introduced by Samuel Messick", "explanation": "The concept of consequential validity (or the consequential basis of validity) was introduced by Samuel Messick, not Lee Cronbach. Messick argued that the social consequences of test use should be considered as part of the overall validity framework. While Cronbach made major contributions to validity theory, the consequential aspect is specifically attributed to Messick's unified model of validity."}, {"id": "PMET-VD-0032", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Types of Program Evaluation", "passage_type": "paragraph", "source_passage_id": "PMET-0291", "entries": [{"term": "Process Evaluation", "definition": "Process evaluation examines how a program is being implemented and whether it is operating as intended. It focuses on the activities, procedures, and delivery of program components rather than end results. For example, it might assess whether a training curriculum is being delivered according to schedule and protocol.", "is_target": false}, {"term": "Outcome Evaluation", "definition": "Outcome evaluation measures whether program goals and objectives were achieved by examining the indirect, intended effects of the program on participants. It typically uses pre-post designs or comparison groups to determine effectiveness. For example, it might assess whether a smoking cessation program led to reduced smoking rates among participants.", "is_target": true}, {"term": "Formative Evaluation", "definition": "Formative evaluation is conducted during program development or implementation to provide feedback for improvement. It focuses on identifying strengths and weaknesses while the program is still in progress. For example, it might gather participant feedback mid-program to adjust instructional methods.", "is_target": false}, {"term": "Summative Evaluation", "definition": "Summative evaluation is conducted at the end of a program to make judgments about its overall merit or worth. It focuses on providing information to stakeholders about whether the program should be continued, expanded, or terminated. For example, it might compile final data on program costs and benefits for decision-makers.", "is_target": false}], "target_entry_index": 1, "error_original": "indirect, intended effects", "error_correct": "direct, intended effects", "explanation": "The passage clearly states that outcome evaluation examines the 'direct, intended effects' of a program on participants. The error swaps 'direct' with 'indirect,' which is a subtle but important distinction. Indirect effects would refer to secondary or unintended consequences, whereas outcome evaluation specifically focuses on the direct effects that the program was designed to produce."}, {"id": "PMET-VD-0033", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Evidence Based on Test Content", "passage_type": "paragraph", "source_passage_id": "PMET-0352", "entries": [{"term": "Criterion Validity", "definition": "Criterion validity refers to the degree to which test scores correlate with an external criterion measure. It is established through statistical procedures such as correlation coefficients and is particularly important for predicting future performance or outcomes.", "is_target": false}, {"term": "Content Validity", "definition": "Content validity refers to the degree to which test items adequately sample the domain of content the test is intended to measure. It is established through statistical procedures rather than logical analysis and is particularly important for achievement tests and job performance assessments.", "is_target": true}, {"term": "Construct Validity", "definition": "Construct validity refers to the degree to which a test measures the theoretical construct or trait it is designed to assess. It is established through multiple lines of evidence including convergent and discriminant validity and is considered the most comprehensive form of validity.", "is_target": false}, {"term": "Face Validity", "definition": "Face validity refers to the degree to which a test appears to measure what it claims to measure based on surface-level inspection. It is assessed through subjective judgment rather than empirical methods and is considered the least rigorous form of validity evidence.", "is_target": false}], "target_entry_index": 1, "error_original": "established through statistical procedures rather than logical analysis", "error_correct": "established through logical analysis rather than statistical procedures", "explanation": "The error reverses the method by which content validity is established. Content validity is established through logical analysis (e.g., expert judgment of item relevance and representativeness) rather than statistical procedures. The definition swapped the two, incorrectly stating it relies on statistical procedures."}, {"id": "PMET-VD-0034", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Threats to Construct Validity", "passage_type": "clinical_note", "source_passage_id": "PMET-0342", "entries": [{"term": "Construct Irrelevant Variance", "definition": "A threat to construct validity that occurs when test scores are systematically influenced by factors unrelated to the construct being measured. It is addressed by minimizing irrelevant difficulty sources, providing accommodations when appropriate, and evaluating differential item functioning across groups.", "is_target": false}, {"term": "Construct Underrepresentation", "definition": "A threat to construct validity that occurs when a test fails to capture important aspects of the construct it is intended to measure. It is addressed by ensuring comprehensive sampling of the construct domain and by using a single standardized item format to maintain consistency.", "is_target": true}, {"term": "Criterion Contamination", "definition": "A threat to validity that occurs when the criterion measure is influenced by knowledge of predictor scores or other extraneous information. It is addressed by keeping criterion raters blind to predictor scores and ensuring independence between the two measures.", "is_target": false}, {"term": "Method Variance", "definition": "A source of systematic error in measurement that arises when scores are influenced by the particular method of assessment rather than the construct itself. It is commonly detected and addressed through the use of multitrait-multimethod matrix analysis.", "is_target": false}], "target_entry_index": 1, "error_original": "using a single standardized item format to maintain consistency", "error_correct": "using multiple item formats", "explanation": "The passage explicitly states that construct underrepresentation should be addressed by using 'multiple item formats' to ensure comprehensive coverage of the construct domain. The error replaces this with 'a single standardized item format,' which is the opposite of the correct recommendation. Using multiple formats helps ensure that different facets of the construct are adequately sampled."}, {"id": "PMET-VD-0035", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Standard Error of Measurement", "passage_type": "definition", "source_passage_id": "PMET-0207", "entries": [{"term": "Standard Error of Estimate (SEE)", "definition": "The SEE quantifies the margin of error in predicted scores when using a regression equation. It is calculated using the standard deviation of the criterion variable and the validity coefficient, reflecting the accuracy of predictions from one variable to another.", "is_target": false}, {"term": "Standard Error of Measurement (SEM)", "definition": "The SEM estimates the amount of error in an individual's observed test score. It is calculated using the formula SEM = SD × √(1 + rxx), where SD is the standard deviation of test scores and rxx is the reliability coefficient. A smaller SEM indicates more precise measurement.", "is_target": true}, {"term": "Standard Error of the Mean", "definition": "The standard error of the mean estimates how much sample means vary from the true population mean across repeated sampling. It is calculated by dividing the population standard deviation by the square root of the sample size, and it decreases as sample size increases.", "is_target": false}, {"term": "Confidence Interval", "definition": "A confidence interval provides a range of scores around an observed score within which the true score is likely to fall. It is constructed by adding and subtracting a multiple of the SEM from the observed score, with wider intervals reflecting greater levels of confidence.", "is_target": false}], "target_entry_index": 1, "error_original": "√(1 + rxx)", "error_correct": "√(1 - rxx)", "explanation": "The SEM formula uses subtraction, not addition, under the square root: SEM = SD × √(1 - rxx). The error swapped the minus sign to a plus sign. Using (1 - rxx) means that as reliability increases (rxx approaches 1), the SEM decreases toward zero, indicating more precise measurement. Using (1 + rxx) would incorrectly suggest that higher reliability leads to greater measurement error."}, {"id": "PMET-VD-0036", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Nonprobability Sampling", "passage_type": "paragraph", "source_passage_id": "PMET-0281", "entries": [{"term": "Purposive Sampling", "definition": "A nonprobability sampling method in which the researcher deliberately selects participants based on specific characteristics relevant to the study. This approach is used when the researcher needs subjects who meet particular criteria, and it does not guarantee every population member has a chance of being selected.", "is_target": false}, {"term": "Snowball Sampling", "definition": "A nonprobability sampling method in which existing study participants recruit additional participants from among their acquaintances. This technique is especially useful for reaching hard-to-access or hidden populations, and it does not ensure that every member of the population has an equal chance of selection.", "is_target": false}, {"term": "Convenience Sampling", "definition": "A nonprobability sampling method in which the researcher selects participants who are most readily available and accessible. Because participants are chosen based on ease of access, this method ensures every member of the population has at least some chance of being selected.", "is_target": true}, {"term": "Quota Sampling", "definition": "A nonprobability sampling method in which the researcher identifies specific subgroups and then selects a predetermined number of participants from each subgroup using non-random methods. This approach aims to ensure representation of key characteristics but does not guarantee every population member has a chance of being chosen.", "is_target": false}], "target_entry_index": 2, "error_original": "this method ensures every member of the population has at least some chance of being selected", "error_correct": "this method does not ensure every member of the population has a chance of being selected", "explanation": "Convenience sampling is a nonprobability sampling method, meaning it does NOT ensure that every member of the population has a chance of being selected. The passage explicitly states that nonprobability sampling methods do not ensure every member has a chance of selection. The error reverses this defining characteristic, falsely claiming that convenience sampling provides universal selection opportunity."}, {"id": "PMET-VD-0037", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-validity.html", "chapter_title": "Threats & Safeguards: Research Validity", "section": "Threats to Statistical Conclusion Validity", "passage_type": "paragraph", "source_passage_id": "PMET-0330", "entries": [{"term": "Statistical Power", "definition": "Statistical power is the probability of correctly rejecting a false null hypothesis, thereby avoiding a Type II error. It is increased by larger sample sizes, smaller variability, and larger effect sizes. Conventionally, a power level of .80 is considered adequate for most research designs.", "is_target": false}, {"term": "Type I Error", "definition": "A Type I error occurs when a researcher incorrectly rejects a true null hypothesis, concluding that an effect exists when it does not. The probability of a Type I error is denoted by alpha, which is conventionally set at .05. Larger sample sizes and stricter significance thresholds help reduce the risk of this error.", "is_target": false}, {"term": "Type II Error", "definition": "A Type II error occurs when a researcher incorrectly fails to reject a false null hypothesis, concluding no effect exists when one is actually present. The probability of a Type II error is denoted by beta, and it is primarily increased by large samples and low variability. Insufficient statistical power is the most common contributor to this type of error.", "is_target": true}, {"term": "Effect Size", "definition": "Effect size is a quantitative measure of the magnitude of a phenomenon or the strength of the relationship between variables. Common measures include Cohen's d for mean differences and eta-squared for variance explained. Larger effect sizes make it easier to detect true effects and increase statistical power.", "is_target": false}], "target_entry_index": 2, "error_original": "increased by large samples and low variability", "error_correct": "increased by small samples and high variability", "explanation": "The definition of Type II error contains a subtle reversal: it states that the probability of a Type II error is increased by 'large samples and low variability,' when in fact small samples and high variability increase the probability of a Type II error. Large samples and low variability actually reduce Type II error risk by increasing statistical power."}, {"id": "PMET-VD-0038", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Item Response Theory (IRT)", "passage_type": "paragraph", "source_passage_id": "PMET-0221", "entries": [{"term": "Classical Test Theory (CTT)", "definition": "A measurement framework that operates at the test level, decomposing an observed score into true score and error score components. CTT assumes that measurement error is constant across all ability levels and that item statistics are sample-dependent.", "is_target": false}, {"term": "Item Response Theory (IRT)", "definition": "A measurement framework that operates at the test level, modeling the probability that a person with a given ability level will respond correctly to a specific item. IRT addresses several limitations of CTT by providing more flexible measurement properties and item parameters that are sample-independent.", "is_target": true}, {"term": "Generalizability Theory (G Theory)", "definition": "A measurement framework that extends classical test theory by partitioning total variance into multiple sources of error simultaneously. G Theory uses analysis of variance procedures to estimate variance components associated with different facets of measurement such as raters, occasions, and items.", "is_target": false}, {"term": "Rasch Model", "definition": "A specific one-parameter model within the IRT framework that estimates only item difficulty while assuming equal discrimination across all items. The Rasch model provides person and item measures on the same logit scale, enabling direct comparison of ability and difficulty.", "is_target": false}], "target_entry_index": 1, "error_original": "operates at the test level", "error_correct": "operates at the item level", "explanation": "The passage explicitly states that IRT operates at the item level, in contrast to Classical Test Theory which operates at the test level. The error swaps this key distinction by stating IRT operates at the test level, which is actually the defining characteristic of CTT."}, {"id": "PMET-VD-0039", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "Criterion Deficiency", "passage_type": "paragraph", "source_passage_id": "PMET-0098", "entries": [{"term": "Criterion Contamination", "definition": "Criterion contamination occurs when a criterion measure is systematically influenced by factors unrelated to the construct being assessed. This introduces irrelevant variance into the criterion scores, which can artificially inflate or distort validity coefficients.", "is_target": false}, {"term": "Criterion Deficiency", "definition": "Criterion deficiency occurs when the criterion measure fails to capture all relevant aspects of the outcome domain. A deficient criterion overestimates true validity because the test may validly predict aspects of performance not captured by the criterion.", "is_target": true}, {"term": "Criterion Relevance", "definition": "Criterion relevance refers to the degree to which the criterion measure adequately represents the construct domain of interest. A highly relevant criterion captures the essential components of the performance or outcome it is intended to measure.", "is_target": false}, {"term": "Content Validity", "definition": "Content validity refers to the extent to which a measure adequately samples the full domain of content it is designed to assess. It is typically established through expert judgment rather than through statistical analysis of test scores.", "is_target": false}], "target_entry_index": 1, "error_original": "overestimates true validity", "error_correct": "underestimates true validity", "explanation": "The passage states that a deficient criterion 'underestimates true validity' because the test may validly predict aspects of performance that the criterion fails to capture. The error swaps 'underestimates' with 'overestimates.' Since the criterion is missing relevant aspects the test actually predicts, the obtained validity coefficient will be lower than the test's actual predictive power, meaning true validity is underestimated, not overestimated."}, {"id": "PMET-VD-0040", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Introduction to Classical Conditioning", "passage_type": "definition", "source_passage_id": "PMET-0001", "entries": [{"term": "Unconditioned Response (UR)", "definition": "The unlearned, naturally occurring response to the unconditioned stimulus. It requires no prior conditioning or training to occur. For example, salivation in response to food placed in the mouth is a UR.", "is_target": false}, {"term": "Conditioned Stimulus (CS)", "definition": "A previously neutral stimulus that, after association with the unconditioned stimulus, comes to trigger a conditioned response. Before conditioning, this stimulus produces no specific response. For example, a bell becomes a CS after repeated pairing with food.", "is_target": false}, {"term": "Conditioned Response (CR)", "definition": "The learned response to the conditioned stimulus that emerges after repeated pairing of the CS and US. It is typically similar in form to the unconditioned response. For example, salivation in response to a bell after conditioning is a CR.", "is_target": false}, {"term": "Unconditioned Stimulus (US)", "definition": "A stimulus that, after repeated exposure, automatically triggers a response through learned association with a neutral stimulus. It serves as the basis for classical conditioning. For example, food causing salivation is considered a US.", "is_target": true}], "target_entry_index": 3, "error_original": "after repeated exposure, automatically triggers a response through learned association with a neutral stimulus", "error_correct": "naturally and automatically triggers a response without prior learning", "explanation": "The definition of the Unconditioned Stimulus (US) incorrectly states that it triggers a response 'after repeated exposure' and 'through learned association.' By definition, the US naturally and automatically triggers a response WITHOUT any prior learning or repeated exposure — that is precisely what makes it 'unconditioned.' The erroneous definition confuses the US with the process that creates a Conditioned Stimulus."}, {"id": "PMET-VD-0041", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0579", "entries": [{"term": "Content Validity", "definition": "Content validity refers to the degree to which items on a test adequately and representatively sample the full domain of content the test is intended to measure. It is typically established through expert judgment rather than statistical analysis. This form of validity is especially important for achievement and licensing examinations.", "is_target": false}, {"term": "Construct Validity", "definition": "Construct validity refers to the degree to which a test measures the theoretical construct or trait it is intended to measure. It encompasses multiple lines of evidence including convergent and discriminant validity. Messick (1989) argued that construct validity is the overarching framework that subsumes all other forms of validity evidence.", "is_target": false}, {"term": "Messick's Unified Validity Framework", "definition": "Messick's (1989) unified validity framework was presented in his seminal chapter in R. L. Linn's Educational Measurement (2nd edition). He argued that validity is a single, integrative concept centered on construct validity and that it must also consider the social consequences of test use. This framework fundamentally reshaped how validity is conceptualized in modern psychometrics.", "is_target": true}, {"term": "Criterion Validity", "definition": "Criterion validity refers to the degree to which scores on a test are related to some external criterion measure. It includes both predictive validity, where the criterion is measured in the future, and concurrent validity, where the criterion is measured at the same time. This type of validity evidence is commonly used in personnel selection contexts.", "is_target": false}], "target_entry_index": 2, "error_original": "2nd edition", "error_correct": "3rd edition", "explanation": "Messick's (1989) seminal chapter on validity appeared in the 3rd edition of Educational Measurement edited by R. L. Linn, not the 2nd edition. The 2nd edition was published in 1971 and edited by Robert L. Thorndike. This is a subtle but factually important bibliographic error."}, {"id": "PMET-VD-0042", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Two-Factor Theory of Avoidance", "passage_type": "paragraph", "source_passage_id": "PMET-0483", "entries": [{"term": "Two-Factor Theory of Avoidance", "definition": "A theory proposing that avoidance behavior is acquired and maintained through two learning processes. Factor 1 involves classical conditioning, where a neutral stimulus becomes a conditioned stimulus for fear through pairing with an aversive unconditioned stimulus. Factor 2 involves operant conditioning, where the organism learns to avoid the conditioned stimulus, and this avoidance is reinforced by fear reduction.", "is_target": false}, {"term": "Escape Learning", "definition": "A form of operant conditioning in which an organism learns to perform a behavior that terminates an aversive stimulus that is already present. Unlike avoidance learning, the organism must first experience the unpleasant event before responding. The behavior is maintained through negative reinforcement, as the removal of the aversive stimulus strengthens the escape response.", "is_target": false}, {"term": "Learned Helplessness", "definition": "A phenomenon first described by Seligman in which an organism exposed to inescapable aversive stimuli later fails to attempt escape or avoidance even when such behavior becomes possible. Factor 1 involves classical conditioning, where a neutral stimulus becomes a conditioned stimulus for fear through pairing with an appetitive unconditioned stimulus. This concept has been applied as a model for understanding depression in humans.", "is_target": true}, {"term": "Avoidance Conditioning", "definition": "A form of learning in which an organism learns to make a response that prevents an aversive stimulus from occurring. The response is typically cued by a warning signal that precedes the aversive event. This behavior is maintained through negative reinforcement, as the successful avoidance of the aversive stimulus reinforces the avoidance response.", "is_target": false}], "target_entry_index": 2, "error_original": "pairing with an appetitive unconditioned stimulus", "error_correct": "pairing with an aversive unconditioned stimulus", "explanation": "The definition of Learned Helplessness incorrectly states that fear conditioning involves pairing with an 'appetitive' (rewarding) unconditioned stimulus. In the Two-Factor Theory's classical conditioning component (which the definition borrows from), the neutral stimulus becomes a conditioned stimulus for fear through pairing with an 'aversive' (unpleasant) unconditioned stimulus. An appetitive stimulus would produce approach behavior, not fear."}, {"id": "PMET-VD-0043", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0532", "entries": [{"term": "Random Assignment", "definition": "Random assignment is the process of allocating participants to different conditions of an experiment using chance procedures. It is the defining feature that distinguishes true experiments from quasi-experiments and is essential for establishing internal validity and causal inferences.", "is_target": false}, {"term": "Random Selection", "definition": "Random selection is a sampling technique in which every member of a population has an equal chance of being chosen for inclusion in a study. It is primarily used to enhance external validity and the generalizability of research findings to the broader population.", "is_target": false}, {"term": "Quasi-Experimental Design", "definition": "A quasi-experimental design is a research approach that resembles a true experiment but lacks random selection of participants into conditions. Because of this limitation, it is more vulnerable to confounding variables and threats to internal validity than a true experiment.", "is_target": true}, {"term": "Counterbalancing", "definition": "Counterbalancing is a procedural technique used in within-subjects designs to control for order effects by varying the sequence in which conditions are presented. It helps ensure that the order of treatments does not systematically bias the results of the study.", "is_target": false}], "target_entry_index": 2, "error_original": "lacks random selection of participants into conditions", "error_correct": "lacks random assignment of participants into conditions", "explanation": "The definition incorrectly states that quasi-experimental designs lack 'random selection.' The actual distinguishing feature is the lack of 'random assignment' (allocation of participants to conditions). Random selection refers to how participants are chosen from a population (affecting external validity), whereas random assignment refers to how participants are allocated to experimental conditions (affecting internal validity). It is the absence of random assignment that defines a quasi-experiment."}, {"id": "PMET-VD-0044", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Introduction to Reliability", "passage_type": "definition", "source_passage_id": "PMET-0201", "entries": [{"term": "True Score", "definition": "A hypothetical score representing an examinee's actual level on the measured attribute. In Classical Test Theory, an individual's observed score is the sum of the true score and measurement error.", "is_target": false}, {"term": "Measurement Error", "definition": "Random fluctuations in scores due to factors unrelated to the construct being measured. These unsystematic influences reduce the reliability of a test and increase the discrepancy between observed and true scores.", "is_target": false}, {"term": "Reliability Coefficient", "definition": "A correlation coefficient indicating the proportion of score variance that is due to measurement error. It ranges from 0 to 1, with higher values indicating greater consistency and less influence from random factors.", "is_target": true}, {"term": "Standard Error of Measurement", "definition": "An index of the amount of error in individual test scores, estimated from the reliability coefficient and the standard deviation of scores. Smaller values indicate more precise measurement of the examinee's true score.", "is_target": false}], "target_entry_index": 2, "error_original": "the proportion of score variance that is due to measurement error", "error_correct": "the proportion of score variance that is systematic (true score variance)", "explanation": "The reliability coefficient indicates the proportion of score variance that is systematic, i.e., true score variance — not variance due to measurement error. A higher reliability coefficient means more of the variance in observed scores is attributable to true differences among examinees, not to random error."}, {"id": "PMET-VD-0045", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "inferential-statistics.html", "chapter_title": "From Samples to Populations: Inferential Statistics", "section": "Commonly Tested Topics for the EPPP", "passage_type": "clinical_note", "source_passage_id": "PMET-0472", "entries": [{"term": "MANCOVA", "definition": "An extension of ANCOVA that incorporates multiple dependent variables while statistically controlling for one or more covariates. It combines the features of MANOVA and ANCOVA to reduce error variance and control for confounding variables.", "is_target": false}, {"term": "MANOVA", "definition": "An extension of ANOVA used when there are multiple dependent variables. It controls for Type I error inflation that would occur if separate ANOVAs were run. Key statistics include Wilks' Lambda and Pillai's Trace, and significant results are typically followed up with factor analysis.", "is_target": true}, {"term": "ANOVA", "definition": "A statistical technique used to compare means across two or more groups on a single dependent variable. It partitions total variance into between-group and within-group components and produces an F-ratio to test the null hypothesis of equal group means.", "is_target": false}, {"term": "Discriminant Function Analysis", "definition": "A multivariate statistical procedure used to determine which variables best distinguish between two or more naturally occurring groups. It creates linear combinations of predictor variables that maximize the separation between group centroids.", "is_target": false}], "target_entry_index": 1, "error_original": "followed up with factor analysis", "error_correct": "followed up with discriminant function analysis", "explanation": "The passage states that MANOVA results are followed up with discriminant function analysis, not factor analysis. Discriminant function analysis identifies which dependent variables contribute most to significant group differences found by MANOVA. Factor analysis is a different technique used to identify underlying latent constructs among a set of observed variables."}, {"id": "PMET-VD-0046", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "inferential-statistics.html", "chapter_title": "From Samples to Populations: Inferential Statistics", "section": "Significance Level (Alpha)", "passage_type": "paragraph", "source_passage_id": "PMET-0128", "entries": [{"term": "Type II Error", "definition": "A Type II error (beta) occurs when a researcher fails to reject the null hypothesis when it is actually false. The probability of making this error is denoted by β, and it is inversely related to statistical power. Researchers can reduce Type II error by increasing sample size or effect size.", "is_target": false}, {"term": "Significance Level (Alpha)", "definition": "The significance level (α) is the probability of rejecting the null hypothesis when it is actually true. Conventionally, α is set at .05, meaning researchers accept a 5% chance of making a Type II error. More stringent levels such as .01 or .001 may be used when the consequences of a false positive are particularly serious.", "is_target": true}, {"term": "Statistical Power", "definition": "Statistical power is the probability of correctly rejecting the null hypothesis when it is false, calculated as 1 minus beta. Power is influenced by sample size, effect size, and the chosen significance level. A commonly accepted minimum power level in behavioral research is .80.", "is_target": false}, {"term": "Effect Size", "definition": "Effect size is a quantitative measure of the magnitude of a phenomenon or the strength of a relationship between variables. Unlike significance testing, effect size is not directly influenced by sample size. Common measures include Cohen's d for mean differences and r² for variance explained.", "is_target": false}], "target_entry_index": 1, "error_original": "5% chance of making a Type II error", "error_correct": "5% chance of making a Type I error", "explanation": "The significance level (alpha) represents the probability of making a Type I error—incorrectly rejecting a true null hypothesis (a false positive). The definition incorrectly states it is the chance of making a Type II error, which is a different concept (failing to reject a false null hypothesis, denoted by beta)."}, {"id": "PMET-VD-0047", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Posttest-Only Control Group Design", "passage_type": "paragraph", "source_passage_id": "PMET-0256", "entries": [{"term": "Pretest-Posttest Control Group Design", "definition": "A true experimental design in which participants are randomly assigned to groups, measured before treatment (pretest), exposed to the treatment or control condition, and then measured again (posttest). The pretest allows researchers to verify that groups were equivalent prior to the intervention.", "is_target": false}, {"term": "Posttest-Only Control Group Design", "definition": "A true experimental design that omits the pretest and relies on matched assignment to equate groups prior to treatment. It is especially useful when pretesting might sensitize participants to the treatment or when the pretest itself could introduce a confound.", "is_target": true}, {"term": "Solomon Four-Group Design", "definition": "A true experimental design that combines both pretest-posttest and posttest-only control group designs, using four groups total. This design allows researchers to assess the effects of pretesting itself as well as the interaction between pretesting and treatment.", "is_target": false}, {"term": "Factorial Design", "definition": "An experimental design in which two or more independent variables are simultaneously manipulated, allowing researchers to examine main effects and interaction effects. Each combination of levels across the independent variables is represented in the design.", "is_target": false}], "target_entry_index": 1, "error_original": "relies on matched assignment to equate groups prior to treatment", "error_correct": "relies on random assignment to equate groups prior to treatment", "explanation": "The posttest-only control group design relies on random assignment, not matched assignment, to equate groups before treatment. Random assignment is what makes this a true experimental design and justifies omitting the pretest, because randomization is expected to produce equivalent groups on both known and unknown variables."}, {"id": "PMET-VD-0048", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0451", "entries": [{"term": "Standards for Educational and Psychological Testing", "definition": "A comprehensive set of guidelines published jointly by AERA, APA, and NCME that provides criteria for evaluating tests, testing practices, and the effects of test use. The most recent edition was published in 2014 and addresses topics such as validity, reliability, and fairness in testing.", "is_target": false}, {"term": "Criterion-Related Validity", "definition": "A type of validity evidence that assesses how well a test score predicts or correlates with an external criterion measure. It includes two subtypes: predictive validity, which examines the relationship between test scores and future criterion performance, and concurrent validity, which examines the relationship between test scores and criterion measures obtained at approximately the same time.", "is_target": false}, {"term": "Content Validity", "definition": "A type of validity evidence based on the degree to which the content of a test adequately represents the domain it is intended to measure. It is typically established through expert judgment and systematic examination of the test items to ensure they are relevant and representative of the construct.", "is_target": false}, {"term": "Construct Validity", "definition": "A type of validity evidence that evaluates whether a test measures the theoretical construct it claims to measure. It was originally articulated by Cronbach and Henry in 1955 and is established through multiple lines of evidence including convergent and discriminant validity as described in multitrait-multimethod analyses.", "is_target": true}], "target_entry_index": 3, "error_original": "Cronbach and Henry in 1955", "error_correct": "Cronbach and Meehl in 1955", "explanation": "The seminal paper on construct validity was authored by Lee Cronbach and Paul Meehl in 1955, not 'Cronbach and Henry.' This is a classic name swap error. Their paper 'Construct Validity in Psychological Tests' is one of the most influential works in psychometrics and is foundational to understanding how psychological constructs are validated."}, {"id": "PMET-VD-0049", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Historical Foundations", "passage_type": "paragraph", "source_passage_id": "PMET-0166", "entries": [{"term": "Law of Contiguity", "definition": "A principle of associative learning stating that events or stimuli that occur close together in time tend to become linked in the mind. This concept was central to early behavioral theories of classical conditioning and stimulus-response associations.", "is_target": false}, {"term": "Law of Effect", "definition": "Formulated by Edward Thorndike based on his research with dogs in puzzle boxes, this principle states that responses followed by satisfying consequences are strengthened, while responses followed by annoying consequences are weakened. It laid the groundwork for the later development of operant conditioning.", "is_target": true}, {"term": "Law of Exercise", "definition": "A principle proposed by Edward Thorndike stating that the more frequently a stimulus-response connection is practiced, the stronger it becomes. Thorndike later revised this law after finding that repetition alone, without reinforcement, does not reliably strengthen associations.", "is_target": false}, {"term": "Law of Readiness", "definition": "A principle proposed by Edward Thorndike suggesting that learning is facilitated when an organism is prepared to respond to a stimulus. When a learner is ready to act, doing so is satisfying, whereas being prevented from acting produces frustration.", "is_target": false}], "target_entry_index": 1, "error_original": "dogs in puzzle boxes", "error_correct": "cats in puzzle boxes", "explanation": "Thorndike's famous puzzle box experiments were conducted with cats, not dogs. He observed cats gradually learning to escape from puzzle boxes more quickly over successive trials, which led him to formulate the Law of Effect."}, {"id": "PMET-VD-0050", "mode": "vocab", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "variables-data.html", "chapter_title": "Variables, Scales & the Language of Data", "section": "Ratio Scale", "passage_type": "paragraph", "source_passage_id": "PMET-0596", "entries": [{"term": "Interval Scale", "definition": "An interval scale has equal distances between values and an arbitrary zero point. It supports addition and subtraction but not meaningful ratios. Examples include temperature measured in Celsius or Fahrenheit.", "is_target": false}, {"term": "Ratio Scale", "definition": "A ratio scale has equal intervals between values and a true zero point, permitting all mathematical operations. It is the only scale where meaningful ratios can be formed. However, only parametric statistical procedures are appropriate for ratio data.", "is_target": true}, {"term": "Ordinal Scale", "definition": "An ordinal scale ranks observations in order but does not ensure equal intervals between ranks. It supports median and percentile calculations but not arithmetic means. Examples include class rankings or Likert-type response categories.", "is_target": false}, {"term": "Nominal Scale", "definition": "A nominal scale classifies data into distinct categories without any inherent order or numeric value. It permits only mode and frequency-based statistics. Examples include gender, ethnicity, or diagnostic categories.", "is_target": false}], "target_entry_index": 1, "error_original": "only parametric statistical procedures are appropriate for ratio data", "error_correct": "all statistical procedures are appropriate for ratio data", "explanation": "The passage states that all statistical procedures are appropriate for ratio data, not only parametric ones. Because a ratio scale is the highest level of measurement, it supports every type of statistical analysis—both parametric and nonparametric. Restricting it to only parametric procedures is incorrect."}, {"id": "PMET-SC-0001", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Meta-Analysis", "passage_type": "paragraph", "source_passage_id": "PMET-0074", "modified_sentence": "When substantial heterogeneity exists, researchers conduct moderator analysis (also called subgroup analysis or meta-correlation) to identify variables that explain why effect sizes differ across studies (e.g., type of intervention, participant age, study quality).", "phrases": ["When substantial heterogeneity exists,", " researchers conduct moderator analysis (also called subgroup analysis or meta-correlation)", " to identify variables that explain why effect sizes differ across studies", " (e.g., type of intervention, participant age, study quality)."], "target_phrase_index": 1, "error_original": "meta-correlation", "error_correct": "meta-regression", "explanation": "Moderator analysis in meta-analysis is also known as subgroup analysis or meta-regression, not meta-correlation. Meta-regression is the technique used to examine whether study-level covariates (moderators) account for heterogeneity in effect sizes across studies. 'Meta-correlation' is not a recognized term for this procedure."}, {"id": "PMET-SC-0002", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0580", "modified_sentence": "Schmidt and Hunter published their landmark 1977 article in the Journal of Applied Psychology, in which they developed a general solution to the problem of criterion contamination, which appeared in volume 62 of the journal, and fundamentally changed how researchers understood the consistency of test validity across settings.", "phrases": ["Schmidt and Hunter published their landmark 1977 article in the Journal of Applied Psychology,", " in which they developed a general solution to the problem of criterion contamination,", " which appeared in volume 62 of the journal,", " and fundamentally changed how researchers understood the consistency of test validity across settings."], "target_phrase_index": 1, "error_original": "the problem of criterion contamination", "error_correct": "the problem of validity generalization", "explanation": "Schmidt and Hunter's 1977 article addressed the problem of validity generalization — specifically, whether employment test validities are situationally specific or generalizable across settings. 'Criterion contamination' is an entirely different psychometric concept referring to when a criterion measure is influenced by knowledge of predictor scores. The article's actual title is 'Development of a general solution to the problem of validity generalization.'"}, {"id": "PMET-SC-0003", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Non-linearity", "passage_type": "paragraph", "source_passage_id": "PMET-0055", "modified_sentence": "If the relationship between variables is curvilinear, such as the Yerkes-Dodson law relating arousal and performance, Pearson's r will overestimate the strength of the actual relationship.", "phrases": ["If the relationship between variables is curvilinear,", " such as the Yerkes-Dodson law relating arousal and performance,", " Pearson's r will overestimate the strength", " of the actual relationship."], "target_phrase_index": 2, "error_original": "overestimate the strength", "error_correct": "underestimate the strength", "explanation": "When a relationship is curvilinear, Pearson's r will underestimate (not overestimate) the strength of the actual relationship because Pearson's r only captures the linear component of the association, missing the non-linear portion. This means the true relationship is stronger than what Pearson's r indicates."}, {"id": "PMET-SC-0004", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Avoidance Learning", "passage_type": "paragraph", "source_passage_id": "PMET-0195", "modified_sentence": "Mowrer's two-factor theory explains why avoidance behavior is so resistant to extinction, because the organism escapes from the presence of the feared stimulus before the conditioned fear response can extinguish, and this negative reinforcement cycle maintains the avoidance behavior indefinitely.", "phrases": ["Mowrer's two-factor theory explains why avoidance behavior is so resistant to extinction,", " because the organism escapes from the presence of the feared stimulus", " before the conditioned fear response can extinguish,", " and this negative reinforcement cycle maintains the avoidance behavior indefinitely."], "target_phrase_index": 1, "error_original": "escapes from the presence of the feared stimulus", "error_correct": "never remains in the presence of the feared stimulus", "explanation": "The key mechanism in Mowrer's two-factor theory of avoidance is that the organism never remains in the presence of the conditioned stimulus (feared stimulus) long enough for extinction to occur — it avoids the stimulus entirely rather than escaping from it. Escape implies contact with the aversive stimulus, whereas avoidance means the organism leaves or acts before encountering the feared stimulus, which is precisely why the conditioned fear never extinguishes."}, {"id": "PMET-SC-0005", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Taste Aversion Learning", "passage_type": "paragraph", "source_passage_id": "PMET-0025", "modified_sentence": "Garcia and Koelling (1966) demonstrated that rats easily learn to avoid flavors paired with illness but not audiovisual cues paired with illness, while they easily learn to avoid flavors paired with shock but not audiovisual cues paired with shock.", "phrases": ["Garcia and Koelling (1966) demonstrated that rats easily learn to avoid flavors paired with illness", " but not audiovisual cues paired with illness,", " while they easily learn to avoid flavors paired with shock", " but not audiovisual cues paired with shock."], "target_phrase_index": 2, "error_original": "while they easily learn to avoid flavors paired with shock", "error_correct": "while they easily learn to avoid audiovisual cues paired with shock", "explanation": "Garcia and Koelling (1966) found that rats easily associate audiovisual cues (not flavors) with shock. The study demonstrated selective association: flavors are readily associated with illness, while audiovisual cues are readily associated with shock. Replacing 'audiovisual cues' with 'flavors' in the shock condition contradicts the key finding of biologically prepared, stimulus-specific learning."}, {"id": "PMET-SC-0006", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Nonequivalent Control Group Design", "passage_type": "clinical_note", "source_passage_id": "PMET-0244", "modified_sentence": "In clinical settings, quasi-experimental designs are often the most feasible option when comparing outcomes between patients who choose medication versus therapy, because random assignment cannot be used ethically if patients have strong preferences, and careful matching and statistical controls can weaken causal inferences in such studies.", "phrases": ["In clinical settings, quasi-experimental designs are often the most feasible option", " when comparing outcomes between patients who choose medication versus therapy,", " because random assignment cannot be used ethically if patients have strong preferences,", " and careful matching and statistical controls can weaken causal inferences in such studies."], "target_phrase_index": 3, "error_original": "can weaken causal inferences in such studies", "error_correct": "can strengthen causal inferences in such studies", "explanation": "The passage states that careful matching and statistical controls can 'strengthen' causal inferences, not 'weaken' them. These techniques are used specifically to compensate for the lack of random assignment in quasi-experimental designs, thereby improving (strengthening) the validity of causal conclusions drawn from such studies."}, {"id": "PMET-SC-0007", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Commonly Tested Topics for the EPPP", "passage_type": "clinical_note", "source_passage_id": "PMET-0493", "modified_sentence": "A discriminative stimulus, often symbolized as S-D, signals that reinforcement is available for a particular response, and it sets the occasion for behavior by directly eliciting it, unlike an unconditioned stimulus in respondent conditioning.", "phrases": ["A discriminative stimulus, often symbolized as S-D,", " signals that reinforcement is available for a particular response,", " and it sets the occasion for behavior by directly eliciting it,", " unlike an unconditioned stimulus in respondent conditioning."], "target_phrase_index": 2, "error_original": "by directly eliciting it", "error_correct": "rather than directly eliciting it", "explanation": "A discriminative stimulus sets the occasion for behavior but does NOT elicit it. Eliciting is characteristic of stimuli in respondent (classical) conditioning. The discriminative stimulus merely signals that reinforcement is available, increasing the probability that the operant behavior will occur, but it does not automatically produce the response the way a stimulus elicits a reflex in classical conditioning. The phrase 'by directly eliciting it' incorrectly states that the discriminative stimulus elicits the behavior, when in fact it specifically does not."}, {"id": "PMET-SC-0008", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "inferential-statistics.html", "chapter_title": "From Samples to Populations: Inferential Statistics", "section": "Chi-Square Tests", "passage_type": "paragraph", "source_passage_id": "PMET-0463", "modified_sentence": "The chi-square goodness of fit test evaluates whether observed frequencies differ significantly from expected frequencies, and it is used with two or more categorical variables to determine if the sample distribution matches a hypothesized pattern.", "phrases": ["The chi-square goodness of fit test evaluates whether observed frequencies differ significantly from expected frequencies,", " and it is used with two or more categorical variables", " to determine if the sample distribution", " matches a hypothesized pattern."], "target_phrase_index": 1, "error_original": "two or more categorical variables", "error_correct": "a single categorical variable", "explanation": "The chi-square goodness of fit test is used with a single categorical variable to test whether the observed frequency distribution matches an expected distribution. The description of 'two or more categorical variables' actually applies to the chi-square test of independence, which examines the relationship between two categorical variables. The goodness of fit test specifically involves only one categorical variable."}, {"id": "PMET-SC-0009", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "The Three-Term Contingency", "passage_type": "paragraph", "source_passage_id": "PMET-0189", "modified_sentence": "The three-term contingency, also known as the ABC model, was a framework developed by Edward Thorndike to describe how an antecedent discriminative stimulus sets the occasion for a behavior, which is then followed by a reinforcing or punishing consequence.", "phrases": ["The three-term contingency, also known as the ABC model,", " was a framework developed by Edward Thorndike to describe", " how an antecedent discriminative stimulus sets the occasion for a behavior,", " which is then followed by a reinforcing or punishing consequence."], "target_phrase_index": 1, "error_original": "Edward Thorndike", "error_correct": "B.F. Skinner", "explanation": "The three-term contingency (ABC model) is a framework central to B.F. Skinner's operant conditioning theory, not Edward Thorndike's. While Thorndike contributed the Law of Effect, which laid groundwork for operant conditioning, the formal three-term contingency model describing the relationship among discriminative stimuli, behavior, and consequences was developed by B.F. Skinner."}, {"id": "PMET-SC-0010", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Evidence Based on Test Content", "passage_type": "paragraph", "source_passage_id": "PMET-0352", "modified_sentence": "Content validity refers to the degree to which test items adequately sample the domain of content the test is intended to measure, and it is established through statistical procedures rather than logical analysis, making it particularly important for achievement tests and job performance assessments.", "phrases": ["Content validity refers to the degree to which test items adequately sample the domain of content the test is intended to measure,", " and it is established through statistical procedures rather than logical analysis,", " making it particularly important for achievement tests", " and job performance assessments."], "target_phrase_index": 1, "error_original": "statistical procedures rather than logical analysis", "error_correct": "logical analysis rather than statistical procedures", "explanation": "Content validity is established through logical analysis (expert judgment of item relevance and representativeness) rather than statistical procedures. The error reverses the two methods, stating that statistical procedures are used instead of logical analysis, when in fact it is the opposite — content validity relies on rational, expert-based evaluation of whether test items adequately cover the intended content domain."}, {"id": "PMET-SC-0011", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Clinical Applications", "passage_type": "paragraph", "source_passage_id": "PMET-0425", "modified_sentence": "Systematic desensitization, developed by Joseph Wolpe, pairs relaxation with a hierarchy of feared stimuli and is based on the principle of stimulus generalization to reduce anxiety responses.", "phrases": ["Systematic desensitization, developed by Joseph Wolpe,", " pairs relaxation with a hierarchy of feared stimuli", " and is based on the principle of stimulus generalization", " to reduce anxiety responses."], "target_phrase_index": 2, "error_original": "stimulus generalization", "error_correct": "reciprocal inhibition", "explanation": "Systematic desensitization is based on the principle of reciprocal inhibition (also called counterconditioning), which holds that a response incompatible with anxiety (such as deep muscle relaxation) can inhibit the anxiety response. Stimulus generalization is a different classical conditioning concept referring to the tendency for stimuli similar to a conditioned stimulus to also elicit the conditioned response. Wolpe explicitly grounded systematic desensitization in reciprocal inhibition theory."}, {"id": "PMET-SC-0012", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Commonly Tested Topics for the EPPP", "passage_type": "clinical_note", "source_passage_id": "PMET-0436", "modified_sentence": "Meta-analysis is a quantitative technique that synthesizes effect sizes across multiple studies, and the funnel plot visually displays individual study results along with the overall weighted average, typically represented at the bottom by a diamond symbol.", "phrases": ["Meta-analysis is a quantitative technique that synthesizes effect sizes across multiple studies,", " and the funnel plot visually displays individual study results", " along with the overall weighted average,", " typically represented at the bottom by a diamond symbol."], "target_phrase_index": 1, "error_original": "the funnel plot visually displays individual study results", "error_correct": "the forest plot visually displays individual study results", "explanation": "The forest plot is the meta-analytic graphic that displays individual study effect sizes (as squares or points on horizontal lines) along with the overall weighted average (as a diamond). The funnel plot is a different meta-analytic tool used to assess publication bias by plotting effect sizes against a measure of study precision (such as standard error). The passage specifically mentions forest plots as the display tool for meta-analytic results."}, {"id": "PMET-SC-0013", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Extinction and Spontaneous Recovery", "passage_type": "clinical_note", "source_passage_id": "PMET-0002", "modified_sentence": "Fear extinguished in a therapist's office may return when the client encounters the feared stimulus in different contexts, and effective treatment should include exposure in a single controlled context to promote generalization of extinction learning.", "phrases": ["Fear extinguished in a therapist's office may return", " when the client encounters the feared stimulus in different contexts,", " and effective treatment should include exposure in a single controlled context", " to promote generalization of extinction learning."], "target_phrase_index": 2, "error_original": "exposure in a single controlled context", "error_correct": "exposure in multiple contexts", "explanation": "The passage states that effective treatment should include exposure in multiple contexts to promote generalization of extinction learning. Replacing 'multiple contexts' with 'a single controlled context' directly contradicts the principle of context-dependence of extinction, which indicates that practicing extinction across varied settings is necessary to prevent renewal of the conditioned fear response."}, {"id": "PMET-SC-0014", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Pretest-Posttest Control Group Design", "passage_type": "paragraph", "source_passage_id": "PMET-0255", "modified_sentence": "The pretest-posttest control group design includes random assignment of participants to treatment and control groups, with measurements taken before and after treatment, and this design allows researchers to assess change over time and control for maturation effects between groups.", "phrases": ["The pretest-posttest control group design includes random assignment of participants to treatment and control groups,", " with measurements taken before and after treatment,", " and this design allows researchers to assess change over time", " and control for maturation effects between groups."], "target_phrase_index": 3, "error_original": "and control for maturation effects between groups", "error_correct": "and control for pre-existing differences between groups", "explanation": "The passage states that the pretest-posttest control group design controls for pre-existing differences between groups (since random assignment and pretesting help ensure groups are equivalent at baseline). The error substitutes 'maturation effects' for 'pre-existing differences.' While the design can help detect maturation effects, its primary strength through random assignment and pretesting is controlling for pre-existing differences between the groups."}, {"id": "PMET-SC-0015", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "Base Rate", "passage_type": "definition", "source_passage_id": "PMET-0078", "modified_sentence": "When the base rate is very high at around 90%, most people succeed and the test has little room to improve prediction, but when the base rate is moderate at around 30%, there is maximum opportunity for the test to improve prediction over chance.", "phrases": ["When the base rate is very high at around 90%,", " most people succeed and the test has little room to improve prediction,", " but when the base rate is moderate at around 30%,", " there is maximum opportunity for the test to improve prediction over chance."], "target_phrase_index": 2, "error_original": "moderate at around 30%", "error_correct": "moderate at around 50%", "explanation": "A moderate base rate is approximately 50%, not 30%. When the base rate is around 50%, there is maximum opportunity for a test to improve prediction beyond what could be achieved by simply predicting the most common outcome. At 30%, the base rate would be considered low rather than moderate."}, {"id": "PMET-SC-0017", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-validity.html", "chapter_title": "Threats & Safeguards: Research Validity", "section": "Threats to Construct Validity", "passage_type": "paragraph", "source_passage_id": "PMET-0327", "modified_sentence": "Researchers' expectations can influence participant behavior or data interpretation, a phenomenon known as the Rosenthal effect, and double-blind procedures are commonly used to control for this threat to internal validity.", "phrases": ["Researchers' expectations can influence participant behavior or data interpretation,", " a phenomenon known as the Rosenthal effect,", " and double-blind procedures are commonly used", " to control for this threat to internal validity."], "target_phrase_index": 3, "error_original": "to control for this threat to internal validity", "error_correct": "to control for this threat to construct validity", "explanation": "Experimenter expectancy (the Rosenthal effect) is classified as a threat to construct validity, not internal validity. It affects construct validity because the researcher's expectations can cause the measured outcomes to reflect the experimenter's beliefs rather than the true effect of the construct being studied. Double-blind procedures are the standard safeguard against this construct validity threat."}, {"id": "PMET-SC-0018", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0572", "modified_sentence": "According to Messick's unified framework, construct validity is the overarching concept, and both content validity and criterion-related validity are considered separate but contributing sources of evidence, while convergent and discriminant validity were formalized by Campbell and Fiske in 1966 using the multitrait-multimethod matrix.", "phrases": ["According to Messick's unified framework, construct validity is the overarching concept,", " and both content validity and criterion-related validity are considered separate but contributing sources of evidence,", " while convergent and discriminant validity were formalized by Campbell and Fiske in 1966", " using the multitrait-multimethod matrix."], "target_phrase_index": 2, "error_original": "in 1966", "error_correct": "in 1959", "explanation": "Campbell and Fiske published their seminal paper introducing the multitrait-multimethod matrix in 1959, not 1966. This 1959 publication is a foundational work in psychometrics that formalized the concepts of convergent and discriminant validity as essential components of construct validation."}, {"id": "PMET-SC-0019", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-validity.html", "chapter_title": "Threats & Safeguards: Research Validity", "section": "Threats to Statistical Conclusion Validity", "passage_type": "paragraph", "source_passage_id": "PMET-0332", "modified_sentence": "Conducting multiple statistical tests without correction inflates the probability of Type I errors, which occur when researchers incorrectly reject a true null hypothesis, and applying a Greenhouse-Geisser correction that divides the alpha level by the number of comparisons is one common safeguard against this threat.", "phrases": ["Conducting multiple statistical tests without correction inflates the probability of Type I errors,", " which occur when researchers incorrectly reject a true null hypothesis,", " and applying a Greenhouse-Geisser correction that divides the alpha level by the number of comparisons", " is one common safeguard against this threat."], "target_phrase_index": 2, "error_original": "Greenhouse-Geisser correction that divides the alpha level by the number of comparisons", "error_correct": "Bonferroni correction that divides the alpha level by the number of comparisons", "explanation": "The Bonferroni correction, not the Greenhouse-Geisser correction, is the procedure that divides the alpha level by the number of comparisons to control for inflated Type I error rates when conducting multiple tests. The Greenhouse-Geisser correction is used to adjust degrees of freedom when the sphericity assumption is violated in repeated-measures ANOVA."}, {"id": "PMET-SC-0020", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Reliability-Validity Relationship", "passage_type": "paragraph", "source_passage_id": "PMET-0527", "modified_sentence": "According to the reliability-validity relationship, a test's validity coefficient cannot exceed the square root of its reliability coefficient, which means that a test with a reliability of .49 could have a maximum validity of .80.", "phrases": ["According to the reliability-validity relationship,", " a test's validity coefficient cannot exceed the square root of its reliability coefficient,", " which means that a test with a reliability of .49", " could have a maximum validity of .80."], "target_phrase_index": 2, "error_original": "a reliability of .49", "error_correct": "a reliability of .64", "explanation": "The maximum validity of a test equals the square root of its reliability coefficient. For a maximum validity of .80, the reliability would need to be .80² = .64, not .49. The square root of .49 is .70, not .80. This formula illustrates that reliability sets an upper limit on validity."}, {"id": "PMET-SC-0021", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-validity.html", "chapter_title": "Threats & Safeguards: Research Validity", "section": "Commonly Tested Topics for the EPPP", "passage_type": "clinical_note", "source_passage_id": "PMET-0568", "modified_sentence": "Research design involves a well-known trade-off: laboratory studies typically have high internal validity but low external validity, while field studies have high internal validity but low external validity.", "phrases": ["Research design involves a well-known trade-off:", " laboratory studies typically have high internal validity but low external validity,", " while field studies have high internal validity", " but low external validity."], "target_phrase_index": 2, "error_original": "while field studies have high internal validity", "error_correct": "while field studies have low internal validity", "explanation": "Field studies are the opposite of laboratory studies in terms of the validity trade-off. Field studies have LOW internal validity (due to less control over extraneous variables) but HIGH external validity (due to greater generalizability to real-world settings). The passage incorrectly states that field studies have high internal validity, which would make them identical to lab studies rather than representing the opposite pattern."}, {"id": "PMET-SC-0022", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "criterion-validity.html", "chapter_title": "Predicting Outcomes: Criterion-Related Validity", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0444", "modified_sentence": "The base rate refers to the proportion of individuals who have a condition or who would succeed without any selection procedure, and a test has its greatest incremental validity when the base rate is extreme, approaching either .00 or 1.00 rather than .50.", "phrases": ["The base rate refers to the proportion of individuals who have a condition", " or who would succeed without any selection procedure,", " and a test has its greatest incremental validity when the base rate is extreme,", " approaching either .00 or 1.00 rather than .50."], "target_phrase_index": 2, "error_original": "when the base rate is extreme", "error_correct": "when the base rate is moderate, around .50", "explanation": "A selection test provides the greatest incremental validity (utility) when the base rate is moderate, approximately .50. When the base rate is extreme (very close to .00 or 1.00), the test adds little useful information because outcomes can already be predicted with high accuracy based on the base rate alone. The error falsely states that test utility is greatest at extreme base rates."}, {"id": "PMET-SC-0023", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "EPPP Exam Focus Points", "passage_type": "clinical_note", "source_passage_id": "PMET-0538", "modified_sentence": "The regression discontinuity design assigns participants to groups based on a cutoff score on a posttest measure, and it has stronger internal validity than other quasi-experimental designs because assignment is based on a known, measured criterion.", "phrases": ["The regression discontinuity design assigns participants to groups", " based on a cutoff score on a posttest measure,", " and it has stronger internal validity than other quasi-experimental designs", " because assignment is based on a known, measured criterion."], "target_phrase_index": 1, "error_original": "on a posttest measure,", "error_correct": "on a pretest measure,", "explanation": "In a regression discontinuity design, participants are assigned to treatment or control groups based on whether their score on a pretest (pre-intervention) measure falls above or below a predetermined cutoff point. The assignment criterion is a pretest measure, not a posttest measure. This pretest-based assignment rule is what gives the design its relatively strong internal validity among quasi-experimental designs."}, {"id": "PMET-SC-0024", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Evidence Based on Relations to Other Variables", "passage_type": "definition", "source_passage_id": "PMET-0338", "modified_sentence": "Campbell and Fiske (1959) introduced the multitrait-multimethod (MTMM) matrix to evaluate both convergent and discriminant validity simultaneously, where convergent validity is shown by high correlations between the same trait measured by different methods, and discriminant validity is shown by lower correlations between different traits even when measured by different methods.", "phrases": ["Campbell and Fiske (1959) introduced the multitrait-multimethod (MTMM) matrix", " to evaluate both convergent and discriminant validity simultaneously,", " where convergent validity is shown by high correlations between the same trait measured by different methods,", " and discriminant validity is shown by lower correlations between different traits even when measured by different methods."], "target_phrase_index": 3, "error_original": "even when measured by different methods", "error_correct": "even when measured by the same method", "explanation": "Discriminant validity in the MTMM matrix is demonstrated by lower correlations between different traits measured by the same method (heterotrait-monomethod correlations). The error changed 'the same method' to 'different methods,' which misrepresents the concept. The key insight of discriminant validity is that different traits should not correlate highly even when method variance is shared (i.e., the same method is used)."}, {"id": "PMET-SC-0025", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "variables-data.html", "chapter_title": "Variables, Scales & the Language of Data", "section": "Ratio Scale", "passage_type": "definition", "source_passage_id": "PMET-0371", "modified_sentence": "The interval scale has equal intervals and no true zero, with examples including temperature and IQ scores, while the ratio scale has equal intervals and a true zero, with the median as its preferred measure of central tendency.", "phrases": ["The interval scale has equal intervals and no true zero,", " with examples including temperature and IQ scores,", " while the ratio scale has equal intervals and a true zero,", " with the median as its preferred measure of central tendency."], "target_phrase_index": 3, "error_original": "the median as its preferred measure of central tendency", "error_correct": "the mean as its preferred measure of central tendency", "explanation": "According to the NOIR scales of measurement, the ratio scale uses the mean as its preferred measure of central tendency, not the median. The median is the preferred measure of central tendency for ordinal scales. Ratio scales, having both equal intervals and a true zero, support the use of the mean."}, {"id": "PMET-SC-0026", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "inferential-statistics.html", "chapter_title": "From Samples to Populations: Inferential Statistics", "section": "One-Sample t-Test", "passage_type": "example", "source_passage_id": "PMET-0119", "modified_sentence": "A researcher measures anxiety before and after a relaxation intervention in the same 30 participants, and the appropriate test is an independent samples t-test, because the same participants are measured twice in a within-subjects design.", "phrases": ["A researcher measures anxiety before and after a relaxation intervention in the same 30 participants,", " and the appropriate test is an independent samples t-test,", " because the same participants are measured twice", " in a within-subjects design."], "target_phrase_index": 1, "error_original": "an independent samples t-test", "error_correct": "a paired samples t-test", "explanation": "When the same participants are measured twice (a within-subjects design), the appropriate test is a paired samples t-test, not an independent samples t-test. An independent samples t-test is used when different participants are in each group (a between-subjects design)."}, {"id": "PMET-SC-0027", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Between-Subjects (Independent Groups) Designs", "passage_type": "paragraph", "source_passage_id": "PMET-0277", "modified_sentence": "Random assignment creates groups that are expected to be equivalent on all variables except the dependent variable, and the main advantage of between-subjects designs is avoiding carryover effects.", "phrases": ["Random assignment creates groups that are expected to be equivalent", " on all variables except the dependent variable,", " and the main advantage of between-subjects designs", " is avoiding carryover effects."], "target_phrase_index": 1, "error_original": "except the dependent variable,", "error_correct": "except the independent variable,", "explanation": "Random assignment is used to ensure that groups are equivalent on all variables except the independent variable, which is the variable being manipulated by the researcher. The dependent variable is the outcome being measured, not the variable on which groups are made equivalent. This error requires understanding the distinction between independent and dependent variables in the context of experimental design."}, {"id": "PMET-SC-0028", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Exposure Therapy", "passage_type": "paragraph", "source_passage_id": "PMET-0416", "modified_sentence": "According to inhibitory learning theory proposed by Craske, cognitive changes such as expectancy violation may be more important than within-session habituation, which was emphasized by earlier emotional processing theory developed by Foa and McNally.", "phrases": ["According to inhibitory learning theory proposed by Craske,", " cognitive changes such as expectancy violation may be more important than within-session habituation,", " which was emphasized by earlier emotional processing theory", " developed by Foa and McNally."], "target_phrase_index": 3, "error_original": "developed by Foa and McNally.", "error_correct": "developed by Foa and Kozak.", "explanation": "Emotional processing theory of exposure was developed by Edna Foa and Michael Kozak (1986), not Foa and McNally. Richard McNally is a prominent anxiety researcher but was not the co-author of emotional processing theory. Foa and Kozak proposed that within-session and between-session habituation were key mechanisms of exposure therapy, a view that has been challenged by Craske's inhibitory learning model emphasizing expectancy violation."}, {"id": "PMET-SC-0029", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-designs.html", "chapter_title": "Blueprints for Discovery: Research Designs", "section": "Solomon Four-Group Design", "passage_type": "paragraph", "source_passage_id": "PMET-0257", "modified_sentence": "The Solomon four-group design combines elements of both previous designs, using six groups: two receive pretests (one treatment, one control) and two do not (one treatment, one control), allowing researchers to detect and control for testing effects and sensitization.", "phrases": ["The Solomon four-group design combines elements of both previous designs,", " using six groups: two receive pretests (one treatment, one control)", " and two do not (one treatment, one control),", " allowing researchers to detect and control for testing effects and sensitization."], "target_phrase_index": 1, "error_original": "six groups", "error_correct": "four groups", "explanation": "The Solomon four-group design uses exactly four groups, not six. The four groups are: (1) pretest + treatment, (2) pretest + control, (3) treatment only (no pretest), and (4) control only (no pretest). The number four is even part of the design's name."}, {"id": "PMET-SC-0030", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "validity.html", "chapter_title": "Measuring What Matters: Content & Construct Validity", "section": "Relationship Between Reliability and Validity", "passage_type": "definition", "source_passage_id": "PMET-0340", "modified_sentence": "Validity cannot exceed the square root of reliability, which is expressed as r_xy ≤ √r_xx, and while high reliability does not guarantee validity, high validity does guarantee that reliability is also high.", "phrases": ["Validity cannot exceed the square root of reliability,", " which is expressed as r_xy ≤ √r_xx,", " and while high reliability does not guarantee validity,", " high validity does guarantee that reliability is also high."], "target_phrase_index": 2, "error_original": "high reliability does not guarantee validity", "error_correct": "low reliability limits the maximum possible validity", "explanation": "While the statement 'high reliability does not guarantee validity' is actually true on its own, the passage's key structural point in that position emphasizes that LOW reliability LIMITS the maximum possible validity — this is the critical relationship being described. However, re-evaluating: the phrase 'high reliability does not guarantee validity' is in fact directly from the passage and is correct. Let me reconsider the error placement."}, {"id": "PMET-SC-0031", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Spearman Rank-Order Correlation (rho)", "passage_type": "paragraph", "source_passage_id": "PMET-0046", "modified_sentence": "Spearman's rho is used when one or both variables are measured on an ordinal scale, or when the relationship is linear but not monotonic, and this coefficient correlates the ranks of the data rather than the raw scores.", "phrases": ["Spearman's rho is used when one or both variables are measured on an ordinal scale,", " or when the relationship is linear but not monotonic,", " and this coefficient correlates the ranks of the data", " rather than the raw scores."], "target_phrase_index": 1, "error_original": "linear but not monotonic", "error_correct": "monotonic but not linear", "explanation": "The passage states that Spearman's rho is appropriate when the relationship between variables is monotonic but not linear. The error reverses these terms: a monotonic relationship means the variables consistently move in the same direction (both increase or both decrease) but not necessarily at a constant rate (which would be linear). Spearman's rho can capture monotonic non-linear relationships, so the correct phrasing is 'monotonic but not linear.'"}, {"id": "PMET-SC-0032", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-validity.html", "chapter_title": "Threats & Safeguards: Research Validity", "section": "Commonly Tested Topics for the EPPP", "passage_type": "clinical_note", "source_passage_id": "PMET-0566", "modified_sentence": "Random assignment controls for threats to internal validity by creating equivalent groups, while random selection controls for threats to construct validity by ensuring the sample is representative of the population.", "phrases": ["Random assignment controls for threats to internal validity", " by creating equivalent groups, while random selection", " controls for threats to construct validity by ensuring", " the sample is representative of the population."], "target_phrase_index": 2, "error_original": "construct validity", "error_correct": "external validity", "explanation": "Random selection (randomly choosing participants from the population) controls for threats to external validity, not construct validity. External validity refers to the generalizability of findings to the broader population, which is precisely what random selection supports. Random assignment controls internal validity by creating equivalent groups, while random selection controls external validity by ensuring representativeness."}, {"id": "PMET-SC-0033", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "The Four Basic Schedules", "passage_type": "paragraph", "source_passage_id": "PMET-0182", "modified_sentence": "Reinforcement is delivered for the first response after a variable time period, averaging to a specified duration, and VI schedules produce steady, high response rates similar to VR schedules.", "phrases": ["Reinforcement is delivered for the first response after a variable time period,", " averaging to a specified duration,", " and VI schedules produce steady, high response rates", " similar to VR schedules."], "target_phrase_index": 2, "error_original": "high response rates", "error_correct": "moderate response rates", "explanation": "VI (variable-interval) schedules produce steady, moderate response rates, not high response rates. High response rates are characteristic of VR (variable-ratio) schedules. VI schedules generate consistent but moderate responding because reinforcement depends on the passage of time rather than the number of responses, so rapid responding does not significantly increase the rate of reinforcement."}, {"id": "PMET-SC-0034", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Commonly Tested Topics for the EPPP", "passage_type": "clinical_note", "source_passage_id": "PMET-0489", "modified_sentence": "Negative reinforcement is commonly confused with punishment, but it is defined as a process that decreases the frequency of a behavior through the removal of an aversive stimulus, rather than through the application of a consequence.", "phrases": ["Negative reinforcement is commonly confused with punishment,", " but it is defined as a process that decreases the frequency of a behavior", " through the removal of an aversive stimulus,", " rather than through the application of a consequence."], "target_phrase_index": 1, "error_original": "decreases the frequency of a behavior", "error_correct": "increases the frequency of a behavior", "explanation": "Negative reinforcement INCREASES the frequency of a behavior by removing an aversive stimulus. This is the most commonly tested distinction on the EPPP — negative reinforcement is not punishment. The word 'negative' refers to the removal of a stimulus, while 'reinforcement' always means behavior increases. Confusing it with punishment (which decreases behavior) is the exact error the passage warns about."}, {"id": "PMET-SC-0035", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "variables-data.html", "chapter_title": "Variables, Scales & the Language of Data", "section": "References", "passage_type": "paragraph", "source_passage_id": "PMET-0591", "modified_sentence": "Gravetter and Wallnau authored Statistics for the Behavioral Sciences, which reached its 12th edition in 2017, and was published by Cengage Learning as a core psychometrics reference.", "phrases": ["Gravetter and Wallnau authored Statistics for the Behavioral Sciences,", " which reached its 12th edition in 2017,", " and was published by Cengage Learning", " as a core psychometrics reference."], "target_phrase_index": 1, "error_original": "12th edition", "error_correct": "10th edition", "explanation": "The 2017 publication of Gravetter and Wallnau's 'Statistics for the Behavioral Sciences' was the 10th edition, not the 12th edition. This is directly stated in the reference citation: Gravetter, F. J., & Wallnau, L. B. (2017). Statistics for the behavioral sciences (10th ed.). Cengage Learning."}, {"id": "PMET-SC-0036", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Internal Consistency Reliability", "passage_type": "paragraph", "source_passage_id": "PMET-0228", "modified_sentence": "Because each half is only half as long as the full test, the Kuder-Richardson formula is used to estimate the reliability of the full-length test.", "phrases": ["Because each half is only half as long as the full test,", " the Kuder-Richardson formula is used", " to estimate the reliability", " of the full-length test."], "target_phrase_index": 1, "error_original": "the Kuder-Richardson formula is used", "error_correct": "the Spearman-Brown formula is used", "explanation": "The Spearman-Brown formula (also called the Spearman-Brown prophecy formula) is used to correct the split-half correlation to estimate the reliability of the full-length test. The Kuder-Richardson formulas (KR-20 and KR-21) are different internal consistency measures used specifically for dichotomously scored items. Replacing Spearman-Brown with Kuder-Richardson is an error that requires specific knowledge of which formula applies in the split-half reliability context."}, {"id": "PMET-SC-0037", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Schedules of Reinforcement", "passage_type": "paragraph", "source_passage_id": "PMET-0176", "modified_sentence": "Skinner and his colleagues discovered that intermittent reinforcement produces behavior that is less resistant to extinction than continuous reinforcement, a phenomenon known as the partial reinforcement effect (Ferster & Skinner, 1957).", "phrases": ["Skinner and his colleagues discovered that", " intermittent reinforcement produces behavior that is less resistant to extinction than continuous reinforcement,", " a phenomenon known as the partial reinforcement effect", " (Ferster & Skinner, 1957)."], "target_phrase_index": 1, "error_original": "less resistant to extinction", "error_correct": "more resistant to extinction", "explanation": "The partial reinforcement effect refers to the well-established finding that intermittent (partial) reinforcement produces behavior that is MORE resistant to extinction than continuous reinforcement, not less. This is because organisms that have experienced intermittent reinforcement take longer to discriminate between the reinforcement schedule and extinction conditions."}, {"id": "PMET-SC-0038", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "correlation-regression.html", "chapter_title": "Relationships in Data: Correlation & Regression", "section": "Heteroscedasticity", "passage_type": "paragraph", "source_passage_id": "PMET-0056", "modified_sentence": "Heteroscedasticity occurs when the variability of one variable differs across levels of the other variable, and the variance in job performance might be larger for people with high test scores than for people with low scores, which violates an assumption of Pearson's r and can affect the accuracy of the correlation coefficient.", "phrases": ["Heteroscedasticity occurs when the variability of one variable differs across levels of the other variable,", " and the variance in job performance might be larger for people with high test scores than for people with low scores,", " which violates an assumption of Pearson's r", " and can affect the accuracy of the correlation coefficient."], "target_phrase_index": 1, "error_original": "larger for people with high test scores than for people with low scores", "error_correct": "larger for people with low test scores than for people with high scores", "explanation": "The passage's classic example of heteroscedasticity states that variance in job performance is larger for people with LOW test scores than for those with HIGH scores (i.e., low scores predict a wider range of performance outcomes). The error reverses this direction, stating that variance is larger for people with high test scores, which contradicts the standard example given in the original passage."}, {"id": "PMET-SC-0039", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "research-validity.html", "chapter_title": "Threats & Safeguards: Research Validity", "section": "Threats to Internal Validity", "passage_type": "paragraph", "source_passage_id": "PMET-0310", "modified_sentence": "Statistical regression occurs when participants are selected based on extreme scores, and extreme scores tend to move away from the mean on retesting due to measurement error, regardless of any treatment effect.", "phrases": ["Statistical regression occurs when participants are selected based on extreme scores,", " and extreme scores tend to move away from the mean on retesting", " due to measurement error,", " regardless of any treatment effect."], "target_phrase_index": 1, "error_original": "move away from the mean", "error_correct": "move toward the mean", "explanation": "Statistical regression (regression to the mean) describes the tendency for extreme scores to move toward the mean on retesting, not away from the mean. This occurs because extreme scores are partly a product of measurement error, and on subsequent testing, that error component is unlikely to be as extreme, pulling scores back toward the average."}, {"id": "PMET-SC-0040", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "variables-data.html", "chapter_title": "Variables, Scales & the Language of Data", "section": "Types of Variables in Research", "passage_type": "example", "source_passage_id": "PMET-0380", "modified_sentence": "In a study examining the effects of sleep deprivation on memory, the researcher manipulates hours of sleep as the dependent variable and measures performance on a memory test as the independent variable.", "phrases": ["In a study examining the effects of sleep deprivation on memory,", " the researcher manipulates hours of sleep as the dependent variable", " and measures performance on a memory test", " as the independent variable."], "target_phrase_index": 1, "error_original": "as the dependent variable", "error_correct": "as the independent variable (IV)", "explanation": "The manipulated variable in an experiment is the independent variable (IV), not the dependent variable. In this study, the researcher manipulates hours of sleep (8 hours vs. 4 hours), making it the independent variable. The dependent variable is the outcome that is measured — in this case, performance on the memory test. The passage incorrectly labels the manipulated variable as the dependent variable and the measured outcome as the independent variable, reversing their definitions."}, {"id": "PMET-SC-0041", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "reliability.html", "chapter_title": "Consistency of Measurement: Test Reliability", "section": "Internal Consistency Reliability", "passage_type": "paragraph", "source_passage_id": "PMET-0227", "modified_sentence": "Internal consistency assesses the homogeneity of items within a single test administration, and it indicates the extent to which items measure different underlying constructs, which is why it is considered the most commonly reported type of reliability in psychological research.", "phrases": ["Internal consistency assesses the homogeneity of items within a single test administration,", " and it indicates the extent to which items measure different underlying constructs,", " which is why it is considered the most commonly reported type", " of reliability in psychological research."], "target_phrase_index": 1, "error_original": "items measure different underlying constructs", "error_correct": "items measure the same construct", "explanation": "Internal consistency reliability indicates the extent to which items on a test measure the same construct (i.e., are homogeneous), not different underlying constructs. High internal consistency means the items are intercorrelated and tapping into a single unified construct."}, {"id": "PMET-SC-0042", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "classical-conditioning.html", "chapter_title": "How Organisms Learn: Classical Conditioning", "section": "Stimulus Generalization and Discrimination", "passage_type": "paragraph", "source_passage_id": "PMET-0015", "modified_sentence": "Discrimination training involves reinforcing responses to one stimulus (CS+) by pairing it with the CR, while not reinforcing responses to similar stimuli (CS-).", "phrases": ["Discrimination training involves reinforcing responses to one stimulus (CS+)", " by pairing it with the CR,", " while not reinforcing responses", " to similar stimuli (CS-)."], "target_phrase_index": 1, "error_original": "by pairing it with the CR", "error_correct": "by pairing it with the US", "explanation": "In classical conditioning, reinforcement of the CS+ occurs by pairing it with the unconditioned stimulus (US), not the conditioned response (CR). The US is the stimulus that naturally elicits the response, and pairing it with the CS+ is what maintains the conditioned response to that stimulus while extinguishing responses to the CS-."}, {"id": "PMET-SC-0043", "mode": "sentence_click", "domain_code": "PMET", "domain_name": "Psychometrics & Research Methods", "chapter_file": "operant-conditioning.html", "chapter_title": "Shaping Behavior: Operant Conditioning", "section": "Stimulus Control", "passage_type": "paragraph", "source_passage_id": "PMET-0186", "modified_sentence": "Stimulus control occurs when a behavior is more likely to occur in the presence of a particular stimulus than in its absence, and the controlling stimulus is called a discriminative stimulus (SD, pronounced \"S-D\"), which signals that punishment is available for a particular response.", "phrases": ["Stimulus control occurs when a behavior is more likely to occur", " in the presence of a particular stimulus than in its absence,", " and the controlling stimulus is called a discriminative stimulus (SD, pronounced \"S-D\"),", " which signals that punishment is available for a particular response."], "target_phrase_index": 3, "error_original": "which signals that punishment is available for a particular response.", "error_correct": "which signals that reinforcement is available for a particular response.", "explanation": "A discriminative stimulus (SD) signals that reinforcement — not punishment — is available for a particular response. The passage clearly states that the SD indicates reinforcement availability. Replacing 'reinforcement' with 'punishment' changes the fundamental definition of a discriminative stimulus in operant conditioning."}]}
};
