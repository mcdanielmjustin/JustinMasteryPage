{
  "domain_code": "PMET",
  "domain_name": "Psychometrics & Research Methods",
  "question_type": "vignette",
  "total": 620,
  "questions": [
    {
      "id": "JQ-LEA-042-vignette-L1",
      "source_question_id": "042",
      "source_summary": "Negative reinforcement occurs when a behavior continues or increases because performing the behavior results in the termination or withdrawal of a stimulus, such as a woman eating chocolate because it reduces her anxiety.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "negative reinforcement",
        "termination",
        "behavior increases"
      ],
      "vignette": "A psychology student is studying operant conditioning principles in her learning course. She notices that every time she feels a headache coming on, she takes ibuprofen, and the headache pain subsides. Because the pain termination follows her pill-taking, her behavior of reaching for ibuprofen increases whenever a headache begins. Her professor identifies this as a classic textbook example of how the removal of an aversive stimulus can strengthen a behavior.",
      "question": "Which operant conditioning principle best explains why the student's pill-taking behavior increases over time?",
      "options": {
        "A": "Positive reinforcement, because taking ibuprofen produces a desirable outcome (pain relief) that strengthens the behavior.",
        "B": "Negative reinforcement, because the termination of an aversive stimulus (headache pain) following the behavior increases its frequency.",
        "C": "Negative punishment, because an unpleasant stimulus is removed contingent on a behavior, thereby reducing the likelihood of that behavior.",
        "D": "Extinction, because the original pairing of headache and no medication is no longer occurring, weakening the association."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Positive reinforcement involves adding a pleasant stimulus to increase behavior. While pain relief feels desirable, the mechanism here is the removal of an aversive stimulus (pain), not the addition of a reward — making this a description of negative, not positive, reinforcement.",
        "B": "This is correct. Negative reinforcement occurs when the removal or termination of an aversive stimulus (headache pain) following a behavior (taking ibuprofen) results in that behavior increasing in frequency. The 'negative' refers to subtraction of the aversive event, not to something bad.",
        "C": "Negative punishment involves removing a desired stimulus to decrease a behavior (e.g., losing screen time for misbehaving). Here, a stimulus is being removed, but the behavioral outcome is an increase, not a decrease — so negative punishment does not apply.",
        "D": "Extinction occurs when a previously reinforced behavior is no longer reinforced, leading to a decrease in that behavior. The student's pill-taking is being reinforced by pain relief each time, not undergoing the elimination of reinforcement, so extinction does not describe this scenario."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-042-vignette-L2",
      "source_question_id": "042",
      "source_summary": "Negative reinforcement occurs when a behavior continues or increases because performing the behavior results in the termination or withdrawal of a stimulus, such as a woman eating chocolate because it reduces her anxiety.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "aversive stimulus",
        "behavior increases"
      ],
      "vignette": "Marcus, a 34-year-old accountant with a history of generalized anxiety disorder, reports to his therapist that he has been leaving work early on days he feels overwhelmed. He explains that when he leaves, the tension in his chest and the feeling of dread immediately subside, and he feels calmer within minutes of getting in his car. Despite wanting to change this pattern, Marcus has found himself leaving work earlier and earlier each week. His therapist notes that this pattern has been persisting for several months and is now interfering with his job performance.",
      "question": "Which learning principle most accurately accounts for why Marcus's early-leaving behavior has progressively increased?",
      "options": {
        "A": "Positive reinforcement, because the feeling of calm Marcus experiences after leaving work functions as a rewarding stimulus that is added to his environment.",
        "B": "Avoidance conditioning, because Marcus is fleeing a feared situation, which reflects a classically conditioned fear response rather than an operant mechanism.",
        "C": "Negative punishment, because Marcus removes himself from the workplace environment, thereby losing access to his professional responsibilities.",
        "D": "Negative reinforcement, because the removal of the aversive stimulus — the overwhelming anxiety and tension — following the behavior of leaving work increases the frequency of that behavior."
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Positive reinforcement involves presenting an appetitive stimulus after a behavior to increase it. While calm feels pleasant, the mechanism is the removal of an aversive state (anxiety and tension), not the addition of a new pleasurable stimulus. Calm resulting from removal of distress is the defining feature of negative reinforcement.",
        "B": "Avoidance conditioning is a specific application of negative reinforcement in which behavior prevents an aversive stimulus from occurring. Marcus's behavior is escape (leaving after anxiety has begun), not avoidance (leaving before anxiety arises). More importantly, avoidance conditioning is itself a form of negative reinforcement, so this option misidentifies the mechanism as classically based rather than operant.",
        "C": "Negative punishment involves taking away a desired stimulus contingent on a behavior to decrease that behavior. Marcus's departure results in increased (not decreased) behavior, and the removal of 'professional responsibilities' is not functioning as a pleasant stimulus being withdrawn. This option does not fit the operant structure or the outcome direction.",
        "D": "This is correct. The behavior (leaving work early) is followed by the termination of an aversive stimulus (intense anxiety and chest tension), which causes the behavior to increase over time. This is the defining mechanism of negative reinforcement: aversive stimulus removal strengthens the preceding behavior."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-042-vignette-L3",
      "source_question_id": "042",
      "source_summary": "Negative reinforcement occurs when a behavior continues or increases because performing the behavior results in the termination or withdrawal of a stimulus, such as a woman eating chocolate because it reduces her anxiety.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "escape"
      ],
      "vignette": "A graduate student named Priya has been struggling to manage her fear of social evaluation during lab meetings. She has noticed that when she excuses herself to use the restroom during particularly intense discussions, the knot in her stomach immediately loosens and she feels relief. Her supervisor comments that Priya seems to leave the room quite frequently and that her absences have become more regular over the past semester. A colleague suggests that Priya's pattern might be related to her history of receiving critical feedback in group settings, which originally caused her distress.",
      "question": "Which principle best explains why Priya's behavior of leaving the room has become more frequent over time?",
      "options": {
        "A": "Classical conditioning, because Priya's physiological distress was originally paired with critical feedback in group settings, making the lab meeting itself a conditioned stimulus.",
        "B": "Positive punishment, because exposure to the aversive evaluation context each time Priya stays in the room suppresses her willingness to participate, thereby shaping her exit behavior.",
        "C": "Negative reinforcement, because Priya's escape behavior is strengthened by the immediate removal of the aversive stimulus — the social anxiety and stomach tension — each time she leaves the room.",
        "D": "Avoidance conditioning, because Priya has learned to exit the room before the most anxiety-provoking moments occur, preventing the onset of her full fear response."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Classical conditioning explains how Priya's anxiety became associated with lab meetings (a plausible mechanism for how her fear developed), and the vignette's reference to critical feedback history is a deliberate red herring supporting this option. However, classical conditioning does not explain why her exit behavior is increasing — that is an operant phenomenon governed by its consequences, not by stimulus–response associations.",
        "B": "Positive punishment involves presenting an aversive stimulus following a behavior to decrease that behavior. While the aversive evaluation context is real, it does not function as a consequence delivered after her exit; rather, the relief following exit is what shapes her behavior. This option inverts the operant logic and describes the wrong mechanism.",
        "C": "This is correct. Priya's escape behavior — leaving the room — is an operant response that is immediately followed by the removal of an aversive internal stimulus (anxiety, stomach tension). This removal functions as negative reinforcement, increasing the frequency of the escape behavior over time. The colleague's comment about her history explains fear acquisition, not the increase in leaving behavior.",
        "D": "Avoidance conditioning is a highly plausible distractor here, and the vignette is designed to suggest it. However, avoidance involves leaving or acting before the aversive stimulus fully arrives to prevent it. Priya's pattern shows she leaves during intense discussions — meaning she is escaping an aversive state already present, not anticipating and preventing it. This makes the behavior escape, not avoidance, though both involve negative reinforcement."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-042-vignette-L4",
      "source_question_id": "042",
      "source_summary": "Negative reinforcement occurs when a behavior continues or increases because performing the behavior results in the termination or withdrawal of a stimulus, such as a woman eating chocolate because it reduces her anxiety.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "contingent"
      ],
      "vignette": "A behavioral researcher is observing a participant, Dana, in a laboratory stress paradigm. When presented with a loud, continuous white noise tone, Dana learns that pressing a lever stops the tone. Initially Dana presses the lever slowly and at irregular intervals, but after several trials the lever-pressing becomes rapid and consistent. The researcher notes in her log that Dana's response rate is clearly contingent on the relationship between the behavior and its outcome, and that the outcome involves something being withdrawn from Dana's environment rather than delivered to it. Interestingly, the researcher initially hypothesizes that the pattern reflects an appetitive learning process, given how eagerly Dana presses the lever.",
      "question": "What operant learning process most precisely accounts for the increase in Dana's lever-pressing behavior?",
      "options": {
        "A": "Positive reinforcement, because the cessation of noise functions as an appetitive outcome — Dana perceives the quiet as a reward — and this reward increases lever-pressing behavior.",
        "B": "Negative reinforcement, because a behavior (lever-pressing) increases in frequency due to the contingent removal of an aversive environmental stimulus (the white noise tone).",
        "C": "Escape conditioning via classical mechanisms, because the white noise is an unconditioned aversive stimulus that elicits a reflexive response, and lever-pressing becomes a conditioned response to terminate it.",
        "D": "Omission training, because the absence of noise is contingent on lever-pressing, structuring the contingency as the withholding of a stimulus rather than its removal."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is the intended first-read distractor. The researcher's own hypothesis about 'appetitive' learning and Dana's eagerness are red herrings designed to suggest positive reinforcement. However, the defining feature of positive reinforcement is the addition of a stimulus, not its removal. The quiet is not a stimulus added to the environment — it results from the termination of the noise. The mechanism is subtractive, which defines negative reinforcement.",
        "B": "This is correct. Negative reinforcement is precisely defined as an increase in behavior due to the contingent removal of an aversive stimulus. Dana's lever-pressing increases because it reliably terminates the aversive white noise tone. The researcher's log explicitly notes the outcome involves 'something being withdrawn,' and the behavior is increasing — both hallmarks of negative reinforcement.",
        "C": "This option exploits a common confusion between classical and operant conditioning. While classical conditioning could explain how the noise became aversive and elicits distress, the lever-pressing behavior is an operant response shaped by its consequences (noise termination), not a reflexive conditioned response to a stimulus. The mechanism increasing lever-pressing is operant, not classical.",
        "D": "Omission training (also called differential reinforcement of other behavior, or DRO) involves delivering a reinforcer only when the target behavior does NOT occur during a specified interval — it is used to decrease behavior by withholding reinforcement contingent on its occurrence. The vignette describes a procedure where pressing the lever terminates the noise, which is escape/negative reinforcement, not a procedure designed to reduce behavior through reinforcer omission."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-042-vignette-L5",
      "source_question_id": "042",
      "source_summary": "Negative reinforcement occurs when a behavior continues or increases because performing the behavior results in the termination or withdrawal of a stimulus, such as a woman eating chocolate because it reduces her anxiety.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "Elena, a 28-year-old woman, grew up in a household where she frequently felt criticized and dismissed. In her adult life, she has developed a strong habit of sending long, detailed text messages to friends whenever she begins to feel a creeping sense of loneliness or rejection. She reports that within minutes of sending these messages, even before receiving any reply, the uncomfortable feeling in her chest eases considerably. Her therapist observes that Elena sends these messages more and more frequently as time goes on, and that they are sent earlier and earlier at the first hint of discomfort. A colleague reviewing the case suggests this pattern likely developed because Elena craves social connection — a positive outcome she is seeking.",
      "question": "Which learning principle most precisely accounts for the increase in Elena's text-messaging behavior over time?",
      "options": {
        "A": "Positive reinforcement, because Elena's messaging is motivated by the desire for social connection and validation — an appetitive goal she is moving toward — and the relief she feels reflects the attainment of that desired outcome.",
        "B": "Avoidance conditioning, because Elena is sending messages increasingly early, before her loneliness fully peaks, suggesting she is acting to prevent the full onset of the aversive emotional state rather than escaping one already fully experienced.",
        "C": "Negative reinforcement, because Elena's messaging behavior consistently increases due to the immediate reduction of an aversive internal state — the discomfort and sense of rejection — that occurs after she sends the message, independent of any reply.",
        "D": "Operant extinction burst, because the increasing frequency and earlier timing of Elena's messages reflects an escalation pattern typically seen when a previously reinforced behavior is no longer being reinforced, prompting the organism to respond more intensely."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is the most tempting distractor. The colleague's framing about Elena 'craving social connection' and the language of 'moving toward a desired outcome' strongly implies positive reinforcement. However, the key distinguishing detail is that Elena's relief occurs before receiving any reply — meaning no social stimulus is actually delivered. The reduction in discomfort is internal and results from the removal of an aversive feeling, not from the addition of a pleasant social reward. This rules out positive reinforcement.",
        "B": "Avoidance conditioning is a highly plausible distractor because Elena does send messages earlier and earlier at the 'first hint' of discomfort, suggesting anticipatory action. However, the vignette specifies she acts when she 'begins to feel' the discomfort — the aversive state has already begun. Avoidance involves acting before the aversive stimulus arrives to prevent it entirely. Additionally, the relief she feels confirms an aversive state was present and is being terminated, which is the hallmark of escape-based negative reinforcement rather than avoidance.",
        "C": "This is correct. The critical distinguishing detail is that Elena's relief occurs before any reply arrives — meaning the reinforcing consequence is entirely the reduction of an internal aversive state (the uncomfortable feeling in her chest), not any external social reward. This is classic negative reinforcement: an operant behavior increases because it reliably terminates an aversive stimulus. The increasing frequency and earlier timing reflect the behavior being well-reinforced, consistent with an escape pattern strengthened over time.",
        "D": "An extinction burst refers to a temporary increase in a behavior's rate or intensity when reinforcement that previously maintained it is suddenly removed. Elena's messaging is increasing because it is being reinforced (the aversive feeling reduces each time), not because reinforcement has been withdrawn. The escalating pattern here reflects successful reinforcement, not the characteristic desperate escalation seen during extinction."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-020-vignette-L1",
      "source_question_id": "020",
      "source_summary": "Spontaneous recovery refers to the return of an extinguished conditioned response without the conditioned stimulus being presented again with the unconditioned stimulus, confirming that the conditioned response is suppressed rather than eliminated by extinction trials.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "extinction",
        "conditioned response",
        "spontaneous recovery"
      ],
      "vignette": "A researcher conditions a dog to salivate in response to a tone by repeatedly pairing the tone with food. After many extinction trials in which the tone is presented without food, the conditioned response of salivation disappears. The dog is then removed from the laboratory for 24 hours. When the tone is presented again the following day — still without any food — the dog salivates at a moderate level, even though no additional conditioning has taken place.",
      "question": "Which classical conditioning phenomenon best explains the return of salivation after the rest interval?",
      "options": {
        "A": "Stimulus generalization",
        "B": "Spontaneous recovery",
        "C": "Higher-order conditioning",
        "D": "Disinhibition"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Stimulus generalization refers to the elicitation of a conditioned response by stimuli that are similar to, but not identical to, the original conditioned stimulus. The scenario involves the identical tone, not a novel stimulus, so generalization does not explain the phenomenon.",
        "B": "Spontaneous recovery is the reappearance of an extinguished conditioned response after a period of rest, without any additional pairings of the conditioned and unconditioned stimuli. The dog's salivation returns following a 24-hour rest interval exactly as this phenomenon predicts, confirming that extinction suppresses rather than eliminates the learned association.",
        "C": "Higher-order conditioning occurs when a previously established conditioned stimulus is used to condition a new, neutral stimulus, creating a second-order conditioned response. This process requires new pairings and a different stimulus sequence; it does not account for the return of a response following extinction.",
        "D": "Disinhibition involves the temporary recovery of an extinguished conditioned response due to the presentation of a novel, extraneous stimulus — not a rest period alone. Because no novel external stimulus is introduced in this scenario, disinhibition does not fit the described pattern."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-020-vignette-L2",
      "source_question_id": "020",
      "source_summary": "Spontaneous recovery refers to the return of an extinguished conditioned response without the conditioned stimulus being presented again with the unconditioned stimulus, confirming that the conditioned response is suppressed rather than eliminated by extinction trials.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "extinction",
        "rest interval"
      ],
      "vignette": "A therapist uses repeated exposure sessions to help a client with a dog phobia reduce her fear response to a photograph of a large dog. The client, who also has generalized anxiety disorder, shows no visible distress by the end of the fourth session. Three weeks later, the client returns for a follow-up appointment and reports feeling notably anxious when she encountered the same photograph while waiting in the lobby — despite no negative incident involving dogs occurring during the intervening weeks.",
      "question": "The client's renewed fear response after a period of no treatment is most consistent with which learning phenomenon?",
      "options": {
        "A": "Stimulus generalization",
        "B": "Spontaneous recovery",
        "C": "Renewal effect",
        "D": "Reconditioning"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Spontaneous recovery is the return of an extinguished conditioned response after a rest period, without re-pairing the conditioned stimulus with the unconditioned stimulus. The client's fear returns following a three-week gap between sessions with no new aversive experience, which is the hallmark of spontaneous recovery.",
        "A": "Stimulus generalization would predict that a stimulus different from the original conditioned stimulus also elicits fear. Here the same photograph is involved and no new stimulus triggers the response, making generalization an incorrect explanation.",
        "C": "The renewal effect refers to the return of an extinguished conditioned response when the organism is returned to the original acquisition context after extinction occurred in a different context. The vignette does not describe a context change; the photograph is encountered in the same clinic setting, so renewal does not apply.",
        "D": "Reconditioning refers to reacquisition of the conditioned response through new pairings of the conditioned and unconditioned stimuli. The client experienced no new frightening encounter with dogs, so there is no basis for reconditioning; the response returned solely from the passage of time."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-020-vignette-L3",
      "source_question_id": "020",
      "source_summary": "Spontaneous recovery refers to the return of an extinguished conditioned response without the conditioned stimulus being presented again with the unconditioned stimulus, confirming that the conditioned response is suppressed rather than eliminated by extinction trials.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "suppressed"
      ],
      "vignette": "A graduate student conducts a study in which rats learn to freeze in response to a light cue that was previously paired with a mild foot shock. The student then presents the light many times without shock until freezing drops to near zero. She notes in her lab journal that the learned fear appears to be 'suppressed rather than eliminated.' One week later, when she places the same rats back in the original chamber and turns on the light — without delivering any shock — the rats display robust freezing behavior once more.",
      "question": "What phenomenon does the rats' freezing behavior upon the light's reappearance after one week best illustrate?",
      "options": {
        "A": "Spontaneous recovery",
        "B": "Renewal effect",
        "C": "Disinhibition",
        "D": "Incubation of fear"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Spontaneous recovery is the reappearance of an extinguished conditioned response following a rest interval, without any new conditioned stimulus–unconditioned stimulus pairings. The one-week gap accounts for the return of freezing, and the journal note that the response was 'suppressed rather than eliminated' is precisely the theoretical basis for spontaneous recovery.",
        "B": "The renewal effect is a plausible distractor because the rats are tested in the original acquisition chamber. However, renewal specifically requires that extinction took place in a context different from the acquisition context (ABA, AAB, or ABC design). If extinction also occurred in the same original chamber, the context has not changed, and the return of the response is better attributed to the rest interval — spontaneous recovery.",
        "C": "Disinhibition involves a temporary reinstatement of an extinguished response triggered by a novel, extraneous stimulus presented alongside the conditioned stimulus. No new external stimulus is introduced in this scenario; the only change is the passage of time, ruling out disinhibition.",
        "D": "Incubation of fear refers to an increase in conditioned fear over time when brief, unreinforced exposures to the feared stimulus occur during the interval. The vignette describes no exposures to the light cue during the week-long gap, so incubation does not explain the returned freezing."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-020-vignette-L4",
      "source_question_id": "020",
      "source_summary": "Spontaneous recovery refers to the return of an extinguished conditioned response without the conditioned stimulus being presented again with the unconditioned stimulus, confirming that the conditioned response is suppressed rather than eliminated by extinction trials.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "inhibitory"
      ],
      "vignette": "A researcher is studying how learning is retained after treatment. She trains rabbits to blink in response to a tone by pairing the tone with an air puff directed at the eye. After successful acquisition, she conducts extensive unreinforced tone presentations across multiple sessions, and blinking drops to baseline levels. She theorizes that an inhibitory process now actively competes with the original excitatory association. Eighteen days later, she tests the rabbits with the tone alone and observes a clear, though attenuated, blink response — a result that her colleague argues instead demonstrates that the original context for testing has changed over the long interval.",
      "question": "The researcher's theoretical interpretation — centered on the inhibitory process — best supports which classical conditioning concept as the explanation for the returned blinking?",
      "options": {
        "A": "Renewal effect",
        "B": "Reconditioning",
        "C": "Spontaneous recovery",
        "D": "Latent inhibition"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Spontaneous recovery is explained theoretically by the weakening of the inhibitory process formed during extinction over the passage of time, allowing the original excitatory conditioned association to express itself again. The researcher's framing — that an inhibitory mechanism competes with excitation and that a rest period allows the response to return — is the precise mechanistic account of spontaneous recovery, confirmed by the 18-day interval without re-pairing.",
        "A": "The renewal effect is deliberately seeded as a red herring by the colleague's remark about context change over the long interval. Renewal, however, requires a demonstrable shift in physical or environmental context, not merely the passage of time. The vignette provides no evidence of a context change; the inhibitory-process framing the researcher uses is the theoretical account for spontaneous recovery, not renewal.",
        "B": "Reconditioning requires new pairings of the conditioned stimulus with the unconditioned stimulus to reestablish the response. No additional tone-air puff pairings occurred during the 18-day gap, so reconditioning cannot explain the returned blinking.",
        "D": "Latent inhibition occurs when extensive pre-exposure to a stimulus before conditioning slows the rate of subsequent acquisition with that stimulus. It is relevant to the acquisition phase, not to the return of a response after extinction, and the inhibitory mechanism described in the vignette refers to extinction inhibition, not pre-exposure inhibition."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-020-vignette-L5",
      "source_question_id": "020",
      "source_summary": "Spontaneous recovery refers to the return of an extinguished conditioned response without the conditioned stimulus being presented again with the unconditioned stimulus, confirming that the conditioned response is suppressed rather than eliminated by extinction trials.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A woman who survived a car accident several years ago spent many months working with a professional to reduce the intense physical tension and dread she felt whenever she heard tires screeching. By the end of her work with the professional, she could listen to recordings of tire sounds in session without any noticeable reaction. She moved to a new city, began a demanding new job, and did not revisit the recordings or any related material for over a year. One afternoon, while walking near a busy intersection, she unexpectedly heard a car brake sharply and felt a wave of dread wash over her — nearly as intense as in the early days of her treatment — even though nothing dangerous actually happened.",
      "question": "Which learning-based phenomenon most precisely accounts for the return of the woman's dread response after more than a year without any new traumatic event?",
      "options": {
        "A": "Renewal effect",
        "B": "Reconditioning",
        "C": "Spontaneous recovery",
        "D": "Incubation of fear"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Spontaneous recovery is the return of a previously extinguished conditioned response after a rest period, without any new aversive pairing. The woman's dread response had been reduced to zero through treatment (extinction), and it returned after a prolonged interval — over a year — with no new traumatic experience. This precisely matches spontaneous recovery, which demonstrates that extinction suppresses rather than permanently erases the conditioned association.",
        "A": "The renewal effect is the strongest distractor here because the woman has moved to a new city and encounters the sound at a busy intersection — a very different environment from the clinical setting where extinction occurred. Renewal would predict that returning to the original acquisition context (or a novel context) reinstates the response. However, the defining feature of renewal is the context change as the mechanism; spontaneous recovery is the better explanation when the defining variable is the passage of time without intervening exposures, which is explicitly emphasized in the vignette ('did not revisit the recordings or any related material for over a year').",
        "B": "Reconditioning requires new pairings of the triggering event with an aversive outcome to rebuild the fear response. The vignette explicitly states that nothing dangerous actually happened during the intersection episode, ruling out a new traumatic learning event and therefore ruling out reconditioning.",
        "D": "Incubation of fear describes the paradoxical increase in conditioned fear over time when brief, unreinforced exposures to the feared stimulus occur during a rest interval, gradually strengthening rather than weakening the response. The vignette emphasizes that the woman had no contact with the recordings or related material for over a year, making incubation — which requires intermittent sub-threshold exposures — an unsuitable explanation."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-064-vignette-L1",
      "source_question_id": "064",
      "source_summary": "Reducing the amount of positive reinforcement, or \"thinning\" the reinforcement schedule, is referred to as thinning.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "positive reinforcement",
        "thinning",
        "reinforcement schedule"
      ],
      "vignette": "A behavioral therapist is working with a child with autism who has just learned to request items using a picture exchange system. Initially, every correct request earns the child a sticker (continuous positive reinforcement). To promote long-term maintenance of the behavior, the therapist begins gradually reducing how often the stickers are provided — moving from every response to every third response, then every fifth. The therapist explains that this systematic reduction in the reinforcement schedule is a necessary step before removing supports entirely.",
      "question": "Which operant conditioning procedure is the therapist implementing when systematically reducing the frequency of positive reinforcement delivered for the child's correct requests?",
      "options": {
        "A": "Thinning",
        "B": "Extinction",
        "C": "Differential reinforcement",
        "D": "Shaping"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. Thinning refers to the systematic reduction in the amount or frequency of positive reinforcement delivered for a target behavior. The therapist is gradually decreasing how often the child receives stickers, which is precisely the definition of thinning a reinforcement schedule.",
        "B": "This is incorrect. Extinction involves withholding all reinforcement for a previously reinforced behavior, causing the behavior to decrease or cease. Here, reinforcement is being reduced gradually — not eliminated — so this is thinning, not extinction.",
        "C": "This is incorrect. Differential reinforcement involves reinforcing one class of behavior while withholding reinforcement for another (e.g., DRI, DRO, DRA). The scenario describes no differentiation between behaviors; rather, the same behavior is reinforced less frequently over time.",
        "D": "This is incorrect. Shaping involves the successive reinforcement of approximations toward a target behavior. The child has already learned the target behavior; the therapist is changing how often reinforcement is delivered, not reinforcing closer and closer approximations to a new behavior."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-064-vignette-L2",
      "source_question_id": "064",
      "source_summary": "Reducing the amount of positive reinforcement, or \"thinning\" the reinforcement schedule, is referred to as thinning.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "reinforcement",
        "maintenance"
      ],
      "vignette": "A school psychologist has been helping a 9-year-old boy with ADHD increase his on-task behavior during independent reading. After weeks of receiving a token for every 5 minutes of on-task behavior, the boy's performance is stable and consistent. The psychologist, noting that the boy's parents are concerned about dependency on external rewards, decides to restructure the program so that tokens are now delivered only after every 15 minutes of on-task behavior, then eventually only after completing a full reading period. This change is intended to support long-term maintenance of the behavior in the natural environment.",
      "question": "What procedure is the psychologist using when progressively reducing how often tokens are delivered for the boy's on-task behavior?",
      "options": {
        "A": "Fading",
        "B": "Thinning",
        "C": "Chaining",
        "D": "Satiation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Thinning refers to systematically reducing the density or frequency of positive reinforcement to promote maintenance of behavior without continuous external support. The psychologist is progressively extending the ratio of on-task behavior required per token, which is the defining feature of thinning.",
        "A": "This is incorrect. Fading involves the gradual removal of prompts or discriminative stimuli (not reinforcement) that support a behavior. The psychologist is changing the delivery of reinforcers, not reducing instructional prompts, so fading does not apply here.",
        "C": "This is incorrect. Chaining is a procedure for teaching complex behaviors by linking discrete steps together in sequence. The scenario does not involve building a multi-step behavior sequence; it involves reducing how often an already-learned behavior is reinforced.",
        "D": "This is incorrect. Satiation occurs when a reinforcer loses its effectiveness because the individual has received too much of it. While the parents' concern about dependency might superficially suggest satiation, the psychologist is deliberately and systematically reducing reinforcement frequency — a planned procedural change, not a loss of reinforcer value."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-064-vignette-L3",
      "source_question_id": "064",
      "source_summary": "Reducing the amount of positive reinforcement, or \"thinning\" the reinforcement schedule, is referred to as thinning.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "schedule"
      ],
      "vignette": "A behavioral consultant is working with a group home for adults with intellectual disabilities. One resident, Marcus, has learned to greet staff appropriately after a structured training program in which he received praise and a small food item after each greeting. Marcus's behavior is now stable. The consultant instructs staff to begin requiring two greetings before delivering praise and food, then four, then eight — following a progressively increasing ratio. Staff note that Marcus's greeting behavior remains strong throughout this process, which the consultant says is the intended outcome. The consultant explains that the goal is for Marcus's behavior to eventually persist with far less frequent programmatic support.",
      "question": "What behavioral procedure is the consultant implementing through this progressive increase in the response requirement before reinforcement is delivered?",
      "options": {
        "A": "Ratio strain induction",
        "B": "Shaping via successive approximations",
        "C": "Thinning",
        "D": "Fixed ratio schedule implementation"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. Thinning involves gradually reducing the frequency or density of positive reinforcement for an already-established behavior to promote its long-term maintenance. By progressively increasing the number of responses required before reinforcement is delivered, the consultant is systematically thinning the schedule of reinforcement.",
        "A": "This is incorrect. Ratio strain refers to a breakdown in behavior that occurs when the ratio requirement is increased too rapidly, resulting in decreased or erratic responding. While ratio strain is a risk during thinning, it is not the procedure itself — and the scenario explicitly notes that Marcus's behavior remains strong, ruling this out as what is being described.",
        "B": "This is incorrect. Shaping via successive approximations involves reinforcing behaviors that incrementally resemble a target behavior not yet in the individual's repertoire. Here, Marcus already performs the target behavior (appropriate greeting); the procedure is altering how often that established behavior is reinforced, not building a new behavior step by step.",
        "D": "This is incorrect. Implementing a fixed ratio schedule means delivering reinforcement after a set, unchanging number of responses (e.g., always after every 5 responses). The consultant is progressively increasing the ratio over time, which is characteristic of thinning, not simply placing behavior on a fixed ratio schedule."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-064-vignette-L4",
      "source_question_id": "064",
      "source_summary": "Reducing the amount of positive reinforcement, or \"thinning\" the reinforcement schedule, is referred to as thinning.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "ratio"
      ],
      "vignette": "A clinical behavior analyst is transitioning a young woman named Dana out of a structured intervention program for compulsive hand-washing. Dana initially received therapist praise and access to preferred activities after every successful 15-minute delay of a washing ritual. Over several months, the behavior analyst has been systematically expanding these delays and adjusting the response demands required before access to preferred activities is provided, always ensuring Dana's behavior remains stable before moving forward. The behavior analyst documents that this gradual restructuring is designed to approximate the naturally occurring contingencies Dana will encounter after discharge. Notably, the overall amount of programmatic reinforcement Dana receives per session has steadily declined across the program.",
      "question": "Which specific operant procedure best characterizes the behavior analyst's approach to gradually reducing the density of programmatic reinforcement while maintaining Dana's treatment gains?",
      "options": {
        "A": "Graduated extinction",
        "B": "Thinning",
        "C": "Prompt fading",
        "D": "Behavioral momentum building"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Thinning is the systematic reduction in the frequency or density of positive reinforcement delivered for a target behavior. Although the scenario is framed in OCD treatment terms and the peripheral hint word 'ratio' is embedded structurally, the defining feature is that the total amount of programmatic reinforcement per session is declining in a controlled, stepwise manner to support maintenance — the precise definition of thinning.",
        "A": "This is incorrect. Graduated extinction refers to a variant of extinction used in sleep training (e.g., Ferber method) and does not describe a procedure for systematically reducing positive reinforcement density while maintaining behavior. Extinction removes all reinforcement, whereas Dana continues to receive reinforcement throughout — just less frequently.",
        "C": "This is incorrect. Prompt fading involves the systematic removal of instructional or physical prompts that cue a behavior, not the reduction of reinforcement frequency. While both fading and thinning involve gradual reduction, fading targets antecedent supports (prompts), not consequential supports (reinforcers). The scenario does not describe removal of prompts.",
        "D": "This is incorrect. Behavioral momentum refers to the persistence of a behavior in the face of disruptions, often built by delivering high rates of reinforcement for easy responses before introducing harder demands. Although behavioral momentum involves reinforcement density, the scenario describes deliberate reduction of reinforcement over time to promote independence — the opposite direction from building momentum."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-064-vignette-L5",
      "source_question_id": "064",
      "source_summary": "Reducing the amount of positive reinforcement, or \"thinning\" the reinforcement schedule, is referred to as thinning.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A consultant is supporting a residential treatment facility for adolescents with developmental disabilities. One teenager, Leo, consistently clears his dinner tray when staff stand beside him and immediately hand him his tablet after each instance. The consultant notices that Leo becomes agitated and stops clearing trays whenever staff are absent or delayed in providing the tablet. To address this, the consultant instructs staff to begin waiting until Leo has cleared his tray twice before handing over the tablet, then three times, then five. Staff are concerned that this change will cause Leo to stop performing the behavior, but over several weeks, Leo continues clearing trays reliably even as the requirements increase. The consultant explains that the current approach is specifically designed so that the behavior will eventually persist without staff needing to respond after every instance, gradually approximating what would happen in a less structured home environment.",
      "question": "What behavioral procedure is the consultant implementing?",
      "options": {
        "A": "Systematic desensitization to staff absence",
        "B": "Thinning",
        "C": "Stimulus control transfer",
        "D": "Differential reinforcement of high rates of behavior"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Thinning involves the systematic reduction in the frequency or density of positive reinforcement delivered for an already-established behavior to support its long-term maintenance. Although the scenario contains no behavioral jargon, the consultant is progressively increasing the number of responses required before Leo receives the tablet — steadily reducing overall reinforcement density — with the explicit goal of maintaining behavior without frequent programmatic support. This is the defining characteristic of thinning.",
        "A": "This is incorrect. Systematic desensitization is a classical conditioning-based technique for reducing conditioned fear responses by pairing a feared stimulus with relaxation, using a graduated hierarchy. While Leo does become agitated when staff are absent (suggesting a stimulus control issue), the intervention does not involve relaxation training or graduated exposure to feared stimuli — it involves altering a reinforcement contingency.",
        "C": "This is incorrect. Stimulus control transfer involves shifting a behavior from responding to one antecedent stimulus to responding to a different (often more natural) stimulus, such as transferring responding from an artificial prompt to a natural cue. The scenario does show concern about Leo's dependence on staff presence, which could suggest stimulus control; however, the consultant's actual procedure — increasing the number of responses required per reinforcer — targets the consequent side of the contingency, not the antecedent, making thinning the more precise answer.",
        "D": "This is incorrect. Differential reinforcement of high rates of behavior (DRH) is a procedure in which reinforcement is delivered only when the rate of behavior exceeds a specified criterion, used to increase low-rate behaviors. The consultant is not reinforcing Leo for performing at a high rate within a time window; rather, the consultant is requiring an increasing number of cumulative responses per reinforcer delivery, which is a ratio-thinning procedure distinct from DRH."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-009-vignette-L1",
      "source_question_id": "009",
      "source_summary": "Stimulus generalization occurs when stimuli similar to the original conditioned stimulus elicit the conditioned response without ever being presented with the unconditioned stimulus.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "conditioned stimulus",
        "stimulus generalization",
        "conditioned response"
      ],
      "vignette": "A researcher conditions a dog to salivate in response to a 1000 Hz tone by repeatedly pairing the tone with food. After conditioning is complete, the researcher tests the dog with tones of 900 Hz and 800 Hz, neither of which was ever paired with food. The dog produces a conditioned response to both novel tones, with response strength decreasing as the tones deviate further from the original conditioned stimulus. This pattern — in which stimuli similar to the original CS elicit the conditioned response without separate pairings — is a well-documented classical conditioning phenomenon.",
      "question": "Which classical conditioning phenomenon best explains the dog's salivation to the novel tones?",
      "options": {
        "A": "Stimulus generalization",
        "B": "Higher-order conditioning",
        "C": "Stimulus discrimination",
        "D": "Spontaneous recovery"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. Stimulus generalization occurs when stimuli similar to the original CS elicit the CR without ever being directly paired with the UCS. The gradient of decreasing response strength as stimuli deviate from the original CS is the classic generalization gradient, precisely what the scenario describes.",
        "B": "Higher-order conditioning occurs when a previously established CS is used as a surrogate UCS to condition a new, neutral stimulus. In this scenario no established CS is being used to train new stimuli; the novel tones were simply tested, not paired with anything.",
        "C": "Stimulus discrimination is the opposite process — the organism learns to respond only to the specific CS and not to similar stimuli. The vignette describes responding to novel tones, not a failure to respond to them, which rules out discrimination.",
        "D": "Spontaneous recovery refers to the reappearance of an extinguished CR after a rest period. There is no mention of extinction or a rest interval in this scenario; the CR to the novel tones results from similarity to the CS, not from recovery after extinction."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-009-vignette-L2",
      "source_question_id": "009",
      "source_summary": "Stimulus generalization occurs when stimuli similar to the original conditioned stimulus elicit the conditioned response without ever being presented with the unconditioned stimulus.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "conditioned fear",
        "generalization"
      ],
      "vignette": "A 34-year-old veteran was involved in a roadside explosion while deployed overseas and now reports intense conditioned fear when he hears any loud sudden noise, including a car backfiring, a slammed door, or a dropped object. He has been diagnosed with PTSD and also struggles with insomnia, which his treatment team attributes to hyperarousal. Notably, none of the civilian sounds that now trigger his fear were present during the original traumatic event. His psychologist notes that his fear responses are strongest to sounds most similar to the explosion and weaker for more distant sounds.",
      "question": "The veteran's fear response to civilian sounds that were never part of the original traumatic event most directly illustrates which learning principle?",
      "options": {
        "A": "Incubation",
        "B": "Generalization",
        "C": "Second-order conditioning",
        "D": "Sensitization"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Generalization accounts for the veteran's fear response to sounds that were never directly paired with the traumatic event (UCS). The gradient — stronger responses to sounds more similar to the explosion — is the hallmark of a generalization gradient, confirming this as the best explanation.",
        "A": "Incubation refers to the paradoxical strengthening of a conditioned fear response over time with brief, unreinforced exposures to the CS. While incubation is relevant to PTSD-related fears, it explains why the fear grows stronger over time, not why it extends to novel stimuli that were never paired with the UCS.",
        "C": "Second-order conditioning involves pairing a new neutral stimulus with an already-established CS to produce a CR. This requires explicit training trials with the established CS; the scenario describes no such pairing procedure — the civilian sounds simply resemble the original trigger.",
        "D": "Sensitization is a non-associative process in which repeated exposure to a stimulus leads to an amplified response, and it does not require any prior CS–UCS pairing. The veteran's responses are organized around similarity to a specific event (the explosion), indicating an associative, CS-based process rather than generalized physiological arousal."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-009-vignette-L3",
      "source_question_id": "009",
      "source_summary": "Stimulus generalization occurs when stimuli similar to the original conditioned stimulus elicit the conditioned response without ever being presented with the unconditioned stimulus.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "extinction"
      ],
      "vignette": "A woman was bitten by a large black Labrador as a child and subsequently became very distressed whenever she encountered large dogs. She later underwent a successful course of exposure therapy, and her distress to large black Labradors was fully eliminated through extinction. Several months after treatment, her therapist is surprised to learn that the woman still becomes noticeably anxious around large brown or golden retrievers, even though these dogs had never caused her harm and were never targeted in treatment. The therapist had assumed that eliminating the response to the original stimulus would resolve her broader reactions as well.",
      "question": "Which phenomenon most precisely explains why the woman continues to experience anxiety around brown and golden retrievers despite successful extinction of her response to the original stimulus?",
      "options": {
        "A": "Spontaneous recovery",
        "B": "Renewal",
        "C": "Stimulus generalization",
        "D": "Disinhibition"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. Stimulus generalization explains why stimuli similar to the original CS (large black Labrador) — in this case, other large dogs of different colors — elicit the CR even without ever being paired with the UCS (the bite). Extinction of the original CS does not automatically extinguish responses to generalized stimuli, which is precisely the clinical problem described.",
        "A": "Spontaneous recovery is the return of an extinguished CR to the original CS after a rest period. The therapist confirmed that extinction to the original stimulus was successful; the problem is not the CR returning to the Labrador but rather its presence for novel, similar stimuli — a distinct phenomenon.",
        "B": "Renewal occurs when an extinguished CR reappears when the individual encounters the original CS in a context different from where extinction occurred. The vignette emphasizes that the anxiety is triggered by different stimuli (golden retrievers), not by encountering the original CS (black Labrador) in a new setting.",
        "D": "Disinhibition refers to the temporary restoration of an extinguished CR following the presentation of a novel, extraneous stimulus — not the systematic pattern of responding based on physical similarity to the original CS. The woman's anxiety gradient across dog types reflects similarity-based responding, not disinhibition."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-009-vignette-L4",
      "source_question_id": "009",
      "source_summary": "Stimulus generalization occurs when stimuli similar to the original conditioned stimulus elicit the conditioned response without ever being presented with the unconditioned stimulus.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "similar"
      ],
      "vignette": "A clinical researcher is studying a participant who developed a strong nausea response after receiving chemotherapy treatments in a hospital infusion room with distinctive teal-painted walls. After completing treatment, the participant reports nausea not only in the original infusion room but also in a differently located clinic with pale green walls, a color he has never encountered during chemotherapy. The researcher notes that the participant does not experience nausea in rooms with red or yellow walls. Interestingly, the participant reports that his reaction to the pale green room feels slightly less intense than his reaction to the original teal room, a detail the researcher considers theoretically significant. The researcher rules out a return of the original conditioned reaction because this green clinic was visited for the first time only after chemotherapy was complete.",
      "question": "The theoretical significance the researcher attaches to the difference in intensity between the response to the original room and the response to the pale green room best reflects which underlying principle?",
      "options": {
        "A": "Occasion setting",
        "B": "The generalization gradient",
        "C": "Second-order conditioning",
        "D": "Semantic generalization"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The generalization gradient is the specific mechanism within stimulus generalization that describes systematically decreasing response strength as test stimuli deviate further from the original CS. The researcher's focus on the reduced intensity of response to the pale green room — compared to the original teal room — is precisely the theoretically significant feature of the generalization gradient, not merely that the response occurred at all.",
        "A": "Occasion setting refers to a stimulus that modulates whether a CS will elicit a CR, essentially setting the occasion for the CS–CR relationship to hold. An occasion setter does not itself elicit the CR directly; it signals when another CS is effective. The pale green room is directly eliciting the nausea, making occasion setting an incorrect fit.",
        "C": "Second-order conditioning would require the pale green room to have been explicitly paired with an established CS (e.g., the teal room) before the nausea response was acquired to it. The scenario explicitly states the participant first visited the pale green clinic only after chemotherapy was complete, ruling out any pairing procedure.",
        "D": "Semantic generalization refers to the spread of a CR to stimuli that are conceptually or linguistically related to the original CS, not physically similar. Teal and pale green are perceptually similar colors on the physical spectrum; the response is spreading along a dimension of physical resemblance, not semantic or symbolic similarity."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-009-vignette-L5",
      "source_question_id": "009",
      "source_summary": "Stimulus generalization occurs when stimuli similar to the original conditioned stimulus elicit the conditioned response without ever being presented with the unconditioned stimulus.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A young boy spent a week in a hospital recovering from a painful surgical procedure. During his stay, the ward was staffed by nurses wearing light blue scrubs. Upon returning home, the boy began crying and expressing intense reluctance whenever his parents dressed him in any light-colored shirt before leaving for school, a situation that had never previously caused distress. His parents attributed this to school anxiety and sought a behavioral consultation. The consultant noted that the boy's distress was markedly stronger to pale blue shirts than to pale yellow or white shirts, and that the boy showed no unusual distress during the hospitalization itself — only afterward when exposed to similar-colored clothing at home. The consultant also confirmed that no particular event at school had occurred around the time the new distress began.",
      "question": "The pattern the consultant observed — particularly the detail about the relative intensity of responses across shirt colors — is most precisely explained by which learning principle?",
      "options": {
        "A": "The generalization gradient in classical conditioning",
        "B": "Avoidance learning maintained by negative reinforcement",
        "C": "Incubation of a conditioned fear response",
        "D": "Context-dependent retrieval of a traumatic memory"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. The boy's distress to pale blue shirts (physically similar to the nurses' light blue scrubs — the CS paired with the painful procedure) that were never present during the hospitalization reflects stimulus generalization. Crucially, the graded intensity — strongest to pale blue, weaker to pale yellow and white — is the defining feature of the generalization gradient, which is what the consultant identifies as theoretically significant.",
        "B": "Avoidance learning maintained by negative reinforcement would explain persistent avoidance behavior maintained by the removal of an aversive stimulus, and it could superficially account for the boy's reluctance to wear the shirts. However, it does not explain the graded pattern of distress across shirt colors — the defining detail the consultant notes — because reinforcement contingencies do not produce a similarity-based response gradient.",
        "C": "Incubation of conditioned fear describes the paradoxical strengthening of fear over time through brief, unreinforced exposures. While the fear did emerge after the hospitalization (suggesting some time-course element), the scenario does not describe repeated brief exposures to the hospital cues; moreover, incubation cannot account for the gradient of response intensity across stimuli of varying physical similarity.",
        "D": "Context-dependent retrieval suggests that memory — including emotional memory — is best retrieved in contexts that match encoding conditions. This could plausibly account for increased distress when wearing similar-colored clothing (matching the hospital context). However, context-dependent retrieval predicts reinstatement of distress in contexts globally similar to encoding; it does not predict a precise gradient ordered by the physical similarity of a single stimulus dimension such as shirt color."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-092-vignette-L1",
      "source_question_id": "092",
      "source_summary": "Of the intermittent schedules of reinforcement, the variable ratio schedule produces the fastest rate of acquisition and the greatest resistance to extinction, as reinforcement is delivered after a variable number of responses.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "variable ratio schedule",
        "resistance to extinction",
        "intermittent reinforcement"
      ],
      "vignette": "A researcher studying gambling behavior notes that slot machine players continue pulling the lever at a high, steady rate even after many consecutive losses. The machine delivers a payout after a variable ratio schedule — sometimes after 3 pulls, sometimes after 20, with no predictable pattern. The researcher observes that this intermittent reinforcement produces some of the greatest resistance to extinction seen in operant conditioning research. Even when the machine is taken offline and no payouts occur, players persist far longer than those who had been winning on every pull.",
      "question": "Which schedule of reinforcement best explains why the slot machine players continue responding at such a high rate and resist stopping even when payouts cease?",
      "options": {
        "A": "Fixed ratio schedule",
        "B": "Variable interval schedule",
        "C": "Variable ratio schedule",
        "D": "Fixed interval schedule"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A fixed ratio schedule delivers reinforcement after a set, predictable number of responses (e.g., every 10 pulls). While it also produces high response rates, it creates a characteristic post-reinforcement pause and does not match the unpredictable payout pattern described here.",
        "B": "A variable interval schedule delivers reinforcement after an unpredictable amount of time has elapsed, regardless of response rate. It produces steady, moderate response rates — not the high, rapid responding associated with slot machine behavior.",
        "C": "Correct. A variable ratio schedule delivers reinforcement after an unpredictable number of responses, which produces the highest response rates and the greatest resistance to extinction among all schedules. Slot machines are the classic real-world example of this schedule.",
        "D": "A fixed interval schedule delivers reinforcement after a set, predictable time period has elapsed. It produces a scallop pattern of responding (slow after reinforcement, rapid near the end of the interval) and is associated with lower resistance to extinction than variable schedules."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-092-vignette-L2",
      "source_question_id": "092",
      "source_summary": "Of the intermittent schedules of reinforcement, the variable ratio schedule produces the fastest rate of acquisition and the greatest resistance to extinction, as reinforcement is delivered after a variable number of responses.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "resistance to extinction",
        "unpredictable"
      ],
      "vignette": "A clinical psychologist is working with a 34-year-old man who compulsively checks his smartphone for social media notifications, even during therapy sessions. He reports that 'likes' and comments appear unpredictably — sometimes he checks and receives many, sometimes none at all. Despite recent efforts to cut back, he finds it nearly impossible to stop checking, and the behavior actually intensifies when he tries to abstain. The psychologist notes that this pattern of behavior is notoriously difficult to eliminate precisely because of how the reinforcement has been delivered over time.",
      "question": "Which schedule of reinforcement most accurately explains why the client's smartphone-checking behavior shows such strong resistance to extinction?",
      "options": {
        "A": "Fixed ratio schedule",
        "B": "Variable ratio schedule",
        "C": "Continuous reinforcement",
        "D": "Variable interval schedule"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A fixed ratio schedule reinforces behavior after a set number of responses, producing high rates of responding but also predictable post-reinforcement pauses. It does not explain unpredictable delivery of social rewards and is associated with less extinction resistance than variable schedules.",
        "B": "Correct. A variable ratio schedule reinforces behavior after an unpredictable number of responses, producing the highest response rates and strongest resistance to extinction. Social media notifications arriving unpredictably after a varying number of checks perfectly exemplify this schedule.",
        "C": "Continuous reinforcement delivers a reward after every single response. While it produces rapid acquisition, it actually leads to the fastest extinction — the opposite of what is described. The client's behavior intensifying during abstinence is inconsistent with a history of continuous reinforcement.",
        "D": "A variable interval schedule delivers reinforcement after unpredictable time intervals, regardless of how many responses are made. Although it also produces extinction resistance, the key feature here is that reinforcement is contingent on the number of checks (responses), not on elapsed time, making variable ratio the better fit."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-092-vignette-L3",
      "source_question_id": "092",
      "source_summary": "Of the intermittent schedules of reinforcement, the variable ratio schedule produces the fastest rate of acquisition and the greatest resistance to extinction, as reinforcement is delivered after a variable number of responses.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "acquisition"
      ],
      "vignette": "A behavioral researcher trains two groups of rats to press a lever for food pellets. Group A receives a pellet every time they press the lever during the acquisition phase, establishing the behavior quickly and reliably. Group B's pellets arrive unpredictably — sometimes after 2 presses, sometimes after 15, averaging around 8. When the researcher later removes all food delivery, Group A rats stop pressing the lever within minutes, while Group B rats continue pressing vigorously for extended periods. Notably, Group B also achieved a faster stable response rate during training than a third group whose pellets arrived on a fixed schedule of every 8 presses.",
      "question": "What schedule of reinforcement was used with Group B, and what best explains both their rapid acquisition rate and their prolonged responding during extinction?",
      "options": {
        "A": "Fixed ratio schedule — because a predictable number of responses leads to efficient habit formation and sustained behavior",
        "B": "Variable interval schedule — because unpredictable timing of reward creates uncertainty that maintains responding even without reinforcement",
        "C": "Variable ratio schedule — because unpredictable delivery after a varying number of responses produces the highest rates of responding and the greatest resistance to extinction",
        "D": "Continuous reinforcement followed by partial reinforcement — because initial continuous reinforcement accelerates acquisition and switching to partial reinforcement increases persistence"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A fixed ratio schedule does produce high response rates but creates a predictable post-reinforcement pause and is associated with less resistance to extinction than variable schedules. The vignette specifies that Group B outperformed the fixed-schedule Group C in response rate, ruling out a fixed ratio explanation.",
        "B": "A variable interval schedule does produce resistance to extinction, but it delivers reinforcement based on elapsed time — not on number of responses. It typically produces moderate, steady response rates rather than the highest rates. The scenario ties reinforcement to response count (2 presses vs. 15 presses), which is a ratio, not interval, contingency.",
        "C": "Correct. A variable ratio schedule delivers reinforcement after an unpredictable number of responses, producing the fastest rate of acquisition and the greatest resistance to extinction among all schedules. Group B's reinforcement structure (averaging 8 presses but varying unpredictably) is a textbook variable ratio schedule, and their outperforming a fixed ratio group on response rate is consistent with research findings.",
        "D": "While transitioning from continuous to partial reinforcement (the partial reinforcement extinction effect) does enhance resistance to extinction, this describes a procedural sequence rather than a single schedule. The vignette describes one consistent training condition for Group B — not a two-phase procedure — and the specific unpredictability of response-number requirements points to variable ratio, not a blended approach."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-092-vignette-L4",
      "source_question_id": "092",
      "source_summary": "Of the intermittent schedules of reinforcement, the variable ratio schedule produces the fastest rate of acquisition and the greatest resistance to extinction, as reinforcement is delivered after a variable number of responses.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "unpredictability"
      ],
      "vignette": "A therapist is consulting on a case involving a woman who spent years in a relationship with an emotionally inconsistent partner who occasionally provided warmth, praise, and affection, but did so without any discernible pattern — sometimes after long silences, sometimes spontaneously mid-conflict. The woman ended the relationship 18 months ago but continues to experience intrusive thoughts about reconnecting and frequently checks her ex-partner's social media profiles. The therapist notes that her behavior resembles that of a subject in a conditioning study: the more she abstains, the more intense and frequent the urge to check becomes. A colleague suggests the behavior reflects a conditioned emotional response to intermittent positive attention, emphasizing the role of unpredictability in maintaining the attachment behavior.",
      "question": "Which operant conditioning principle most precisely accounts for the persistence and intensification of the woman's reconnection-seeking behavior following the end of the reinforcing relationship?",
      "options": {
        "A": "Partial reinforcement extinction effect produced by a variable interval schedule, because affection was delivered unpredictably over time regardless of her behavior",
        "B": "Negative reinforcement, because checking her ex-partner's social media temporarily reduces the anxiety associated with uncertainty about the relationship",
        "C": "Variable ratio schedule producing high resistance to extinction, because affection was delivered unpredictably contingent on a varying number of approach or connection behaviors",
        "D": "Fixed ratio schedule producing a partial reinforcement extinction effect, because affection was delivered consistently after a certain threshold of emotional effort was reached"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A variable interval schedule is highly defensible here because the affection appeared unpredictably over time, and interval schedules do produce strong extinction resistance. However, interval schedules deliver reinforcement based on time elapsed rather than on the subject's responses. The vignette emphasizes that affection followed behavioral bids (approach, connection attempts) — a ratio contingency — making variable ratio the more precise fit.",
        "B": "Negative reinforcement is also plausible because checking social media may reduce anxiety, which would reinforce the checking behavior through escape/avoidance. However, the question asks about the persistence of the broader reconnection-seeking pattern established during the relationship, not the function of the current checking behavior alone. The therapist's framing explicitly attributes maintenance to the conditioning history of intermittent positive attention, not to anxiety reduction.",
        "C": "Correct. A variable ratio schedule delivers positive reinforcement after an unpredictable number of responses, producing the highest resistance to extinction. The partner's affection was contingent (even if loosely) on the woman's approach and connection behaviors, with no predictable pattern in how many such behaviors were required. This is the hallmark of variable ratio conditioning, explaining why the behavior persists and intensifies even 18 months after the relationship ended.",
        "D": "A fixed ratio schedule requires a set, predictable number of responses before reinforcement is delivered. This does not match the described unpredictability of the partner's affection, and fixed schedules produce less resistance to extinction than variable schedules. While the partial reinforcement extinction effect does apply to variable schedules, it is not specifically associated with fixed ratio schedules in a way that explains prolonged, intensifying behavior after extinction."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-092-vignette-L5",
      "source_question_id": "092",
      "source_summary": "Of the intermittent schedules of reinforcement, the variable ratio schedule produces the fastest rate of acquisition and the greatest resistance to extinction, as reinforcement is delivered after a variable number of responses.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher studying persistence in task performance recruits participants to play a computer game in which they must repeatedly click a target on the screen to earn points. Unknown to participants, the game is programmed so that points are awarded in four different groups: Group 1 earns a point on every click; Group 2 earns a point after every tenth click; Group 3 earns a point after a click, but only once a set amount of time has passed since the last reward; Group 4 earns a point after clicking, but the number of clicks required before the next point is awarded changes constantly without pattern. After the game phase, the researcher disables all point delivery and observes how long each group continues clicking. Group 4 clicks significantly longer and faster than all other groups, even longer than Group 2 despite Group 2 also requiring effort for each reward. Interestingly, Group 1 stops clicking almost immediately after points stop.",
      "question": "What feature of Group 4's reward structure is most directly responsible for their superior persistence after rewards are removed, and which principle does this exemplify?",
      "options": {
        "A": "Group 4 persists longest because their reward was delivered after the passage of time, making it difficult to determine when rewards have truly stopped — consistent with a variable interval schedule",
        "B": "Group 4 persists longest because they were initially rewarded on every click, making them expect rewards to resume — consistent with a history of continuous reinforcement and subsequent frustration",
        "C": "Group 4 persists longest because the number of clicks required for each reward was unpredictable, making it impossible to distinguish non-reward trials from a longer-than-usual run before the next reward — consistent with a variable ratio schedule",
        "D": "Group 4 persists longest because they exerted effort for each reward, creating a cognitive justification for continued investment — consistent with a fixed ratio schedule and effort-based commitment"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A variable interval schedule is a strong distractor because it also produces high resistance to extinction through unpredictability. However, interval schedules deliver rewards based on elapsed time — the subject need not click at a particular rate to earn the reward. The scenario specifies that clicking is required and that the number of clicks between rewards varies, which is the defining feature of ratio schedules, not interval schedules.",
        "B": "Continuous reinforcement (Group 1 in the scenario) actually produces the fastest extinction, not the longest persistence — and the scenario directly shows Group 1 stopping almost immediately. This option inverts the actual relationship between continuous reinforcement and extinction resistance, making it incorrect despite sounding plausible at first glance.",
        "C": "Correct. The variable ratio schedule delivers reinforcement after an unpredictable number of responses, making it cognitively impossible for the subject to know whether the next click will yield a reward or whether rewards have simply stopped. This uncertainty drives continued responding long after rewards cease and explains why Group 4 outlasts even the effort-matched Group 2, which operates on a predictable (fixed) number of clicks per reward.",
        "D": "A fixed ratio schedule does require effort for each reward, and Group 2 (every 10th click) fits this description. However, fixed ratio schedules produce predictable post-reinforcement pauses and lower extinction resistance than variable ratio schedules, precisely because subjects can detect the absence of reward more readily when the required response count is known. The scenario shows Group 4 outperforming Group 2, ruling out the fixed ratio explanation."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-053-vignette-L1",
      "source_question_id": "053",
      "source_summary": "Mowrer's (1960) two-factor theory of learning is most useful for understanding avoidance conditioning, which combines classical conditioning and negative reinforcement (operant conditioning).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "avoidance conditioning",
        "classical conditioning",
        "negative reinforcement"
      ],
      "vignette": "A researcher is studying how a rat first learned to fear a tone that had been paired with a shock (classical conditioning) and then learned to jump over a barrier whenever the tone sounded in order to prevent the shock from occurring. The jumping behavior is maintained through negative reinforcement, because it successfully prevents the aversive outcome. A theorist argues that this two-stage process — first fear acquisition, then escape/avoidance — best explains the persistence of avoidance conditioning. The theorist notes that classical conditioning accounts for the emotional fear response, while operant learning accounts for the active escape behavior.",
      "question": "The theorist's two-stage explanation of how the rat's avoidance behavior was acquired and maintained most directly reflects which theoretical framework?",
      "options": {
        "A": "Rescorla-Wagner model",
        "B": "Mowrer's two-factor theory",
        "C": "Thorndike's law of effect",
        "D": "Hull's drive reduction theory"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The Rescorla-Wagner model is a mathematical model of classical conditioning that predicts how the strength of a conditioned response changes with each trial; it addresses associative learning but does not incorporate a two-stage operant avoidance mechanism.",
        "B": "Mowrer's two-factor theory explicitly proposes that avoidance is acquired in two stages: first, classical conditioning establishes a conditioned fear response to a neutral stimulus, and second, operant (negative reinforcement) learning maintains the escape/avoidance behavior that reduces that fear. This perfectly matches the described scenario.",
        "C": "Thorndike's law of effect states that responses followed by satisfying consequences are strengthened, which is a foundational principle of operant conditioning but describes a single-factor process and does not incorporate the role of classically conditioned fear in initiating avoidance.",
        "D": "Hull's drive reduction theory proposes that behavior is motivated by the reduction of biological drives; while it shares the idea that organisms act to reduce aversive states, it does not specifically address the two-stage classical-then-operant sequence that defines avoidance conditioning."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-053-vignette-L2",
      "source_question_id": "053",
      "source_summary": "Mowrer's (1960) two-factor theory of learning is most useful for understanding avoidance conditioning, which combines classical conditioning and negative reinforcement (operant conditioning).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "conditioned fear",
        "avoidance"
      ],
      "vignette": "A 34-year-old woman with no prior psychiatric history reports that after being involved in a serious car accident three years ago, she developed intense anxiety whenever she approached her vehicle. She now refuses to drive or even sit in cars, and her anxiety diminishes immediately whenever she successfully avoids getting into one. Her therapist notes that she is otherwise high-functioning, works full-time, and has strong social support, making her anxiety particularly circumscribed. The therapist conceptualizes her avoidance behavior as being maintained by two distinct learning processes.",
      "question": "The therapist's conceptualization that two distinct learning processes underlie this client's pattern of behavior is most consistent with which theoretical framework?",
      "options": {
        "A": "Bandura's social learning theory",
        "B": "Seligman's preparedness theory",
        "C": "Mowrer's two-factor theory",
        "D": "Skinner's operant conditioning model"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Bandura's social learning theory emphasizes observational learning and modeling as mechanisms of behavior acquisition; while it can address fear, it does not specifically account for the two-stage process of classically conditioned fear followed by operant avoidance that the therapist is describing.",
        "B": "Seligman's preparedness theory proposes that humans are biologically predisposed to acquire certain fears more readily than others; it can explain why some phobias develop quickly, but it describes a single-factor preparedness mechanism rather than a two-stage classical-then-operant process.",
        "C": "Mowrer's two-factor theory is the correct answer. The accident paired the car (neutral stimulus) with pain/fear, establishing conditioned fear via classical conditioning. The client's avoidance is then maintained by negative reinforcement (the immediate reduction in anxiety when she avoids the car), precisely the two-factor sequence Mowrer described.",
        "D": "Skinner's operant conditioning model addresses only how consequences shape voluntary behavior; it can explain why avoidance is maintained through negative reinforcement but does not incorporate the classical conditioning component that first established the conditioned fear, making it an incomplete account of this two-stage process."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-053-vignette-L3",
      "source_question_id": "053",
      "source_summary": "Mowrer's (1960) two-factor theory of learning is most useful for understanding avoidance conditioning, which combines classical conditioning and negative reinforcement (operant conditioning).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "fear acquisition"
      ],
      "vignette": "A graduate student presents a case study of a combat veteran who, after repeated exposure to explosions in the field, began experiencing intense dread whenever he heard loud noises — even in safe civilian settings. The veteran now systematically leaves any environment where loud noises are possible and reports feeling immediate relief each time he exits. The student emphasizes that the veteran's exit behavior appears to be self-perpetuating: the more he leaves, the stronger the urge to leave becomes. Notably, the veteran has never observed another person model this avoidance behavior, and his clinician suspects that the exit behavior would be difficult to eliminate without also addressing the underlying dread.",
      "question": "Which theoretical framework best explains both the origin of the veteran's dread and the self-perpetuating nature of his exit behavior?",
      "options": {
        "A": "Eysenck's incubation theory",
        "B": "Mowrer's two-factor theory",
        "C": "Observational learning theory",
        "D": "Reciprocal inhibition"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Eysenck's incubation theory proposes that brief, unreinforced exposures to a conditioned stimulus can paradoxically strengthen a conditioned fear response rather than extinguish it; while it addresses fear persistence and can partially explain why the veteran's fear doesn't diminish, it does not provide a two-stage account of how the exit behavior was acquired and maintained through negative reinforcement.",
        "B": "Mowrer's two-factor theory is correct. Stage one (classical conditioning) explains how explosions (UCS) became associated with loud noises (CS), establishing conditioned fear. Stage two (operant/negative reinforcement) explains why the exit behavior is self-perpetuating: each successful exit reduces fear, reinforcing the avoidance response and making it progressively stronger. The note about the clinician needing to address underlying dread also aligns with the theory's implication that treating only the operant behavior is insufficient.",
        "C": "Observational learning theory (Bandura) holds that behaviors are acquired by watching and imitating others; the vignette explicitly notes the veteran never observed another person model the avoidance, making this mechanism an implausible explanation for how the exit behavior originated.",
        "D": "Reciprocal inhibition is the principle underlying Wolpe's systematic desensitization: pairing a relaxation response with an anxiety-provoking stimulus to inhibit fear; it is a therapeutic technique rather than an explanatory framework for how avoidance behavior is acquired and maintained, so it does not account for the origin or self-perpetuation of the veteran's exit behavior."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-053-vignette-L4",
      "source_question_id": "053",
      "source_summary": "Mowrer's (1960) two-factor theory of learning is most useful for understanding avoidance conditioning, which combines classical conditioning and negative reinforcement (operant conditioning).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "relief"
      ],
      "vignette": "A clinical researcher is presenting data showing that a group of patients with obsessive-compulsive disorder consistently perform ritualistic checking behaviors immediately after experiencing intrusive thoughts. Neuroimaging shows heightened amygdala activation coinciding with the intrusive thoughts and rapid amygdala deactivation following each checking ritual. The researcher argues that the checking behaviors are not simply habitual — they were originally acquired through an associative pairing process and are now maintained by a fundamentally different learning mechanism. A senior clinician in the audience argues that this two-process account is overly complex and that a single-process operant model is sufficient.",
      "question": "The researcher's two-process account, which the senior clinician is disputing, most closely aligns with which theoretical framework?",
      "options": {
        "A": "Gray's behavioral inhibition system model",
        "B": "Mowrer's two-factor theory",
        "C": "Rescorla-Wagner associative learning model",
        "D": "Mackintosh's attentional theory of conditioning"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Gray's behavioral inhibition system (BIS) model describes a neurobiological system that responds to signals of punishment, non-reward, or novelty by inhibiting behavior and increasing arousal and attention; while it is relevant to anxiety and shares some neurobiological overlap with the described phenomena, it is a single-system neurobiological model and does not articulate a two-stage classical-then-operant acquisition sequence.",
        "B": "Mowrer's two-factor theory is correct. The intrusive thoughts function as conditioned stimuli that elicit conditioned fear (acquired via classical conditioning/associative pairing), and the checking rituals are maintained by negative reinforcement — the 'relief' (rapid amygdala deactivation) that follows each ritual. This precisely describes the two distinct learning mechanisms the researcher is defending against the single-process operant critique.",
        "C": "The Rescorla-Wagner model is a sophisticated mathematical model that predicts associative strength in classical conditioning based on prediction error; it describes only one learning process (associative/classical) and does not incorporate an operant maintenance mechanism, making it insufficient to account for the two-process account described.",
        "D": "Mackintosh's attentional theory proposes that organisms selectively allocate attention to stimuli that are good predictors of outcomes during classical conditioning; like the Rescorla-Wagner model, it is a single-process classical conditioning theory focused on how associative learning occurs, not a two-factor framework that separately accounts for operant maintenance of avoidance."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-053-vignette-L5",
      "source_question_id": "053",
      "source_summary": "Mowrer's (1960) two-factor theory of learning is most useful for understanding avoidance conditioning, which combines classical conditioning and negative reinforcement (operant conditioning).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A child who previously had an unpleasant experience at a dentist's office — involving an unexpected loud drilling sound and physical discomfort — now begins crying and trembling each time the family drives into the dental office parking lot, even before entering the building. The parents notice that whenever the child is allowed to stay home, the crying stops within minutes and the child becomes cheerful again. Over several months, the parents find it increasingly difficult to bring the child to any medical appointment, as the tantrums and distress have spread to similar contexts. The child's pediatrician notes that the tantrums seem paradoxically to worsen each time the parents capitulate, and that the behavior would be very difficult to address without first tackling the underlying emotional response the child has to medical settings.",
      "question": "Which theoretical framework best accounts for both the initial development of the child's emotional response to the parking lot and the progressive worsening of the tantrum behavior over time?",
      "options": {
        "A": "Operant extinction with spontaneous recovery",
        "B": "Mowrer's two-factor theory",
        "C": "Stimulus generalization gradient",
        "D": "Learned helplessness"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Operant extinction with spontaneous recovery describes the weakening of a previously reinforced behavior when reinforcement is withheld, followed by the temporary return of that behavior after a rest period; while spontaneous recovery could superficially explain why the behavior 'keeps coming back,' it does not account for how the emotional response to the parking lot was initially established through associative pairing, which is a critical part of the clinical picture.",
        "B": "Mowrer's two-factor theory is correct. The dental experience (pain/loud noise) was associatively paired with the dental context, causing the parking lot — through stimulus generalization — to elicit a conditioned emotional fear response (stage one: classical conditioning). Staying home removes the child from the feared context, and the rapid cessation of crying constitutes a reduction in conditioned fear that reinforces the avoidance (staying home) through negative reinforcement (stage two: operant conditioning). The pediatrician's observation that the behavior worsens each time parents capitulate and that the underlying emotional response must be addressed first are both signature features of Mowrer's model.",
        "C": "The stimulus generalization gradient describes how a conditioned response spreads to stimuli that are similar to the original conditioned stimulus, which does explain why the child's distress has spread to other medical contexts; however, stimulus generalization is a phenomenon within classical conditioning and does not constitute a full theoretical framework that also accounts for the operant mechanism maintaining the tantrum behavior through parental capitulation.",
        "D": "Learned helplessness, developed by Seligman, describes a state in which repeated exposure to uncontrollable aversive events leads an organism to stop attempting to escape even when escape becomes possible; this framework would predict passivity and depression rather than active, escalating tantrum behavior, and it does not account for the two-stage acquisition and maintenance process evident in this child's case."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-085-vignette-L1",
      "source_question_id": "085",
      "source_summary": "In the context of operant conditioning, fading refers to the gradual removal of prompts so that, eventually, the desired behavior occurs without prompts.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "fading",
        "prompts",
        "operant conditioning"
      ],
      "vignette": "A behavior therapist is working with a child with autism on independently requesting a snack. Using operant conditioning principles, the therapist initially provides full physical guidance (a full prompt) to help the child point to a picture on a communication board. Over successive sessions, the therapist systematically reduces assistance — moving from physical guidance to a light touch, then to a gestural cue, and finally withdrawing all prompts entirely. By the end of intervention, the child spontaneously points to the picture without any assistance from the therapist.",
      "question": "Which operant conditioning procedure does this scenario most directly illustrate?",
      "options": {
        "A": "Fading",
        "B": "Shaping",
        "C": "Chaining",
        "D": "Extinction"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Fading is the gradual removal of prompts (from full physical guidance to gestural to no assistance) so that the target behavior eventually occurs independently. The therapist systematically withdraws support across sessions, which is the defining feature of fading.",
        "B": "Incorrect. Shaping involves differentially reinforcing successive approximations of a target behavior, gradually changing the behavior itself. In this scenario, the behavior (pointing to the picture) does not change; what changes is the level of external assistance provided — the hallmark of fading, not shaping.",
        "C": "Incorrect. Chaining is a procedure for teaching a sequence of behaviors that together form a complex skill, linking individual steps together. This scenario does not describe teaching a multi-step behavior sequence; it describes the withdrawal of assistance for a single target response.",
        "D": "Incorrect. Extinction involves discontinuing reinforcement for a previously reinforced behavior, leading to a decrease in that behavior over time. In this scenario, reinforcement continues; it is the prompts — not the reinforcement — that are being removed."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-085-vignette-L2",
      "source_question_id": "085",
      "source_summary": "In the context of operant conditioning, fading refers to the gradual removal of prompts so that, eventually, the desired behavior occurs without prompts.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "prompts",
        "independently"
      ],
      "vignette": "A special education teacher is helping a 7-year-old with an intellectual disability learn to write his name. The child has been diagnosed with a mild intellectual disability and also shows some fine motor delays that initially concerned the teacher. Early in instruction, the teacher physically guides the child's hand through each letter. Over the next several weeks, the teacher moves to providing only verbal reminders ('now curve down'), then to simply pointing to the starting position, and finally offers no assistance at all. The child now writes his name independently and correctly without any support from the teacher.",
      "question": "The instructional technique the teacher used to move the child from full physical guidance to independent performance is best described as which of the following?",
      "options": {
        "A": "Shaping",
        "B": "Fading",
        "C": "Forward chaining",
        "D": "Differential reinforcement of other behavior (DRO)"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Fading is the systematic, gradual withdrawal of prompts — in this case moving from physical guidance to verbal cues to gestural cues to no assistance — until the behavior occurs without external support. The fine motor delay detail is a neutral clinical feature that does not change the classification of the technique.",
        "A": "Incorrect. Shaping involves reinforcing progressively closer approximations to a target behavior, meaning the response itself changes step by step. Here, the target behavior (writing the name) remains constant across sessions; only the level of assistance changes, distinguishing this procedure as fading.",
        "C": "Incorrect. Forward chaining teaches a multi-step task by first training the initial step and then adding subsequent steps in sequence. Writing one's name could theoretically be broken into a chain, but the scenario specifically describes the systematic reduction of assistance levels for the whole task — the defining feature of fading.",
        "D": "Incorrect. Differential reinforcement of other behavior (DRO) involves providing reinforcement when the target behavior is absent for a specified interval, typically used to reduce an unwanted behavior. This scenario focuses on building an independent skill by removing prompts, which is fading rather than DRO."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-085-vignette-L3",
      "source_question_id": "085",
      "source_summary": "In the context of operant conditioning, fading refers to the gradual removal of prompts so that, eventually, the desired behavior occurs without prompts.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "prompts"
      ],
      "vignette": "A board-certified behavior analyst (BCBA) is teaching a 10-year-old with autism to independently complete a hand-washing routine. Early in treatment, the BCBA uses picture cards placed directly above each sink station to remind the child of each step. Notably, the child has learned every individual step of the routine in isolation during earlier sessions. Over the next month, the BCBA gradually moves the picture cards farther from the sink, then to across the room, and finally removes them entirely. By the end of treatment, the child completes the entire hand-washing routine without referencing any visual supports.",
      "question": "Although the child's hand-washing routine was already taught step-by-step in earlier sessions, the BCBA's current approach — systematically relocating and then removing the visual supports — is best classified as which behavioral procedure?",
      "options": {
        "A": "Backward chaining",
        "B": "Stimulus generalization",
        "C": "Shaping",
        "D": "Fading"
      },
      "correct_answer": "D",
      "option_explanations": {
        "D": "Correct. Fading is the gradual removal of prompts (here, visual picture cards) across sessions so that the behavior eventually occurs without external support. The detail that individual steps were already learned is a red herring; the current procedure is specifically about withdrawing the visual prompts that were cueing independent performance.",
        "A": "Incorrect. Backward chaining is a method of teaching a multi-step task by first reinforcing completion of the last step, then the second-to-last, and so on. The vignette states that all steps were already taught in isolation; the current focus is on removing supports, not on sequencing how steps were originally taught.",
        "B": "Incorrect. Stimulus generalization refers to the occurrence of a conditioned response in the presence of stimuli similar to, but different from, the original training stimulus. While moving the cards to a different location could superficially resemble manipulating stimulus conditions, the goal here is not to broaden responding across similar stimuli but to eliminate the prompt entirely — the defining feature of fading.",
        "C": "Incorrect. Shaping requires differentially reinforcing successive approximations toward a target behavior that does not yet exist in the individual's repertoire. Since the child can already perform the hand-washing steps, the behavior itself is not being shaped; rather, external supports are being systematically withdrawn, which is fading."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-085-vignette-L4",
      "source_question_id": "085",
      "source_summary": "In the context of operant conditioning, fading refers to the gradual removal of prompts so that, eventually, the desired behavior occurs without prompts.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "independence"
      ],
      "vignette": "A psychologist is consulting on a vocational training program for adults with developmental disabilities. One client has been successfully completing a multi-step filing task when a supervisor stands directly at her workstation and occasionally points to the next folder to be filed. The psychologist notes that the client performs each step correctly and consistently. To build greater independence, the psychologist instructs the supervisor to begin standing slightly farther from the workstation each week — first an arm's length away, then across the room, then briefly stepping out of sight — while continuing to praise correct completion of the task. After two months, the supervisor is fully absent during the task and the client files accurately without incident. The psychologist records this as a successful outcome of the planned intervention.",
      "question": "On first analysis, the ongoing praise from the supervisor might suggest a reinforcement-based procedure, but the critical mechanism driving the client's progress toward independent performance is best identified as which of the following?",
      "options": {
        "A": "Shaping through differential reinforcement",
        "B": "Fading",
        "C": "Stimulus control transfer via generalization training",
        "D": "Backward chaining with reinforcement"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Although praise (reinforcement) is maintained throughout, the defining mechanism of the intervention is the systematic, gradual removal of the supervisor's physical presence — which functioned as a prompt cueing the client's filing behavior. Progressively increasing the supervisor's distance until full absence is the hallmark of fading, and it is this prompt-withdrawal process, not the reinforcement, that is responsible for the client achieving independent performance.",
        "A": "Incorrect. Shaping through differential reinforcement would require that the client's filing behavior itself be reinforced in successively closer approximations to a final target form. Here, the filing behavior is already at criterion and does not change; what changes is the proximity of the supervisory prompt. The presence of praise does not make this shaping.",
        "C": "Incorrect. Stimulus control transfer via generalization training involves expanding a well-established stimulus-response relationship to new, related stimuli in different contexts or settings. In this scenario, the training environment does not change, and no new stimulus is being introduced; rather, an existing prompt (supervisor proximity) is being systematically withdrawn — which is fading.",
        "D": "Incorrect. Backward chaining teaches a complex task by first training the last step, then adding steps in reverse sequence toward the beginning. The scenario states the client already performs all steps correctly; the intervention is not teaching step sequence but removing supervisory proximity as a prompt, making this fading rather than chaining."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-085-vignette-L5",
      "source_question_id": "085",
      "source_summary": "In the context of operant conditioning, fading refers to the gradual removal of prompts so that, eventually, the desired behavior occurs without prompts.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A clinician is working with a teenager who became dependent on her mother's verbal reminders to complete her morning routine after a period of significant anxiety and school avoidance. The teenager can perform every step of the routine correctly when reminded but has never completed it on her own since the anxiety began. Rather than reducing the mother's involvement all at once, the clinician coaches the mother to shift from announcing each step to offering a single general reminder at the start, then to pausing in the doorway rather than the bedroom, then to remaining in the kitchen while the teen gets ready, and finally to simply being present elsewhere in the house without any communication. After eight weeks, the teenager completes her full morning routine every day without any input from her mother. The clinician notes that the teenager's actual morning routine behaviors did not need to be rebuilt — they were already in her repertoire.",
      "question": "The clinician's structured approach to modifying the mother's involvement represents which behavioral procedure?",
      "options": {
        "A": "Systematic desensitization",
        "B": "Shaping",
        "C": "Fading",
        "D": "Graduated extinction"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The critical detail is that the target behaviors were already fully in the teenager's repertoire — she could perform every step when prompted. The clinician's intervention systematically reduced the mother's reminders and physical presence step by step until the teen performed independently with no external cues. This is precisely fading: the gradual withdrawal of prompts until the behavior occurs without them. The anxiety history is a red herring that suggests a fear-reduction procedure.",
        "A": "Incorrect. Systematic desensitization is a classical conditioning-based procedure in which a hierarchy of anxiety-provoking stimuli is paired with relaxation to reduce conditioned fear responses. Although the teen has an anxiety history, the clinician's intervention targets the mother's verbal and physical presence as cues for behavior — not the teen's anxiety responses — and no relaxation pairing is described.",
        "B": "Incorrect. Shaping involves differentially reinforcing successive approximations toward a behavior not yet in the individual's repertoire, gradually building the behavior itself. The vignette explicitly states that the teenager's morning routine behaviors did not need to be rebuilt; this rules out shaping. What was changed systematically was the level of external support, not the behavior.",
        "D": "Incorrect. Graduated extinction — sometimes called 'fading extinction' in sleep or behavioral intervention literature — refers to systematically reducing the duration or intensity of reinforcement or parental response to gradually allow a behavior to extinguish. In this scenario, nothing is being extinguished; the teenager's routine behavior is maintained and the goal is its continuation without prompts, which is the defining purpose of fading rather than extinction-based procedures."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-008-vignette-L1",
      "source_question_id": "008",
      "source_summary": "In Watson's research, the white rat was the conditioned stimulus because it produced a startle response after being paired with the loud noise, which was the unconditioned stimulus.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "conditioned stimulus",
        "unconditioned stimulus",
        "classical conditioning"
      ],
      "vignette": "A researcher replicates Watson's Little Albert experiment in a controlled laboratory setting. An infant is shown a white rat, which initially produces no fear response. The researcher then pairs the white rat with a loud noise — the unconditioned stimulus — on several successive trials. After repeated pairings, the infant begins to cry and show distress whenever the white rat alone is presented. This is a textbook demonstration of classical conditioning.",
      "question": "In this scenario, what role does the white rat play in the conditioning process?",
      "options": {
        "A": "It is the unconditioned stimulus because it naturally elicits distress in infants without any prior learning.",
        "B": "It is the conditioned stimulus because it elicits a fear response only after being paired with the loud noise.",
        "C": "It is the unconditioned response because it automatically triggers a startle reaction when presented.",
        "D": "It is the conditioned response because it was learned through operant reinforcement of avoidance behavior."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. The unconditioned stimulus is the loud noise, which naturally elicits a startle or fear response without prior learning. The white rat initially produced no distress, so it cannot be the unconditioned stimulus.",
        "B": "Correct. The white rat began as a neutral stimulus and acquired the ability to elicit fear only after repeated pairings with the loud noise. This transformation from neutral to fear-eliciting defines it as the conditioned stimulus.",
        "C": "Incorrect. An unconditioned response is a reflexive, unlearned reaction to an unconditioned stimulus — in this case, the infant's startle to the loud noise. The white rat is a stimulus, not a response, and it plays a different role in the conditioning paradigm.",
        "D": "Incorrect. A conditioned response is the learned reaction (fear/crying) that the organism produces to the conditioned stimulus. The white rat is the eliciting stimulus, not the behavioral response, and operant reinforcement of avoidance is not involved in this classical conditioning demonstration."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-008-vignette-L2",
      "source_question_id": "008",
      "source_summary": "In Watson's research, the white rat was the conditioned stimulus because it produced a startle response after being paired with the loud noise, which was the unconditioned stimulus.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "conditioned stimulus",
        "neutral stimulus"
      ],
      "vignette": "A nine-month-old boy in a developmental psychology study is initially unafraid of small furry animals. Over the course of several sessions, each presentation of a white rabbit is immediately followed by a sudden, jarring clang of a steel bar struck behind the child's head. The child, who was born prematurely and has mild auditory hypersensitivity, shows an especially intense startle to the loud sound. After five pairings, the child begins to whimper and reach for his caregiver as soon as the rabbit is brought into view, even when no sound is produced.",
      "question": "What term best describes the role of the white rabbit in producing the child's fear response by the end of the study?",
      "options": {
        "A": "Unconditioned stimulus, because it is the object most directly associated with the child's emotional distress.",
        "B": "Conditioned stimulus, because it was originally a neutral stimulus that acquired the power to elicit fear through repeated pairing with the loud sound.",
        "C": "Discriminative stimulus, because the child learned to use the rabbit's presence as a signal to predict when a aversive event would occur.",
        "D": "Secondary reinforcer, because the rabbit gained motivational salience through its consistent association with an unpleasant outcome."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The white rabbit started as a neutral stimulus with no fear-eliciting properties. After repeated pairings with the loud clang — the unconditioned stimulus — the rabbit alone came to elicit a fear response, fulfilling the defining criteria of a conditioned stimulus in classical conditioning.",
        "A": "Incorrect. The unconditioned stimulus is the loud clang, which produced a startle without any prior learning. The rabbit was initially neutral and required training to elicit fear, which disqualifies it from being the unconditioned stimulus regardless of how prominent a role it plays in the child's distress by the end.",
        "C": "Incorrect. A discriminative stimulus is an operant conditioning concept referring to a cue that signals when a particular behavior will be reinforced. While the rabbit does come to 'predict' the loud sound, the mechanism here is classical (Pavlovian) pairing, not operant discrimination learning, making 'conditioned stimulus' the more precise and accurate term.",
        "D": "Incorrect. A secondary (conditioned) reinforcer is an operant concept describing a previously neutral stimulus that gains reinforcing value through association with a primary reinforcer. The rabbit does not reinforce any behavior; rather, it elicits a fear response, placing this squarely in classical conditioning rather than operant conditioning."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-008-vignette-L3",
      "source_question_id": "008",
      "source_summary": "In Watson's research, the white rat was the conditioned stimulus because it produced a startle response after being paired with the loud noise, which was the unconditioned stimulus.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "pairing"
      ],
      "vignette": "A behaviorist studying fear acquisition in toddlers documents the following: each time a particular hand puppet is introduced into the playroom, an experimenter simultaneously activates a very loud alarm. The toddlers initially laugh at the puppet but startle and cry when the alarm sounds. After several sessions, a new experimenter enters and simply holds up the puppet — no alarm is activated — and the children immediately begin crying and backing away. One child's mother reports that her son now refuses to enter any room where he has previously seen the puppet, even days later. The behaviorist notes that the alarm's pairing with the puppet was the key event in producing this outcome.",
      "question": "Based on the behaviorist's account, which element of this situation is best described as the conditioned stimulus?",
      "options": {
        "A": "The loud alarm, because it is the event that originally and reliably produced crying and fear without any prior learning history with the toddlers.",
        "B": "The toddlers' crying and backing away, because this behavior was directly shaped by the repeated co-occurrence of the puppet and the alarm.",
        "C": "The hand puppet, because it was initially neutral but came to reliably elicit fear after repeated temporal association with the alarm.",
        "D": "The playroom environment, because the children's avoidance generalized to any setting where the puppet had been previously encountered."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. The loud alarm is the unconditioned stimulus — it naturally elicits a startle and fear response without any prior training. The conditioned stimulus, by contrast, must begin as a neutral element and acquire its fear-eliciting properties through association, which describes the puppet rather than the alarm.",
        "B": "Incorrect. The toddlers' crying and backing away is the conditioned response — the learned behavioral reaction that now occurs to the puppet alone. A conditioned response is a reaction, not a stimulus, so it cannot also be the conditioned stimulus.",
        "C": "Correct. The hand puppet was initially neutral (children laughed at it) and elicited no fear before the experiment began. Through repeated pairing with the loud alarm (unconditioned stimulus), the puppet acquired the ability to elicit fear independently, which is the defining feature of a conditioned stimulus.",
        "D": "Incorrect. The playroom environment, having been associated with both the puppet and the alarm, represents a context that could support higher-order conditioning or stimulus generalization, but it is not the primary conditioned stimulus described in this scenario. The behaviorist specifically identifies the puppet-alarm pairing as the key event, focusing on the puppet as the trained fear cue."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-008-vignette-L4",
      "source_question_id": "008",
      "source_summary": "In Watson's research, the white rat was the conditioned stimulus because it produced a startle response after being paired with the loud noise, which was the unconditioned stimulus.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "acquisition"
      ],
      "vignette": "A research team studying emotional learning in infants publishes findings showing that a previously innocuous visual object — a stuffed bear — reliably predicts an aversive auditory event during the acquisition phase of their protocol. By session six, infants display heart rate acceleration, withdrawal, and facial expressions of distress the moment the stuffed bear is placed in their field of view, well before any sound is produced. The team emphasizes that it is the stuffed bear, not the sound, that has undergone the critical psychological transformation during learning. Notably, if the stuffed bear had been presented without the aversive sound from the start, the current emotional reactions would never have developed.",
      "question": "Which classical conditioning concept most precisely describes what the stuffed bear has become by session six?",
      "options": {
        "A": "Unconditioned stimulus, because the bear is now a powerful, reliable trigger for the infants' distress responses observed throughout the trials.",
        "B": "Conditioned stimulus, because the bear acquired fear-eliciting properties through its predictive relationship with an inherently aversive event.",
        "C": "Higher-order conditioned stimulus, because the bear's aversive properties were derived indirectly through prior conditioning of the aversive sound itself.",
        "D": "Conditioned response, because the distress reactions displayed by the infants to the stuffed bear represent the learned outcome of the pairing procedure."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The stuffed bear began as a neutral stimulus and through its consistent temporal pairing with an inherently aversive auditory event, it acquired the capacity to elicit fear responses on its own. This is the precise definition of a conditioned stimulus: a previously neutral cue that gains psychological significance through association with an unconditioned stimulus.",
        "A": "Incorrect. An unconditioned stimulus elicits a response naturally, without prior learning, such as the aversive sound that reflexively startles infants. The bear required multiple sessions of paired exposure before it could elicit fear, indicating it is a learned (conditioned) rather than a natural (unconditioned) stimulus, despite its current reliability as a trigger.",
        "C": "Incorrect. A higher-order (second-order) conditioned stimulus is one that acquires fear-eliciting properties through pairing with an already-established conditioned stimulus, not with an unconditioned stimulus directly. In this study, the bear was paired with the aversive sound — the unconditioned stimulus itself — making it a first-order conditioned stimulus, not a higher-order one.",
        "D": "Incorrect. A conditioned response is the learned behavioral or physiological reaction — here, the infants' heart rate acceleration, withdrawal, and distress expressions. The stuffed bear is a stimulus that elicits these reactions, not the reaction itself. Confusing the eliciting stimulus with the elicited response is a common error that this distractor is designed to probe."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-008-vignette-L5",
      "source_question_id": "008",
      "source_summary": "In Watson's research, the white rat was the conditioned stimulus because it produced a startle response after being paired with the loud noise, which was the unconditioned stimulus.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "During a series of laboratory sessions, a young child is shown an ordinary wooden block. The child reaches for it, smiles, and attempts to play with it — typical exploratory behavior with no sign of distress. In each session, a researcher waits until the child touches the block and then immediately produces an extremely jarring sound by striking a metal pipe directly behind the child's head. By the fourth session, the child begins crying and withdrawing from the room as soon as the wooden block is placed on the table, even before touching it and even when the researcher has left the room entirely. The child's parents note that, at home, the child now reacts with similar distress to other rectangular objects of similar size.",
      "question": "In this scenario, which element best fits the role traditionally described as the stimulus that underwent a transformation in its psychological properties as a result of the experimental procedure?",
      "options": {
        "A": "The jarring sound produced by the metal pipe, because it is the event that drove the entire learning process by reliably producing distress across all sessions.",
        "B": "The child's withdrawal and crying, because these behaviors were absent before the sessions began and emerged specifically as a consequence of repeated exposure to the two events together.",
        "C": "The wooden block, because it began with no capacity to elicit distress and developed that capacity solely through its repeated temporal association with the jarring sound.",
        "D": "The other rectangular objects at home, because they are now producing fear reactions in the child even though they were never directly involved in the laboratory sessions."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The wooden block is the conditioned stimulus. It was originally a neutral object that the child approached playfully. Through repeated pairing with the loud, aversive sound (the unconditioned stimulus), the block alone acquired the ability to elicit fear — a psychological transformation from neutral to fear-eliciting that defines the conditioned stimulus in classical conditioning.",
        "A": "Incorrect. The jarring sound is the unconditioned stimulus — it naturally produced distress without any prior learning. The question asks about the element that underwent a transformation in psychological properties through the procedure, which is not the sound (whose aversive properties were present from the outset) but the wooden block.",
        "B": "Incorrect. The child's withdrawal and crying constitute the conditioned response — the learned behavioral reaction that came to be elicited by the wooden block. While this response did emerge as a result of the procedure, it is a reaction, not a stimulus, and it did not undergo the psychological transformation of gaining signal value. The question specifically asks about the stimulus that was transformed, not the response that was acquired.",
        "D": "Incorrect. The child's fear of similar rectangular objects at home illustrates stimulus generalization — the spread of a conditioned response to stimuli resembling the original conditioned stimulus. These objects were never directly involved in the pairing procedure and did not undergo transformation through the experimental sessions themselves, making them examples of generalization from the conditioned stimulus rather than the conditioned stimulus itself."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-019-vignette-L1",
      "source_question_id": "019",
      "source_summary": "Higher-order conditioning involves using the initial conditioned stimulus (a tone) as an unconditioned stimulus by pairing it with a neutral stimulus (a light) so that the neutral stimulus also becomes a conditioned stimulus and elicits a conditioned response (salivation) when it's presented alone.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "conditioned stimulus",
        "higher-order conditioning",
        "neutral stimulus"
      ],
      "vignette": "A researcher first establishes classical conditioning in a dog by repeatedly pairing a tone with food until the tone alone elicits salivation, making the tone a conditioned stimulus. In a second phase, the researcher pairs a flashing light — a neutral stimulus — with the tone (without ever presenting food again). After several such pairings, the light alone is sufficient to elicit salivation. The researcher notes that this response to the light was established entirely through its association with the tone, not through any direct pairing with food. This two-stage process is the focus of the study.",
      "question": "Which learning phenomenon best describes the process by which the flashing light came to elicit salivation?",
      "options": {
        "A": "Higher-order conditioning",
        "B": "Stimulus generalization",
        "C": "Sensory preconditioning",
        "D": "Extinction"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. Higher-order conditioning occurs when an already-established conditioned stimulus (the tone) is used as a surrogate unconditioned stimulus to condition a new neutral stimulus (the light), producing a conditioned response without direct pairing with the original unconditioned stimulus (food).",
        "B": "Stimulus generalization refers to the tendency for stimuli similar to the original conditioned stimulus to also elicit the conditioned response. It does not involve a two-stage pairing procedure; the light would need to physically resemble the tone, not be paired with it.",
        "C": "Sensory preconditioning involves pairing two neutral stimuli before either is conditioned; conditioning one later causes the other to also elicit the response. Here, the tone was already a conditioned stimulus when paired with the light, so this is higher-order conditioning rather than sensory preconditioning.",
        "D": "Extinction is the weakening and eventual disappearance of a conditioned response when the conditioned stimulus is repeatedly presented without the unconditioned stimulus. The scenario describes acquisition of a new response, not the elimination of one."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-019-vignette-L2",
      "source_question_id": "019",
      "source_summary": "Higher-order conditioning involves using the initial conditioned stimulus (a tone) as an unconditioned stimulus by pairing it with a neutral stimulus (a light) so that the neutral stimulus also becomes a conditioned stimulus and elicits a conditioned response (salivation) when it's presented alone.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "conditioned response",
        "second-order"
      ],
      "vignette": "A behavioral researcher works with a 7-year-old child who has developed a fear response to a specific alarm sound after it was repeatedly paired with a mildly startling air puff. Despite the child having no known anxiety disorder, the researcher observes that the child now shows visible distress whenever the classroom lights flicker — even though the lights were never directly paired with the air puff. The researcher's notes indicate that the flickering lights had been consistently present in the room only during alarm-sound presentations over the preceding weeks. The researcher hypothesizes that the lights acquired their fear-eliciting power through a second-order pairing process, producing a conditioned response to a stimulus never paired with the original aversive event.",
      "question": "The process the researcher is describing, in which the lights came to elicit fear, is best explained by which phenomenon?",
      "options": {
        "A": "Stimulus discrimination",
        "B": "Sensory preconditioning",
        "C": "Higher-order conditioning",
        "D": "Incubation"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. Higher-order (second-order) conditioning explains why the lights elicit fear: the alarm sound had first become a conditioned stimulus, and the lights were then paired with that already-conditioned stimulus, allowing the lights to independently elicit the conditioned fear response.",
        "A": "Stimulus discrimination is the learned ability to respond differently to stimuli that differ from the conditioned stimulus. It refers to restricting a response, not to acquiring a new conditioned response through pairing with an existing conditioned stimulus.",
        "B": "Sensory preconditioning would apply if the lights and alarm sound had been paired before either was conditioned — that is, both were neutral at the time of their pairing. In this vignette, the alarm was already a conditioned stimulus when paired with the lights, which is the defining feature of higher-order conditioning.",
        "D": "Incubation refers to the paradoxical strengthening of a conditioned fear response during brief, unreinforced exposures to the conditioned stimulus. While relevant to fear acquisition, it does not explain how a new stimulus acquires fear-eliciting properties through pairing with an existing conditioned stimulus."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-019-vignette-L3",
      "source_question_id": "019",
      "source_summary": "Higher-order conditioning involves using the initial conditioned stimulus (a tone) as an unconditioned stimulus by pairing it with a neutral stimulus (a light) so that the neutral stimulus also becomes a conditioned stimulus and elicits a conditioned response (salivation) when it's presented alone.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "neutral"
      ],
      "vignette": "During a study on associative learning, a rat was trained in a two-phase paradigm. In the first phase, a brief buzzer sound was reliably followed by a mild foot shock until the buzzer reliably produced a freezing response. In the second phase, a previously neutral odor was introduced into the chamber at the same time as the buzzer, but no foot shock was ever administered. The experimenter was careful to ensure that the odor had never been present during any foot-shock trials. After several sessions of odor-buzzer pairings, the rat was placed in a new chamber with only the odor present and showed a clear freezing response, even though the odor had never been directly paired with the aversive shock. The researchers note that this result would not have occurred if the buzzer had been extinguished before the second phase.",
      "question": "The rat's fear response to the odor alone is best explained by which learning phenomenon?",
      "options": {
        "A": "Latent inhibition",
        "B": "Higher-order conditioning",
        "C": "Blocking",
        "D": "Sensory preconditioning"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Higher-order conditioning explains how the odor acquired fear-eliciting power: the buzzer was already an established conditioned stimulus, and pairing it with the neutral odor allowed the odor to independently elicit the conditioned freezing response without ever being paired with the shock itself.",
        "A": "Latent inhibition occurs when prior exposure to a neutral stimulus without consequences makes it harder for that stimulus to later become a conditioned stimulus. The odor in this vignette was not pre-exposed before conditioning; it was introduced during the second phase with the buzzer, making latent inhibition inapplicable.",
        "C": "Blocking occurs when prior conditioning to one stimulus prevents a simultaneously presented new stimulus from becoming conditioned, because the first stimulus already fully predicts the unconditioned stimulus. This might seem to apply here, but the key distinction is that blocking prevents conditioning whereas the scenario shows successful conditioning of the odor — the odor did acquire the response, which is the hallmark of higher-order conditioning.",
        "D": "Sensory preconditioning requires that two neutral stimuli be paired before either is conditioned, and conditioning one later causes the other to also respond. In this vignette, the buzzer was already a conditioned stimulus before it was paired with the odor, ruling out sensory preconditioning and pointing instead to higher-order conditioning."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-019-vignette-L4",
      "source_question_id": "019",
      "source_summary": "Higher-order conditioning involves using the initial conditioned stimulus (a tone) as an unconditioned stimulus by pairing it with a neutral stimulus (a light) so that the neutral stimulus also becomes a conditioned stimulus and elicits a conditioned response (salivation) when it's presented alone.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "surrogate"
      ],
      "vignette": "A clinician reviewing a case file notes that a patient with no history of cardiac illness developed intense physiological arousal upon entering hospital waiting rooms. Investigation revealed that the patient had previously undergone a series of painful dental procedures always preceded by the sound of a specific overhead announcement chime. That chime eventually produced anticipatory anxiety on its own. Hospital waiting rooms, it turned out, featured the same style of chime — but the patient reported that the arousal actually began upon seeing the rows of chairs arranged along the walls, even in waiting rooms where the chime was absent. Chart notes from the treating dentist confirm that chairs were always visible in the dental suite during chime presentations but that chairs were never present during actual painful procedures. The clinician hypothesizes that the chairs function as a surrogate-conditioned elicitor, one step removed from the original aversive event.",
      "question": "The process by which the rows of chairs came to elicit physiological arousal is best characterized as which of the following?",
      "options": {
        "A": "Stimulus generalization from the dental environment to hospital settings",
        "B": "Higher-order conditioning in which the chime served as the conditioning stimulus for the chairs",
        "C": "Sensory preconditioning in which the chairs and chime were paired as neutral stimuli before either was conditioned",
        "D": "Incubation of conditioned fear mediated by brief, unreinforced exposures in the waiting room"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The chime had already been established as a conditioned stimulus (producing anticipatory anxiety on its own) before it was repeatedly paired with the chairs. Because the chairs were paired with the already-conditioned chime — never with the painful procedure itself — the chairs' fear-eliciting power was acquired through higher-order conditioning, with the chime functioning as a surrogate unconditioned stimulus.",
        "A": "Stimulus generalization could superficially explain why hospital settings produce arousal if they physically resemble the dental office. However, the vignette specifies that arousal occurs specifically to the rows of chairs even in settings without the chime, and that chairs were present during chime (not during pain) presentations — a directional pairing relationship that generalization alone does not account for.",
        "C": "Sensory preconditioning requires that the chairs and chime be paired when both were still neutral, before the chime became conditioned. The vignette's timeline clearly indicates that the chime had already become a conditioned stimulus (producing anxiety independently) at the time it was experienced alongside the chairs in the waiting room, disqualifying sensory preconditioning.",
        "D": "Incubation refers to the strengthening of conditioned fear during brief exposures to the conditioned stimulus alone, typically through a mechanism involving partial reinforcement or anxiety-response amplification. While the patient may experience incubation over time in waiting rooms, it does not explain the mechanism by which the chairs initially acquired fear-eliciting properties."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-019-vignette-L5",
      "source_question_id": "019",
      "source_summary": "Higher-order conditioning involves using the initial conditioned stimulus (a tone) as an unconditioned stimulus by pairing it with a neutral stimulus (a light) so that the neutral stimulus also becomes a conditioned stimulus and elicits a conditioned response (salivation) when it's presented alone.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A woman reports that she feels intense nausea whenever she walks past a particular bakery on her street. She recalls that several months ago she frequently ate pastries from that bakery while recovering from a gastrointestinal illness, and she attributes her nausea to bad memories of being sick. Closer questioning, however, reveals that she never actually ate the pastries during the worst phase of her illness; rather, she had become deeply anxious about eating anything during that period, and this anxiety was reliably triggered by the sound of her kitchen timer, which she had used daily to time her medication doses. She began purchasing bakery pastries only after recovering, during a brief period when she always set her kitchen timer as a reminder to leave the house for her daily walk — and the bakery was the first stop on that walk. She has since stopped using the timer entirely, yet the nausea remains whenever she approaches the bakery.",
      "question": "The mechanism most precisely explaining why the bakery now elicits nausea is which of the following?",
      "options": {
        "A": "Taste aversion conditioning, in which the pastries were paired with gastrointestinal illness and the bakery became associated through generalization",
        "B": "Higher-order conditioning, in which the timer had acquired the capacity to elicit nausea and subsequently conditioned the bakery as a new elicitor of that response",
        "C": "Sensory preconditioning, in which the bakery and timer were paired as neutral stimuli before the timer became associated with illness",
        "D": "Stimulus generalization, in which the bakery environment resembles the home kitchen setting where illness-related associations were formed"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The kitchen timer had already become an established elicitor of anxiety/nausea through repeated pairing with medication doses during illness — that is, it was already a conditioned stimulus. Later, the bakery was consistently paired with the timer (she set the timer as a reminder before her walk to the bakery) but never with the illness itself. The bakery thus acquired its nausea-eliciting power through pairing with the already-conditioned timer, which is the defining mechanism of higher-order conditioning.",
        "A": "Taste aversion conditioning (a form of classical conditioning) would require that ingesting the pastries was followed by gastrointestinal illness, making the taste of the pastries the conditioned stimulus. The vignette explicitly states she did not eat pastries during the illness phase, only after recovery — so no direct pairing between the pastries or bakery and illness occurred, ruling out taste aversion as the primary mechanism.",
        "C": "Sensory preconditioning would apply only if the bakery and timer had been paired when both were still neutral — that is, before the timer had acquired any conditioned properties. The timeline in the vignette shows the timer became a conditioned elicitor (during illness) before the woman began associating it with the bakery (after recovery), so the bakery was paired with an already-conditioned stimulus, not a neutral one.",
        "D": "Stimulus generalization would predict that environments physically similar to the home kitchen — where illness associations were formed — would also elicit nausea. However, a bakery does not physically resemble a home kitchen in the ways that would drive generalization, and the mechanism described (timer pairing) points to a direct associative chain rather than perceptual similarity, making higher-order conditioning the more precise explanation."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-041-vignette-L1",
      "source_question_id": "041",
      "source_summary": "Thorndike's research with cats in a puzzle box led to his development of the law of effect, which predicts that behaviors followed by satisfying consequences are more likely to recur.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "satisfying consequences",
        "law of effect",
        "recur"
      ],
      "vignette": "A psychology professor describes a classic experiment in which a hungry cat was placed inside a puzzle box and had to learn to escape by pressing a lever. Initially the cat made many random movements, but over repeated trials the escape behavior became faster and more reliable. The professor explains that behaviors followed by satisfying consequences are more likely to recur, a principle he identifies as the foundation of modern operant conditioning. He notes that this empirical observation, derived from systematic animal research, was formalized into what became one of psychology's most influential early learning laws.",
      "question": "Which learning principle is the professor describing?",
      "options": {
        "A": "The law of effect",
        "B": "The law of contiguity",
        "C": "The principle of reinforcement schedules",
        "D": "Classical conditioning"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Thorndike's law of effect states that behaviors followed by satisfying outcomes are more likely to be repeated, which is precisely what the professor describes. The puzzle-box scenario is the canonical illustration of this principle.",
        "B": "Incorrect. The law of contiguity, associated with early associationist philosophers such as Hume, holds that events occurring close together in time become associated. While temporal proximity is involved in Thorndike's work, contiguity does not capture the critical role of satisfying consequences in strengthening behavior.",
        "C": "Incorrect. Reinforcement schedules (e.g., fixed-ratio, variable-interval) describe patterns of delivering reinforcers after a behavior is already established. They were developed later by Skinner and do not describe the foundational principle relating satisfying outcomes to behavioral recurrence.",
        "D": "Incorrect. Classical conditioning, developed by Pavlov, involves pairing a neutral stimulus with an unconditioned stimulus to elicit a reflexive response. It does not involve voluntary behavior or the role of satisfying consequences in increasing response probability."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-041-vignette-L2",
      "source_question_id": "041",
      "source_summary": "Thorndike's research with cats in a puzzle box led to his development of the law of effect, which predicts that behaviors followed by satisfying consequences are more likely to recur.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "satisfying outcome",
        "repeated"
      ],
      "vignette": "A behavioral researcher studying skill acquisition in children notes that a 7-year-old quickly learned to press a particular button on a tablet after discovering that pressing it produced an animated star and a cheerful sound. The child, who was described by parents as generally anxious and slow to try new tasks, nonetheless increased button-pressing behavior markedly over ten sessions. The researcher records that the pleasurable outcome following the button press appeared to stamp in the connection between the action and its result, leading the behavior to be repeated with increasing frequency.",
      "question": "The researcher's explanation of the child's learning most directly reflects which theoretical principle?",
      "options": {
        "A": "Negative reinforcement",
        "B": "Thorndike's law of effect",
        "C": "Observational learning",
        "D": "Premack's principle"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Thorndike's law of effect holds that responses followed by satisfying outcomes are strengthened and more likely to recur. The researcher's language — 'stamp in the connection' and 'pleasurable outcome' — directly mirrors Thorndike's original formulation.",
        "A": "Incorrect. Negative reinforcement involves the removal of an aversive stimulus to increase a behavior. The scenario describes the addition of a pleasant outcome (animated star and sound), not the removal of something unpleasant, so negative reinforcement does not apply.",
        "C": "Incorrect. Observational learning, associated with Bandura, involves acquiring behavior by watching others. The child in this scenario learns directly from the consequences of her own actions rather than by observing a model.",
        "D": "Incorrect. Premack's principle states that a high-frequency behavior can reinforce a low-frequency behavior. While it involves preferred outcomes, it specifically addresses using one activity to reinforce another, and it does not describe the broader foundational principle linking satisfying consequences to behavioral repetition as Thorndike formulated."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-041-vignette-L3",
      "source_question_id": "041",
      "source_summary": "Thorndike's research with cats in a puzzle box led to his development of the law of effect, which predicts that behaviors followed by satisfying consequences are more likely to recur.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "stamped in"
      ],
      "vignette": "A historian of psychology is reviewing early twentieth-century animal learning research. She describes an investigator who was puzzled by the way animals seemed to gradually eliminate ineffective escape responses over many trials, as though the unsuccessful movements were progressively weakened while one particular successful movement was 'stamped in.' Notably, the animals showed no evidence of insight or sudden comprehension — the improvement was gradual and appeared tied entirely to the outcome following each response. The historian contrasts this with later work suggesting that animals can form cognitive maps and solve problems in the absence of direct reward.",
      "question": "The early research described by the historian is best identified as the empirical basis for which learning principle?",
      "options": {
        "A": "Latent learning",
        "B": "Thorndike's law of effect",
        "C": "Operant extinction",
        "D": "The law of exercise"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The description captures Thorndike's puzzle-box findings: gradual trial-and-error learning in which satisfying consequences stamp in successful responses and unsatisfying consequences stamp out unsuccessful ones. This is the empirical foundation of the law of effect.",
        "A": "Incorrect. Latent learning, demonstrated by Tolman, refers to learning that occurs without direct reinforcement and is revealed only when a reward is later introduced. The historian explicitly contrasts the described research with this type of learning, noting the animals showed no cognitive insight and improvement depended on direct outcome.",
        "C": "Incorrect. Operant extinction refers to the weakening of a previously reinforced behavior when reinforcement is withheld. While weakening of unsuccessful responses is mentioned, the scenario centrally describes the strengthening mechanism tied to satisfying outcomes — the defining feature of the law of effect, not extinction.",
        "D": "Incorrect. Thorndike's law of exercise holds that connections are strengthened by repetition or practice, independent of outcome. The historian's description emphasizes that improvement was tied to the outcome following each response, not mere repetition, which distinguishes the law of effect from the law of exercise."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-041-vignette-L4",
      "source_question_id": "041",
      "source_summary": "Thorndike's research with cats in a puzzle box led to his development of the law of effect, which predicts that behaviors followed by satisfying consequences are more likely to recur.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "trial-and-error"
      ],
      "vignette": "A researcher in an early learning laboratory repeatedly places a hungry animal in an enclosure from which it must act to obtain food. On initial trials the animal emits a wide variety of behaviors; across sessions the researcher observes that the latency to perform the food-producing action decreases systematically while other behaviors drop out entirely. The researcher is careful to note that no external signal precedes the food — the food appears only as a direct consequence of the animal's action — and that the animal shows no sudden change in behavior consistent with a cognitive reorganization of the problem. A colleague suggests the results demonstrate contiguous stimulus–response bonding driven by temporal proximity alone, but the original researcher disagrees, arguing that the pleasurable nature of the food outcome, not merely its timing, is the critical explanatory variable.",
      "question": "The original researcher's position is most consistent with which learning principle?",
      "options": {
        "A": "Guthrie's law of contiguity",
        "B": "Hull's drive-reduction theory",
        "C": "Thorndike's law of effect",
        "D": "Skinner's principle of operant reinforcement"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The researcher explicitly argues that the pleasurable quality of the consequence — not mere temporal contiguity — strengthens the preceding response. This is the defining claim of Thorndike's law of effect, distinguishing it from purely contiguity-based accounts. The trial-and-error learning context and the puzzle-box-like scenario further identify this as Thorndike's framework.",
        "A": "Incorrect. Guthrie's law of contiguity argues that learning occurs through simple temporal pairing of stimulus and response, with no role for the satisfying or unsatisfying nature of the outcome. The colleague in the scenario endorses this view, but the original researcher explicitly rejects it, which rules out Guthrie's position as the answer.",
        "B": "Incorrect. Hull's drive-reduction theory holds that reinforcement reduces a biological drive state, which strengthens S-R connections. While Hull acknowledged the importance of reinforcement consequences, his framework is a mechanistic, drive-based elaboration developed well after Thorndike's original work; the scenario describes the foundational principle itself, not Hull's theoretical refinement.",
        "D": "Incorrect. Skinner's operant reinforcement principle also holds that consequences control behavior, but Skinner deliberately avoided mentioning mental states such as 'pleasurable' or 'satisfying,' instead defining reinforcement purely in functional terms. The researcher's explicit reference to the pleasurable nature of the outcome aligns more precisely with Thorndike's original hedonistic formulation of the law of effect."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-041-vignette-L5",
      "source_question_id": "041",
      "source_summary": "Thorndike's research with cats in a puzzle box led to his development of the law of effect, which predicts that behaviors followed by satisfying consequences are more likely to recur.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A scientist working in the late nineteenth century repeatedly places a hungry animal in a small enclosure with a single mechanical release mechanism. The animal initially moves about randomly, accidentally activating the release and obtaining food. Over many repetitions, the scientist notices that the time between placement and activation grows shorter and shorter, while unproductive movements gradually disappear, without any apparent moment of sudden understanding. A contemporary critic argues that the animal is simply becoming calmer and less agitated with each session, making the faster escape a byproduct of reduced emotionality rather than any selective process. The original scientist counters that the pattern of improvement is too systematic and too specifically tied to the outcome-producing action to be explained by reduced agitation alone, pointing instead to a direct relationship between the outcome and the specific action that produced it.",
      "question": "The original scientist's argument most directly anticipates which theoretical principle in the psychology of learning?",
      "options": {
        "A": "The principle that behaviors occurring most recently before a change in the environment are the ones most likely to be repeated",
        "B": "The principle that a satisfying outcome following a specific action increases the probability that the action will occur again",
        "C": "The principle that biological needs create internal tension states that are relieved by specific consummatory behaviors",
        "D": "The principle that an organism learns the structure of its environment through exploration, independent of any external reward"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The scenario describes Thorndike's puzzle-box research and the scientist's argument is a plain-language articulation of the law of effect: the satisfying consequence (food) directly strengthens the specific action (activating the release mechanism) that produced it, explaining the systematic decrease in response latency over trials.",
        "A": "Incorrect. This option describes Guthrie's contiguity principle, which holds that the last response made before a change in stimulation is the one that becomes associated with those stimuli. The scientist's argument goes beyond timing alone, emphasizing that the nature of the outcome — not just its contiguity — is responsible for the selective strengthening of the correct response.",
        "C": "Incorrect. This option describes Hull's drive-reduction theory, in which internal biological deprivation creates a drive state and reinforcement consists of reducing that drive. While hunger is present in the scenario, the scientist's argument focuses on the relationship between the action and its pleasant outcome, not on internal tension states or their reduction, which is Hull's theoretical emphasis.",
        "D": "Incorrect. This option describes Tolman's concept of latent learning and cognitive maps, in which organisms acquire knowledge of environmental structure without external reward. The scenario explicitly notes the absence of sudden comprehension and emphasizes that improvement is tied to the outcome-producing action, directly contradicting the reward-independent learning that Tolman's principle describes."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-052-vignette-L1",
      "source_question_id": "052",
      "source_summary": "Negative punishment occurs when a behavior decreases or stops because a stimulus (a loss of 50 cents from a weekly allowance) is removed following the behavior (teasing a younger sibling).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "negative punishment",
        "allowance",
        "behavior decreases"
      ],
      "vignette": "A school psychologist is consulting with parents about their 8-year-old son, Marcus, who repeatedly teases his younger sister during dinner. The parents decide to implement a consequence: each time Marcus teases his sister, they remove 50 cents from his weekly allowance. After two weeks, the teasing behavior decreases noticeably. The psychologist explains that this is a clear example of negative punishment at work, where removal of a valued stimulus leads to a reduction in the target behavior.",
      "question": "Which operant conditioning procedure best describes what the parents used to reduce Marcus's teasing behavior?",
      "options": {
        "A": "Negative reinforcement",
        "B": "Positive punishment",
        "C": "Negative punishment",
        "D": "Extinction"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Negative reinforcement involves the removal of an aversive stimulus to increase a behavior. Here, the parents are not removing something unpleasant to make Marcus more likely to do something; they are removing something pleasant (money) to decrease teasing, so this does not apply.",
        "B": "Positive punishment involves adding an aversive stimulus following a behavior to decrease it. While the goal (behavior decrease) is shared, the mechanism differs — the parents are taking something away rather than adding something unpleasant, so positive punishment does not describe this scenario.",
        "C": "Negative punishment involves removing a desirable stimulus (money from the allowance) following a behavior (teasing) in order to decrease that behavior. This exactly matches the scenario described, making it the correct answer.",
        "D": "Extinction involves withholding reinforcement that previously maintained a behavior, leading to its gradual decline. However, there is no indication that teasing was previously reinforced and that reinforcement is now being withheld; rather, an active consequence (removing money) is being applied, which distinguishes this from extinction."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-052-vignette-L2",
      "source_question_id": "052",
      "source_summary": "Negative punishment occurs when a behavior decreases or stops because a stimulus (a loss of 50 cents from a weekly allowance) is removed following the behavior (teasing a younger sibling).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "removed",
        "behavior decreases"
      ],
      "vignette": "Nine-year-old Daniela has recently been diagnosed with ADHD, and her parents are working with a behavioral therapist to manage her disruptive behavior at home. Daniela earns tokens throughout the week that can be redeemed for screen time on weekends. Whenever she interrupts her parents during phone calls, several tokens are removed from her collection. Over several weeks, her interrupting behavior decreases significantly. Her therapist notes that Daniela's improved behavior is due to the consistent application of this procedure rather than her ADHD medication, which was not changed during this period.",
      "question": "The procedure the therapist used to decrease Daniela's interrupting behavior is best described as which of the following?",
      "options": {
        "A": "Response cost",
        "B": "Positive reinforcement",
        "C": "Negative reinforcement",
        "D": "Positive punishment"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Response cost is a form of negative punishment in which a specified amount of a positive reinforcer (tokens) is removed contingent on an undesirable behavior, resulting in a decrease of that behavior. This precisely describes what Daniela's therapist implemented, making it the correct answer.",
        "B": "Positive reinforcement involves adding a desirable stimulus following a behavior to increase that behavior. In this scenario, nothing is being added to strengthen a behavior; rather, tokens are being removed to weaken interrupting, so this option does not apply.",
        "C": "Negative reinforcement involves removing an aversive stimulus to increase a target behavior. Although the word 'negative' might seem to fit because something is removed, what is removed here is a positive reinforcer (tokens) rather than something aversive, and the goal is to decrease rather than increase behavior.",
        "D": "Positive punishment involves adding an aversive stimulus to decrease a behavior. While it shares the goal of decreasing behavior, this procedure requires adding something unpleasant, not removing something pleasant, so it does not describe the token-removal procedure used here."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-052-vignette-L3",
      "source_question_id": "052",
      "source_summary": "Negative punishment occurs when a behavior decreases or stops because a stimulus (a loss of 50 cents from a weekly allowance) is removed following the behavior (teasing a younger sibling).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "token economy"
      ],
      "vignette": "A residential treatment program for adolescents with conduct disorder uses a token economy in which residents earn points for prosocial behaviors. Fourteen-year-old Jordan has been doing well and accumulating a large number of points. However, staff notice that Jordan has begun bullying a peer in unsupervised areas. Each time bullying is confirmed, the program director deducts 20 points from Jordan's total. Jordan initially argues loudly that the system is unfair and the bullying briefly intensifies before gradually declining over the following two weeks. Staff are encouraged by this trend and continue the procedure consistently.",
      "question": "The brief intensification of bullying Jordan showed before the behavior declined is best explained by which phenomenon, and what is the primary procedure being used to reduce bullying?",
      "options": {
        "A": "Extinction burst during an extinction procedure",
        "B": "Spontaneous recovery during a punishment procedure",
        "C": "Extinction burst during a negative punishment procedure",
        "D": "Counterconditioning during a positive punishment procedure"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "An extinction burst can occur during extinction, but extinction involves withholding the reinforcer that was maintaining the behavior. Here, points are actively removed as a consequence — this is a punishment procedure, not extinction. Therefore, while the burst-like increase is correctly identified, the procedure label is wrong.",
        "B": "Spontaneous recovery is the reappearance of a previously extinguished response after a rest period, not a temporary intensification during an active consequence procedure. The increase seen here occurred during ongoing point removal, not after a break, so spontaneous recovery does not explain it.",
        "C": "When a previously reinforced behavior first encounters a punishment or omission contingency, a temporary increase — analogous to an extinction burst — can occur before the behavior declines. Combined with the active removal of positive reinforcers (points) following bullying, this correctly identifies both the burst-like phenomenon and the negative punishment procedure being applied.",
        "D": "Counterconditioning involves pairing a conditioned stimulus with a new, incompatible response to change an emotional reaction, which is a classical conditioning technique. This does not describe the operant procedure of removing points following bullying, nor does it explain the brief behavioral increase observed."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-052-vignette-L4",
      "source_question_id": "052",
      "source_summary": "Negative punishment occurs when a behavior decreases or stops because a stimulus (a loss of 50 cents from a weekly allowance) is removed following the behavior (teasing a younger sibling).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "omission"
      ],
      "vignette": "A behavioral psychologist is supervising a case involving a 10-year-old named Priya who frequently talks back to her teacher. The team implements an omission-based contingency using classroom privileges — specifically, Priya loses five minutes of her preferred free-play period every time she talks back. During supervision, a practicum student argues that because Priya finds the free-play loss 'punishing,' the procedure must be positive punishment. The supervising psychologist gently corrects this reasoning, explaining that what defines a procedure is not the subjective experience of the recipient but the structural relationship between the behavior and the environmental change that follows it. Priya's talking back decreases steadily over the following month.",
      "question": "The supervising psychologist's correction implies that the procedure being used is best classified as which of the following, and why?",
      "options": {
        "A": "Positive punishment, because Priya experiences the loss of free play as aversive and her behavior decreases",
        "B": "Negative reinforcement, because removing an unpleasant contingency (loss) eventually allows access to free play, increasing compliance",
        "C": "Negative punishment, because a desirable stimulus (free-play time) is removed following the behavior, decreasing its future occurrence",
        "D": "Extinction, because reinforcement for talking back is being systematically withheld by the teacher"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Positive punishment requires that an aversive stimulus be added following the behavior. Priya's subjective experience of loss as unpleasant does not change the structural fact that something (free-play time) is being removed rather than added. The practicum student's reasoning conflates subjective aversiveness with the operant definition of positive punishment.",
        "B": "Negative reinforcement involves removing an aversive event following a behavior in order to increase that behavior. Here, nothing aversive is being removed to strengthen a desirable response; rather, a positive stimulus is removed to weaken an undesirable one. The word 'negative' and the mention of 'loss' make this option superficially attractive but mechanistically incorrect.",
        "C": "Negative punishment is defined by the removal of a positive stimulus following a behavior, resulting in a decrease of that behavior. Free-play time (a positive reinforcer) is removed contingent on talking back, and the behavior decreases. The supervising psychologist's point — that structural relationship, not subjective experience, defines the procedure — confirms this classification as correct.",
        "D": "Extinction involves discontinuing the reinforcer that was previously maintaining the behavior, without adding or removing other stimuli. There is no indication that a specific reinforcer was maintaining talking back and is now being withheld; instead, an active consequence (loss of free play) is imposed after each instance, which is a punishment procedure rather than extinction."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-052-vignette-L5",
      "source_question_id": "052",
      "source_summary": "Negative punishment occurs when a behavior decreases or stops because a stimulus (a loss of 50 cents from a weekly allowance) is removed following the behavior (teasing a younger sibling).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "Seven-year-old Eli loves the special dessert his grandmother sets aside for him each Friday evening as part of a cherished family tradition. Eli has recently started pinching his baby brother when adults are not watching, and his parents have tried ignoring the behavior and redirecting him without success. A family consultant suggests a new approach: each time pinching is confirmed through their home monitoring system, Eli's parents quietly inform him that Friday's dessert will not be available that week. Eli cries and protests each time he is told, but after the procedure is applied consistently over five weeks, the pinching drops to near zero. Eli's parents note that he never seems to connect his pinching to the dessert loss — he simply stops pinching.",
      "question": "Which learning principle most accurately accounts for the reduction in Eli's pinching behavior?",
      "options": {
        "A": "Extinction, because the parents stopped providing the attention that had been reinforcing the pinching behavior",
        "B": "Negative punishment, because a valued positive stimulus was removed contingent on the behavior, resulting in its decrease",
        "C": "Positive punishment, because the announcement of dessert loss functioned as an aversive event added after the behavior",
        "D": "Negative reinforcement, because once Eli stopped pinching, the unpleasant contingency was lifted and his behavior increased in a positive direction"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Extinction requires withholding the reinforcer that was specifically maintaining the behavior. The vignette states that ignoring (which would approximate extinction if attention were the reinforcer) was already tried and failed, and the effective change involved removing dessert access — a procedure structurally distinct from withholding a maintaining reinforcer. This makes extinction an incorrect classification for the successful procedure.",
        "B": "Negative punishment occurs when a positive stimulus is removed following a behavior, causing that behavior to decrease. The Friday dessert was a valued reward, and its removal was made contingent on each instance of pinching, after which the pinching reliably decreased. Even though Eli did not consciously connect the two events, the behavioral outcome and the structural contingency satisfy the definition of negative punishment.",
        "C": "Positive punishment involves adding an aversive stimulus following a behavior. The announcement of dessert loss might seem like an 'added' aversive event, but the actual mechanism is the removal of access to a positive reinforcer (the dessert itself), not the presentation of something noxious. The announcement is merely the communication of the contingency, not a punishing stimulus in itself.",
        "D": "Negative reinforcement involves removing an aversive event contingent on a behavior in order to increase that behavior. While Eli did stop pinching (a behavior that increased), the procedure removed a positive stimulus (dessert) rather than an aversive one, and the goal was to decrease pinching, not to strengthen a different behavior by lifting an unpleasant consequence."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-063-vignette-L1",
      "source_question_id": "063",
      "source_summary": "When two behaviors are being reinforced and the reinforcement for one behavior is suddenly stopped, the behavior that is no longer reinforced will decrease, while the behavior that is still receiving the same amount of reinforcement will increase.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "reinforcement",
        "extinction",
        "behavioral contrast"
      ],
      "vignette": "A researcher is studying two operant responses in a pigeon: pecking a red key and pecking a blue key. Both behaviors are maintained on concurrent schedules of reinforcement. The researcher then places pecking the red key on extinction while keeping reinforcement unchanged for the blue key. Over the following sessions, pecking the blue key increases substantially above its pre-extinction baseline, while red-key pecking declines steadily.",
      "question": "Which operant conditioning phenomenon best explains the observed increase in blue-key pecking following the extinction of red-key pecking?",
      "options": {
        "A": "Behavioral contrast",
        "B": "Spontaneous recovery",
        "C": "Stimulus generalization",
        "D": "Negative reinforcement"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Behavioral contrast occurs when the reinforcement for one response in a concurrent schedule is reduced or eliminated, causing the rate of the other concurrently reinforced response to increase above its prior baseline. This is exactly what is described: red-key extinction leads to a rise in blue-key responding.",
        "B": "Incorrect. Spontaneous recovery refers to the temporary reappearance of a previously extinguished response after a rest period, not an increase in a different, still-reinforced response. The scenario describes a rise in an alternative behavior, not a return of the extinguished one.",
        "C": "Incorrect. Stimulus generalization describes the spread of a conditioned response to stimuli similar to the original conditioned stimulus. It does not account for an increase in a separate operant response maintained by its own reinforcement contingency.",
        "D": "Incorrect. Negative reinforcement involves the removal of an aversive stimulus following a behavior, which strengthens that behavior. There is no mention of aversive stimuli being removed; the change in blue-key pecking is driven by the alteration in the competing reinforcement schedule."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-063-vignette-L2",
      "source_question_id": "063",
      "source_summary": "When two behaviors are being reinforced and the reinforcement for one behavior is suddenly stopped, the behavior that is no longer reinforced will decrease, while the behavior that is still receiving the same amount of reinforcement will increase.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "concurrent schedule",
        "behavioral contrast"
      ],
      "vignette": "A 10-year-old boy with ADHD frequently engages in two behaviors during homework time: asking relevant questions of his tutor and getting out of his seat. His tutor has been providing praise for both behaviors to encourage engagement. The tutor's supervisor notes that out-of-seat behavior should not be reinforced, so the tutor stops praising it while continuing to praise question-asking at the same rate. Over the next two weeks, out-of-seat behavior declines, but the boy's rate of asking questions increases well above what it was before the change.",
      "question": "Which phenomenon most precisely accounts for the unexpected increase in the boy's question-asking behavior?",
      "options": {
        "A": "Differential reinforcement of other behavior (DRO)",
        "B": "Positive punishment",
        "C": "Behavioral contrast",
        "D": "Shaping"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. Behavioral contrast occurs on concurrent schedules when reinforcement for one behavior is reduced or withdrawn, causing the rate of the concurrently reinforced behavior to rise above its prior baseline. The tutor's removal of praise for out-of-seat behavior, while keeping praise for question-asking constant, produced exactly this effect.",
        "A": "Incorrect. Differential reinforcement of other behavior (DRO) involves delivering reinforcement contingent on the absence of a target behavior for a specified interval; it is a procedure used to reduce a specific behavior. While the tutor did stop reinforcing out-of-seat behavior, DRO does not explain the supra-baseline increase in question-asking.",
        "B": "Incorrect. Positive punishment involves adding an aversive stimulus following a behavior to decrease it. The tutor simply withheld praise rather than adding anything aversive, and positive punishment does not predict a rise in an alternative behavior.",
        "D": "Incorrect. Shaping involves reinforcing successive approximations of a target behavior to build a new response. There is no progressive reinforcement of closer approximations described here; question-asking was already established and increased without any gradual shaping procedure."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-063-vignette-L3",
      "source_question_id": "063",
      "source_summary": "When two behaviors are being reinforced and the reinforcement for one behavior is suddenly stopped, the behavior that is no longer reinforced will decrease, while the behavior that is still receiving the same amount of reinforcement will increase.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "concurrent"
      ],
      "vignette": "A graduate student is running a study in which rats press two levers simultaneously available in their home cage. For months, both lever presses have yielded food pellets on variable interval schedules. The student then discontinues the pellet dispenser connected to the left lever while leaving the right-lever schedule completely unchanged. Although the left-lever pressing drops as expected, the student notices that right-lever pressing climbs to rates noticeably higher than those recorded before the left-lever food was discontinued. The student's advisor tells her this finding is a classic and well-replicated laboratory phenomenon.",
      "question": "What learning phenomenon does the advisor most likely have in mind?",
      "options": {
        "A": "Resurgence",
        "B": "Behavioral contrast",
        "C": "Induction",
        "D": "Matching law reallocation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Behavioral contrast is the well-replicated finding that, within concurrent operant schedules, reducing or eliminating reinforcement for one response produces a supra-baseline increase in the rate of the concurrently reinforced alternative response, even though that alternative's reinforcement schedule has not changed. This is precisely what the rat data show.",
        "A": "Incorrect. Resurgence refers to the reappearance of a previously reinforced (and later extinguished) behavior when a more recently reinforced behavior is itself placed on extinction. The scenario involves an increase in a currently reinforced behavior, not the return of a previously extinguished one, making resurgence an inadequate explanation.",
        "C": "Incorrect. Induction (or behavioral induction) refers to the phenomenon in which reinforcing one behavior can temporarily increase other behaviors in the organism's repertoire. While related, induction predicts a broad, transient increase across many behaviors, whereas behavioral contrast specifically predicts the supra-baseline increase in the alternative concurrent response and is the more precise and classic label for this procedure.",
        "D": "Incorrect. The matching law describes how organisms distribute responses across concurrent schedules in proportion to relative reinforcement rates. While the matching law would predict a shift in response allocation after one schedule is changed, it does not specifically predict a supra-baseline overshoot in the remaining reinforced behavior. The observed rate exceeds what matching law reallocation alone would predict, which is the defining feature of behavioral contrast."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-063-vignette-L4",
      "source_question_id": "063",
      "source_summary": "When two behaviors are being reinforced and the reinforcement for one behavior is suddenly stopped, the behavior that is no longer reinforced will decrease, while the behavior that is still receiving the same amount of reinforcement will increase.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "supra-baseline"
      ],
      "vignette": "A behavior analyst is treating a child who exhibits both self-injurious behavior (SIB) and appropriate toy play, both of which have been maintained by adult attention in naturalistic observations. As part of the intervention plan, the analyst instructs caregivers to fully withdraw attention following any instance of SIB while ensuring attention for toy play remains at exactly the same frequency and duration as before the intervention. Within three weeks, SIB drops markedly. Caregivers report, however, that toy-play episodes have become noticeably longer and more frequent than they ever observed prior to the intervention, even though they are certain they have not increased their attention for play. The analyst is unsurprised and documents this outcome as an expected and desirable side effect.",
      "question": "Which principle from the operant literature best explains why toy play increased beyond its pre-intervention baseline without any change in the reinforcement delivered for it?",
      "options": {
        "A": "Extinction-induced variability",
        "B": "Differential reinforcement of incompatible behavior (DRI)",
        "C": "Behavioral contrast",
        "D": "Response generalization"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. Behavioral contrast predicts that when reinforcement for one behavior in a concurrent arrangement is removed, the rate of the alternative concurrently reinforced behavior will increase above its prior baseline — even when the reinforcement schedule for that alternative is held constant. The supra-baseline rise in toy play, despite unchanged caregiver attention for play, is the hallmark finding of behavioral contrast, which is why an experienced behavior analyst would anticipate it.",
        "A": "Incorrect. Extinction-induced variability refers to the phenomenon in which organisms placed on extinction emit a wider variety of novel behaviors, including responses not previously in their repertoire. While it is associated with the extinction of SIB, it does not specifically predict a supra-baseline increase in a particular already-established alternative behavior that retains its reinforcement.",
        "B": "Incorrect. Differential reinforcement of incompatible behavior (DRI) is a behavior-reduction procedure in which a behavior that cannot physically co-occur with the target behavior is selectively reinforced. Although the intervention here involves reinforcing toy play, caregivers did not increase or modify the reinforcement for toy play in any way, so DRI cannot account for the increase beyond prior baseline.",
        "D": "Incorrect. Response generalization refers to the spread of a behavior change to other topographically similar responses that were not directly targeted. Toy play is a functionally distinct behavior in a concurrent schedule, not a topographic variant of SIB or of another targeted response; furthermore, response generalization does not specifically predict a supra-baseline increase in a concurrently reinforced alternative."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-063-vignette-L5",
      "source_question_id": "063",
      "source_summary": "When two behaviors are being reinforced and the reinforcement for one behavior is suddenly stopped, the behavior that is no longer reinforced will decrease, while the behavior that is still receiving the same amount of reinforcement will increase.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A preschool teacher has been giving sticker rewards to children both for cleaning up their materials and for working quietly at their desks, and both activities occur regularly throughout the day. During a three-week observation, the teacher stops giving stickers for cleaning up because the classroom aide now handles that transition without rewards. The teacher keeps giving stickers for quiet desk work at exactly the same rate as before. By the end of the three weeks, the children's cleaning-up behavior has become more inconsistent and brief, but the children are spending substantially more time sitting quietly at their desks than they ever did before — more time, in fact, than would be expected simply because cleanup is taking less time.",
      "question": "What learning principle most precisely explains why quiet desk-work time increased beyond what it had been before the sticker policy changed, given that the sticker reward for desk work never changed?",
      "options": {
        "A": "The removal of stickers for cleanup freed up more minutes in the day, making more desk-work time structurally available.",
        "B": "The children's desk-work behavior was strengthened because stickers became relatively more valuable once they were no longer available for cleanup.",
        "C": "The increase in desk-work reflects the supra-baseline rise in a concurrently reinforced behavior that occurs when reinforcement for an alternative behavior in the same context is discontinued.",
        "D": "The teacher inadvertently increased her attention and encouragement during desk work to compensate for losing cleanup as a reinforcement opportunity."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. This option describes behavioral contrast: when one of two concurrently reinforced behaviors loses its reinforcement, the rate of the other, still-reinforced behavior rises above its prior baseline even without any change to that behavior's reinforcement schedule. The vignette specifies that stickers for desk work were unchanged and that the increase exceeded what could be explained by time freed up from cleanup — the defining criterion of behavioral contrast rather than a structural or motivational account.",
        "A": "Incorrect. This option offers a plausible structural explanation — more available minutes could mean more desk-work time. However, the vignette explicitly states that the increase in desk work exceeds what the newly available time would account for, ruling out a purely scheduling-based explanation and pointing instead to an operant learning phenomenon.",
        "B": "Incorrect. This option invokes motivating operations and the relative value of reinforcers: when one source of stickers is removed, the remaining stickers may become more motivating (establishing operation), potentially increasing behavior maintained by those stickers. While this is a legitimate operant mechanism and superficially compelling, behavioral contrast is specifically defined by a supra-baseline increase in a concurrently reinforced response under unchanged reinforcement, not by a change in reinforcer value — the two mechanisms make similar predictions but are conceptually distinct, and behavioral contrast is the more precise and directly applicable concept here.",
        "D": "Incorrect. This option attributes the increase in desk-work time to an unintentional change in teacher behavior — specifically, more attention or encouragement — which would represent an inadvertent increase in reinforcement for desk work. The vignette, however, is explicit that sticker delivery for desk work remained at exactly the same rate as before, and there is no evidence that the teacher altered other behaviors toward the children during desk work, making this explanation unsupported by the described facts."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-074-vignette-L1",
      "source_question_id": "074",
      "source_summary": "Skinner referred to the unusual behaviors, such as bowing, turning, and hopping on one foot, performed by pigeons that were receiving food pellets at a fixed interval regardless of their behavior as superstitious behaviors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "superstitious behavior",
        "fixed interval",
        "pigeons"
      ],
      "vignette": "A researcher studying operant conditioning places pigeons in individual chambers and delivers food pellets on a fixed interval schedule, regardless of what the pigeons are doing at the moment each pellet is released. Over time, each pigeon develops a unique, repetitive behavior — one bows its head, another hops on one foot, and a third repeatedly turns in circles. The researcher notes that these behaviors are reliably performed before each food delivery, even though they have no actual effect on whether the food is delivered. Skinner famously described this phenomenon as analogous to human rituals performed in hopes of influencing outcomes.",
      "question": "What behavioral phenomenon do the pigeons' repetitive actions best illustrate?",
      "options": {
        "A": "Superstitious behavior",
        "B": "Shaping",
        "C": "Chaining",
        "D": "Adventitious reinforcement schedule effects on variable ratio responding"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. Superstitious behavior, as described by Skinner, occurs when an organism's behavior is adventitiously reinforced by a response-independent schedule. The pigeon's idiosyncratic actions were accidentally paired with food delivery, leading to the strengthening of whatever behavior happened to precede reinforcement, despite no true contingency existing.",
        "B": "This is incorrect. Shaping involves the systematic reinforcement of successive approximations toward a target behavior. In this scenario, the experimenter is not differentially reinforcing any specific behavior; food is delivered independently of what the pigeon does, making shaping inapplicable.",
        "C": "This is incorrect. Chaining refers to a procedure in which a sequence of discrete behaviors is linked together, with each behavior serving as both a conditioned reinforcer for the preceding response and a discriminative stimulus for the next. No intentional behavioral sequence is being constructed here.",
        "D": "This is incorrect. A variable ratio schedule delivers reinforcement after an unpredictable number of responses, which tends to produce high, steady rates of responding. The scenario describes a fixed interval schedule with response-independent delivery, not a variable ratio arrangement."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-074-vignette-L2",
      "source_question_id": "074",
      "source_summary": "Skinner referred to the unusual behaviors, such as bowing, turning, and hopping on one foot, performed by pigeons that were receiving food pellets at a fixed interval regardless of their behavior as superstitious behaviors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "adventitious reinforcement",
        "interval"
      ],
      "vignette": "A behavioral psychologist working with a 7-year-old child with intellectual disability notices that the child engages in repetitive rocking immediately before snack time, which occurs every 20 minutes throughout the school day regardless of the child's behavior. The child has no history of self-injurious behavior, and the rocking is described by teachers as rhythmic and calm rather than distressed. The psychologist hypothesizes that the rocking intensified because it was inadvertently paired with the delivery of snacks through adventitious reinforcement on an interval-based delivery schedule. No teacher had intentionally reinforced rocking at any point.",
      "question": "Which learning phenomenon most accurately accounts for the strengthening of the child's rocking behavior?",
      "options": {
        "A": "Operant chaining",
        "B": "Negative reinforcement",
        "C": "Superstitious behavior",
        "D": "Fixed interval scalloping"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. Superstitious behavior arises when a response is accidentally strengthened because it happened to precede a response-independent reinforcer. Here, the rocking was coincidentally occurring before snack delivery at a regular interval, creating an adventitious contingency that strengthened the behavior despite no true response-reinforcer relationship.",
        "A": "This is incorrect. Operant chaining involves the deliberate linking of a series of behaviors into a sequence, with each step serving as a discriminative stimulus for the next. There is no intentional chain being built here, and the child's rocking is a single, isolated behavior rather than a link in a trained sequence.",
        "B": "This is incorrect. Negative reinforcement involves the removal of an aversive stimulus contingent on a behavior, which increases the frequency of that behavior. Snack delivery is a positive event added to the environment, not the removal of something aversive, so negative reinforcement does not apply.",
        "D": "This is incorrect. Fixed interval scalloping refers to the characteristic pattern of responding under fixed interval schedules — slow responding after reinforcement, followed by accelerating rates as the interval end approaches. While the snack delivery is on a fixed interval, 'fixed interval scalloping' describes a response rate pattern under deliberate contingency, not the inadvertent strengthening of an accidental behavior."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-074-vignette-L3",
      "source_question_id": "074",
      "source_summary": "Skinner referred to the unusual behaviors, such as bowing, turning, and hopping on one foot, performed by pigeons that were receiving food pellets at a fixed interval regardless of their behavior as superstitious behaviors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "adventitious"
      ],
      "vignette": "A behavioral researcher observes that a college student studying for exams has begun tapping her pen three times on her notebook before beginning each study session, a habit she developed after a semester in which she received unexpectedly high exam scores. She reports feeling that the tapping 'helps her focus,' but careful review of her study logs reveals that her performance was not reliably better on days she tapped compared to days she did not. The researcher notes that positive academic outcomes during that semester were delivered at roughly predictable intervals regardless of her pen-tapping, and the tapping was adventitiously present just before several high-scoring results. She remains convinced the ritual is effective despite evidence to the contrary.",
      "question": "Which behavioral concept best explains why the student continues to tap her pen before studying?",
      "options": {
        "A": "Superstitious behavior",
        "B": "Positive reinforcement of a preparatory behavior",
        "C": "Stimulus control",
        "D": "Covert sensitization"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. The pen-tapping was adventitiously paired with high exam scores that were delivered independently of the behavior, strengthening the tapping through a non-contingent reinforcement process. This is the defining feature of Skinnerian superstitious behavior: a behavior is maintained because it accidentally preceded reinforcement even though no true contingency exists.",
        "B": "This is incorrect. Positive reinforcement requires a genuine contingency between the behavior and a subsequent reinforcer. Although the student believes the tapping helps her, the data show no reliable relationship between tapping and performance, ruling out a true operant reinforcement process.",
        "C": "This is incorrect. Stimulus control refers to a situation in which a discriminative stimulus reliably evokes a behavior because the behavior has been reinforced in the presence of that stimulus. The pen-tapping itself is a behavior, not a stimulus controlling other behaviors, and the scenario does not describe differential reinforcement in the presence of a specific cue.",
        "D": "This is incorrect. Covert sensitization is a behavioral therapy technique that pairs imagined aversive consequences with an unwanted behavior to reduce it. It is a therapeutic procedure designed to decrease behavior, not a naturally occurring process that explains the acquisition or maintenance of a ritual."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-074-vignette-L4",
      "source_question_id": "074",
      "source_summary": "Skinner referred to the unusual behaviors, such as bowing, turning, and hopping on one foot, performed by pigeons that were receiving food pellets at a fixed interval regardless of their behavior as superstitious behaviors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "coincidental"
      ],
      "vignette": "A competitive tennis player began bouncing the ball exactly five times before serving during a tournament week in which he won every match. He had never counted bounces deliberately before that week, but the wins came at coincidental moments that did not depend on how he warmed up or served. Over the following season, his win rate returned to his historical average, yet the five-bounce ritual became more rigid — he feels anxious and restarts the entire routine if he loses count. His sports psychologist notes that the behavior intensified most during the original winning streak and that the player attributes causation where none statistically exists. The psychologist is careful to distinguish this from obsessive-compulsive patterns because no intrusive cognitions are reported and the behavior causes only mild functional interference.",
      "question": "The tennis player's rigid pre-serve ritual is most precisely explained by which psychological concept?",
      "options": {
        "A": "Avoidance conditioning",
        "B": "Superstitious behavior",
        "C": "Habit formation via variable ratio reinforcement",
        "D": "Preparatory response theory of classical conditioning"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The player's ritual was strengthened because it coincidentally preceded wins that were delivered independently of his serve preparation — a hallmark of Skinnerian superstitious behavior, where an accidental temporal contiguity between a response and a non-contingent reinforcer strengthens the response. The psychologist's note that causation is attributed where none exists and that the ritual intensified during the winning streak both precisely match this phenomenon.",
        "A": "This is incorrect. Avoidance conditioning involves a behavior that is maintained because it prevents or postpones an aversive stimulus. While the player experiences anxiety when he cannot complete the ritual, the original acquisition of the behavior was not driven by avoidance of an aversive outcome; it was driven by accidental pairing with positive outcomes, making avoidance conditioning a partial but incorrect explanation.",
        "C": "This is incorrect. Variable ratio reinforcement delivers a reinforcer after an unpredictable number of responses and produces highly resistant, high-rate responding. Although wins during a season might seem variable, the wins were not contingent on the bouncing behavior at all — the reinforcement was response-independent, not response-dependent on a variable ratio, which is the critical distinction.",
        "D": "This is incorrect. Preparatory response theory posits that conditioned responses prepare the organism for the unconditioned stimulus by mimicking physiological preparatory reactions. This is a classical conditioning framework focused on automatic physiological responses, whereas the player's ritual is a voluntary, operant behavior that was accidentally rather than systematically paired with an outcome."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-074-vignette-L5",
      "source_question_id": "074",
      "source_summary": "Skinner referred to the unusual behaviors, such as bowing, turning, and hopping on one foot, performed by pigeons that were receiving food pellets at a fixed interval regardless of their behavior as superstitious behaviors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A professional basketball player always eats a peanut butter sandwich exactly 90 minutes before tip-off, a routine he adopted during a stretch of games three seasons ago in which he scored career highs. His nutritionist confirms the sandwich provides no performance advantage over his normal pre-game meal, and statistical review of his career shows his scoring during that original streak was well within normal fluctuation. He has since refused to alter the meal timing even on road trips when logistics make it genuinely difficult, and he becomes irritable and distracted if the routine is disrupted. Teammates report his performance is no better on days the routine is perfectly executed than on the rare occasions it is interrupted, yet his belief in its necessity strengthens with each passing season.",
      "question": "Which psychological phenomenon most precisely accounts for the origin and persistence of the player's pre-game meal ritual?",
      "options": {
        "A": "Negative reinforcement via anxiety reduction",
        "B": "Superstitious behavior",
        "C": "Classical conditioning of a preparatory response",
        "D": "Habitual behavior maintained by a variable ratio schedule"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The ritual originated when the meal coincidentally preceded strong performances that occurred independently of what the player ate, creating an accidental temporal pairing that strengthened the behavior — the defining feature of superstitious behavior as described by Skinner. Crucially, the reinforcer (good performance) was not contingent on the behavior, and the data confirm no true performance benefit, yet the behavior persists due to the original adventitious pairing.",
        "A": "This is incorrect. Negative reinforcement could explain the maintenance of the ritual if the meal timing removes anxiety, and the player's irritability when the routine is disrupted is a compelling red herring. However, negative reinforcement does not account for the origin of the ritual, which preceded any anxiety about disruption; the anxiety arose because the ritual was already established, not the other way around. Superstitious behavior better explains the full acquisition and maintenance picture.",
        "C": "This is incorrect. Classical conditioning of a preparatory response would predict that a neutral stimulus (the meal) becomes associated with a biologically significant event (physical exertion) and begins to elicit preparatory physiological reactions. While this framework could explain some pre-game routines, it does not account for the critical detail that the behavior was selected because it accidentally preceded positive outcomes on a non-contingent basis — an operant rather than classical process.",
        "D": "This is incorrect. A variable ratio schedule requires that reinforcement is delivered contingent on a varying number of responses, producing high resistance to extinction. Although wins do occur unpredictably across a season, wins are not contingent on the meal ritual at all — the reinforcement is entirely response-independent. This non-contingency is what distinguishes superstitious behavior from behavior maintained by a variable ratio schedule."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-03-vignette-L1",
      "source_question_id": "03",
      "source_summary": "When using classical conditioning to elicit a new response from a neutral stimulus, the neutral stimulus becomes a conditioned stimulus and elicits a conditioned response that is usually less in intensity than the unconditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "conditioned stimulus",
        "unconditioned response",
        "conditioned response"
      ],
      "vignette": "A researcher pairs a tone (neutral stimulus) with a mild electric shock (unconditioned stimulus) repeatedly. The shock reliably produces a strong startle reflex (unconditioned response) in participants. After several pairings, the tone alone begins to elicit a startle reflex in participants. The researcher notes that the startle reflex produced by the tone alone is noticeably weaker than the startle produced by the shock itself.",
      "question": "Which of the following best describes the relationship between the conditioned response and the unconditioned response observed in this experiment?",
      "options": {
        "A": "The conditioned response is typically weaker in magnitude than the unconditioned response",
        "B": "The conditioned response equals the unconditioned response in magnitude once conditioning is complete",
        "C": "The conditioned stimulus eventually replaces the unconditioned stimulus and produces an equally strong response",
        "D": "The conditioned response is stronger than the unconditioned response because of repeated pairing"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. A hallmark of classical conditioning is that the conditioned response (CR), while functionally similar to the unconditioned response (UCR), is typically reduced in intensity. The tone elicits a startle, but it is weaker than the startle caused by the shock, consistent with the principle that the CS-elicited CR is usually a diminished version of the UCR.",
        "B": "This is incorrect. Classical conditioning does not result in equivalent magnitudes of the CR and UCR. The UCR is a full, reflexive biological reaction to the unconditioned stimulus, while the CR is a learned, preparatory response that is generally less intense.",
        "C": "This is incorrect. The conditioned stimulus does not replace the unconditioned stimulus in terms of biological potency. The CS acquires the ability to elicit a response, but that response is attenuated compared to the UCR produced by the original UCS.",
        "D": "This is incorrect. Repeated pairing does not cause the CR to exceed the UCR in strength. If anything, the CR typically asymptotes at a level below the UCR, because the CR is a learned anticipatory reaction rather than a full reflexive response."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-03-vignette-L2",
      "source_question_id": "03",
      "source_summary": "When using classical conditioning to elicit a new response from a neutral stimulus, the neutral stimulus becomes a conditioned stimulus and elicits a conditioned response that is usually less in intensity than the unconditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "neutral stimulus",
        "extinction"
      ],
      "vignette": "A five-year-old boy was hospitalized for a painful medical procedure during which a nurse in blue scrubs administered several injections. Prior to hospitalization, the boy showed no fear response to people in blue scrubs. After returning home, the boy begins to cry and show mild distress whenever he sees anyone wearing blue scrubs, although his distress is noticeably less intense than the crying and distress he displayed during the actual injections. His parents attempt to help by repeatedly exposing him to nurses in blue scrubs in a safe pediatric clinic, with no injections occurring, until the fear response diminishes. The boy is otherwise healthy and has no prior history of anxiety disorders.",
      "question": "The reduction in the boy's fear response to blue scrubs compared to his response during the actual injections is best explained by which principle of classical conditioning?",
      "options": {
        "A": "Extinction reduces the conditioned response to zero after sufficient non-reinforced exposures",
        "B": "The conditioned response to the formerly neutral stimulus is typically less intense than the unconditioned response",
        "C": "Stimulus generalization causes the conditioned response to spread to related stimuli with equal intensity",
        "D": "Higher-order conditioning produces a response to the neutral stimulus that matches the original unconditioned response"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. After classical conditioning, the formerly neutral stimulus (blue scrubs) becomes a conditioned stimulus that elicits a conditioned response (fear/crying). However, the CR is typically less intense than the UCR (the distress during the painful injections), which is precisely what the vignette describes.",
        "A": "This is incorrect. Extinction refers to the procedure of repeatedly presenting the CS without the UCS, leading to a gradual reduction in the CR over time. While the parents' intervention describes an extinction procedure, extinction does not explain why the initial CR was weaker than the UCR — it explains later reduction of the CR after non-reinforced exposures.",
        "C": "This is incorrect. Stimulus generalization refers to the phenomenon whereby stimuli similar to the CS also elicit the CR. Generalization does not explain the difference in intensity between the CR and the UCR; it describes the breadth of stimuli that elicit a response, not the relative magnitude of the response.",
        "D": "This is incorrect. Higher-order conditioning occurs when a well-established CS is used to condition a new neutral stimulus, creating a second-order CS. This process actually tends to produce weaker CRs with each successive order of conditioning, but it does not explain why the first-order CR is weaker than the UCR in a single conditioning episode."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-03-vignette-L3",
      "source_question_id": "03",
      "source_summary": "When using classical conditioning to elicit a new response from a neutral stimulus, the neutral stimulus becomes a conditioned stimulus and elicits a conditioned response that is usually less in intensity than the unconditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "preparatory response"
      ],
      "vignette": "A clinical researcher studying anxiety disorders notices that a combat veteran flinches and tenses his muscles whenever he hears a car backfire, even though he displays a far more pronounced startle and physiological arousal response when he unexpectedly encounters a loud explosion in a film. The veteran reports that he cannot control either response. The researcher hypothesizes that the car backfire has come to signal danger through repeated association with actual explosions during deployment, and that the veteran's body generates a preparatory response to the car backfire that anticipates, but does not fully replicate, the full alarm reaction caused by the explosion itself. A colleague suggests the difference in intensity might instead reflect habituation to the car backfire over time.",
      "question": "Which principle most directly explains why the veteran's response to the car backfire is less intense than his response to the film explosion?",
      "options": {
        "A": "Habituation has reduced the veteran's response to frequently encountered stimuli such as the car backfire",
        "B": "The conditioned response elicited by a learned signal is inherently less intense than the unconditioned response to the original aversive stimulus",
        "C": "Spontaneous recovery causes a partial return of the response after a period of reduced exposure, accounting for the residual but diminished response",
        "D": "Stimulus discrimination has caused the veteran to produce a graded response calibrated to the relative danger of each stimulus"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The researcher's hypothesis aligns precisely with the classical conditioning principle that the CR is typically a diminished version of the UCR. The car backfire (CS) elicits a preparatory tensing and flinch (CR), while the actual explosion (UCS analogue) produces the full startle and arousal (UCR). This difference in magnitude is an inherent feature of classical conditioning, not an artifact of habituation or discrimination.",
        "A": "This is incorrect and represents the red herring planted by the colleague. Habituation does involve decreased responding to repeated stimuli, but habituation is a non-associative process that does not require pairing with an aversive stimulus. More importantly, habituation would reduce the response over time, whereas the vignette presents the weaker CR as a characteristic feature of the conditioned relationship, not as a time-dependent decline.",
        "C": "This is incorrect. Spontaneous recovery refers to the reappearance of a previously extinguished CR after a rest period, and it produces a partial, weaker response compared to the peak CR. However, the vignette does not describe any extinction procedure, so spontaneous recovery is not applicable here.",
        "D": "This is incorrect. Stimulus discrimination refers to the ability to distinguish between stimuli that are paired with the UCS and those that are not, producing differential responses. While the veteran does respond differently to the two stimuli, the explanation here is not discrimination — it is the inherent property of conditioning that the CR is weaker than the UCR, regardless of the organism's discriminative ability."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-03-vignette-L4",
      "source_question_id": "03",
      "source_summary": "When using classical conditioning to elicit a new response from a neutral stimulus, the neutral stimulus becomes a conditioned stimulus and elicits a conditioned response that is usually less in intensity than the unconditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "magnitude"
      ],
      "vignette": "A psychophysiology researcher pairs a soft 500 Hz tone with a moderately painful air puff to the cornea across 40 trials with healthy adult volunteers. By trial 40, participants reliably blink to the tone alone, and the researcher carefully measures peak eyelid velocity during the tone-only trials versus the air puff-only trials. Strikingly, and despite complete acquisition of the blink to the tone, the peak eyelid velocity during tone-alone presentations is consistently 35–40% lower than during air puff-alone presentations. The researcher rules out motor fatigue and sensory adaptation as explanations. A graduate student argues that this magnitude difference indicates that conditioning is incomplete and that more trials would eliminate the discrepancy.",
      "question": "The graduate student's interpretation is incorrect because the observed magnitude difference is best explained by which principle?",
      "options": {
        "A": "Incomplete conditioning — the CS has not yet fully acquired the capacity to substitute for the UCS, and additional trials would close the magnitude gap",
        "B": "Response competition — the tone simultaneously elicits an orienting response that inhibits the full expression of the conditioned eyeblink",
        "C": "An inherent property of classical conditioning in which the conditioned response is characteristically less intense than the unconditioned response even after full acquisition",
        "D": "Overshadowing — the intensity of the air puff outcompetes the tone, preventing the tone from acquiring full associative strength"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The magnitude difference is not a sign of incomplete conditioning; it is a characteristic and expected feature of classical conditioning. Even after full acquisition (as evidenced by reliable responding on tone-alone trials), the CR is inherently less intense than the UCR. The 35–40% reduction in eyelid velocity is consistent with this principle, and additional trials would not eliminate it.",
        "A": "This is incorrect and represents the graduate student's erroneous view. Incomplete conditioning would mean the CS has not yet reached asymptotic associative strength, but the vignette states that responding is reliable by trial 40. More trials would not close the magnitude gap because the CR being weaker than the UCR is an intrinsic property of conditioning, not a sign of insufficient training.",
        "B": "This is incorrect. Response competition is a plausible distractor because the tone could theoretically elicit an orienting response that partially antagonizes the eyeblink. However, this mechanism is not the primary explanation for the magnitude difference and would predict variable interference depending on novelty of the tone, which would diminish as the tone becomes familiar — the vignette presents the effect as stable across acquisition.",
        "D": "This is incorrect. Overshadowing occurs during the acquisition phase when a more salient stimulus in a compound CS captures disproportionate associative strength, leaving the less salient stimulus with weak conditioning. Overshadowing predicts weak or absent responding to the overshadowed CS, not reliable but attenuated responding — the vignette clearly states reliable acquisition has occurred."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-03-vignette-L5",
      "source_question_id": "03",
      "source_summary": "When using classical conditioning to elicit a new response from a neutral stimulus, the neutral stimulus becomes a conditioned stimulus and elicits a conditioned response that is usually less in intensity than the unconditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A young woman grew up near a factory that occasionally released a distinctive chemical odor just before its emergency alarm sounded. After years of this pattern, she notices that whenever she detects the odor while driving past the factory, she grips the steering wheel more tightly, her heart rate increases slightly, and she feels a vague sense of unease — even when the alarm never sounds. Her coworker, who also drives past the factory daily but began doing so only recently, shows no such reaction to the odor. Notably, the woman's reaction to the odor alone, while real and measurable, is substantially less intense than the full alarm reaction she displays on the rare occasions when the alarm actually does sound.",
      "question": "The difference in intensity between the woman's reaction to the odor alone and her reaction to the alarm is best explained by which learning principle?",
      "options": {
        "A": "The woman has become sensitized to the odor through repeated pairings, but sensitization produces only a partial amplification of the baseline response, accounting for the reduced intensity compared to the alarm",
        "B": "The woman's reaction to the odor reflects a learned association formed over time, and such learned reactions characteristically occur with less intensity than the original reflexive reaction that established them",
        "C": "The coworker's lack of response demonstrates that the woman has undergone operant conditioning, reinforcing her vigilance behavior near the factory, which is maintained only partially by the odor cue",
        "D": "The woman's reduced reaction to the odor compared to the alarm indicates that repeated exposure to the odor without the alarm has already begun to weaken the association through a process of gradual non-reinforced exposure"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The vignette describes a scenario consistent with classical conditioning: the odor (formerly neutral, now conditioned stimulus) was repeatedly paired with the alarm (unconditioned stimulus), producing a learned reaction (conditioned response) to the odor alone. The key detail is that her reaction to the odor is real but substantially less intense than her reaction to the alarm itself, which is the defining characteristic of the CR being weaker in magnitude than the UCR even after full acquisition.",
        "A": "This is incorrect. Sensitization is a non-associative process in which repeated exposure to a stimulus increases the magnitude of a response to that stimulus or to other stimuli. Sensitization does not require pairing between two stimuli, and it does not produce a response that is characteristically weaker than a separately defined reflexive response. The woman's differential reaction to odor versus alarm cannot be explained by sensitization.",
        "C": "This is incorrect. Operant conditioning involves behavior being strengthened or weakened by its consequences — reinforcement or punishment following voluntary responses. The woman's physiological reactions (increased heart rate, tension) are involuntary responses, not operant behaviors shaped by their outcomes. The coworker's lack of response reflects absence of prior pairing history, not a control condition for operant learning.",
        "D": "This is incorrect and represents a carefully planted red herring. The description could suggest that the woman has experienced extinction trials (odor without alarm) that have partially weakened the association. However, the vignette does not describe recent changes in her responding — her reaction is presented as a stable, established phenomenon. More importantly, the question asks about the inherent difference in intensity between CR and UCR, which exists even at peak conditioning, not as a product of extinction."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-01-vignette-L1",
      "source_question_id": "01",
      "source_summary": "Pavlov found that requiring dogs to make difficult discriminations between similar stimuli provoked agitation and aggression, which he referred to as experimental neurosis.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "classical conditioning",
        "discrimination",
        "experimental neurosis"
      ],
      "vignette": "A researcher replicates one of Pavlov's classic studies in classical conditioning. Dogs are trained to salivate to a circle but not to an ellipse. Over successive trials, the discrimination task is made progressively harder by making the ellipse more and more circular until the two shapes are nearly indistinguishable. As the discrimination becomes nearly impossible, the dogs begin to squirm in their harnesses, bark aggressively, and show signs of extreme agitation that were not present earlier in the experiment.",
      "question": "The behavioral deterioration observed in the dogs when required to make an extremely difficult discrimination is best described as which of the following phenomena?",
      "options": {
        "A": "Stimulus generalization",
        "B": "Experimental neurosis",
        "C": "Extinction burst",
        "D": "Conditioned inhibition"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Stimulus generalization refers to the tendency for a conditioned response to be elicited by stimuli similar to the original conditioned stimulus, not the breakdown of behavior under impossible discrimination demands. The dogs here are showing agitation, not responding to a novel similar stimulus.",
        "B": "Experimental neurosis is the term Pavlov used to describe the agitated, aggressive, and disorganized behavior that emerged when animals were forced to make extremely difficult or impossible discriminations between similar stimuli. The progressive narrowing of the ellipse toward a circle until the task became nearly impossible is the defining feature of this phenomenon.",
        "C": "An extinction burst is a temporary increase in the frequency or intensity of a previously reinforced operant behavior when reinforcement is suddenly withdrawn. This is an operant conditioning concept and does not apply to the breakdown of discriminative responding in a classical conditioning paradigm.",
        "D": "Conditioned inhibition occurs when a stimulus signals the absence of the unconditioned stimulus and thereby suppresses the conditioned response. While it is a classical conditioning concept, it describes a learned suppression of responding rather than the emotional and behavioral disruption seen here."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-01-vignette-L2",
      "source_question_id": "01",
      "source_summary": "Pavlov found that requiring dogs to make difficult discriminations between similar stimuli provoked agitation and aggression, which he referred to as experimental neurosis.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "discrimination",
        "agitation"
      ],
      "vignette": "A behavioral neuroscience laboratory is studying associative learning in rats. Initially, rats are successfully trained to differentiate between two tones of clearly different pitches, responding to one and not the other. Over several weeks, the experimenter gradually increases the similarity of the two tones. Although the rats had no difficulty with the original task, as the tones become nearly identical, individual rats begin to exhibit erratic behavior including biting the equipment, freezing, and uncharacteristic vocalizations. The lead investigator notes that several rats had also recently been moved to new cage locations, though this change appears unrelated to the observed behavioral disruption.",
      "question": "The behavioral disruption observed in the rats as the discrimination task becomes increasingly difficult is best explained by which of the following concepts?",
      "options": {
        "A": "Learned helplessness",
        "B": "Stimulus generalization decrement",
        "C": "Experimental neurosis",
        "D": "Blocking"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Learned helplessness is a state in which an organism that has experienced repeated inescapable aversive events becomes passive and fails to take action even when escape is possible. While it involves behavioral disruption, it arises from uncontrollability, not from the impossibility of a perceptual discrimination task.",
        "B": "Stimulus generalization decrement refers to the reduction in conditioned responding that occurs when the test stimulus differs from the original training stimulus. This describes a weakening of a conditioned response, not the emergence of agitation and aggression that results from an overly demanding discrimination requirement.",
        "C": "Experimental neurosis is the correct answer. Pavlov observed that when animals were required to discriminate between stimuli so similar that accurate discrimination became nearly impossible, they developed disorganized, agitated, and aggressive behavior. The progressive tightening of the discrimination until it exceeded the animal's perceptual capacity is precisely the defining feature of experimental neurosis, and the cage relocation detail is a red herring.",
        "D": "Blocking is a classical conditioning phenomenon in which prior conditioning to one stimulus prevents or reduces conditioning to a second stimulus when both are paired together with the unconditioned stimulus. It relates to the acquisition of associations, not to behavioral breakdown under discrimination demands."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-01-vignette-L3",
      "source_question_id": "01",
      "source_summary": "Pavlov found that requiring dogs to make difficult discriminations between similar stimuli provoked agitation and aggression, which he referred to as experimental neurosis.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "Pavlov"
      ],
      "vignette": "A psychologist reads about a classic set of studies by Pavlov in which animals were trained to respond to one stimulus and to withhold responding to another. The animals performed quite well when the two stimuli were easy to distinguish. However, as the experimenter systematically altered one stimulus to make it progressively more similar to the other, the animals' behavior changed dramatically. Rather than simply failing to discriminate correctly, they became highly agitated, showed signs of emotional distress, and in some cases attacked the experimental apparatus. The psychologist notes that the animals had previously shown no signs of emotional disturbance and had been stable throughout early training phases.",
      "question": "The phenomenon described in Pavlov's research — in which animals subjected to an increasingly difficult perceptual discrimination task developed emotional and behavioral disruption — is best identified as which of the following?",
      "options": {
        "A": "Conditioned emotional response",
        "B": "Experimental neurosis",
        "C": "Preparedness and biological constraints on learning",
        "D": "Stimulus discrimination failure"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A conditioned emotional response (CER) is an autonomic or emotional reaction (e.g., fear, suppression of behavior) that becomes associated with a neutral stimulus through classical conditioning, as demonstrated in Watson's studies. While it involves emotional components, it arises from pairing a neutral stimulus with an aversive unconditioned stimulus, not from making discrimination demands impossible.",
        "B": "Experimental neurosis is the correct answer. Pavlov coined this term specifically to describe the agitated, aggressive, and emotionally disorganized behavior that emerged in animals when required to make discriminations that exceeded their perceptual limits. The scenario precisely maps onto the original procedure: initial successful discrimination training followed by progressive narrowing of stimulus differences until breakdown occurs.",
        "C": "Preparedness and biological constraints on learning refer to the evolutionary predisposition of organisms to more readily acquire certain associations (e.g., taste aversions) than others. This concept explains differential rates of conditioning across stimulus types, not the behavioral deterioration arising from overly demanding discrimination tasks.",
        "D": "Stimulus discrimination failure is a descriptive phrase referring to an organism's inability to distinguish between two stimuli, which would simply result in responding to both stimuli equally. This does not capture the defining element of the phenomenon — the emergence of dramatic emotional disturbance and aggression that goes far beyond mere perceptual error."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-01-vignette-L4",
      "source_question_id": "01",
      "source_summary": "Pavlov found that requiring dogs to make difficult discriminations between similar stimuli provoked agitation and aggression, which he referred to as experimental neurosis.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "agitation"
      ],
      "vignette": "A researcher studying stress responses in animals designs a procedure in which a subject is first trained to respond differentially to two tactile stimuli. Training proceeds smoothly, and reliable differential responding is established. The researcher then initiates a series of sessions in which the physical properties of the two stimuli are systematically brought closer together. The subject begins making more errors, and the researcher increases the number of trials per session to give the subject more opportunity to 'learn' the harder distinction. Over subsequent sessions, the subject's error rate does not improve, and the subject instead develops persistent agitation, refusal to engage with the apparatus, and episodic aggressive outbursts that carry over into its home environment between sessions.",
      "question": "The researcher's inadvertent induction of behavioral and emotional pathology in this subject is best understood as an instance of which of the following?",
      "options": {
        "A": "Overtraining reversal effect",
        "B": "Learned helplessness",
        "C": "Experimental neurosis",
        "D": "Motivational conflict"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The overtraining reversal effect refers to the finding that animals trained extensively on a discrimination before a reversal procedure learn the reversal faster than animals given minimal training. It concerns the efficiency of learning a new rule, not the pathological emotional and behavioral breakdown resulting from impossibly demanding discriminations.",
        "B": "Learned helplessness is a compelling distractor here because the subject's persistent errors and eventual disengagement might suggest uncontrollability. However, learned helplessness develops from repeated exposure to inescapable aversive events and is characterized by passive failure to respond, not by the emergence of agitation, aggression, and emotional disturbance — which are the hallmarks of experimental neurosis.",
        "C": "Experimental neurosis is the correct answer. Although 'agitation' is the only domain-adjacent hint word, the full scenario matches Pavlov's experimental neurosis: initial successful differential conditioning, systematic narrowing of stimulus differences beyond the organism's discriminative capacity, increasing errors that persist despite more trials, and the development of pervasive emotional and aggressive disruption that generalizes beyond the experimental setting.",
        "D": "Motivational conflict (approach-approach, avoidance-avoidance, or approach-avoidance) describes the behavioral and emotional disruption that arises when an organism is caught between competing motivational pulls. While Dollard and Miller noted that conflict can produce neurotic-like behavior, the scenario describes no competing goals or avoidance contingencies — it describes perceptual overload in a discrimination task, making experimental neurosis the more precise and accurate construct."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-01-vignette-L5",
      "source_question_id": "01",
      "source_summary": "Pavlov found that requiring dogs to make difficult discriminations between similar stimuli provoked agitation and aggression, which he referred to as experimental neurosis.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "An animal subject in a research laboratory had spent several weeks successfully completing a perceptual task that required it to respond differently to two distinct patterns. The task was then modified so that the two patterns were made progressively harder to tell apart, with the researcher assuming that repeated practice would eventually sharpen the subject's performance. Instead of improving, the subject began to fail at a much higher rate than before, and its behavior between trials shifted markedly — it became restless and difficult to handle, began biting the equipment, and showed what the attending staff described as 'personality changes,' becoming snappy with handlers who had previously had no difficulties with it. These behavioral changes persisted even on days when the subject was not tested, and they could not be attributed to changes in housing, diet, or health status.",
      "question": "The pattern of behavioral and emotional deterioration observed in this subject following the modification of the task is best described by which of the following concepts?",
      "options": {
        "A": "Conditioned suppression",
        "B": "Learned helplessness",
        "C": "Experimental neurosis",
        "D": "Behavioral contrast"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Conditioned suppression refers to the reduction in an ongoing operant behavior that occurs when a stimulus previously paired with an aversive unconditioned stimulus is presented. While it involves behavioral change in a conditioning context, it describes the suppression of responding due to a conditioned fear signal — not the global emotional and aggressive breakdown that results from impossible discrimination demands.",
        "B": "Learned helplessness is the strongest distractor in this scenario because the subject's persistent failure and apparent disengagement despite continued trials closely resembles the passive deterioration seen in helplessness paradigms. However, learned helplessness requires exposure to inescapable aversive events, whereas this subject's deterioration stems from a perceptual discrimination that exceeds its capacity — and crucially, the behavior is characterized by agitation and aggression rather than passivity and withdrawal.",
        "C": "Experimental neurosis is the correct answer. Despite the absence of any technical terminology, the scenario replicates Pavlov's defining procedure and outcome: a subject with established differential responding is exposed to a progressively more demanding discrimination until the task exceeds its perceptual limits, resulting in persistent, pervasive agitation, aggression, and emotional disruption that generalize outside the experimental setting and cannot be explained by extraneous environmental factors.",
        "D": "Behavioral contrast refers to the phenomenon in which a change in the reinforcement schedule for one behavior produces an inverse change in the rate of another behavior — for example, when reinforcement for behavior A is reduced, behavior B that is still reinforced increases in rate. While it involves unexpected changes in behavior following procedural modifications, it describes a reallocation of response rates rather than the emergence of emotional pathology and aggression."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-09-vignette-L1",
      "source_question_id": "09",
      "source_summary": "When using stimulus control to increase a behavior, the presence of a discriminative stimulus indicates that the behavior will be reinforced.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "discriminative stimulus",
        "stimulus control",
        "reinforced"
      ],
      "vignette": "A behavior analyst is working with a child who is learning to request snacks appropriately. The analyst places a green light on the counter to signal that asking for a snack will be reinforced with praise and access to the snack. When the red light is on, requests go unreinforced. Over time, the child learns to ask for snacks only when the green light is on, demonstrating that the green light has acquired stimulus control over the requesting behavior. The behavior analyst explains that the green light functions as a discriminative stimulus indicating the behavior will be reinforced.",
      "question": "Which operant conditioning concept BEST explains the function of the green light in this scenario?",
      "options": {
        "A": "Negative reinforcement",
        "B": "Stimulus control",
        "C": "Fixed ratio schedule",
        "D": "Shaping"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Negative reinforcement involves increasing a behavior by removing an aversive stimulus; it does not describe how a cue signals the availability of reinforcement for a specific behavior.",
        "B": "Stimulus control occurs when the presence of a discriminative stimulus (the green light) increases the probability that a behavior will occur because it signals that reinforcement is available — precisely what is described in this scenario.",
        "C": "A fixed ratio schedule refers to delivering reinforcement after a set number of responses; it describes when reinforcement is delivered, not how an environmental cue signals its availability.",
        "D": "Shaping involves reinforcing successive approximations toward a target behavior; it describes a procedure for building new behaviors, not how a cue comes to control an already-learned behavior."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-09-vignette-L2",
      "source_question_id": "09",
      "source_summary": "When using stimulus control to increase a behavior, the presence of a discriminative stimulus indicates that the behavior will be reinforced.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "discriminative stimulus",
        "reinforcement"
      ],
      "vignette": "A 9-year-old boy with ADHD has difficulty knowing when it is appropriate to raise his hand in class versus speak freely. His teacher introduces a small blue card on his desk: when the card is visible, hand-raising is followed by praise and attention; when the card is removed, hand-raising is ignored. Despite the child's impulsivity — which initially led his teacher to wonder if behavioral strategies would work — he soon raises his hand almost exclusively when the blue card is present. The teacher notes that the card now reliably predicts whether reinforcement will follow hand-raising.",
      "question": "The blue card is functioning as which of the following in this behavioral intervention?",
      "options": {
        "A": "A conditioned reinforcer",
        "B": "A token economy component",
        "C": "A discriminative stimulus",
        "D": "An unconditioned stimulus"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A conditioned reinforcer is a previously neutral stimulus that has acquired reinforcing properties through pairing with a primary reinforcer (e.g., money, tokens). The blue card does not directly reinforce behavior; it signals when reinforcement is available.",
        "B": "A token economy involves delivering tokens that are exchanged for backup reinforcers. The blue card is not exchanged for anything; it simply signals the availability of reinforcement, which is a different function.",
        "C": "A discriminative stimulus signals that a particular behavior will be reinforced. The blue card has come to predict praise and attention for hand-raising, making the child more likely to emit that behavior in its presence — the defining feature of stimulus control.",
        "D": "An unconditioned stimulus is a concept from classical conditioning — it naturally and automatically elicits an unconditioned response. This scenario involves operant conditioning and a cue for reinforcement availability, not an eliciting stimulus."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-09-vignette-L3",
      "source_question_id": "09",
      "source_summary": "When using stimulus control to increase a behavior, the presence of a discriminative stimulus indicates that the behavior will be reinforced.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "stimulus control"
      ],
      "vignette": "A clinician is working with a man who has been trying to reduce his late-night snacking. The client reports that he tends to eat impulsively only while sitting in his recliner watching television after 10 p.m., and almost never snacks in any other location or at any other time. The clinician notes that over many months, eating behavior has become tightly linked to this specific context because snacking while reclining and watching TV has consistently been followed by feelings of pleasure and relaxation. The clinician considers implementing stimulus control strategies to help the client break this pattern, such as restricting eating to the kitchen table only.",
      "question": "The clinician's observation that eating occurs almost exclusively in the recliner while watching TV after 10 p.m. BEST illustrates which principle?",
      "options": {
        "A": "Chaining",
        "B": "Stimulus control",
        "C": "Negative reinforcement",
        "D": "Classical conditioning"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Chaining involves linking a sequence of discrete behaviors together so that each behavior serves as a cue for the next; it describes the structure of a behavioral chain, not how an environmental context comes to occasion a single behavior.",
        "B": "Stimulus control explains why the client eats almost exclusively in the recliner while watching TV: that specific context has become a discriminative stimulus because snacking in that setting has been repeatedly reinforced by pleasure and relaxation. The clinician's intervention strategy of restricting eating to the kitchen is a classic stimulus control technique.",
        "C": "Negative reinforcement increases a behavior by removing an aversive stimulus. While relaxation could be framed as relief from tension, the scenario emphasizes a specific environmental context controlling behavior — a hallmark of stimulus control, not negative reinforcement.",
        "D": "Classical conditioning involves learning an association between a neutral stimulus and an unconditioned stimulus that elicits a reflexive response. Here the eating behavior is operant (voluntary and reinforced by consequences), and the focus is on a context signaling reinforcement availability, not on elicited reflexive responses."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-09-vignette-L4",
      "source_question_id": "09",
      "source_summary": "When using stimulus control to increase a behavior, the presence of a discriminative stimulus indicates that the behavior will be reinforced.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "signal"
      ],
      "vignette": "A researcher studying workplace productivity notices that employees on one team submit completed reports almost exclusively in the afternoons, even though the work could technically be done at any time of day. Closer analysis reveals that the team supervisor sends a brief Slack message each morning listing which reports are due, and that reports submitted by 3 p.m. consistently receive immediate supervisor acknowledgment, while those submitted later are logged but never acknowledged. Employees who join the team initially submit reports at various times, but within a few weeks they too cluster their submissions in the afternoon. The researcher notes that the morning Slack message appears to serve as a reliable signal for when a particular work behavior will produce a meaningful outcome.",
      "question": "The pattern the researcher observes is MOST precisely explained by which operant conditioning principle?",
      "options": {
        "A": "Variable interval schedule of reinforcement",
        "B": "Stimulus control via a discriminative stimulus",
        "C": "Shaping through successive approximations",
        "D": "Conditioned reinforcement"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A variable interval schedule delivers reinforcement after unpredictable amounts of time have passed since the last reinforcement; it would predict a steady, moderate response rate across all times of day, not tight clustering in the afternoon following a specific morning cue.",
        "B": "Stimulus control via a discriminative stimulus is the correct answer. The morning Slack message has become a discriminative stimulus: it reliably signals that report submission by 3 p.m. will be acknowledged (reinforced), causing employees to concentrate the behavior during the signaled window. New employees acquire this pattern because they experience the same contingency.",
        "C": "Shaping involves reinforcing successive approximations toward a terminal behavior that has not yet been exhibited. In this scenario, submitting reports is already in the employees' repertoire; the issue is when the behavior is performed, not building the behavior incrementally.",
        "D": "Conditioned reinforcement refers to a previously neutral stimulus that has acquired reinforcing value through association with primary reinforcement. Supervisor acknowledgment functions as a reinforcer here, but the Slack message is not reinforcing behavior — it is signaling when reinforcement will be available, which is the function of a discriminative stimulus."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-09-vignette-L5",
      "source_question_id": "09",
      "source_summary": "When using stimulus control to increase a behavior, the presence of a discriminative stimulus indicates that the behavior will be reinforced.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A woman adopted a rescue dog that had lived with a previous family for five years. She notices that the dog barks at the front door almost exclusively between 5:00 and 6:00 p.m. on weekdays. A neighbor mentions that the previous owner always arrived home from work at around 5:30 p.m. and would immediately play with the dog and give it a treat upon entering. The woman also notices the dog frequently circles the kitchen in the early morning, which is when she typically prepares its meals. Despite a change in home, people, and routine, the dog's behavior at the door persists strongly at the old arrival time and has not generalized to other hours, even though strangers walk past the door throughout the day.",
      "question": "The dog's tendency to bark at the front door specifically between 5:00 and 6:00 p.m. on weekdays, but not at other times when similar environmental sounds occur, is MOST accurately explained by which learning principle?",
      "options": {
        "A": "Higher-order classical conditioning",
        "B": "Stimulus control via a discriminative stimulus",
        "C": "Variable interval reinforcement generalization",
        "D": "State-dependent learning"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Higher-order classical conditioning involves a conditioned stimulus being used to condition a new neutral stimulus, producing a conditioned response to the new stimulus. While the scenario superficially resembles classical conditioning (a time of day eliciting anticipatory behavior), the key feature is that the barking behavior was previously followed by play and treats — an operant outcome. The 5:00–6:00 p.m. window functions as a cue for when the operant behavior will be reinforced, not as a stimulus eliciting a reflexive response through CS–US pairing.",
        "B": "Stimulus control via a discriminative stimulus is correct. The 5:00–6:00 p.m. time window has become a discriminative stimulus because barking and door-orienting during that period was repeatedly followed by reinforcement (play and treats from the previous owner). The behavior does not generalize to other times when similar sounds occur, which is the hallmark of stimulus control rather than generalization — the time context specifically signals reinforcement availability.",
        "C": "Variable interval reinforcement would predict a steady, moderate rate of the behavior across time rather than clustering at a specific hour. Generalization would predict the behavior spreading to other times or stimuli, which is the opposite of what occurs — the behavior is actually highly specific to the one time window.",
        "D": "State-dependent learning refers to better retrieval of information when the internal physiological or psychological state during retrieval matches the state during encoding. This is a memory phenomenon, not a behavioral one, and it does not explain why a specific environmental time context controls the frequency and specificity of an operant behavior."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-08-vignette-L1",
      "source_question_id": "08",
      "source_summary": "A parent who is using positive reinforcement to establish a desirable behavior in her child will switch from a continuous schedule of reinforcement to an intermittent schedule in order to reduce the risk for satiation.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "positive reinforcement",
        "continuous schedule",
        "satiation"
      ],
      "vignette": "A behavior therapist is coaching a mother on how to use positive reinforcement to teach her five-year-old to put toys away after playtime. The mother initially praises and gives a small sticker every single time the child completes the task, using a continuous schedule of reinforcement. After the behavior is well established, the therapist advises the mother to shift to an intermittent schedule to prevent satiation, warning that delivering a reward after every single response risks making the reward lose its effectiveness. The therapist explains that once a behavior is learned, intermittent delivery of reinforcement helps maintain it more durably.",
      "question": "The therapist's recommendation to switch from a continuous to an intermittent reinforcement schedule primarily reflects concern about which operant conditioning phenomenon?",
      "options": {
        "A": "Extinction, in which withholding reinforcement causes the established behavior to decrease in frequency",
        "B": "Satiation, in which a reinforcer loses its effectiveness when it is delivered too frequently or in excess",
        "C": "Stimulus generalization, in which the child begins emitting the behavior in response to stimuli other than the original cue",
        "D": "Shaping, in which successive approximations to the target behavior are selectively reinforced"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Extinction refers to the process by which a previously reinforced behavior decreases when reinforcement is withheld entirely. While switching schedules involves changing how often reinforcement is given, the concern here is not that reinforcement will stop but that delivering it too frequently will reduce its motivational value — which is satiation, not extinction.",
        "B": "Satiation occurs when a reinforcer is presented so frequently that it loses its rewarding value and therefore its ability to motivate behavior. Switching to an intermittent schedule reduces the rate of reinforcer delivery, preserving the reward's effectiveness and maintaining motivation over time. This is precisely the concern described in the vignette.",
        "C": "Stimulus generalization is the spread of a conditioned response to stimuli that resemble the original discriminative stimulus. This concept is unrelated to the frequency of reinforcement delivery and does not address why continuous reinforcement could undermine behavioral maintenance.",
        "D": "Shaping involves reinforcing successive approximations toward a target behavior and is relevant during the acquisition phase of learning. The vignette describes a behavior that is already well established, and the concern is about maintaining it — not about differentially reinforcing approximations."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-08-vignette-L2",
      "source_question_id": "08",
      "source_summary": "A parent who is using positive reinforcement to establish a desirable behavior in her child will switch from a continuous schedule of reinforcement to an intermittent schedule in order to reduce the risk for satiation.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "intermittent schedule",
        "satiation"
      ],
      "vignette": "A seven-year-old boy diagnosed with ADHD is working with a behavioral consultant to increase on-task behavior during homework. His mother has been giving him a piece of his favorite candy every time he completes a homework problem without prompting, and the behavior has become reliably established over three weeks. However, the consultant notices the boy has recently started refusing the candy and completing fewer problems independently. The consultant recommends that the mother switch to an intermittent schedule of reinforcement rather than rewarding every single correct response.",
      "question": "The consultant's recommendation most directly addresses which operant conditioning concern?",
      "options": {
        "A": "Response cost, in which a penalty for unwanted behavior inadvertently suppresses the desired behavior",
        "B": "Satiation, in which frequent delivery of a reinforcer reduces its motivational value",
        "C": "Behavioral contrast, in which a change in reinforcement conditions in one context increases behavior in another context",
        "D": "Overjustification, in which extrinsic rewards undermine the child's intrinsic motivation to engage in the task"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Response cost involves removing a positive reinforcer as a consequence of an unwanted behavior and is a form of punishment. The scenario describes the reward losing effectiveness from overuse, not a procedure in which something is taken away following a misbehavior, making response cost incorrect here.",
        "B": "Satiation occurs when a reinforcer is delivered so frequently that the individual is no longer motivated by it, as evidenced by the boy refusing candy and declining performance. Switching to an intermittent schedule reduces exposure to the reward and restores its reinforcing value, which is exactly the rationale described.",
        "C": "Behavioral contrast refers to the phenomenon in which a decrease in reinforcement rate in one setting leads to an increase in behavior in a different setting. While it involves reinforcement schedule changes, it does not explain why the boy is refusing the reward or why intermittent delivery would help maintain the behavior.",
        "D": "Overjustification is a cognitive-motivational concept from self-determination theory suggesting that external rewards can undermine pre-existing intrinsic motivation. While superficially plausible because the boy is resisting the reward, satiation better explains the pattern because it predicts diminished reinforcer effectiveness specifically from excessive repeated delivery — the defining feature of this case."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-08-vignette-L3",
      "source_question_id": "08",
      "source_summary": "A parent who is using positive reinforcement to establish a desirable behavior in her child will switch from a continuous schedule of reinforcement to an intermittent schedule in order to reduce the risk for satiation.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "intermittent"
      ],
      "vignette": "A school psychologist helps a teacher establish a reward system for a student who rarely volunteers to answer questions in class. For several weeks, the teacher has given the student a small token every single time she raises her hand and answers a question, and the behavior has become frequent and consistent. Lately, however, the student seems indifferent to receiving the tokens and her hand-raising has dropped sharply despite the reward system remaining in place. The teacher wonders whether the problem is that she has been too generous, and the psychologist suggests adjusting to an intermittent delivery approach going forward.",
      "question": "What is the most likely explanation for the student's declining behavior, and what problem does switching to intermittent delivery directly address?",
      "options": {
        "A": "Extinction, because the tokens have been paired with so many responses that they have lost their conditioned reinforcing properties",
        "B": "Overjustification effect, because the external token system has eroded the student's intrinsic interest in answering questions",
        "C": "Satiation, because the frequent and consistent delivery of tokens has reduced their motivating value",
        "D": "Learned helplessness, because unpredictable outcomes have led the student to stop responding"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Extinction involves decreasing a behavior by withholding a reinforcer that had previously been delivered, not by delivering it too often. The teacher has not stopped delivering tokens — she has been delivering them consistently — so the mechanism described is not extinction. A conditioned reinforcer losing its value through repeated presentation without the primary backup reinforcer is a related concept (token devaluation), but the core issue here is the reward's declining motivational power from overuse, which is satiation.",
        "B": "The overjustification effect proposes that introducing external rewards for an already intrinsically motivated activity reduces intrinsic interest. This is a plausible distractor because the student may have had some natural interest in class participation, but the vignette emphasizes that the tokens are being delivered too frequently as the critical mechanism. Overjustification would not be solved by making delivery less frequent — it would persist regardless of schedule.",
        "C": "Satiation is the correct answer. When a reinforcer is delivered after every single response over a prolonged period, the individual accumulates so much of it that it no longer functions as a motivator. The student's indifference to tokens and declining performance is the hallmark behavioral signature of satiation. Switching to intermittent delivery reduces the rate of accumulation and preserves the reward's value.",
        "D": "Learned helplessness develops when an organism repeatedly experiences uncontrollable, inescapable aversive events, leading to passive and avoidant behavior. The scenario describes the opposite: a reliable, consistent reward system where the student's responses were always followed by tokens. There is nothing uncontrollable or aversive in the described situation, so learned helplessness does not apply."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-08-vignette-L4",
      "source_question_id": "08",
      "source_summary": "A parent who is using positive reinforcement to establish a desirable behavior in her child will switch from a continuous schedule of reinforcement to an intermittent schedule in order to reduce the risk for satiation.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "deprivation"
      ],
      "vignette": "A parent has successfully used a small treat to get her six-year-old to greet guests politely, and the greeting behavior now occurs reliably every time a visitor arrives. The child initially ran eagerly to earn the treat, but after several months of consistent reward delivery the child has become noticeably sluggish about performing the greeting and sometimes ignores the offered treat entirely. A behavioral consultant reviewing the case notes that the child's motivational state with respect to the treat is the inverse of what would be needed to maximize the treat's behavioral control. The consultant recommends spacing out when the treat is offered so that the child's level of deprivation relative to the treat can be partially restored between earning opportunities.",
      "question": "The consultant's reference to the child's motivational state being the 'inverse' of what is needed, and the recommendation to restore deprivation, most directly identifies which operant conditioning problem?",
      "options": {
        "A": "Habituation, in which the child's orienting response to the treat has been reduced through repeated non-consequential exposure",
        "B": "Stimulus overselectivity, in which the treat has lost discriminative control because it is present in too many contexts",
        "C": "Satiation, in which the reinforcer has been delivered so frequently that deprivation state is minimized and the treat no longer effectively motivates behavior",
        "D": "Conditioned inhibition, in which the consistent pairing of the treat with greeting has transformed the treat into an inhibitor of exploratory behavior"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Habituation is a non-associative learning process in which repeated presentation of a neutral or orienting stimulus leads to decreased responsiveness to that stimulus. It does not involve motivational states, deprivation, or the relationship between a reinforcer and behavior. While the child's declining responsiveness superficially resembles habituation, the mechanism described by the consultant — a depleted deprivation state — is specific to the satiation-deprivation dimension of reinforcer effectiveness in operant conditioning.",
        "B": "Stimulus overselectivity refers to a condition in which an organism attends to a very limited subset of available stimuli, often discussed in the context of autism spectrum disorder. It concerns which stimuli gain control over behavior, not the motivational effectiveness of a reinforcer. The vignette's focus on deprivation state and frequency of reinforcer delivery is entirely inconsistent with overselectivity.",
        "C": "Satiation is the correct answer. The consultant's language is precise: deprivation is the motivational operation that increases a reinforcer's effectiveness, and satiation is its opposite — the state produced by excess delivery that reduces reinforcer value. Restoring deprivation by spacing out delivery is the standard operant solution to satiation. The vignette is written so that 'habituation' and 'inhibition' appear superficially plausible, but the explicit reference to motivational state and deprivation unmistakably points to satiation.",
        "D": "Conditioned inhibition is a Pavlovian phenomenon in which a stimulus that has been paired with the absence of an unconditioned stimulus comes to inhibit conditioned responses. It applies to classical, not operant, conditioning and involves associative suppression of reflexive responses — not the motivational relationship between a freely emitted operant behavior and its reinforcer. The scenario describes operant behavior maintained by a treat, not a reflex inhibited by a signal."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-08-vignette-L5",
      "source_question_id": "08",
      "source_summary": "A parent who is using positive reinforcement to establish a desirable behavior in her child will switch from a continuous schedule of reinforcement to an intermittent schedule in order to reduce the risk for satiation.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A mother has been rewarding her eight-year-old son with a small piece of chocolate every single time he sets the dinner table without being asked. After about two months, the boy began setting the table reliably and consistently. Lately, however, the mother has noticed that even though she continues to offer the chocolate, her son often shrugs, says 'I don't really feel like it,' and sometimes does not set the table at all. The mother finds this puzzling because she has not changed anything about the system, and the chocolate is the same brand he always loved. A family consultant suggests the problem is structural and recommends that the mother offer the chocolate after only some instances of the behavior, rather than every time, so that the boy's relationship to the reward can reset.",
      "question": "The family consultant's explanation and recommendation most precisely reflect which underlying principle?",
      "options": {
        "A": "The boy's intrinsic motivation for table-setting has been undermined by the prolonged use of external rewards, and reducing reward frequency may allow intrinsic interest to re-emerge",
        "B": "The chocolate has become so predictable that it no longer functions as an incentive, and varying its delivery will re-establish its ability to guide behavior by increasing the child's desire for it",
        "C": "The boy has learned that setting the table always produces chocolate, and because he is no longer hungry for chocolate, the association between the behavior and the reward has weakened",
        "D": "The consistency of reward delivery has conditioned the boy to expect the chocolate regardless of his behavior, and intermittent delivery will restore a contingency he can detect"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This option describes the overjustification effect, a well-supported phenomenon in which externally rewarded tasks lose their intrinsic appeal. It is a strong distractor because the boy's indifference and the mother's unchanged system superficially fit. However, the consultant's recommendation is not to remove the reward or rely on intrinsic motivation — it is to deliver the same external reward less frequently so that the boy desires it more. Overjustification would not be resolved by reducing delivery frequency; it requires removing or diminishing the external contingency altogether.",
        "B": "This is the correct answer and describes satiation and the role of deprivation in reinforcer effectiveness. When a reinforcer is delivered after every single response, the individual accumulates it so thoroughly that motivational deprivation is eliminated, and the reward loses its capacity to drive behavior. Switching to delivering the reward after only some responses reduces the rate of accumulation, partially restores deprivation, and thereby re-establishes the chocolate as an effective motivator. The consultant's recommendation to let the boy's 'relationship to the reward reset' is precisely this logic.",
        "C": "This option is a plausible distractor that blends extinction logic with satiation language. It correctly identifies that the boy is not hungry for the chocolate, but frames the mechanism as a weakening of the learned association — which is extinction. Extinction requires that reinforcement be withheld, not merely that the child is full of chocolate. The association itself has not weakened; the problem is motivational (deprivation state), not associative, making extinction an inaccurate framing.",
        "D": "This option suggests the boy has developed a superstitious or unconditional expectation of the reward — that he believes he will get it regardless of his behavior, resembling a form of learned irrelevance or lack of contingency awareness. While plausible, the vignette specifies that the mother continues to offer the chocolate specifically when he sets the table, meaning the contingency remains intact. The problem is not that the boy perceives no contingency but that the reward itself no longer functions as a motivator due to its excessive accumulation — which is satiation, not a contingency perception failure."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-02-vignette-L1",
      "source_question_id": "02",
      "source_summary": "Of the three conditioning methods (trace, simultaneous, and delay), delay conditioning is the most effective in establishing a conditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "classical conditioning",
        "conditioned response",
        "delay conditioning"
      ],
      "vignette": "A researcher is designing a classical conditioning experiment to produce the strongest possible conditioned response in participants. She is comparing three procedures: one in which the conditioned stimulus (CS) onset precedes the unconditioned stimulus (US) onset and the CS remains present until the US arrives, one in which the CS and US are presented at exactly the same time, and one in which the CS ends before the US begins. She reviews the literature to determine which procedure most reliably produces a robust conditioned response.",
      "question": "Based on the classical conditioning literature, which procedure should the researcher select to most effectively establish a conditioned response?",
      "options": {
        "A": "Simultaneous conditioning, because presenting the CS and US together maximizes their temporal association",
        "B": "Trace conditioning, because the gap between CS and US forces deeper cognitive processing of the stimulus",
        "C": "Delay conditioning, because the CS overlaps with and remains present until the onset of the US, producing the strongest conditioned response",
        "D": "Backward conditioning, because presenting the US before the CS creates a strong predictive relationship"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Simultaneous conditioning, in which the CS and US begin and end together, actually produces weaker conditioning than delay conditioning because the CS does not serve as a reliable predictor or signal for the upcoming US.",
        "B": "Trace conditioning involves a gap between the offset of the CS and the onset of the US, which weakens the association between them and produces less effective conditioning than the delay procedure.",
        "C": "Correct. In delay conditioning, the CS onset precedes the US onset and the CS remains on until the US arrives. This temporal arrangement allows the CS to function as an effective predictor of the US and consistently produces the strongest conditioned response among the three major conditioning procedures.",
        "D": "Backward conditioning, in which the US precedes the CS, is generally the least effective procedure for establishing a conditioned response because the CS cannot serve as a predictive signal for a stimulus that has already occurred."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-02-vignette-L2",
      "source_question_id": "02",
      "source_summary": "Of the three conditioning methods (trace, simultaneous, and delay), delay conditioning is the most effective in establishing a conditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "conditioned stimulus",
        "classical conditioning"
      ],
      "vignette": "A behavioral neuroscience lab is training rabbits in an eyeblink classical conditioning paradigm, a well-established model for studying associative learning. The lead researcher is an expert in the cerebellum's role in motor learning and has been documenting individual differences in acquisition rates across animals. The team debates whether to use a procedure in which the conditioned stimulus ends before the air puff begins, a procedure in which both stimuli start and end simultaneously, or a procedure in which the conditioned stimulus starts first and stays on until the air puff is delivered. The researcher selects the method that the literature consistently identifies as producing the fastest and most reliable acquisition.",
      "question": "Which conditioning procedure did the researcher most likely select?",
      "options": {
        "A": "Trace conditioning, because the inter-stimulus interval allows the animal time to form a stronger associative memory",
        "B": "Delay conditioning, because the conditioned stimulus overlaps with the unconditioned stimulus and reliably produces the strongest conditioned response",
        "C": "Simultaneous conditioning, because pairing stimuli at the exact same moment produces the most direct associative link",
        "D": "Second-order conditioning, because chaining stimuli together accelerates acquisition beyond what a single CS–US pairing can achieve"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Trace conditioning does involve an inter-stimulus interval, but this gap is precisely what weakens the CS–US association, making it less effective than delay conditioning rather than more effective.",
        "B": "Correct. Delay conditioning, in which the CS onset precedes the US and both stimuli overlap until the US is delivered, is the most effective of the three primary conditioning procedures for establishing a conditioned response, including in eyeblink paradigms.",
        "C": "Simultaneous conditioning is less effective than delay conditioning because the CS provides no advance predictive information about the impending US when both stimuli occur at exactly the same time.",
        "D": "Second-order conditioning is a phenomenon in which a neutral stimulus acquires conditioned properties by being paired with an already-established CS, not a temporal procedure for optimizing first-order CS–US acquisition. It does not compete directly with delay, trace, or simultaneous procedures as a method for initial conditioning."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-02-vignette-L3",
      "source_question_id": "02",
      "source_summary": "Of the three conditioning methods (trace, simultaneous, and delay), delay conditioning is the most effective in establishing a conditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "inter-stimulus interval"
      ],
      "vignette": "A clinical researcher studying anxiety acquisition in humans finds that participants in one training group develop a robust fear response to a neutral image after only a few trials. In this group, the neutral image appears on screen and remains visible for three seconds before a mild electric shock is delivered; the image stays on during the shock. A second group receives the image and shock at the exact same moment, and a third group sees the image but it disappears one second before the shock occurs. The researcher notes that the first group's short inter-stimulus interval and continuous stimulus overlap seem to be critical factors in their faster and stronger fear acquisition. Participants in the first group also had higher baseline trait anxiety scores, which the researcher carefully controls for statistically.",
      "question": "The superior fear acquisition observed in the first group is best explained by which conditioning procedure?",
      "options": {
        "A": "Trace conditioning, because the brief temporal separation between stimuli sharpens attention and enhances the predictive value of the neutral image",
        "B": "Simultaneous conditioning, because simultaneous onset of image and shock maximizes the associative strength between the two stimuli",
        "C": "Delay conditioning, because the neutral image precedes and overlaps with the shock, producing the strongest conditioned fear response",
        "D": "Higher trait anxiety, because participants with elevated baseline anxiety are neurobiologically primed to form stronger conditioned fear responses regardless of procedure"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Trace conditioning involves the CS terminating before the US begins, which describes the third group, not the first. The first group's stimulus remained on through shock delivery, which is the defining feature of delay conditioning, not trace conditioning.",
        "B": "Simultaneous conditioning describes the second group, in which image and shock co-occur from the start. The first group is distinguished by the CS preceding the US, which is the hallmark of delay conditioning.",
        "C": "Correct. The first group received delay conditioning: the CS (neutral image) onset preceded the US (shock) onset, and the CS remained present until the US was delivered. This procedure consistently produces the strongest conditioned response, explaining the superior fear acquisition. The higher trait anxiety scores are a controlled covariate and do not determine which procedure was used.",
        "D": "While trait anxiety can influence the magnitude of fear responses, the question asks which conditioning procedure explains the superior acquisition. Higher trait anxiety was statistically controlled, and the procedural difference between groups remains the most parsimonious explanation for differential outcomes."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-02-vignette-L4",
      "source_question_id": "02",
      "source_summary": "Of the three conditioning methods (trace, simultaneous, and delay), delay conditioning is the most effective in establishing a conditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "predictive signal"
      ],
      "vignette": "A researcher examining phobia acquisition reviews data from three groups of participants who were exposed to a neutral geometric shape paired with an aversive tone. She notices that Group A, whose participants showed the highest autonomic reactivity during training, were exposed to the shape for several seconds before the tone sounded, and the shape remained on screen throughout the tone's duration. Group B participants were exposed to the shape and tone concurrently from start to finish, while Group C participants saw the shape, which then disappeared, followed by a half-second silent pause before the tone. The researcher comments that Group A's conditioning outcome aligns precisely with the procedure that produces the most efficient stimulus-response association because the shape functioned as an optimal predictive signal for the tone. She is puzzled that Group C participants, despite being the most cognitively engaged during training according to self-report data, showed the weakest conditioned responses.",
      "question": "The researcher's findings are best accounted for by which of the following explanations?",
      "options": {
        "A": "Group A benefited from trace conditioning, and the continuous presence of the CS throughout training strengthened the cognitive representation of the CS–US association",
        "B": "Group B's simultaneous conditioning procedure should have produced the strongest response due to maximal temporal contiguity between shape and tone",
        "C": "Group A's superior conditioning reflects delay conditioning, in which the CS preceded and overlapped with the US, optimizing the CS's role as a predictive cue",
        "D": "Group C's cognitive engagement during the silent gap enhanced encoding of the CS–US relationship, and their weak response reflects a measurement artifact rather than inferior conditioning"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Trace conditioning is defined by the CS terminating before the US begins, which describes Group C's procedure, not Group A's. Group A's shape remained on screen throughout the tone, making their procedure delay conditioning. Labeling Group A's procedure as trace conditioning is a misclassification that this option exploits.",
        "B": "Simultaneous conditioning, used by Group B, does not produce the strongest conditioned response despite the maximum temporal overlap between CS and US. Without the CS preceding the US, it cannot function as a forward predictive signal, which is why delay conditioning outperforms simultaneous conditioning.",
        "C": "Correct. Group A received delay conditioning: the CS (shape) preceded the US (tone) and remained present until the US was delivered. This procedure allows the CS to serve as an optimal predictive signal for the US, consistently producing the strongest conditioned responses. Group C's procedure — where the CS ended before the US began — is trace conditioning and yields weaker responses, consistent with the researcher's data.",
        "D": "While Group C participants reported higher cognitive engagement, this does not translate into stronger conditioning outcomes in delay versus trace comparisons. The vignette explicitly states they showed the weakest conditioned responses, and attributing this to measurement artifact contradicts the established finding that trace conditioning is less effective than delay conditioning."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-02-vignette-L5",
      "source_question_id": "02",
      "source_summary": "Of the three conditioning methods (trace, simultaneous, and delay), delay conditioning is the most effective in establishing a conditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "Three groups of volunteers each watch a series of short video clips. In each clip, a specific colored light appears on screen and is later followed by a brief, unpleasant vibration to the wrist. For Group 1, the light stays on from the moment it appears until the vibration begins and ends. For Group 2, the light and vibration begin and stop at exactly the same moment. For Group 3, the light flicks off a moment before the vibration starts, leaving a brief dark interval. Afterward, when volunteers from all three groups see the colored light alone, Group 1 shows the strongest and most consistent physiological reaction, Group 2 shows a moderate reaction, and Group 3 shows the weakest reaction — despite reporting in post-task interviews that they spent the dark interval actively thinking about what was coming next. A reviewer of the study argues that Group 3's active anticipation during the gap should have strengthened their learning, but the data clearly contradict this intuition.",
      "question": "The pattern of results across the three groups is best explained by which of the following principles?",
      "options": {
        "A": "Group 1 benefited from a longer exposure to the light, giving volunteers more time to encode it as a meaningful cue, an advantage that reflects elaborative processing rather than a timing-specific conditioning effect",
        "B": "Group 3's weak reaction reflects retroactive interference, because the dark interval introduced a competing stimulus representation that disrupted the association formed between the light and vibration",
        "C": "The results reflect the relative effectiveness of delay, simultaneous, and trace conditioning procedures, with the procedure in which the cue precedes and overlaps with the aversive event producing the strongest learned reaction",
        "D": "Group 2's moderate reaction suggests that temporal contiguity alone is the primary driver of associative learning strength, and Group 1's advantage is attributable to the additional time allowed for conscious expectancy to develop"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This option reframes Group 1's advantage as elaborative encoding rather than a conditioning-procedure effect. However, longer CS exposure alone does not explain the differential outcomes; the critical variable is the temporal relationship between the CS and US. Group 1's superior performance reflects delay conditioning, not simply more encoding time, as Group 3 participants — who actively thought during the gap — did not outperform Group 1.",
        "B": "Retroactive interference is a memory phenomenon in which later-learned material disrupts recall of earlier-learned material and applies to declarative memory tasks rather than to physiological conditioning outcomes. The dark interval in Group 3 represents a trace conditioning gap, not a competing stimulus, and the correct framework is comparative conditioning procedure effectiveness, not interference theory.",
        "C": "Correct. Group 1 received delay conditioning (CS onset precedes US onset, CS remains on until US), Group 2 received simultaneous conditioning, and Group 3 received trace conditioning (CS terminates before US onset). The rank ordering of outcomes — Group 1 strongest, Group 2 moderate, Group 3 weakest — precisely mirrors the established hierarchy of these three procedures, with delay conditioning being the most effective. Group 3's active anticipation during the gap is a red herring; conscious expectancy does not compensate for the weakened CS–US association created by the gap.",
        "D": "This option correctly identifies temporal contiguity as relevant but misattributes Group 1's advantage to conscious expectancy development over time rather than to the structural properties of delay conditioning. The data specifically show that Group 3, whose participants actively anticipated the vibration during the gap, had the weakest responses — directly contradicting the claim that expectancy development explains Group 1's superiority."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-07-vignette-L1",
      "source_question_id": "07",
      "source_summary": "If reinforcement is stopped for pressing Bar B, the rat will press Bar A with greater frequency and Bar B with less frequency, demonstrating behavioral contrast.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "extinction",
        "behavioral contrast",
        "reinforcement"
      ],
      "vignette": "A researcher trains a rat to press two levers, Bar A and Bar B, both of which deliver food pellets on identical reinforcement schedules. After stable responding is established, the researcher places responding on Bar B under extinction, completely withholding reinforcement for that lever. Over the next several sessions, the rat's pressing of Bar B decreases steadily, while its pressing of Bar A increases substantially above its original baseline rate. The researcher notes that the increase in Bar A responding is directly proportional to the loss of reinforcement on Bar B.",
      "question": "Which operant conditioning phenomenon BEST explains the observed increase in Bar A responding following the extinction of Bar B responding?",
      "options": {
        "A": "Behavioral contrast",
        "B": "Spontaneous recovery",
        "C": "Positive reinforcement schedule thinning",
        "D": "Response generalization"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Behavioral contrast refers to the phenomenon in which withholding reinforcement (extinction) for one response in a multiple-schedule environment produces an increase in responding on the alternative reinforced response, beyond the original baseline. This is precisely what is described here.",
        "B": "Incorrect. Spontaneous recovery refers to the temporary reappearance of a previously extinguished response after a rest period, without any new reinforcement. It describes the return of the extinguished behavior, not an increase in an alternative behavior.",
        "C": "Incorrect. Schedule thinning refers to the deliberate gradual reduction of reinforcement density on a single response, a procedure used to maintain behavior on leaner schedules. It does not describe a contrast effect across two concurrently available responses.",
        "D": "Incorrect. Response generalization refers to the spread of a trained response to topographically similar behaviors. While it involves increases in other responses, it applies to similar behaviors elicited by the same stimulus, not to a discrete alternative operant in a multiple-schedule arrangement."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-07-vignette-L2",
      "source_question_id": "07",
      "source_summary": "If reinforcement is stopped for pressing Bar B, the rat will press Bar A with greater frequency and Bar B with less frequency, demonstrating behavioral contrast.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "extinction",
        "multiple schedule"
      ],
      "vignette": "A behavioral researcher is studying lever-pressing in rats using a multiple schedule in which two levers are simultaneously available. Both levers initially yield food on comparable schedules, and the rat, an older male with a history of stable responding, presses both levers at roughly equal rates. The researcher then removes all food delivery for pressing the left lever while keeping the right lever's schedule unchanged. Within days, the rat presses the left lever far less often, but notably presses the right lever at a rate well above its pre-intervention baseline.",
      "question": "The increase in right-lever pressing above its original baseline rate is BEST explained by which of the following concepts?",
      "options": {
        "A": "Negative reinforcement",
        "B": "Behavioral contrast",
        "C": "Resurgence",
        "D": "Premack principle"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Behavioral contrast occurs in multiple-schedule situations when extinction of responding on one alternative leads to an increase in responding on the remaining reinforced alternative above its original baseline. The rat's above-baseline increase on the right lever following removal of reinforcement from the left lever is a textbook example.",
        "A": "Incorrect. Negative reinforcement involves the removal of an aversive stimulus contingent on a response, thereby increasing that response. While the right-lever rate does increase, there is no aversive stimulus being removed; reinforcement was simply withheld from the left lever, making negative reinforcement an inaccurate account.",
        "C": "Incorrect. Resurgence refers to the reappearance of a previously reinforced but extinguished behavior when a currently reinforced alternative behavior is itself placed on extinction. Here, the right-lever behavior is not being extinguished; instead, it is increasing — the pattern is the opposite of what drives resurgence.",
        "D": "Incorrect. The Premack principle holds that a higher-probability behavior can serve as a reinforcer for a lower-probability behavior. While it involves relative response probabilities, it does not explain why a previously stable response would increase above baseline when its competitor is extinguished."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-07-vignette-L3",
      "source_question_id": "07",
      "source_summary": "If reinforcement is stopped for pressing Bar B, the rat will press Bar A with greater frequency and Bar B with less frequency, demonstrating behavioral contrast.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "contrast"
      ],
      "vignette": "A graduate student is running a series of sessions with a pigeon trained to peck two keys in an operant chamber. Both keys have been concurrently available and reinforced, but the student begins a new phase in which pecking the green key no longer produces grain while pecking the red key continues to be reinforced on the same schedule as before. The pigeon begins pecking the green key less and less, which the student attributes simply to the green-key response being extinguished. However, the student is puzzled to find that the red-key response rate climbs notably higher than it ever was during the original training phase, even though nothing has changed about the red-key contingency.",
      "question": "What learning phenomenon BEST accounts for the unexpectedly elevated rate of red-key pecking?",
      "options": {
        "A": "Extinction burst on the red key",
        "B": "Positive induction",
        "C": "Behavioral contrast",
        "D": "Concurrent reinforcement schedule reallocation"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. Behavioral contrast is the phenomenon whereby removing or reducing reinforcement for one response in a multiple-schedule context produces a sustained increase in responding on the concurrently available reinforced alternative above its pre-change baseline. The unchanged red-key contingency producing above-baseline responding after green-key extinction is the defining signature of behavioral contrast.",
        "A": "Incorrect. An extinction burst refers to a temporary increase in the rate, intensity, or variability of the response that is being extinguished — in this case, green-key pecking. It does not explain an increase in a separate, still-reinforced response such as red-key pecking.",
        "B": "Incorrect. Positive induction (also called induction in classical conditioning contexts) refers to a phenomenon from Pavlov's work where inhibition in one area of the cortex leads to excitation in adjacent areas. While it superficially resembles behavioral contrast in producing a contrast effect, it is a classical-conditioning construct and does not precisely map onto operant multiple-schedule outcomes.",
        "D": "Incorrect. 'Concurrent reinforcement schedule reallocation' is not an established technical term in operant conditioning. While matching law concepts describe how animals distribute responses across concurrent schedules, they predict proportional allocation, not an above-baseline increase on one alternative when nothing about that alternative has changed."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-07-vignette-L4",
      "source_question_id": "07",
      "source_summary": "If reinforcement is stopped for pressing Bar B, the rat will press Bar A with greater frequency and Bar B with less frequency, demonstrating behavioral contrast.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "baseline"
      ],
      "vignette": "A researcher studying operant behavior in pigeons notes that when sessions alternate between two components — one in which key-pecking is reinforced and one in which it is not — the birds' responding in the reinforced component is consistently higher than when that same component is presented alone throughout an entire session. The researcher initially suspects the birds are simply showing better discrimination between the two components over time. However, careful analysis reveals that the elevated responding cannot be accounted for by changes in the discriminative stimuli, the reinforcement rate of the reinforced component, or the session history of that component taken in isolation. The effect is most pronounced when the non-reinforcement component immediately precedes the reinforcement component.",
      "question": "The elevated responding in the reinforced component, relative to its standalone baseline, is BEST explained by which of the following?",
      "options": {
        "A": "Stimulus discrimination training",
        "B": "Behavioral contrast",
        "C": "Schedule-induced polydipsia",
        "D": "Partial reinforcement extinction effect"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Behavioral contrast, as studied in multiple-schedule research, produces above-baseline responding in a reinforced component precisely when it is preceded or surrounded by components with lower or absent reinforcement. The key diagnostic feature — elevated responding in the reinforced component that exceeds the standalone baseline and cannot be attributed to the reinforcement rate of that component alone — is the hallmark of behavioral contrast.",
        "A": "Incorrect. Stimulus discrimination training could plausibly explain more precise responding during the reinforced component, but it would not produce responding that exceeds the standalone baseline rate. Discrimination training predicts sharper differentiation, not an absolute increase in response rate beyond what the reinforcement schedule alone would support.",
        "C": "Incorrect. Schedule-induced polydipsia refers to the excessive drinking that occurs in the post-reinforcement pause on intermittent schedules, an adjunctive behavior not related to the distribution of responding across components in a multiple schedule. It shares the context of reinforcement schedules but addresses an entirely different behavioral mechanism.",
        "D": "Incorrect. The partial reinforcement extinction effect (PREE) refers to the finding that intermittently reinforced responses are more resistant to extinction than continuously reinforced ones. It describes persistence of behavior after reinforcement is withdrawn, not an increase in responding above baseline during an ongoing reinforced component."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-07-vignette-L5",
      "source_question_id": "07",
      "source_summary": "If reinforcement is stopped for pressing Bar B, the rat will press Bar A with greater frequency and Bar B with less frequency, demonstrating behavioral contrast.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher sets up two identical response opportunities for an animal. The animal learns over many sessions to use both options roughly equally, earning rewards from each. The researcher then silently stops offering rewards for one of the two options, making no other changes. As expected, the animal gradually stops using the unrewarded option. What the researcher did not anticipate, however, is that the animal begins using the still-rewarded option at a rate noticeably higher than it ever had before — even though the rules and rewards for that option have not changed at all. A colleague suggests the animal may have simply learned to prefer the rewarded option over time, but the researcher points out that this preference appeared abruptly right after the change and the response rate exceeds anything seen during the original training.",
      "question": "The abrupt increase in responding on the still-rewarded option beyond its original training levels is BEST accounted for by which of the following processes?",
      "options": {
        "A": "Habituation to the unrewarded option freeing up behavioral capacity",
        "B": "Learned preference developing through differential reinforcement history",
        "C": "Behavioral contrast arising from removal of reinforcement in a concurrent context",
        "D": "Resurgence of a previously favored response pattern"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. Behavioral contrast is defined by exactly this pattern: when reinforcement is removed from one option in a concurrent or multiple-schedule arrangement, responding on the remaining reinforced option rises above its original baseline level, even though nothing about that option's contingencies has changed. The abrupt onset tied specifically to the removal of reinforcement on the alternative option, and the above-baseline level of responding, are the defining signatures of behavioral contrast.",
        "A": "Incorrect. Habituation refers to the decrease in a response to a repeatedly presented, inconsequential stimulus. Although it could loosely suggest that reduced use of the unrewarded option 'frees up' behavior, habituation does not predict that responding on a separate option will exceed its prior baseline; it would only predict redistribution, not an above-baseline elevation.",
        "B": "Incorrect. A learned preference through differential reinforcement could explain why the animal comes to favor the rewarded option over the unrewarded one, and a student who focuses on the reward differential might select this. However, differential reinforcement history would produce a gradual shift, not an abrupt above-baseline increase, and cannot explain why the rate exceeds the original training levels when nothing about the rewarded option has changed.",
        "D": "Incorrect. Resurgence occurs when a behavior that was previously reinforced but then extinguished reappears when the current reinforced behavior is itself placed on extinction. The scenario describes the opposite pattern: the reinforced option's rate is increasing, not being extinguished, and there is no indication that the elevated responding represents a return of an older, suppressed behavior."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-013-vignette-L1",
      "source_question_id": "013",
      "source_summary": "Overshadowing occurs when two neutral stimuli that differ in salience are repeatedly presented together before an unconditioned stimulus (US) until the paired stimuli become conditioned stimuli (CS) and presentation of the two stimuli together elicits a conditioned response (CR), but when each CS is subsequently presented alone, only the more salient CS will elicit the CR because the less salient CS was overshadowed by the more salient CS.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "overshadowing",
        "salient",
        "conditioned stimulus"
      ],
      "vignette": "A researcher pairs a loud buzzer and a dim light simultaneously as neutral stimuli, always followed immediately by a mild electric shock (the unconditioned stimulus). After many conditioning trials, the researcher tests each conditioned stimulus separately. The animal displays a robust fear response to the buzzer alone but shows almost no response to the dim light alone. The researcher concludes that the more salient stimulus prevented the less salient stimulus from acquiring associative strength.",
      "question": "Which classical conditioning phenomenon best explains why the dim light failed to elicit a conditioned response when presented alone?",
      "options": {
        "A": "Blocking",
        "B": "Overshadowing",
        "C": "Latent inhibition",
        "D": "Stimulus generalization"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Blocking occurs when prior conditioning to one stimulus prevents a subsequently added stimulus from acquiring associative strength. In this scenario, both stimuli were introduced simultaneously from the start — there was no pre-training phase with only one stimulus — so blocking does not apply.",
        "B": "Overshadowing is correct. When two neutral stimuli of differing salience are paired together with a US, the more salient conditioned stimulus (the loud buzzer) overshadows the less salient one (the dim light), leaving the less salient stimulus unable to elicit the conditioned response when tested alone.",
        "C": "Latent inhibition refers to the slowed conditioning that occurs when an organism has been repeatedly exposed to a stimulus alone (without consequence) before conditioning trials begin. The dim light was never presented alone prior to conditioning, so latent inhibition does not account for the finding.",
        "D": "Stimulus generalization describes the tendency for a conditioned response to be evoked by stimuli that are similar to, but not identical to, the original conditioned stimulus. This concept involves responding to new stimuli, not the failure of a co-presented stimulus to acquire associative strength."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-013-vignette-L2",
      "source_question_id": "013",
      "source_summary": "Overshadowing occurs when two neutral stimuli that differ in salience are repeatedly presented together before an unconditioned stimulus (US) until the paired stimuli become conditioned stimuli (CS) and presentation of the two stimuli together elicits a conditioned response (CR), but when each CS is subsequently presented alone, only the more salient CS will elicit the CR because the less salient CS was overshadowed by the more salient CS.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "conditioned response",
        "salience"
      ],
      "vignette": "A 9-year-old boy who was previously healthy develops a fear response after being repeatedly present during his older brother's violent asthma attacks. During each attack, he heard the extremely loud sound of labored breathing and also noticed the faint odor of the particular air-freshener used in the room. The boy has since been evaluated and carries no anxiety disorder diagnosis. When the psychologist later tests his reactions, the boy shows marked autonomic arousal when he hears similar labored breathing sounds but displays no discernible conditioned response when only the faint scent is introduced.",
      "question": "What conditioning phenomenon most accurately explains why the faint scent fails to elicit a fear response in the boy?",
      "options": {
        "A": "Extinction",
        "B": "Overshadowing",
        "C": "Blocking",
        "D": "Higher-order conditioning"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Extinction involves the repeated presentation of a previously conditioned stimulus in the absence of the unconditioned stimulus, resulting in a gradual reduction of the conditioned response. The scent never reliably elicited a response to begin with, so there is nothing to extinguish; this is not an extinction process.",
        "B": "Overshadowing is correct. The highly salient loud breathing noise and the faint, low-salience odor were paired simultaneously with the distressing US (the asthma attack). The more salient stimulus acquired greater associative strength and overshadowed the weaker stimulus, leaving the scent unable to elicit the conditioned fear response on its own.",
        "C": "Blocking requires that one stimulus was conditioned to the US in an earlier, separate phase before the second stimulus was introduced. Because both the sound and the scent were present together from the start of the boy's experience, there was no prior conditioning phase that could block the scent, ruling out blocking.",
        "D": "Higher-order conditioning occurs when a well-established conditioned stimulus is used to condition a new neutral stimulus, without further involvement of the original unconditioned stimulus. That process is not described here; both stimuli were present during the same episodes involving the actual aversive event."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-013-vignette-L3",
      "source_question_id": "013",
      "source_summary": "Overshadowing occurs when two neutral stimuli that differ in salience are repeatedly presented together before an unconditioned stimulus (US) until the paired stimuli become conditioned stimuli (CS) and presentation of the two stimuli together elicits a conditioned response (CR), but when each CS is subsequently presented alone, only the more salient CS will elicit the CR because the less salient CS was overshadowed by the more salient CS.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "neutral stimuli"
      ],
      "vignette": "In a graduate research seminar, students review data from a study in which two neutral stimuli — a vivid flashing light and a barely perceptible background hum — were always presented together and immediately followed by an aversive loud noise. The instructor notes that the hum and the light were introduced simultaneously on the very first trial. After extensive conditioning, animals reliably tensed and startled when the light was shown alone but showed no measurable defensive reaction to the hum alone, even though the hum was objectively detectable by the animals as confirmed by audiological testing. A student suggests that the hum must have undergone some form of prior habituation, but the instructor points out that neither stimulus had any prior history before the experiment.",
      "question": "Which conditioning phenomenon does this pattern of results most directly illustrate?",
      "options": {
        "A": "Blocking",
        "B": "Latent inhibition",
        "C": "Conditioned inhibition",
        "D": "Overshadowing"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Blocking requires that one of the two stimuli was pre-trained as a conditioned stimulus before the compound stimulus was introduced. Here both stimuli debuted simultaneously on trial one, explicitly ruling out a prior conditioning phase and therefore ruling out blocking as the mechanism.",
        "B": "Latent inhibition occurs when a stimulus that has been repeatedly presented alone (without pairing with a US) later shows retarded conditioning. The instructor explicitly states that neither stimulus had any prior history, which is the necessary condition for latent inhibition and therefore eliminates this option.",
        "C": "Conditioned inhibition develops when a stimulus signals the absence of a US, actively inhibiting the conditioned response. The hum was always presented with the US, never signaling its omission, so it could not have become a conditioned inhibitor; the finding is better explained by a failure to acquire excitatory strength.",
        "D": "Overshadowing is correct. Two simultaneously introduced neutral stimuli of unequal physical intensity were paired with the same US. The more salient stimulus (vivid flashing light) dominated associative learning and overshadowed the less salient stimulus (barely perceptible hum), preventing it from independently eliciting the conditioned response."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-013-vignette-L4",
      "source_question_id": "013",
      "source_summary": "Overshadowing occurs when two neutral stimuli that differ in salience are repeatedly presented together before an unconditioned stimulus (US) until the paired stimuli become conditioned stimuli (CS) and presentation of the two stimuli together elicits a conditioned response (CR), but when each CS is subsequently presented alone, only the more salient CS will elicit the CR because the less salient CS was overshadowed by the more salient CS.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "associative strength"
      ],
      "vignette": "A behavioral pharmacologist trains two groups of rats on a compound conditioning protocol in which a moderately loud tone and a faint tactile vibration on the cage floor are always co-presented before a foot shock. In Group A, both stimuli are novel and introduced simultaneously on day one. In Group B, the tone had been paired with the foot shock for 20 sessions before the vibration was added to the protocol. After equivalent total compound trials, both groups show strong conditioned freezing to the tone alone. However, only Group B shows any significant conditioned response to the vibration alone, while Group A animals treated to the vibration alone fail to freeze above baseline. The researcher concludes that a different acquisition-limiting process is operating in each group.",
      "question": "Which phenomenon best accounts for the failure of Group A animals — but not Group B animals — to respond to the vibration alone?",
      "options": {
        "A": "Blocking",
        "B": "Overshadowing",
        "C": "Learned helplessness",
        "D": "Second-order conditioning"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Overshadowing is correct and specifically explains Group A's failure. Because the tone and vibration were introduced simultaneously as co-equal neutral stimuli, the more salient tone competed with and overshadowed the vibration during acquisition, preventing the vibration from accumulating sufficient associative strength to elicit freezing independently.",
        "A": "Blocking is the mechanism at work in Group B, not Group A. In Group B, prior conditioning to the tone meant that the tone's associative strength was already at asymptote when the vibration was added, leaving no prediction error to drive learning about the vibration. Choosing blocking for Group A requires ignoring the critical detail that both stimuli debuted simultaneously in that group.",
        "C": "Learned helplessness involves an organism's failure to attempt escape or avoidance after repeated exposure to inescapable aversive events, resulting in a general motivational and learning deficit. It does not predict the specific pattern of one stimulus being ineffective while another remains effective in the same training context.",
        "D": "Second-order conditioning occurs when a well-established CS is used to condition a new neutral stimulus in the absence of the original US. This process would predict that the tone — once strongly conditioned — could confer associative value on the vibration, which is the opposite of what is observed in Group A and is not the mechanism being tested."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-013-vignette-L5",
      "source_question_id": "013",
      "source_summary": "Overshadowing occurs when two neutral stimuli that differ in salience are repeatedly presented together before an unconditioned stimulus (US) until the paired stimuli become conditioned stimuli (CS) and presentation of the two stimuli together elicits a conditioned response (CR), but when each CS is subsequently presented alone, only the more salient CS will elicit the CR because the less salient CS was overshadowed by the more salient CS.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A woman who survived a severe car accident consistently becomes panicked whenever she is near a busy highway. Interestingly, she also was wearing a particular brand of perfume during the accident, yet she experiences no distress whatsoever when she smells that perfume on other occasions. Her therapist notes that the highway environment — with its roaring engine noise, visual rush of fast vehicles, and sensory intensity — was present throughout the accident, just as the perfume was. Both were experienced at the same moment during the traumatic event and neither had any prior association with danger for the woman. A colleague hypothesizes that the woman must have simply stopped noticing the perfume over time, but the therapist disagrees, pointing out that the woman had not encountered the perfume in any context since the accident until testing.",
      "question": "Which learning phenomenon most precisely explains why the perfume does not trigger fear while the highway environment does?",
      "options": {
        "A": "Extinction of a conditioned fear response to the perfume",
        "B": "Blocking of the perfume by the highway environment",
        "C": "Overshadowing of the perfume by the highway environment",
        "D": "Latent inhibition of the perfume's conditioned properties"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Extinction requires that a stimulus was first successfully conditioned and then repeatedly presented without the aversive outcome, gradually reducing the response. The therapist specifically notes the woman had not re-encountered the perfume since the accident, making it impossible for extinction to have occurred. Additionally, if the perfume never strongly acquired associative strength in the first place, there would be nothing to extinguish.",
        "B": "Blocking would apply if the highway environment had previously been associated with danger before the accident, creating a fully conditioned response, and was then present when the perfume was first introduced. The vignette specifies that neither stimulus had any prior danger association for the woman, ruling out the pre-training phase that is the defining and necessary condition for blocking.",
        "C": "Overshadowing is correct. The highway environment — with its overwhelming multimodal intensity — was a far more salient stimulus than the subtle perfume scent, despite both being present simultaneously during the traumatic event. The high-salience environment monopolized associative learning and prevented the low-salience perfume from acquiring the capacity to elicit fear on its own.",
        "D": "Latent inhibition develops when a stimulus is repeatedly experienced alone and without consequence before any conditioning takes place, slowing subsequent conditioning to that stimulus. The vignette explicitly states neither stimulus had a prior association with danger, and there is no indication the perfume was extensively pre-exposed; the colleague's habituation hypothesis is specifically rejected by the therapist."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-001-vignette-L1",
      "source_question_id": "001",
      "source_summary": "Delay conditioning, in which the presentation of the conditioned stimulus precedes and overlaps presentation of the unconditioned stimulus, is the most effective method for establishing a conditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "conditioned stimulus",
        "unconditioned stimulus",
        "conditioned response"
      ],
      "vignette": "A researcher is designing a classical conditioning experiment to teach laboratory rats to salivate in response to a tone. She decides to present the conditioned stimulus (the tone) first and keep it active while introducing the unconditioned stimulus (food), so that both stimuli overlap during each trial. After several pairings, the rats reliably produce a conditioned response (salivation) to the tone alone. The researcher notes that this procedure consistently yields the strongest and fastest acquisition compared to other timing arrangements she has tested.",
      "question": "Which classical conditioning procedure is the researcher using, and why is it considered the most effective for establishing a conditioned response?",
      "options": {
        "A": "Trace conditioning, because a brief gap between stimuli forces the organism to maintain a memory trace of the CS, which strengthens encoding.",
        "B": "Delay conditioning, because the CS precedes and overlaps with the US, providing the most reliable temporal contiguity and predictive signal for learning.",
        "C": "Simultaneous conditioning, because presenting both stimuli at exactly the same moment creates the strongest associative bond between them.",
        "D": "Backward conditioning, because following the US with the CS allows the organism to use the CS as a retrospective predictor of the US."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Trace conditioning involves a gap between the offset of the CS and the onset of the US. Because the CS has already terminated before the US appears, the organism must maintain an internal memory trace. This additional cognitive demand makes trace conditioning less efficient than delay conditioning for establishing a robust conditioned response.",
        "B": "Correct. In delay conditioning the CS begins before the US and remains present (overlapping) until the US onset or co-termination. This arrangement provides optimal temporal contiguity and a clear predictive relationship between the two stimuli, making it the most effective procedure for acquiring a conditioned response.",
        "C": "Simultaneous conditioning presents the CS and US at exactly the same moment with no lead time for the CS. Because the CS offers no advance predictive information about the US, this procedure produces very weak conditioning and is generally considered ineffective compared to delay conditioning.",
        "D": "Backward conditioning presents the US before the CS, meaning the CS cannot predict the US. This reverse temporal order disrupts the predictive relationship required for efficient conditioning and typically results in little or no conditioned response, and may even produce conditioned inhibition."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-001-vignette-L2",
      "source_question_id": "001",
      "source_summary": "Delay conditioning, in which the presentation of the conditioned stimulus precedes and overlaps presentation of the unconditioned stimulus, is the most effective method for establishing a conditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "classical conditioning",
        "acquisition"
      ],
      "vignette": "A clinical researcher studying anxiety disorders wants to identify the most efficient way to establish a fear response in a rodent model for later use in extinction studies. The rat has a well-documented history of exploratory behavior and shows no baseline fear of neutral tones. The researcher pairs a tone with a mild foot shock, ensuring the tone begins approximately one second before the shock and continues sounding throughout the shock delivery. After only five pairings, robust fear acquisition is observed, which the researcher attributes to the specific timing arrangement used.",
      "question": "Which timing procedure in classical conditioning best accounts for the rapid acquisition observed in this study?",
      "options": {
        "A": "Trace conditioning, because the delay between tone offset and shock onset heightens the animal's anticipatory arousal, speeding acquisition.",
        "B": "Backward conditioning, because the shock preceding the tone creates a strong retroactive association that accelerates learning.",
        "C": "Simultaneous conditioning, because presenting both stimuli together creates the maximal overlap and therefore the strongest associative bond.",
        "D": "Delay conditioning, because the CS onset precedes and overlaps with the US, optimizing temporal contiguity and predictive signal strength during acquisition."
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Trace conditioning requires the CS to terminate before the US begins, creating a stimulus-free gap. Although this gap does engage memory processes, it generally slows rather than speeds acquisition compared to delay conditioning, making it a poor explanation for the rapid learning observed here.",
        "B": "Backward conditioning presents the US before the CS, reversing the predictive relationship. This arrangement is largely ineffective for establishing conditioned responses and in some cases produces conditioned inhibition, so it cannot account for the rapid fear acquisition described.",
        "C": "Simultaneous conditioning presents the CS and US at the same instant with no CS lead time. Without any temporal advance of the CS over the US, the CS carries no predictive value, resulting in weak or no conditioning. It does not explain rapid acquisition.",
        "D": "Correct. The scenario explicitly describes the tone beginning before the shock and remaining on during shock delivery — the defining feature of delay conditioning. This procedure maximizes temporal contiguity while maintaining the CS as a forward-predictive signal, producing the fastest and most robust acquisition among standard classical conditioning arrangements."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-001-vignette-L3",
      "source_question_id": "001",
      "source_summary": "Delay conditioning, in which the presentation of the conditioned stimulus precedes and overlaps presentation of the unconditioned stimulus, is the most effective method for establishing a conditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "overlap"
      ],
      "vignette": "A behavioral neuroscientist is comparing four stimulus-timing protocols to determine which produces the highest conditioned eyeblink response rate in rabbits. In the protocol of interest, a light flash begins 500 ms before an air puff to the eye and continues until the air puff is delivered, so the two stimuli briefly overlap at the moment of air puff onset. Despite the experiment appearing deceptively simple relative to other protocols that use more complex interstimulus intervals or memory-demand manipulations, this protocol consistently outperforms the others in both speed of acquisition and asymptotic response strength. A graduate student suggests the advantage is due to the requirement for an internal memory representation, but the supervising researcher disagrees.",
      "question": "The supervisor is correct to disagree. Which conditioning procedure does the winning protocol represent, and what is the actual reason for its superiority?",
      "options": {
        "A": "Trace conditioning; its superiority results from the hippocampally mediated memory trace that bridges the interstimulus gap, creating a stronger engram than simpler procedures.",
        "B": "Simultaneous conditioning; presenting both stimuli with maximal temporal overlap at the same moment optimizes associative strength by eliminating any predictive uncertainty.",
        "C": "Delay conditioning; the CS precedes and overlaps with the US, providing optimal temporal contiguity and a clear predictive signal without requiring an internal memory bridge, which is why it outperforms procedures demanding a memory trace.",
        "D": "Higher-order conditioning; the light flash acts as a secondary CS that derives its associative strength from a prior association with the air puff, explaining the rapid and robust responding."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Trace conditioning does engage hippocampal memory processes to bridge the gap between CS offset and US onset, and this is a genuine and well-studied phenomenon. However, the described protocol shows no gap — the CS remains active until US onset — and trace conditioning's memory demands make it slower and less robust, not superior. The graduate student's incorrect suggestion of a memory trace is the red herring this option is designed to exploit.",
        "B": "Simultaneous conditioning presents CS and US with no temporal lead for the CS. Although the scenario mentions overlap, the critical detail is that the CS begins 500 ms before the US — it is not simultaneous. Simultaneous conditioning is among the weakest procedures; the overlap described here is the tail end of a delay arrangement, not a fully simultaneous one.",
        "C": "Correct. The protocol described — CS onset 500 ms before US, CS continuing through US onset — is the textbook definition of delay conditioning. Its superiority stems from the CS serving as a reliable forward-predictive signal while maintaining direct temporal contiguity with the US, requiring no internal memory bridge. This combination of predictiveness and contiguity is why delay conditioning produces the strongest and fastest conditioned responses.",
        "D": "Higher-order (second-order) conditioning occurs when a previously neutral stimulus is paired with an already-established CS rather than directly with a US. Nothing in the protocol description suggests the light flash has any prior conditioning history or that the US is being replaced by an established CS. This option introduces a plausible conditioning concept but does not fit the described experimental arrangement."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-001-vignette-L4",
      "source_question_id": "001",
      "source_summary": "Delay conditioning, in which the presentation of the conditioned stimulus precedes and overlaps presentation of the unconditioned stimulus, is the most effective method for establishing a conditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "contiguity"
      ],
      "vignette": "A researcher reports that among several classical conditioning variants tested on human volunteers, one procedure consistently produced the fastest fear acquisition and the strongest galvanic skin response at asymptote. The researcher noted that the critical variable appeared to be contiguity between stimuli — specifically, the temporal arrangement ensured that participants could use the first stimulus as a reliable anticipatory signal before the aversive event arrived. Interestingly, subjects in this group also verbally reported less confusion about the stimulus relationship than subjects in groups where no advance warning signal was available. A colleague reviewing the paper argues that the results are best explained by the formation of a memory trace that bridges an intervening gap between the two stimuli.",
      "question": "The colleague's interpretation is incorrect. Which procedure most precisely explains the pattern of results, and why does the colleague's proposed mechanism not apply?",
      "options": {
        "A": "Simultaneous conditioning; the maximal contiguity achieved by presenting both stimuli together eliminated predictive uncertainty, and the colleague's memory-trace account does not apply because there is no gap to bridge.",
        "B": "Trace conditioning; the gap between stimuli does require a memory trace, but the colleague is wrong because the trace in this case is too short to produce hippocampal involvement.",
        "C": "Delay conditioning; the CS overlaps with the US after preceding it, providing optimal contiguity and a clear predictive signal without any interstimulus gap — precisely why no memory trace is needed and the colleague's account is inapplicable.",
        "D": "Backward conditioning; the aversive event preceding the warning signal created a retroactive association that paradoxically increased predictive clarity, and no memory trace is needed because the US itself serves as a retrieval cue."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Simultaneous conditioning involves co-onset of CS and US with no temporal lead for the CS, meaning the CS cannot function as an advance anticipatory signal. The scenario explicitly states participants used the first stimulus as an anticipatory cue before the aversive event — a feature incompatible with simultaneous conditioning. Additionally, simultaneous conditioning produces weak, not robust, acquisition.",
        "B": "Trace conditioning does involve a memory trace bridging a gap between CS offset and US onset, and the colleague's language directly mirrors this concept — making this option a compelling distractor. However, the scenario describes the CS as providing an anticipatory signal that remains useful up to US onset with participants reporting clarity about the stimulus relationship; trace conditioning's gap and associated memory demands typically reduce rather than enhance clarity and response strength relative to delay conditioning.",
        "C": "Correct. Delay conditioning is defined by the CS preceding and overlapping with the US, creating both temporal contiguity and forward predictability without any interstimulus gap. These features explain faster acquisition, stronger asymptotic response, and greater verbal clarity about stimulus relationships. Because there is no gap between CS offset and US onset, no memory trace is required to bridge the two stimuli — directly refuting the colleague's account.",
        "D": "Backward conditioning presents the US before the CS, meaning the CS cannot serve as a forward-anticipatory signal. The scenario clearly states participants used the first stimulus as an anticipatory warning before the aversive event, which is logically impossible in backward conditioning. Backward procedures also typically produce inhibitory or no conditioning rather than the rapid excitatory acquisition described."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-001-vignette-L5",
      "source_question_id": "001",
      "source_summary": "Delay conditioning, in which the presentation of the conditioned stimulus precedes and overlaps presentation of the unconditioned stimulus, is the most effective method for establishing a conditioned response.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "In a laboratory study, participants are exposed to a brief visual cue followed immediately by a mildly unpleasant noise, with the visual cue still visible on the screen at the moment the noise begins. After a small number of repetitions, participants reliably show a measurable physiological reaction to the visual cue alone, even before the noise sounds. A second group of participants is given an identical number of exposures, but in their version the visual cue disappears from the screen for a short interval before the noise begins. A third group sees the noise begin at the same instant as the visual cue, with no advance warning. When all three groups are tested, the first group demonstrates the strongest and fastest-learned physical reaction, while the third group shows almost no learned reaction at all and the second group falls in between.",
      "question": "The pattern of results across all three groups is most precisely explained by differences in which classical conditioning procedure?",
      "options": {
        "A": "Simultaneous conditioning versus trace conditioning versus delay conditioning, with simultaneous conditioning producing the weakest learning because the cue provides no advance predictive information.",
        "B": "Higher-order conditioning versus first-order conditioning versus backward conditioning, with the first group benefiting from a pre-established associative chain that amplifies responding.",
        "C": "Delay conditioning versus trace conditioning versus simultaneous conditioning, with delay conditioning producing the strongest learning because the cue both precedes and overlaps with the aversive event, optimizing predictive signal and temporal closeness without requiring an internal memory bridge.",
        "D": "Forward conditioning versus backward conditioning versus inhibitory conditioning, with the first group benefiting from forward temporal order while the third group develops inhibitory associations due to stimulus co-occurrence."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This option correctly identifies the three procedures and correctly notes that simultaneous conditioning is weakest. However, it misassigns which group maps to which procedure — it implies simultaneous conditioning is being compared to trace and delay without specifying which arrangement produced the intermediate result. More critically, it does not provide a mechanistic explanation for why delay conditioning outperforms trace conditioning, which is the conceptually pivotal distinction in the study. Option C captures this distinction precisely.",
        "B": "Higher-order conditioning involves pairing a neutral stimulus with an already-conditioned CS rather than with an unconditioned stimulus directly. Nothing in the scenario suggests any prior conditioning history for the stimuli, no pre-established conditioned stimulus is mentioned, and the three groups differ only in the temporal arrangement of the same two stimuli. This option misidentifies the fundamental variable being manipulated.",
        "C": "Correct. Group 1 (cue precedes and overlaps with noise) represents delay conditioning, which produces the strongest acquisition because the cue serves as a reliable forward predictor while maintaining direct temporal contiguity, requiring no internal memory bridge. Group 2 (gap between cue offset and noise onset) represents trace conditioning, which produces intermediate learning because memory-trace demands reduce efficiency. Group 3 (simultaneous onset) represents simultaneous conditioning, which produces near-zero learning because the cue offers no predictive advance information. This pattern of results is the empirical signature distinguishing these three temporal procedures.",
        "D": "While 'forward conditioning' is a broad category that includes both delay and trace conditioning, this option conflates two distinct procedures under one label and introduces 'inhibitory conditioning' for the simultaneous group. Simultaneous conditioning typically produces weak excitatory or null responding rather than active inhibition; conditioned inhibition is produced by specific procedures such as explicitly unpaired presentations or compound conditioning paradigms. The option's characterization of the third group is therefore mechanistically inaccurate and fails to distinguish the three groups as precisely as option C does."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-004-vignette-L1",
      "source_question_id": "004",
      "source_summary": "The matching law in the context of operant conditioning predicts the effects of two or more concurrent schedules of reinforcement on the behaviors that are being reinforced, such that the relative rate of responding to the stimuli is equal to the relative rate of reinforcement received for responding.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "concurrent schedules",
        "relative rate of responding",
        "relative rate of reinforcement"
      ],
      "vignette": "A behavioral researcher sets up a choice chamber in which a pigeon can peck either of two keys. Key A delivers food on a variable-interval schedule that provides reinforcement twice as often as Key B. After extended exposure to these concurrent schedules, the pigeon allocates its pecks across the two keys such that its relative rate of responding to each key closely matches the relative rate of reinforcement obtained from each key. The researcher notes that the pigeon directs roughly twice as many pecks toward Key A as toward Key B, consistent with the quantitative prediction being studied.",
      "question": "Which principle of operant conditioning best explains the pigeon's pattern of key pecking?",
      "options": {
        "A": "Behavioral momentum",
        "B": "The matching law",
        "C": "The premack principle",
        "D": "Response deprivation hypothesis"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Behavioral momentum describes how response rates under high reinforcement density are more resistant to disruption (extinction, distraction), not how an organism distributes behavior proportionally across two simultaneously available reinforcement options.",
        "B": "The matching law, formulated by Herrnstein, states that when two or more concurrent reinforcement schedules are available, the relative rate of responding to each alternative will equal (match) the relative rate of reinforcement obtained from that alternative. The pigeon's 2:1 allocation of pecks mirroring the 2:1 reinforcement ratio is the textbook example of this principle.",
        "C": "The Premack principle states that a higher-probability behavior can reinforce a lower-probability behavior, addressing the relative reinforcing value of activities rather than how response allocation is distributed proportionally across simultaneous alternatives.",
        "D": "The response deprivation hypothesis extends the Premack principle by specifying that a behavior functions as a reinforcer when access to it has been restricted below its baseline rate; it does not predict proportional distribution of responses across concurrent options."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-004-vignette-L2",
      "source_question_id": "004",
      "source_summary": "The matching law in the context of operant conditioning predicts the effects of two or more concurrent schedules of reinforcement on the behaviors that are being reinforced, such that the relative rate of responding to the stimuli is equal to the relative rate of reinforcement received for responding.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "concurrent schedules",
        "response allocation"
      ],
      "vignette": "A 9-year-old boy diagnosed with ADHD is observed during free time in his classroom. His teacher has set up two activity stations: a tablet game station and a drawing station. Unknown to the teacher, the tablet game delivers social praise (via recorded messages) on average every 2 minutes, while the drawing station generates teacher attention on average every 6 minutes. After two weeks of free-time sessions, data show that the child spends approximately 75% of his time at the tablet station and 25% at the drawing station. His response allocation across these concurrent schedules closely mirrors the ratio of reinforcement rates delivered at each station.",
      "question": "Which behavioral principle most directly accounts for the proportion of time the child allocates between the two stations?",
      "options": {
        "A": "The matching law",
        "B": "Differential reinforcement of other behavior (DRO)",
        "C": "Variable-ratio schedule effects",
        "D": "Stimulus generalization gradient"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "The matching law predicts that the proportion of behavior directed toward each alternative equals the proportion of total reinforcement obtained from that alternative. A 3:1 ratio of reinforcement rates (every 2 min vs. every 6 min) predicts 75%:25% response allocation, which is precisely what is observed here.",
        "B": "DRO is a procedure in which reinforcement is delivered contingent on the absence of a target behavior for a specified interval; it is a behavior reduction strategy, not a principle that predicts how a learner distributes time across simultaneously available reinforcement sources.",
        "C": "Variable-ratio schedules produce high, steady rates of responding and strong resistance to extinction, but this describes the properties of a single schedule. The scenario involves two simultaneously available schedules, and the proportional distribution of behavior — not just rate or persistence — is what requires explanation.",
        "D": "Stimulus generalization gradient describes how response strength varies as a function of how similar a new stimulus is to the original conditioned stimulus; it pertains to classical or operant discrimination training, not to the proportional allocation of behavior across concurrent reinforcement alternatives."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-004-vignette-L3",
      "source_question_id": "004",
      "source_summary": "The matching law in the context of operant conditioning predicts the effects of two or more concurrent schedules of reinforcement on the behaviors that are being reinforced, such that the relative rate of responding to the stimuli is equal to the relative rate of reinforcement received for responding.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "proportional"
      ],
      "vignette": "A behavioral consultant is analyzing the phone-use habits of college students. She notes that students who have two social media apps open simultaneously distribute their time across the two apps in a way that is proportional to how frequently each app delivers 'likes' and comments on their posts. Although students report that one app 'feels more engaging' because of its visual design — a plausible explanation for their preference — careful measurement reveals that the actual number of social interactions (reinforcing events) delivered per hour by each app fully accounts for the observed time distribution. The consultant also rules out the possibility that students are simply more skilled at using one app, as performance metrics are equivalent across apps.",
      "question": "Which operant learning principle best explains why students distribute their time between the two apps in proportion to the rate of social reinforcement each delivers?",
      "options": {
        "A": "Optimal foraging theory",
        "B": "Behavioral contrast",
        "C": "The matching law",
        "D": "Conditioned reinforcement"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Optimal foraging theory, borrowed from behavioral ecology, predicts that organisms maximize net rate of energy gain and may switch between patches when local returns fall below the average — a related but distinct framework. It does not specifically predict that response proportions will equal reinforcement proportions; matching is a more precise and directly applicable principle here.",
        "B": "Behavioral contrast refers to the phenomenon in which decreasing reinforcement on one component of a multiple schedule produces an increase in responding on the other component, and vice versa; it describes changes in response rate following schedule changes, not the proportional allocation of behavior across simultaneously available alternatives.",
        "C": "The matching law directly predicts that when multiple reinforcement sources are simultaneously available, the proportion of responding allocated to each source will equal the proportion of total reinforcement obtained from that source. The consultant's finding that social interaction rates (reinforcement rates) fully account for time distribution is the hallmark of matching.",
        "D": "Conditioned reinforcement describes the process by which a previously neutral stimulus acquires reinforcing properties through association with a primary reinforcer; it explains how 'likes' become reinforcing in the first place but does not predict how behavior will be distributed proportionally across two simultaneously available reinforcement alternatives."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-004-vignette-L4",
      "source_question_id": "004",
      "source_summary": "The matching law in the context of operant conditioning predicts the effects of two or more concurrent schedules of reinforcement on the behaviors that are being reinforced, such that the relative rate of responding to the stimuli is equal to the relative rate of reinforcement received for responding.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "ratio"
      ],
      "vignette": "A clinic specializing in pediatric feeding disorders uses two therapists who differ in how frequently they deliver praise during mealtime intervention sessions. Without formal planning, Therapist A delivers praise approximately four times per session on average, while Therapist B delivers praise approximately once per session on average. Over several months, staff observe that the child spontaneously seeks out Therapist A's room far more often when both rooms are available, and that the child's approach ratio across rooms tracks the reinforcement ratio with striking precision. The treatment team initially attributes this to the child's 'personality fit' with Therapist A, but a behavior analyst consulting on the case argues that the pattern reflects a fundamental quantitative law governing how organisms distribute behavior under simultaneously available reinforcement sources.",
      "question": "Which principle does the behavior analyst most likely invoke to explain the child's room-choice pattern?",
      "options": {
        "A": "Differential reinforcement of high rates (DRH)",
        "B": "The matching law",
        "C": "Relative value theory of reinforcement",
        "D": "Resistance to extinction as a function of reinforcement history"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Differential reinforcement of high rates (DRH) is a schedule in which reinforcement is delivered only when responses occur at or above a specified rate within an interval; it is a procedure to increase response rate on a single operant, not a principle predicting how behavior is proportionally distributed across two simultaneously available reinforcement alternatives.",
        "B": "The matching law is the quantitative principle stating that when two or more concurrent reinforcement alternatives exist, the proportion of responses (or time) allocated to each alternative will equal the proportion of total reinforcement obtained from that alternative. A 4:1 praise ratio predicting a 4:1 approach ratio is a precise, textbook illustration of matching. The 'ratio' hint word points to this proportional relationship.",
        "C": "Relative value theory addresses how the reinforcing effectiveness of a stimulus changes depending on what other reinforcers are available (i.e., substitutability and complementarity among reinforcers); while related to concurrent choice, it does not provide the specific quantitative prediction that response proportions will equal reinforcement proportions as the matching law does.",
        "D": "Resistance to extinction as a function of reinforcement history (related to the partial reinforcement extinction effect) predicts that behaviors reinforced intermittently will persist longer during extinction; this concerns persistence of a single behavior over time after reinforcement ceases, not the proportional allocation of behavior across two simultaneously active reinforcement sources."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-004-vignette-L5",
      "source_question_id": "004",
      "source_summary": "The matching law in the context of operant conditioning predicts the effects of two or more concurrent schedules of reinforcement on the behaviors that are being reinforced, such that the relative rate of responding to the stimuli is equal to the relative rate of reinforcement received for responding.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher studying decision-making in children places a 7-year-old in a room with two toy bins on opposite sides. Each bin dispenses a small sticker when the child reaches into it, but the two bins are set up so that one dispenses stickers more frequently than the other — specifically, at three times the rate. The child, who has never been told anything about the bins, freely moves between them over a 45-minute session. By the end of the session, the child has visited the more generous bin roughly three times as often as the less generous bin. The researcher notes that the child never appeared to deliberately count or compare outcomes; the behavior emerged spontaneously through natural exploration. A developmental psychologist watching suggests the pattern reflects the child's growing cognitive ability to track and compare reward histories, but the researcher disagrees, arguing the pattern follows a simpler, more universal behavioral law.",
      "question": "Which principle does the researcher most likely argue is responsible for the child's bin-visiting pattern?",
      "options": {
        "A": "Probability matching",
        "B": "The matching law",
        "C": "Melioration theory",
        "D": "Optimal foraging theory"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Probability matching is a cognitive phenomenon in which individuals match their choice frequencies to the probability of reward at each option (e.g., choosing a 70%-rewarding option 70% of the time rather than always), typically studied in judgment and decision-making research. It shares the surface feature of proportional choice but is framed as a cognitive heuristic about probability estimation rather than a universal quantitative law derived from reinforcement rates, making it a compelling but incorrect choice here.",
        "B": "The matching law is the correct answer. It states that when organisms have simultaneous access to multiple reinforcement sources, the proportion of behavior directed toward each source will equal the proportion of total reinforcement obtained from that source. The child's 3:1 visit ratio precisely mirrors the 3:1 reinforcement rate ratio, and the researcher explicitly argues for a simple, universal behavioral law — language that directly evokes the matching law's status in behavior analysis.",
        "C": "Melioration theory, proposed by Herrnstein and Vaughan, holds that organisms shift behavior toward whichever alternative currently delivers a higher local rate of reinforcement until rates are equalized — a process that often produces matching as an outcome. It is closely related to the matching law and describes a dynamic process that can generate matching behavior, but it is a mechanistic account of how matching arises rather than the descriptive law itself. The researcher is identifying the observed pattern, not the underlying mechanism.",
        "D": "Optimal foraging theory predicts that organisms maximize their overall rate of return by choosing the option with the highest long-term yield and switching when local returns fall below the environmental average — which would predict exclusive preference for the richer option rather than proportional distribution. The child's proportional (rather than exclusive) allocation of visits actually violates optimal foraging predictions, making this a plausible but ultimately incorrect explanation."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-011-vignette-L1",
      "source_question_id": "011",
      "source_summary": "Fading refers to the gradual removal of visual or auditory hints or cues (prompts) used to help students recall information, while thinning refers to the reduction of reinforcers.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "prompts",
        "fading",
        "reinforcers"
      ],
      "vignette": "A behavior analyst is training a child with autism to independently identify letters of the alphabet. Initially, the therapist provides bold visual prompts beneath each letter card to help the child respond correctly. Over successive sessions, the therapist systematically reduces the intensity of these visual prompts until the child can identify letters without any additional cues. The therapist is careful to distinguish this technique from the separate process of reducing how often reinforcers are delivered once the skill is mastered.",
      "question": "The therapist's systematic reduction of visual prompts to help the child recall letters independently is best described as which behavioral procedure?",
      "options": {
        "A": "Thinning",
        "B": "Shaping",
        "C": "Fading",
        "D": "Chaining"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Thinning refers to the gradual reduction of the frequency or magnitude of reinforcers delivered for a behavior, not the removal of cues or prompts. While thinning was mentioned in the vignette as a contrasting procedure, it does not describe what the therapist is doing with the visual prompts.",
        "B": "Shaping involves the differential reinforcement of successive approximations toward a target behavior. While shaping is used to build new behaviors, it does not describe the systematic removal of visual cues or prompts as illustrated in this scenario.",
        "C": "Fading is the correct answer. Fading refers to the gradual removal of visual or auditory hints or prompts used to help a learner recall or perform a behavior, which precisely matches the therapist's systematic reduction of visual cues beneath the letter cards.",
        "D": "Chaining is a procedure used to teach complex behaviors by linking a sequence of individual stimulus-response components together. It does not involve the removal of prompts or cues and does not fit the scenario described."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-011-vignette-L2",
      "source_question_id": "011",
      "source_summary": "Fading refers to the gradual removal of visual or auditory hints or cues (prompts) used to help students recall information, while thinning refers to the reduction of reinforcers.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "prompts",
        "thinning"
      ],
      "vignette": "A special education teacher is working with a 9-year-old boy diagnosed with an intellectual disability to help him learn to read sight words. The teacher initially highlights each word with a bright yellow border to draw his attention to it. Over the course of eight weeks, she progressively reduces the brightness and thickness of the yellow highlighting on each card. Separately, once the boy demonstrates consistent accuracy, the teacher plans to reduce how often she provides praise and stickers for correct responses. The boy's mother notes that he has been more motivated since the colorful cards were introduced.",
      "question": "The teacher's plan to reduce the frequency of praise and stickers once consistent accuracy is established is best described as which behavioral procedure?",
      "options": {
        "A": "Fading",
        "B": "Thinning",
        "C": "Extinction",
        "D": "Differential reinforcement"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Fading describes the gradual removal of prompts or cues used to help a learner perform a target behavior. In this vignette, fading is what the teacher does with the yellow highlighting on the cards — not the reduction of praise and stickers, which is what the question asks about.",
        "B": "Thinning is the correct answer. Thinning refers to the gradual reduction in the frequency or magnitude of reinforcers delivered for a target behavior. The teacher's plan to reduce how often praise and stickers are given once consistent accuracy is established precisely matches this definition.",
        "C": "Extinction involves withholding reinforcement entirely for a previously reinforced behavior, leading to its eventual decrease. The teacher is not eliminating reinforcers altogether but gradually reducing their frequency, which distinguishes this from extinction.",
        "D": "Differential reinforcement involves providing reinforcement for specific target behaviors while withholding it for others. It describes the selective delivery of reinforcers based on response quality, not the gradual reduction of reinforcement frequency for an already-established behavior."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-011-vignette-L3",
      "source_question_id": "011",
      "source_summary": "Fading refers to the gradual removal of visual or auditory hints or cues (prompts) used to help students recall information, while thinning refers to the reduction of reinforcers.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "prompts"
      ],
      "vignette": "A speech-language pathologist is helping a 7-year-old girl learn to name common household objects during therapy sessions. At the start of treatment, the therapist provides the first syllable of each word aloud whenever the child hesitates. The child's parents report that she has shown rapid progress and seems to be memorizing words quickly. Over the following weeks, the therapist begins whispering the initial syllable rather than saying it at full volume, then mouthing it silently, and finally offering no auditory cues at all — while the child continues to respond correctly. The therapist notes that the child's motivation remains high because she still earns a token for every correct response.",
      "question": "What procedure does the therapist's gradual transition from spoken syllable cues to mouthed cues to no cue at all represent?",
      "options": {
        "A": "Thinning",
        "B": "Shaping",
        "C": "Fading",
        "D": "Stimulus generalization"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Thinning refers to the reduction of the frequency or amount of reinforcement delivered for a target behavior. In this vignette, the child continues to earn a token for every correct response, meaning the reinforcement schedule is unchanged. The therapist is reducing auditory cues, not reinforcers, so thinning does not apply here.",
        "B": "Shaping involves the differential reinforcement of successive approximations to a target behavior that does not yet exist in the learner's repertoire. While the child is improving, the therapist is not reinforcing closer and closer approximations to a new behavior — she is removing auditory supports for a behavior the child can already perform, which is the defining feature of fading.",
        "C": "Fading is the correct answer. Fading is the systematic and gradual reduction of prompts or cues — in this case, auditory cues — used to assist a learner in performing a target behavior. The therapist's progression from full spoken syllables to whispered syllables to mouthed syllables to no cue perfectly exemplifies this procedure.",
        "D": "Stimulus generalization refers to the tendency for a conditioned response to occur in the presence of stimuli similar to the original training stimulus. Although the child is successfully responding across changing cue conditions, this reflects planned prompt removal rather than an untrained spread of responding to novel stimuli."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-011-vignette-L4",
      "source_question_id": "011",
      "source_summary": "Fading refers to the gradual removal of visual or auditory hints or cues (prompts) used to help students recall information, while thinning refers to the reduction of reinforcers.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "schedule"
      ],
      "vignette": "A school psychologist consults with a teacher who has successfully used a structured intervention to help a third-grader named Marcus learn multiplication facts. During the initial weeks, Marcus received a sticker every time he answered a problem correctly. After Marcus achieved 90% accuracy consistently, the teacher began adjusting the intervention: she started delivering stickers after every second correct answer, then every fourth, and eventually only after every tenth correct response. The teacher wanted to ensure Marcus would maintain his multiplication skills in the regular classroom without relying on constant external rewards. The school psychologist commends this approach as a critical step in promoting long-term independence.",
      "question": "The teacher's systematic adjustment of how frequently Marcus receives stickers following correct responses is best described as which procedure?",
      "options": {
        "A": "Fading",
        "B": "Thinning",
        "C": "Token economy",
        "D": "Shifting to a variable ratio schedule"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Fading refers to the gradual removal of prompts or cues — such as visual highlights or auditory hints — used to support a learner's correct responding. In this vignette, no prompts or cues are being removed; instead, the frequency of sticker delivery is being reduced after correct responses are already occurring without additional supports. Fading does not describe changes to reinforcement delivery.",
        "B": "Thinning is the correct answer. Thinning refers to the gradual reduction of the frequency or magnitude of reinforcement provided for a target behavior that is already established. The teacher's progression from continuous reinforcement (every correct answer) to leaner schedules (every 2nd, 4th, then 10th) is the defining procedure of thinning, aimed at promoting behavioral maintenance without dependence on dense reinforcement.",
        "C": "A token economy is a behavior management system in which tokens are earned for target behaviors and later exchanged for backup reinforcers. While stickers function as token-like reinforcers here, the teacher's action of systematically reducing sticker delivery is not the definition or function of a token economy — it describes a specific adjustment within a reinforcement system.",
        "D": "Shifting to a variable ratio schedule describes moving to an unpredictable reinforcement pattern based on an average number of responses. While the teacher is indeed thinning reinforcement, the specific mechanism described — moving from every 1st to every 2nd, 4th, and 10th response — reflects a fixed ratio progression, and the overarching procedure being asked about is thinning, not schedule type. Identifying only the schedule change misses the broader behavioral procedure the question targets."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-011-vignette-L5",
      "source_question_id": "011",
      "source_summary": "Fading refers to the gradual removal of visual or auditory hints or cues (prompts) used to help students recall information, while thinning refers to the reduction of reinforcers.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Operant Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A classroom aide is helping a kindergartner named Lily learn to independently write her first name. At first, the aide places her hand over Lily's and guides each letter stroke from start to finish. Over several weeks, the aide shifts to resting her hand lightly on Lily's wrist, then moves to only touching Lily's elbow, and eventually withdraws all physical contact while Lily writes correctly on her own. During the same period, the aide notices that Lily has become so proficient that she begins giving her a small reward only after Lily completes the entire name correctly rather than after each individual letter. A supervising teacher observes the process and points out that the aide is implementing two distinct but complementary procedures simultaneously.",
      "question": "The aide's systematic withdrawal of physical guidance across sessions — moving from full hand-over-hand contact to no physical contact — is best described as which behavioral procedure?",
      "options": {
        "A": "Thinning",
        "B": "Shaping",
        "C": "Fading",
        "D": "Chaining"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Thinning refers to the gradual reduction in the frequency or magnitude of reinforcement delivered for an established target behavior. In this vignette, thinning is also occurring — the aide shifts from rewarding each letter to rewarding only the completed name — but thinning describes the reinforcement change, not the gradual withdrawal of physical guidance, which is what the question specifically asks about.",
        "B": "Shaping involves delivering reinforcement for successive approximations toward a target behavior that is not yet in the learner's repertoire. While Lily is learning a new skill, the aide is not reinforcing progressively closer approximations; she is systematically removing physical guidance supports while Lily performs the target behavior, which is the hallmark of fading rather than shaping.",
        "C": "Fading is the correct answer. Fading refers to the gradual and systematic removal of physical, visual, or auditory prompts used to support correct responding. The aide's progression from full hand-over-hand guidance to wrist contact, then elbow contact, then no contact at all is a textbook example of physical prompt fading, designed to transfer stimulus control to the natural environment.",
        "D": "Chaining is a procedure used to teach complex multi-step behavioral sequences by linking individual stimulus-response units together, either forward or backward. Although writing a name involves a sequence of letter strokes, the aide is not teaching individual steps in a chain and connecting them; she is removing physical guidance for a behavior the child is already performing, which is fading."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-33-vignette-L1",
      "source_question_id": "33",
      "source_summary": "Pavlov proposed that spontaneous recovery of a conditioned response after extinction trials provides evidence that extinction of the conditioned response is due to internal inhibition.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "extinction",
        "spontaneous recovery",
        "internal inhibition"
      ],
      "vignette": "A researcher conditions a dog to salivate to the sound of a bell by repeatedly pairing the bell with food. After many extinction trials in which the bell is presented without food, the conditioned salivation response drops to near zero. The researcher then allows a rest period with no trials. When the bell is presented again after the rest period, the dog salivates once more, even though no additional pairings occurred. Pavlov argued that the return of the conditioned response after this rest period could only be explained by the nature of what extinction actually does to the original learning.",
      "question": "According to Pavlov, what does spontaneous recovery following extinction trials reveal about the mechanism of extinction?",
      "options": {
        "A": "Extinction permanently erases the original conditioned response through unlearning, making any subsequent response a new acquisition.",
        "B": "Extinction produces internal inhibition that suppresses but does not destroy the conditioned response, as evidenced by its return after a rest period.",
        "C": "Spontaneous recovery reflects the formation of a new conditioned stimulus-unconditioned stimulus association during the rest period.",
        "D": "Extinction weakens the unconditioned stimulus pathway, and rest allows the pathway to recover its original strength."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect. Pavlov specifically argued against the idea that extinction erases learning. The spontaneous recovery of the response after a rest period — without any new pairings — shows the original association was not destroyed, which is the key evidence for internal inhibition rather than unlearning.",
        "B": "This is correct. Pavlov proposed that extinction generates internal inhibition that actively suppresses the conditioned response rather than eliminating it. Spontaneous recovery — the return of the conditioned response after a rest period — is precisely the evidence he used to support this view, because a truly erased response would not return.",
        "C": "This is incorrect. Spontaneous recovery occurs during a rest period in which no stimuli are presented, so no new CS-US association could form. This option confuses spontaneous recovery with reconditioning or reacquisition, which require additional stimulus pairings.",
        "D": "This is incorrect. Pavlov's internal inhibition account focused on the conditioned response pathway, not on changes in unconditioned stimulus strength. Additionally, extinction does not target or weaken the unconditioned stimulus; the US-UR relationship remains intact throughout extinction procedures."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-33-vignette-L2",
      "source_question_id": "33",
      "source_summary": "Pavlov proposed that spontaneous recovery of a conditioned response after extinction trials provides evidence that extinction of the conditioned response is due to internal inhibition.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "extinction",
        "spontaneous recovery"
      ],
      "vignette": "A 34-year-old woman with a dog phobia undergoes systematic desensitization, and after several weeks of treatment her fear response to images of dogs drops to an unmeasurable level. She is a graduate student under significant academic stress during this period, a detail her therapist notes but does not consider clinically relevant to her conditioning history. Three months later, following a semester break with no therapy sessions, the woman contacts her therapist reporting that her fear of dog images has partially returned despite no new negative experiences with dogs.",
      "question": "What classical conditioning phenomenon best explains the partial return of the woman's fear response after the rest period?",
      "options": {
        "A": "Stimulus generalization, in which the fear response broadened during the rest period to encompass stimuli similar to but not identical to the original conditioned stimulus.",
        "B": "Higher-order conditioning, in which neutral stimuli encountered during the break became associated with the original conditioned stimulus through indirect pairing.",
        "C": "Spontaneous recovery, in which the inhibitory process that suppressed the extinguished fear response partially dissipates over time, allowing the original conditioned response to re-emerge.",
        "D": "Reconditioning, in which an unnoticed aversive encounter with a dog-related stimulus during the break re-established the conditioned fear response."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect. Stimulus generalization refers to the conditioned response occurring to stimuli that are similar to but distinct from the original conditioned stimulus — it describes the spread of a response across stimuli, not the return of a previously extinguished response to the same stimulus after a rest period.",
        "B": "This is incorrect. Higher-order conditioning involves a neutral stimulus acquiring conditioned properties by being paired with an already-established conditioned stimulus. The woman's fear returned to the original stimulus (dog images) without evidence of new pairings, so this mechanism does not apply here.",
        "C": "This is correct. Spontaneous recovery describes the re-emergence of an extinguished conditioned response following a rest period, without any new CS-US pairings. Pavlov explained this by arguing that extinction produces internal inhibition — a suppressive process — that weakens over time during the rest interval, allowing the original conditioned response to resurface.",
        "D": "This is incorrect. Reconditioning requires an actual pairing of the conditioned stimulus with an aversive unconditioned stimulus. The scenario explicitly states no new negative experiences with dogs occurred, ruling out this mechanism; the return is spontaneous rather than the result of a new conditioning event."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-33-vignette-L3",
      "source_question_id": "33",
      "source_summary": "Pavlov proposed that spontaneous recovery of a conditioned response after extinction trials provides evidence that extinction of the conditioned response is due to internal inhibition.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "inhibition"
      ],
      "vignette": "A psychophysiology researcher extinguishes a conditioned galvanic skin response (GSR) in human participants by repeatedly presenting a tone that was previously paired with a mild shock, until skin conductance responses to the tone reach baseline. She is careful to note that participants report feeling 'over it' and show no signs of residual anxiety. After a 48-hour interval with no experimental contact, participants return to the lab, and upon hearing the tone again they exhibit a modest but statistically significant skin conductance response — even though no shock has been administered in the interim. The researcher argues that this finding challenges the view that extinction is simply the reversal of original learning.",
      "question": "What theoretical explanation, rooted in Pavlov's analysis of extinction, best accounts for the researcher's observation and supports her conclusion?",
      "options": {
        "A": "Disinhibition, in which a novel extraneous stimulus introduced during extinction disrupted the inhibitory process and allowed the conditioned response to emerge temporarily.",
        "B": "External inhibition, in which the experimental context served as a competing inhibitory stimulus that suppressed responding during extinction but lost its effect over the 48-hour interval.",
        "C": "Spontaneous recovery arising from internal inhibition, in which extinction generated a suppressive process that decayed during the rest interval, revealing the intact underlying conditioned association.",
        "D": "Incubation of anxiety, in which the absence of the feared stimulus during the rest interval paradoxically strengthened the conditioned fear response through a rehearsal-like cognitive process."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect. Disinhibition is also a Pavlovian concept and involves the temporary release of an extinguished response when a novel or surprising stimulus is introduced during the extinction phase itself — not simply after a rest period. In this scenario, no novel external stimulus is introduced; the response returns after a time interval alone, which is the defining feature of spontaneous recovery, not disinhibition.",
        "B": "This is incorrect. External inhibition refers to the disruption or suppression of an ongoing conditioned response by the presentation of a novel external stimulus during the trial — it is a transient effect caused by an orienting response. The scenario involves no new external stimulus; the response returns after a rest period, which is not consistent with external inhibition's mechanism.",
        "C": "This is correct. Pavlov proposed that extinction produces an active internal inhibitory process that overlays but does not erase the original conditioned association. Over time — particularly during a rest period — this inhibitory process decays, allowing the original conditioned response to spontaneously recover. The researcher's finding is the classic evidence for this account: the conditioned response returns after a time interval without any new CS-US pairings.",
        "D": "This is incorrect. Incubation of anxiety is a concept from learning theory (associated with Eysenck) referring to the paradoxical strengthening of a fear CR through repeated brief, unreinforced exposures — not through a simple rest interval. While superficially appealing given the return of the GSR, incubation involves a different mechanism and set of conditions than what is described here."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-33-vignette-L4",
      "source_question_id": "33",
      "source_summary": "Pavlov proposed that spontaneous recovery of a conditioned response after extinction trials provides evidence that extinction of the conditioned response is due to internal inhibition.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "suppression"
      ],
      "vignette": "A behavioral neuroscientist presents data at a conference showing that after a well-trained fear response was reduced to zero through repeated unreinforced stimulus presentations, reintroduction of the conditioned stimulus following a prolonged delay — with absolutely no intervening training — produced robust responding that was indistinguishable from that seen early in the original acquisition phase. Critically, the scientist notes that pharmacological blockade of inhibitory interneurons in the prefrontal cortex during the delay period accelerated and amplified the return of responding. She concludes that the reduction in responding following unreinforced trials does not reflect the elimination of the learned association but rather its active suppression by a competing neural process.",
      "question": "The scientist's interpretation is most consistent with which principle derived from Pavlov's analysis of the extinction process?",
      "options": {
        "A": "Conditioned inhibition, in which the conditioned stimulus itself acquired inhibitory properties during unreinforced trials and directly suppressed the unconditioned response pathway.",
        "B": "Spontaneous recovery as evidence of internal inhibition, in which extinction produces an active suppressive process that degrades over time, allowing the original conditioned association to re-emerge intact.",
        "C": "Counterconditioning, in which the rest interval permitted a competing response to weaken, thereby unmasking a previously suppressed excitatory association between the conditioned and unconditioned stimuli.",
        "D": "Extinction as new learning, in which the rest interval caused the newly formed inhibitory CS–no US association to decay faster than the original CS–US excitatory association, producing apparent recovery."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect. Conditioned inhibition refers specifically to a stimulus that has been trained to signal the absence of a US and thereby actively inhibits responding — it is a property acquired by a stimulus through specific training procedures (e.g., A+ / AX–). While this concept involves inhibition, it does not explain why a previously excitatory CS recovers responding after a rest period; it describes the development of a new inhibitory signal rather than the temporary suppression of an excitatory one.",
        "B": "This is correct. Pavlov's internal inhibition account holds that extinction does not destroy the original CS-US association but instead overlays it with an active inhibitory process. The return of the conditioned response after a rest period — spontaneous recovery — is the primary behavioral evidence for this account. The pharmacological finding that blocking inhibitory interneurons accelerated recovery further supports the claim that an active suppressive (inhibitory) mechanism, not erasure, underlies extinction.",
        "C": "This is incorrect. Counterconditioning involves pairing the conditioned stimulus with a stimulus that elicits a response incompatible with the original conditioned response — an active training procedure. The scenario involves no new pairings or training during the delay period; the recovery is spontaneous and requires only the passage of time, which is inconsistent with a counterconditioning explanation.",
        "D": "This is incorrect. The 'extinction as new learning' view (associated with contemporary inhibitory learning models) proposes that extinction forms a new CS–no US memory that competes with the original memory. While this view acknowledges that the original memory is preserved, it differs from Pavlov's internal inhibition account in important ways: Pavlov attributed the suppression to an intrinsic inhibitory neural process generated by the CS itself, not to competition between two separately encoded memories. The scientist's framing — active suppression by a competing neural process that degrades over time — aligns more precisely with Pavlov's internal inhibition concept."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-33-vignette-L5",
      "source_question_id": "33",
      "source_summary": "Pavlov proposed that spontaneous recovery of a conditioned response after extinction trials provides evidence that extinction of the conditioned response is due to internal inhibition.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A laboratory animal has been thoroughly trained so that a particular sound reliably triggers a measurable physiological response. The animal is then subjected to an extended series of sessions in which the sound is presented many dozens of times without the event that originally made the sound meaningful, until the physiological response disappears entirely and remains absent for two consecutive sessions. The animal is then removed from the laboratory and housed in its home cage for several weeks with no experimental contact whatsoever. When the sound is presented again upon return, the physiological response reappears at a level comparable to that seen after the original training, despite the complete absence of any re-pairing of the sound with the original meaningful event. A theorist observing this result argues that the weeks of sessions eliminating the response did not undo the original change in the animal but instead added a new layer that temporarily masked it — and that this masking layer, unlike the original change, is inherently unstable.",
      "question": "The theorist's account of these findings most directly reflects which classical conditioning principle?",
      "options": {
        "A": "Disinhibition, in which the extended rest period functioned as a novel contextual event that disrupted an inhibitory process and transiently released the suppressed response.",
        "B": "Latent inhibition, in which pre-exposure to the sound before meaningful pairings reduced the association's initial strength, causing it to appear eliminated but allowing it to re-emerge after sufficient time had passed.",
        "C": "Spontaneous recovery as evidence of internal inhibition, in which the response-eliminating sessions produced a temporary, unstable suppressive process that decayed during the rest period, leaving the original underlying association intact.",
        "D": "Renewal, in which the change in environmental context between the response-eliminating sessions and the test session released the original response by removing the contextual cue that had been maintaining inhibition."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect. Disinhibition requires the introduction of a novel external stimulus during extinction or shortly thereafter to temporarily disrupt the inhibitory process and release the response. In this scenario, the response returns after a pure rest period with no novel stimuli presented — the key feature of spontaneous recovery. Disinhibition is not produced merely by the passage of time.",
        "B": "This is incorrect. Latent inhibition refers to the retardation of conditioning that results from prior non-reinforced exposure to the to-be-conditioned stimulus before training begins. It describes a reduction in conditionability due to pre-exposure, not the return of a previously eliminated response after a rest period. The scenario involves the re-emergence of a well-established response, which is inconsistent with latent inhibition.",
        "C": "This is correct. The theorist's claim maps precisely onto Pavlov's internal inhibition account of extinction and spontaneous recovery. The 'original change' that was not undone is the CS-US association formed during training. The 'new layer' that is inherently unstable is the internal inhibitory process generated during the response-eliminating (extinction) sessions. Its decay during the rest period explains why the original response re-emerges — not because new pairings occurred, but because the masking inhibition dissipated over time.",
        "D": "This is incorrect. Renewal refers to the return of an extinguished conditioned response when the organism is tested in a context different from the one in which extinction occurred — it is fundamentally a context-dependent phenomenon. The scenario does not describe any context change between extinction and test; the animal simply rests in its home cage and is returned to the same laboratory. Renewal requires a specific ABA, AAB, or ABC contextual manipulation, which is absent here."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-217-vignette-L1",
      "source_question_id": "217",
      "source_summary": "John Watson used delay conditioning to establish Little Albert's fear response to white rats.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "classical conditioning",
        "unconditioned stimulus",
        "conditioned fear"
      ],
      "vignette": "A researcher pairs a neutral white rat with a loud noise—an unconditioned stimulus—while an infant watches. After several pairings, the infant displays conditioned fear whenever the white rat is presented alone. The researcher notes that the rat is always shown slightly before and overlapping with the noise, so the infant can anticipate what is coming. This experiment, conducted by John Watson, is a landmark demonstration of classical conditioning in humans.",
      "question": "Which classical conditioning procedure most accurately describes the temporal arrangement used in Watson's experiment to establish Little Albert's fear of the white rat?",
      "options": {
        "A": "Trace conditioning",
        "B": "Simultaneous conditioning",
        "C": "Delay conditioning",
        "D": "Backward conditioning"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Trace conditioning involves presenting the conditioned stimulus (CS) and then removing it before the unconditioned stimulus (US) begins, leaving a 'trace' interval between them. In Watson's experiment the CS (rat) remained present and overlapped with the US (noise), so trace conditioning does not apply.",
        "B": "Simultaneous conditioning occurs when the CS and US begin and end at exactly the same moment with no forward offset. Watson's procedure had the CS onset precede the US onset, making this an incorrect characterization of his temporal arrangement.",
        "C": "In delay conditioning, the CS is presented first and remains present until — and through — the onset of the US. Watson showed the rat to Albert just before and overlapping with the loud noise, which is precisely the delay-conditioning arrangement, making this the correct answer.",
        "D": "Backward conditioning presents the US before the CS, which is the reverse of what Watson did. This arrangement typically produces weak or no conditioning and does not describe the procedure used to condition Little Albert's fear."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-217-vignette-L2",
      "source_question_id": "217",
      "source_summary": "John Watson used delay conditioning to establish Little Albert's fear response to white rats.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "conditioned stimulus",
        "fear response"
      ],
      "vignette": "A 9-month-old infant with no prior aversive history with animals is brought into a laboratory. During each trial, a white rat is introduced and a sudden, sharp clang is produced shortly afterward while the rat is still visible to the child. The infant, who is generally described by observers as placid and easy-going by temperament, quickly begins to cry and withdraw whenever the white rat enters the room, even when the clanging sound is absent. By the seventh trial, the fear response is robust and consistent.",
      "question": "The procedure used to establish the infant's fear of the white rat is best described as which type of classical conditioning arrangement?",
      "options": {
        "A": "Backward conditioning",
        "B": "Delay conditioning",
        "C": "Trace conditioning",
        "D": "Higher-order conditioning"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Backward conditioning presents the aversive stimulus before the neutral stimulus, which is the reverse of this procedure. Because backward conditioning typically fails to produce strong learned fear, it cannot account for the robust fear response described after only seven trials.",
        "B": "Delay conditioning is defined by the CS (white rat) being presented first and remaining present through the onset of the US (loud clang). This matches the trial structure described — rat visible, clang follows while rat is still present — making delay conditioning the correct answer.",
        "C": "Trace conditioning requires the CS to terminate before the US is presented, leaving a gap between them. Here the rat remains visible when the sound occurs, so no trace interval exists and trace conditioning does not apply.",
        "D": "Higher-order conditioning involves using an already-established CS to condition a new neutral stimulus; it does not describe the initial pairing of a neutral stimulus with an unconditioned aversive stimulus, which is what the scenario depicts."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-217-vignette-L3",
      "source_question_id": "217",
      "source_summary": "John Watson used delay conditioning to establish Little Albert's fear response to white rats.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "pairing"
      ],
      "vignette": "In a now-infamous early study, an infant was repeatedly exposed to a furry animal immediately followed by a startling noise; crucially, the animal was still present at the moment the noise occurred on every pairing. Some commentators later argued the child's distress was partly caused by the researcher's own anxious demeanor during testing, suggesting observational learning as an alternative explanation. However, the primary mechanism demonstrated was the direct pairing of the neutral stimulus with the aversive one, and the child soon reacted fearfully to the animal even when presented silently and alone. The same fear generalized to other white, furry objects the child encountered.",
      "question": "Setting aside the alternative explanations raised by later critics, which specific conditioning procedure best accounts for how the infant's fear was originally established through the pairing sequence described?",
      "options": {
        "A": "Trace conditioning",
        "B": "Simultaneous conditioning",
        "C": "Higher-order conditioning",
        "D": "Delay conditioning"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Trace conditioning would require the furry animal to be removed before the startling noise began, creating a stimulus-free gap. The scenario explicitly states the animal was still present at the moment the noise occurred, ruling out trace conditioning.",
        "B": "Simultaneous conditioning requires the CS and US to begin at exactly the same instant with no forward offset from the CS. The vignette describes the furry animal being introduced first and then the noise following — a forward, overlapping arrangement inconsistent with true simultaneity.",
        "C": "Higher-order conditioning uses a previously conditioned CS to condition a new neutral stimulus rather than pairing a novel neutral stimulus directly with an unconditioned aversive event. The procedure described is a first-order pairing of the animal with an inherently startling noise, not higher-order conditioning.",
        "D": "Delay conditioning is defined by the CS onset preceding the US onset while the CS remains present until the US occurs. The description — animal presented first, noise following while the animal is still visible — precisely fits delay conditioning, making this the correct answer."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-217-vignette-L4",
      "source_question_id": "217",
      "source_summary": "John Watson used delay conditioning to establish Little Albert's fear response to white rats.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "anticipatory"
      ],
      "vignette": "Archival accounts of Watson's 1920 experiment reveal that each trial was structured so that the child could develop an anticipatory reaction: the furry object appeared first, and the aversive sound arrived while the object was still in view, never before it and never after it was withdrawn. Later researchers have debated whether the child's responses reflected true associative learning or merely a sensitization effect caused by repeated loud noises, since the child showed some distress to the noise alone before conditioning began. Despite this debate, the specific temporal relationship between the two stimuli — one consistently predicting and overlapping with the other — is what defines the procedure Watson actually used. Importantly, the procedure differed from an arrangement in which a gap of silence separated the two stimuli.",
      "question": "Based on the precise temporal arrangement described in the archival accounts, which classical conditioning procedure did Watson employ to condition the infant's fear response?",
      "options": {
        "A": "Simultaneous conditioning",
        "B": "Trace conditioning",
        "C": "Delay conditioning",
        "D": "Pseudoconditioning"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Simultaneous conditioning is defined by the CS and US beginning at exactly the same moment, offering no forward interval in which the CS predicts the US. The accounts describe the furry object appearing first and the noise arriving subsequently — a forward offset that is inconsistent with simultaneous conditioning.",
        "B": "Trace conditioning is distinguished by a gap between CS offset and US onset, meaning the CS would have been removed before the aversive noise began. The vignette explicitly states the object was still in view when the sound occurred and contrasts the procedure with one involving 'a gap of silence,' ruling out trace conditioning.",
        "C": "Delay conditioning is characterized by CS onset preceding US onset while the CS remains continuously present until the US is delivered. The archival description — object visible first, sound arriving while object is still present, with no intervening gap — precisely matches delay conditioning, making this the correct answer.",
        "D": "Pseudoconditioning refers to an apparent conditioned response that is actually a generalized sensitization to stimuli following repeated exposure to an aversive US, without true CS–US association. While the debate about sensitization is raised in the vignette, Watson's procedure involved systematic CS–US pairing rather than unpaired presentations, so pseudoconditioning does not describe the procedure he used."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-LEA-217-vignette-L5",
      "source_question_id": "217",
      "source_summary": "John Watson used delay conditioning to establish Little Albert's fear response to white rats.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Classical Conditioning",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "An infant who had never previously shown any distress around animals was brought into a room where a soft, white, furry object was placed within reach. Each time the object appeared, a sudden jarring sound was produced a moment later while the object was still fully visible; the object was never taken away before the sound, and the sound never came before the object. By the fifth session, the child would begin crying and pulling away the instant the fuzzy object came into view, before any sound was made. Observers noted that the child's caregiver was present and visibly tense during several sessions, leading some to suggest the child was merely mirroring the adult's emotions. The same withdrawal reaction later emerged with a white cotton ball and even a fur coat, none of which had ever been in the room during the original sessions.",
      "question": "Which procedure most precisely accounts for the mechanism by which the infant's distress to the soft, furry object was originally established, based on the specific timing structure described?",
      "options": {
        "A": "Trace conditioning",
        "B": "Delay conditioning",
        "C": "Observational learning",
        "D": "Simultaneous conditioning"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Trace conditioning requires the first stimulus to be fully removed before the second begins, creating a brief silent interval between them. The scenario specifies that the object was never taken away before the sound occurred, directly contradicting the defining feature of trace conditioning.",
        "B": "Delay conditioning is defined by the first stimulus (the furry object) being presented before — and remaining continuously present through — the onset of the second stimulus (the jarring sound). The precise temporal description — object visible first, sound arriving while object is still present, object never removed before the sound — matches delay conditioning exactly, making this the correct answer despite the red herring of the caregiver's visible tension.",
        "C": "Observational learning involves acquiring behaviors or emotional reactions by watching another individual respond to stimuli, and the caregiver's visible tension is a deliberate red herring pointing toward this option. However, observational learning cannot account for the original acquisition described, because the mechanism specified is a repeated, structured timing relationship between the object and the sound rather than the child watching an adult react.",
        "D": "Simultaneous conditioning requires the two stimuli to begin at precisely the same instant, with no temporal gap in which one predicts the other. The scenario describes a moment of delay — the object appears first and the sound arrives 'a moment later' — ruling out strict simultaneity as the defining feature of the procedure."
      },
      "legacy_domain_code": "LEA",
      "legacy_domain_name": "Learning"
    },
    {
      "id": "JQ-RMS-094-vignette-L1",
      "source_question_id": "094",
      "source_summary": "The analysis of covariance (ANCOVA) is used to statistically remove the effects of an extraneous variable on the dependent variable so that it's easier to detect the effects of the independent variable on the dependent variable, and the extraneous variable is the \"covariate\".",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "ANCOVA",
        "covariate",
        "extraneous variable"
      ],
      "vignette": "A researcher is studying the effect of a new cognitive-behavioral therapy protocol on depression scores across two treatment groups. She suspects that baseline depression severity, an extraneous variable, may be confounding the results. To statistically remove the influence of pre-treatment depression scores before comparing group outcomes, she enters them as a covariate in her analysis. The researcher uses ANCOVA to ensure that any differences in post-treatment scores can be more clearly attributed to the therapy protocol itself.",
      "question": "Which statistical procedure is this researcher using, and what is its primary purpose in this context?",
      "options": {
        "A": "Repeated-measures ANOVA, which controls for individual differences by measuring the same participants across multiple time points rather than adjusting for a specific extraneous variable",
        "B": "Multiple regression, which models the relationship between several predictors and an outcome but does not partition variance in the same way as ANCOVA to isolate the effect of a specific extraneous variable",
        "C": "ANCOVA, which statistically removes the variance attributable to the covariate (baseline depression) so that the independent variable's effect on post-treatment scores can be detected more clearly",
        "D": "MANOVA, which simultaneously tests multiple dependent variables across groups but does not specifically target a single extraneous continuous variable for statistical removal"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. Repeated-measures ANOVA controls for individual variability by measuring the same subjects across conditions, but it does not statistically adjust for a continuous extraneous variable measured prior to treatment in the way ANCOVA does.",
        "B": "Incorrect. Multiple regression can model relationships among predictors and an outcome, and while it can include a control variable, ANCOVA is the specific procedure designed to partition and remove a covariate's influence before testing group differences on the dependent variable.",
        "C": "Correct. ANCOVA is precisely the procedure that enters an extraneous continuous variable—here, baseline depression—as a covariate to remove its variance from the dependent variable, making it easier to detect the independent variable's (therapy type's) effect.",
        "D": "Incorrect. MANOVA is used when there are multiple dependent variables and tests whether group means differ across that set of outcomes. It does not specifically target a single extraneous continuous variable for statistical removal as ANCOVA does."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-094-vignette-L2",
      "source_question_id": "094",
      "source_summary": "The analysis of covariance (ANCOVA) is used to statistically remove the effects of an extraneous variable on the dependent variable so that it's easier to detect the effects of the independent variable on the dependent variable, and the extraneous variable is the \"covariate\".",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "covariate",
        "extraneous"
      ],
      "vignette": "A clinical psychologist is comparing three mindfulness-based interventions for reducing anxiety in a sample of middle-aged adults. Participants were not randomly assigned because the groups had pre-existing differences in trait anxiety. The psychologist is concerned that trait anxiety at baseline will cloud the picture of which intervention is most effective. He decides to measure trait anxiety before the study begins and enters it as an extraneous variable in his statistical model, effectively adjusting each group's post-treatment anxiety scores as though they began at the same starting point. The researcher also notes that participants in two of the groups had higher rates of comorbid insomnia, but he determines this factor is not part of his current statistical adjustment.",
      "question": "Which statistical approach is the psychologist employing to address his primary methodological concern?",
      "options": {
        "A": "Analysis of covariance (ANCOVA), which adjusts post-treatment scores for the pre-existing extraneous variable so that group differences more clearly reflect the intervention's effect rather than pre-existing differences in trait anxiety",
        "B": "One-way ANOVA, which tests for differences among the three group means but does not account for pre-existing group differences on a continuous variable like baseline trait anxiety",
        "C": "Hierarchical multiple regression, which enters predictors in sequential blocks and can control for baseline anxiety, but frames the analysis as predicting a continuous outcome rather than formally comparing adjusted group means",
        "D": "Propensity score matching, which attempts to equate non-randomly assigned groups on measured confounds by removing or weighting cases rather than statistically adjusting outcome scores within the analysis"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The psychologist is using ANCOVA, which treats the pre-existing trait anxiety measure as a covariate—an extraneous variable whose variance is statistically removed from the dependent variable before group comparisons are made, addressing the non-random assignment problem.",
        "B": "Incorrect. One-way ANOVA compares means across three or more groups but has no mechanism for statistically adjusting those means based on a pre-existing continuous variable like baseline trait anxiety; it would leave the confound unaddressed.",
        "C": "Incorrect. Hierarchical regression can control for baseline anxiety in a sequential model and is related conceptually, but it frames the problem as predicting variance in an outcome rather than comparing adjusted group means across conditions, which is ANCOVA's specific function.",
        "D": "Incorrect. Propensity score matching is a design-level technique used to equate non-randomly assigned groups before analysis by selecting or weighting cases, not a within-analysis statistical adjustment like ANCOVA."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-094-vignette-L3",
      "source_question_id": "094",
      "source_summary": "The analysis of covariance (ANCOVA) is used to statistically remove the effects of an extraneous variable on the dependent variable so that it's easier to detect the effects of the independent variable on the dependent variable, and the extraneous variable is the \"covariate\".",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "adjusted"
      ],
      "vignette": "A researcher compares two groups of adolescents—one receiving a school-based social skills program and one receiving no intervention—on post-program peer relationship quality scores. The two groups were drawn from different schools and showed meaningful differences in socioeconomic status (SES) at the outset. Rather than excluding participants or resampling, the researcher decides to include SES as a continuous variable in her statistical model so that the final group comparison reflects adjusted means. The output reports not raw group means but means that have been statistically equated for SES, and the F-test comparing the two groups is evaluated after accounting for this source of variance. The researcher explicitly notes that she is not interested in SES as a predictor in its own right but solely in isolating the program's effect.",
      "question": "Which analytical strategy is the researcher employing, and what distinguishes it from related approaches?",
      "options": {
        "A": "Blocked ANOVA, in which participants are grouped into blocks based on SES levels so that between-block variance is separated from treatment variance, reducing error without statistically adjusting individual scores",
        "B": "Analysis of covariance (ANCOVA), in which SES is entered as a covariate to remove its variance from the dependent variable before comparing adjusted group means, making the group comparison more sensitive to the program's effect",
        "C": "Two-way ANOVA with SES as a second independent variable, which tests main effects and interactions but treats SES as a categorical factor rather than removing its continuous variance from the outcome",
        "D": "Partial correlation, which statistically controls for SES when examining the relationship between program participation and peer relationship quality but does not frame the comparison in terms of group mean differences"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. ANCOVA is precisely the procedure in which a continuous variable (SES) is entered not as a primary factor but as a covariate whose variance is partitioned out, leaving adjusted group means that more purely reflect the treatment effect—matching all details of the described approach.",
        "A": "Incorrect. Blocked ANOVA (randomized block design) assigns participants to blocks based on a nuisance variable to reduce error variance, but this is a design-level strategy that segments participants rather than statistically adjusting individual scores as ANCOVA does.",
        "C": "Incorrect. Two-way ANOVA tests a second factor and its interaction with the primary factor, but SES would need to be categorized (e.g., low/medium/high), and the purpose would be to test SES as an independent variable of interest, not to remove its variance from the outcome.",
        "D": "Incorrect. Partial correlation controls for a third variable when estimating the association between two continuous variables, not when comparing group means. It does not produce adjusted group means or an F-test of group differences as described in the vignette."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-094-vignette-L4",
      "source_question_id": "094",
      "source_summary": "The analysis of covariance (ANCOVA) is used to statistically remove the effects of an extraneous variable on the dependent variable so that it's easier to detect the effects of the independent variable on the dependent variable, and the extraneous variable is the \"covariate\".",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "partition"
      ],
      "vignette": "A neuropsychologist is investigating whether a rehabilitation program reduces cognitive errors in two groups of stroke patients: those who received the program and those in a waitlist condition. He is aware that the groups differed substantially on age at time of stroke, and older age is known to independently reduce post-stroke cognitive performance. Rather than stratifying the sample or redesigning the study, he incorporates age into the statistical model as a continuous variable whose variance in cognitive error scores is first partitioned out before the group comparison is evaluated. The resulting F-statistic testing the group difference is computed on residual variance from which the age-related portion has been removed, and the group means reported in the paper are not the raw observed means. On first review, a colleague suggests the design looks like a straightforward between-groups comparison, but the neuropsychologist clarifies that the analysis goes a step further.",
      "question": "What statistical procedure is the neuropsychologist using, and what is the key feature that differentiates it from the approach his colleague assumed?",
      "options": {
        "A": "Two-way ANOVA treating age as a categorical between-subjects factor, which partitions variance attributable to age strata but requires dichotomizing or categorizing the continuous variable and tests age as an independent variable of interest rather than removing it as a nuisance",
        "B": "Analysis of covariance (ANCOVA), in which age is entered as a continuous covariate whose variance is statistically removed from cognitive error scores before the adjusted group means are compared, producing a more sensitive test of the rehabilitation program's effect",
        "C": "Hierarchical multiple regression in which age is entered in Block 1 and group membership in Block 2, which controls for age but frames the analysis as predicting a continuous outcome and does not produce adjusted group means for a discrete between-groups comparison",
        "D": "Randomized block ANOVA, in which patients are matched into age-based blocks to reduce error variance, which requires a design-level grouping of participants rather than a statistical adjustment of outcome scores within an ongoing between-groups comparison"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Two-way ANOVA could incorporate age as a second factor, but it requires categorizing the continuous age variable and treats age as an independent variable to be tested, not as a nuisance variable whose variance is removed before the primary comparison. This matches some surface features but misrepresents the mechanism.",
        "B": "Correct. ANCOVA is the procedure in which age, a continuous extraneous variable, is entered as a covariate so that its variance is partitioned out before group means are compared. The key distinguishing feature is that the reported means are adjusted (not raw) and the F-test reflects residual variance after the covariate's influence is removed—exactly what the vignette describes.",
        "C": "Incorrect. Hierarchical regression is conceptually closely related and does control for age in the first step, but its framework is predicting a continuous outcome from a set of predictors, not comparing adjusted group means. It does not produce the adjusted cell means characteristic of ANCOVA.",
        "D": "Incorrect. Randomized block ANOVA is a design strategy in which participants are grouped into homogeneous blocks before data collection to reduce error variance. It does not statistically adjust scores on a continuous variable post hoc as ANCOVA does, and it requires prospective blocking rather than incorporating a covariate into an existing analysis."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-094-vignette-L5",
      "source_question_id": "094",
      "source_summary": "The analysis of covariance (ANCOVA) is used to statistically remove the effects of an extraneous variable on the dependent variable so that it's easier to detect the effects of the independent variable on the dependent variable, and the extraneous variable is the \"covariate\".",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher studying memory improvement notices that participants in her two groups—one of which completed a structured practice regimen and one of which did not—differed meaningfully on how many hours per night they slept on average before the study began. She considers simply reporting both groups' final memory test scores, but realizes that sleep differences would likely account for some of the score variation and could mask or inflate any real difference due to the practice regimen. Instead, she mathematically accounts for sleep hours in her model before she computes the group comparison, so that the comparison is made as though both groups had been sleeping the same amount all along. The final numbers she reports for each group's memory performance are not their actual average scores but values that reflect what the scores would have looked like had the sleep difference not existed, and the statistical test she runs is evaluated after sleep's contribution has been extracted from the outcome.",
      "question": "Which statistical procedure best describes the approach the researcher used to isolate the effect of the practice regimen?",
      "options": {
        "A": "Matched-pairs t-test, in which participants from each group are paired on sleep hours before comparison so that sleep-related variance is controlled through the matching design rather than through a within-analysis mathematical adjustment of scores",
        "B": "Two-way factorial ANOVA, in which sleep is treated as a second grouping factor alongside practice regimen, testing main effects and an interaction but requiring sleep to be categorized and treating it as a variable of theoretical interest rather than removing it as a background nuisance",
        "C": "Analysis of covariance (ANCOVA), in which sleep hours are entered as a continuous variable whose contribution to memory scores is mathematically removed before the groups are compared, yielding adjusted group means and a group comparison test based on the remaining variance",
        "D": "Partial eta-squared adjustment, in which the effect size estimate for the practice regimen is corrected for the proportion of variance explained by sleep, producing a more accurate index of the regimen's practical importance without altering the group comparison itself"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. ANCOVA is uniquely identified by the combination of three features in the vignette: (1) a continuous background variable (sleep) is entered into the model, (2) the group comparison is made after that variable's variance is mathematically extracted from the outcome, and (3) the reported group values are adjusted means rather than raw observed means. No other listed procedure has all three features.",
        "A": "Incorrect. Matched-pairs design controls for a nuisance variable by pairing participants before data collection, not by mathematically adjusting scores within the analysis. The vignette specifies that the adjustment happens computationally after the fact, not through a design-level pairing strategy.",
        "B": "Incorrect. Two-way factorial ANOVA could incorporate sleep as a second factor, but it requires sleep to be categorized into discrete levels and treats sleep as a variable whose main effect and interaction with practice are of interest. The vignette describes sleep as a background nuisance to be removed, not a factor to be tested, which is the hallmark of a covariate rather than a second independent variable.",
        "D": "Incorrect. Partial eta-squared is an effect size measure that quantifies how much variance in the outcome is attributable to a specific factor after accounting for other factors; it is computed after the analysis and does not alter the group comparison itself or produce adjusted means. The vignette explicitly describes a procedure that changes the reported group averages and the comparison test, not just an effect size index."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-004-vignette-L1",
      "source_question_id": "004",
      "source_summary": "In a study to evaluate the effects of an anti-drug program on attitudes toward drug use for middle school students from low-income families, the biggest threat to the internal validity is history, as external events that occur during the course of the study may have a systematic effect on subjects' scores or status on the dependent variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "internal validity",
        "history",
        "external events"
      ],
      "vignette": "A researcher designs a pre-post study to evaluate an anti-drug curriculum delivered to seventh-graders from low-income urban neighborhoods over a six-month period. Midway through the study, a well-publicized local overdose tragedy receives extensive media coverage and is discussed in classrooms citywide. At posttest, students show markedly more negative attitudes toward drug use compared to pretest. The researcher is concerned that internal validity may have been compromised. When reviewing threats to internal validity, the research team debates which threat best explains the observed change in attitudes.",
      "question": "Which threat to internal validity most likely accounts for the change in students' attitudes toward drug use in this study?",
      "options": {
        "A": "Maturation, because students naturally develop more mature attitudes about risk over a six-month developmental period.",
        "B": "History, because the widely publicized overdose event is an external event that occurred during the study and could systematically affect students' scores on the dependent variable.",
        "C": "Testing, because repeated administration of an attitude survey can sensitize participants and alter their subsequent responses.",
        "D": "Instrumentation, because changes in how the outcome measure is scored or interpreted across time points can produce spurious pre-post differences."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Maturation refers to biological or psychological changes that occur within participants simply as a function of the passage of time — such as growing older or becoming more cognitively developed — independent of any external event. Here, a specific identifiable external event (the overdose tragedy), not maturational progression, is the plausible driver of attitude change.",
        "B": "History is the threat to internal validity in which an external event — one not part of the experimental treatment — occurs during the course of the study and systematically influences participants' standing on the dependent variable. The highly publicized overdose tragedy is precisely such an event, making history the most fitting threat here.",
        "C": "Testing (or test sensitization) refers to the effect of prior exposure to a measure on later scores — participants may become more aware of what is being measured and adjust responses accordingly. While attitude surveys can produce this effect, there is no indication here that familiarity with the survey instrument, rather than the real-world event, drove the change.",
        "D": "Instrumentation refers to changes in the measurement instrument itself, the scoring procedures, or the observers across time points that create artificial pre-post differences. Nothing in the scenario suggests the measurement tool changed; the threat here is an external world event, not a shift in how the dependent variable was assessed."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-004-vignette-L2",
      "source_question_id": "004",
      "source_summary": "In a study to evaluate the effects of an anti-drug program on attitudes toward drug use for middle school students from low-income families, the biggest threat to the internal validity is history, as external events that occur during the course of the study may have a systematic effect on subjects' scores or status on the dependent variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "internal validity",
        "confound"
      ],
      "vignette": "A university research team implements a school-based substance prevention program for eighth-graders from economically disadvantaged families over one academic year. Students complete an attitude survey at the beginning and end of the year. Three months into the program, the state government launches a high-profile anti-drug media campaign that includes television spots, school posters, and community events specifically targeting youth in low-income areas. Although the students are predominantly from immigrant families who have varying degrees of media exposure, the campaign is pervasive enough that virtually all students encountered it. At year's end, attitudes toward drug use had shifted significantly in the desired direction, raising questions about internal validity.",
      "question": "What is the primary threat to internal validity that the research team should be most concerned about?",
      "options": {
        "A": "Selection bias, because the sample is drawn exclusively from low-income, predominantly immigrant families, making the groups non-equivalent at the outset.",
        "B": "Maturation, because eighth-graders undergo significant developmental change over a full academic year that could independently shift their risk attitudes.",
        "C": "History, because the statewide anti-drug media campaign is an external event occurring concurrently with the study that could systematically influence students' drug attitudes.",
        "D": "Statistical regression, because students selected from a disadvantaged population may have had extreme baseline attitudes that naturally regress toward the mean over time."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Selection bias refers to pre-existing differences between groups that make them non-comparable before the intervention begins, and is most relevant in designs comparing two or more groups. This is a single-group pre-post design, and the immigrant family detail, while potentially interesting, does not introduce group non-equivalence — making selection bias a less central concern here.",
        "B": "Maturation is a plausible concern over a full academic year, as adolescents do change developmentally. However, the scenario identifies a specific, pervasive external event — the statewide media campaign — that provides a more direct and proximate explanation for the attitude change than general developmental progression.",
        "C": "History is the threat in which an identifiable external event, separate from the treatment, occurs during the study and systematically affects participants on the dependent variable. The comprehensive statewide anti-drug media campaign targeting the same population and attitudes measured in the study is a textbook example of a history threat.",
        "D": "Statistical regression (regression to the mean) is a threat when participants are selected because of extreme scores on a measure, predicting their scores will move toward the average on retest. There is no indication that students were selected for extreme drug attitudes at baseline; the concern here is a concurrent external event, not regression from extreme scores."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-004-vignette-L3",
      "source_question_id": "004",
      "source_summary": "In a study to evaluate the effects of an anti-drug program on attitudes toward drug use for middle school students from low-income families, the biggest threat to the internal validity is history, as external events that occur during the course of the study may have a systematic effect on subjects' scores or status on the dependent variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "confound"
      ],
      "vignette": "Researchers conduct a longitudinal pre-post study of a drug prevention program delivered to middle schoolers in a low-income urban district over eight months. The students' parents are asked to complete a brief demographic survey at enrollment. Notably, in month four, a popular social media influencer with a massive following among teenagers publicly discussed a close friend's death from a drug overdose, generating millions of views and widespread peer discussion in schools. The research team notes that the program is delivered consistently and the same facilitators conduct all sessions, so procedural integrity appears sound. At posttest, students' self-reported attitudes toward drug use have shifted substantially more than pilot data had predicted.",
      "question": "Which confound most directly threatens the conclusion that the drug prevention program caused the observed attitude change?",
      "options": {
        "A": "Instrumentation, because even though the same facilitators were used, subtle shifts in how program content was delivered over eight months may have altered the effective dose of the intervention.",
        "B": "Maturation, because middle school students undergo rapid social and cognitive development over eight months that could independently produce more negative attitudes toward drug use.",
        "C": "History, because the viral social media event involving the influencer's public grief over a drug-related death constitutes an external occurrence that could systematically shift students' drug attitudes during the study period.",
        "D": "Attrition, because families who chose to complete the demographic survey at enrollment may differ systematically from those who did not, creating a biased final sample."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Instrumentation concerns changes in the measurement instrument or delivery procedure that introduce error. The vignette explicitly states that procedural integrity was maintained with the same facilitators, making this a red herring designed to sound plausible — it does not account for the unexpectedly large attitude shift or the clearly identified external event.",
        "B": "Maturation is a genuinely plausible alternative explanation over eight months in adolescents, and serves as the primary red herring in this vignette. However, it cannot explain why the shift exceeded pilot data predictions — maturation would be constant across replications. The novel, specific viral social media event provides a more proximate and distinguishing explanation.",
        "C": "History refers to external events occurring during the study that systematically affect participants' scores on the dependent variable. The influencer's viral post about a drug-related death is precisely this kind of event: identifiable, timed during the study, widely encountered by the target population, and directly relevant to drug attitudes — making history the best explanation for the unexpectedly large attitude shift.",
        "D": "Attrition (or mortality) is a threat when differential dropout during a study creates a biased remaining sample. The demographic survey completion at enrollment is a baseline activity, not evidence of differential dropout over time, and the vignette provides no information suggesting meaningful participant loss — making this distractor inapplicable."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-004-vignette-L4",
      "source_question_id": "004",
      "source_summary": "In a study to evaluate the effects of an anti-drug program on attitudes toward drug use for middle school students from low-income families, the biggest threat to the internal validity is history, as external events that occur during the course of the study may have a systematic effect on subjects' scores or status on the dependent variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "systematic"
      ],
      "vignette": "A nonprofit organization partners with a school district to evaluate a new curriculum designed to change attitudes about substance use among seventh-graders in an under-resourced community. The curriculum is delivered by trained teachers over a single semester, and pre- and post-program surveys are administered using identical instruments by the same staff. Participation is high and attrition is minimal throughout. Partway through the semester, the district itself launches a broader wellness initiative — including assemblies, parent communications, and hallway signage — addressing substance use, mental health, and healthy choices in response to a district-wide policy mandate unrelated to the research study. The evaluation team observes that the post-program attitude scores are substantially more favorable than those from a similar cohort studied the prior year under the same curriculum without the wellness initiative.",
      "question": "The discrepancy between this cohort's outcomes and those of the prior year's cohort most directly reflects which threat to the study's ability to draw valid causal conclusions?",
      "options": {
        "A": "Testing effects, because administering the same attitude survey at pre- and post-program may have cued students to the program's goals, resulting in demand characteristics that inflated post-program scores.",
        "B": "Selection by maturation interaction, because students from under-resourced communities may mature at a different developmental rate than normative samples, producing a systematic difference in outcome trajectories between cohorts.",
        "C": "History, because the concurrent district wellness initiative represents a systematic external event occurring during the study period that could independently influence participants' attitudes toward substance use beyond what the curriculum alone produced.",
        "D": "Instrumentation, because even with identical survey instruments and staff, the social context created by the wellness initiative may have altered how students interpreted and responded to survey items, effectively changing the measurement process."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Testing effects are a legitimate concern when repeated exposure to a measure sensitizes participants to its content and artificially elevates scores. However, the prior-year cohort used the same pre-post survey design without the wellness initiative and produced lower scores, which rules out the testing procedure as the explanation for the discrepancy — the key differentiating factor between the two cohorts is the wellness initiative, not the survey.",
        "B": "A selection-by-maturation interaction is relevant in quasi-experimental designs where non-equivalent groups are compared and one group may be at a different point in a developmental trend. While this sounds appealing given the under-resourced community framing (a red herring), both cohorts are from the same community and school context; the observed difference is more parsimoniously explained by the identified external event occurring only in the current cohort's semester.",
        "C": "History is the threat in which an external event occurring during the study period — separate from the intended treatment — systematically influences the dependent variable. The district wellness initiative is precisely this: an identifiable, concurrent, co-occurring event that affected only the current cohort and that directly addresses the same attitudinal outcomes being measured. It provides the most direct explanation for the cohort discrepancy.",
        "D": "Instrumentation concerns changes in the measurement instrument, scoring rubric, or observer behavior that introduce systematic error in assessment. While the wellness initiative context could theoretically affect how students interpret items, the vignette states instruments and staff were identical — and the threat of instrumentation does not explain why this specific cohort, exposed to the wellness initiative, differs from a prior cohort that was not. The external event itself, not the measurement context, is the more precise explanation."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-004-vignette-L5",
      "source_question_id": "004",
      "source_summary": "In a study to evaluate the effects of an anti-drug program on attitudes toward drug use for middle school students from low-income families, the biggest threat to the internal validity is history, as external events that occur during the course of the study may have a systematic effect on subjects' scores or status on the dependent variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team studying behavioral change in young adolescents from financially struggling neighborhoods recruits a group of seventh-graders and collects information about their opinions on a particular risky behavior before and after a structured school-based program lasting about half a school year. The program is delivered consistently by the same people using identical materials throughout. Dropout rates are negligible, and the young people who remained were similar to those who enrolled. Midway through the program, a classmate of many participants was hospitalized following a well-known incident in the community that received considerable local news coverage and became a major topic of conversation among students and parents alike. When the team compares beginning and end scores, they find the shift in opinions is considerably larger than what a comparable group of students showed in a prior run of the identical program in the same type of setting.",
      "question": "The unexpectedly large shift in opinions compared to the prior group is most directly attributable to which methodological concern?",
      "options": {
        "A": "The fact that the same questionnaire was given twice may have made participants more aware of what the program was targeting, leading them to answer in ways that aligned with what they believed was expected of them.",
        "B": "Because the students were drawn from a financially disadvantaged background, those with the most extreme initial opinions may have been overrepresented, and their scores naturally moved closer to the average over the course of the program.",
        "C": "A notable real-world incident that occurred in the participants' immediate community during the program's run — and was widely discussed among them — may have independently influenced their opinions in ways that have nothing to do with the program itself.",
        "D": "Because the same instructors delivered the program over many months, their own engagement and enthusiasm may have subtly declined over time, altering the effective quality of the program experienced by this cohort compared to prior ones."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This option describes testing effects or demand characteristics — the idea that prior exposure to a questionnaire sensitizes participants and biases their later responses. While plausible in a pre-post design, this mechanism would have been equally present in the prior run of the program, which used the same design; it cannot explain why this cohort showed a larger shift than the earlier cohort who faced the same repeated-measurement procedure.",
        "B": "This option describes regression to the mean — the statistical tendency for extreme scores to move toward the group average upon retesting. It could be relevant if participants were selected specifically because of extreme baseline opinions, but the vignette does not indicate extreme-score selection. Moreover, regression to the mean would also have operated in the prior cohort run under comparable conditions, so it cannot account for the difference between the two cohorts.",
        "C": "This option describes the threat known as history — a specific, identifiable external event occurring during the study period that is unrelated to the intervention but could systematically influence participants' opinions on the very outcome being measured. The community incident involving a classmate, widely discussed among participants and their families, is precisely such an event. It was present for this cohort but absent from the prior cohort, making it the most direct explanation for the unexpectedly larger pre-to-post shift.",
        "D": "This option describes a form of instrumentation or treatment fidelity degradation — the idea that the delivery quality changed over time in a way that altered the participants' experience of the program. While instructor fatigue is a real concern in longitudinal research, the vignette states the program was delivered consistently by the same people using identical materials; more importantly, this mechanism would predict a smaller effect than prior cohorts (due to reduced quality), not a larger one."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-015-vignette-L1",
      "source_question_id": "015",
      "source_summary": "A Latin square is a type of counterbalanced design that ensures that the different levels of the independent variable are assigned to the groups of subjects so that each level appears an equal number of times in each ordinal position.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "counterbalanced",
        "ordinal position",
        "Latin square"
      ],
      "vignette": "A researcher is studying the effects of three different therapy modalities (Cognitive-Behavioral, Psychodynamic, and Humanistic) on anxiety reduction. To control for order effects, she uses a counterbalanced design in which each modality appears in each ordinal position exactly once across three groups of participants. She arranges the sequence of therapy exposure so that the assignment matrix forms a Latin square. Each group of participants receives the modalities in a different sequence, but every modality is experienced first, second, and third by exactly one group.",
      "question": "Which research design is best illustrated by this study?",
      "options": {
        "A": "Complete counterbalancing",
        "B": "Latin square design",
        "C": "Randomized block design",
        "D": "Repeated measures ANOVA"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Complete counterbalancing would require all possible orderings of the three conditions (six sequences for three conditions), whereas the Latin square uses a reduced subset in which each condition appears exactly once in each ordinal position. The scenario specifies a structured, not exhaustive, arrangement, ruling this out.",
        "B": "A Latin square design is a form of counterbalancing in which each level of the independent variable appears in each ordinal position exactly once, precisely as described. The three-group, three-condition matrix in which every modality is first, second, and third once matches the defining feature of this design.",
        "C": "A randomized block design groups participants into blocks based on a nuisance variable and randomly assigns treatments within each block. It does not specifically address the ordinal sequencing of conditions to control for order or carryover effects, which is the central concern in this scenario.",
        "D": "Repeated measures ANOVA is a statistical analysis technique used when the same participants are measured across multiple conditions or time points. It is an analytic method, not a design strategy for controlling order effects, so it does not describe what the researcher implemented in constructing the sequence matrix."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-015-vignette-L2",
      "source_question_id": "015",
      "source_summary": "A Latin square is a type of counterbalanced design that ensures that the different levels of the independent variable are assigned to the groups of subjects so that each level appears an equal number of times in each ordinal position.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "counterbalanced",
        "order effects"
      ],
      "vignette": "A developmental psychologist is examining how three different instruction formats—visual, auditory, and combined—affect memory recall in older adults with mild cognitive impairment. Because the same participants will experience all three formats, the researcher is concerned about order effects that could confound the results. She assigns participants to one of three groups and arranges the sequences so that each instruction format appears exactly once in each sequential position across the groups. The researcher specifically chose this approach over presenting all possible orderings because only three groups were needed rather than six.",
      "question": "Which design strategy did the researcher employ to control for order effects?",
      "options": {
        "A": "Complete counterbalancing",
        "B": "Random assignment",
        "C": "Randomized block design",
        "D": "Latin square design"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Complete counterbalancing involves using all possible orderings of conditions. With three conditions, this requires six groups. The scenario explicitly states that only three groups were used instead of six, which is the hallmark advantage of a Latin square over complete counterbalancing—ruling this option out.",
        "B": "Random assignment refers to the process of assigning participants to conditions by chance to equate groups at baseline. While it addresses group equivalence, it does not specifically address the sequential placement of conditions across ordinal positions, which is the central design feature described here.",
        "C": "A randomized block design assigns participants to homogeneous blocks and randomly assigns treatments within those blocks, primarily to reduce variance due to a nuisance variable. It does not systematically sequence conditions so that each appears equally in every ordinal position, which is what the researcher accomplished.",
        "D": "A Latin square design is a counterbalanced approach that uses a reduced set of sequences ensuring each condition appears in each ordinal position exactly once. The scenario's key details—three groups, three conditions, each appearing once per ordinal position, and fewer groups than complete counterbalancing requires—precisely define this design."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-015-vignette-L3",
      "source_question_id": "015",
      "source_summary": "A Latin square is a type of counterbalanced design that ensures that the different levels of the independent variable are assigned to the groups of subjects so that each level appears an equal number of times in each ordinal position.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "confound"
      ],
      "vignette": "A researcher studying the effects of three relaxation techniques on physiological arousal has participants complete all three techniques across separate sessions. To prevent practice, fatigue, or habituation from acting as a confound, she carefully constructs the administration sequences across her three participant groups. The sequences are not randomly generated; rather, they are systematically selected so that no technique occupies the same session slot in more than one group and every technique appears in every session slot exactly once. A colleague reviewing the study notes that this approach does not test every possible ordering and suggests it may be a limitation, but the researcher argues that the approach is sufficient for balancing sequential influences.",
      "question": "Which methodological approach did the researcher use to manage potential confounds related to administration order?",
      "options": {
        "A": "Complete counterbalancing",
        "B": "Latin square design",
        "C": "Matched-pairs design",
        "D": "Within-subjects factorial design"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Complete counterbalancing does test every possible ordering, which is precisely what the colleague was suggesting as an alternative. The scenario explicitly states the researcher did not use all possible orderings, making this the surface-level distractor that the red herring (the colleague's comment) is designed to push students toward. The correct answer requires recognizing that the systematic, partial approach described is a Latin square.",
        "B": "A Latin square design uses a systematically selected subset of all possible sequences so that each condition appears in each ordinal position exactly once. The researcher's approach—three groups, three techniques, each appearing once per session slot, without testing all orderings—is the defining characteristic of this design, including the acknowledged trade-off that not all sequences are tested.",
        "C": "A matched-pairs design pairs participants on a relevant variable and assigns one member of each pair to each condition, reducing variance due to individual differences. It does not address the sequential positioning of conditions across multiple administrations, which is the focus of this vignette.",
        "D": "A within-subjects factorial design involves the same participants being exposed to all levels of two or more factors. While this study does use a within-subjects element, the key feature being described is the systematic sequencing strategy applied to control order, not the factorial structure of the independent variables."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-015-vignette-L4",
      "source_question_id": "015",
      "source_summary": "A Latin square is a type of counterbalanced design that ensures that the different levels of the independent variable are assigned to the groups of subjects so that each level appears an equal number of times in each ordinal position.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "ordinal"
      ],
      "vignette": "A neuropsychologist administering a battery of three cognitive assessments to the same patients is concerned that patients who take Test A before Test B may perform differently than those who take Test B first, due to fatigue or practice. She decides not to randomly vary the test order, as she wants the variation to be systematic enough to ensure interpretability. She divides her patient sample into exactly three groups and assigns a unique sequence to each group such that no two groups receive the same test in the same ordinal position. A statistician reviewing the protocol points out that this approach sacrifices coverage of some possible sequences in favor of parsimony, and recommends checking whether differential carryover effects could still pose a problem given this limitation.",
      "question": "Which design feature is most precisely described by the neuropsychologist's approach to controlling for the influence of test administration order?",
      "options": {
        "A": "Matched-subjects design to equate groups on baseline cognitive functioning",
        "B": "Randomized block design that assigns patients to blocks based on cognitive severity",
        "C": "Latin square design that ensures each assessment appears once in each ordinal position",
        "D": "Complete counterbalancing that exhaustively tests all possible test sequences"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A matched-subjects design pairs or groups participants based on a measured characteristic to reduce between-subjects variability. While cognitive baseline could be a relevant matching variable here, this approach does not address or control for the ordinal sequencing of conditions—the explicit focus of the neuropsychologist's strategy.",
        "B": "A randomized block design organizes participants into homogeneous groups and randomly assigns conditions within blocks to reduce error variance. While it could account for cognitive severity differences, it does not systematically place each condition in each ordinal position exactly once, which is the distinguishing feature described in the scenario.",
        "C": "The Latin square design is characterized by assigning conditions so that each appears exactly once in each ordinal position across a set number of groups, using fewer sequences than complete counterbalancing. The scenario's details—three groups, three assessments, unique ordinal placement per group, and the statistician's note about incomplete sequence coverage—together identify this design precisely.",
        "D": "Complete counterbalancing would use all possible orderings (six sequences for three conditions), which the scenario explicitly contrasts against by noting the design sacrifices coverage for parsimony and that some sequences are not tested. This is the primary red herring in this scenario, as students may conflate systematic sequencing with exhaustive counterbalancing."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-015-vignette-L5",
      "source_question_id": "015",
      "source_summary": "A Latin square is a type of counterbalanced design that ensures that the different levels of the independent variable are assigned to the groups of subjects so that each level appears an equal number of times in each ordinal position.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team wants to expose each of their participants to all three versions of an experimental task, but they are worried that doing them in the same order each time might change how well people do simply because of how many times they've already completed something similar. They divide participants into exactly three groups and carefully write out which version each group does first, second, and third, making sure that across the three groups, each version shows up once in the first slot, once in the second slot, and once in the third slot. A methodologist reviewing the plan notes approvingly that while this approach does not cover every possible way the three versions could be arranged, it does a reasonable job of spreading the sequential influence evenly. One graduate student argues the team should instead let a computer randomly decide the order for each participant independently, while another insists they should write out all six possible arrangements and assign participants to each.",
      "question": "Which research design principle best describes the team's approach to managing the influence of task order on performance?",
      "options": {
        "A": "Complete counterbalancing, which uses all possible orderings to fully eliminate sequence bias",
        "B": "Simple randomization of condition order, which distributes order effects by chance across participants",
        "C": "Latin square design, which uses a reduced set of sequences ensuring each condition appears once per ordinal position",
        "D": "Randomized block design, which groups participants by a background variable and assigns conditions within blocks"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Complete counterbalancing is designed to control sequence effects by using every possible ordering of conditions. The first graduate student's suggestion—using a computer to randomly assign orders—does not describe complete counterbalancing, and the second student's suggestion of all six arrangements does. However, the team's actual approach is explicitly contrasted with that six-arrangement strategy, making complete counterbalancing incorrect here despite being thematically related.",
        "B": "Simple randomization of condition order distributes order effects probabilistically rather than systematically. This is what the first graduate student advocated for, not what the team chose. The team's method was deliberate and structured, ensuring a specific pattern of ordinal placement, which distinguishes it from random assignment of sequence.",
        "C": "The Latin square design is defined by constructing a set of sequences—fewer than all possible orderings—such that each condition appears exactly once in each sequential position across groups. The team's method of three groups, three versions, each appearing once in every slot, along with the methodologist's observation that not all orderings are covered, precisely and uniquely identifies this design.",
        "D": "A randomized block design clusters participants into groups based on a pre-existing characteristic and randomly allocates conditions within each cluster to reduce error variance. The scenario contains no reference to grouping participants by any background variable; the groupings exist solely to vary the sequence of task versions, not to account for individual differences on a nuisance variable."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-081-vignette-L1",
      "source_question_id": "081",
      "source_summary": "When conducting a one-way ANOVA, the mean square between (MSB) provides an estimate of variability in dependent variable scores due to treatment effects plus error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "ANOVA",
        "mean square between",
        "treatment effects"
      ],
      "vignette": "A researcher conducts a one-way ANOVA to compare anxiety scores across three different therapy conditions: cognitive-behavioral therapy, mindfulness-based therapy, and a waitlist control. After partitioning the total variance, she computes the mean square between groups (MSB). She wants to understand precisely what sources of variability this statistic captures. Her advisor reminds her that the MSB reflects both systematic and unsystematic sources of variability in the dependent variable.",
      "question": "What does the mean square between groups (MSB) in a one-way ANOVA estimate?",
      "options": {
        "A": "Variability in scores due solely to random sampling error, independent of group assignment",
        "B": "Variability in scores due to treatment effects plus random error",
        "C": "Variability in scores due to individual differences within each treatment group",
        "D": "Variability in scores due to the interaction between two or more independent variables"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This describes what the mean square within groups (MSW) estimates — the unsystematic, random error component only. MSB, by contrast, includes both treatment effects and error, which is why it can exceed MSW when treatments differ.",
        "B": "Correct. In a one-way ANOVA, MSB is calculated from deviations of group means from the grand mean. When the null hypothesis is false, group means differ due to treatment effects, so MSB reflects both those systematic treatment effects and random error, making it a larger estimate than MSW under real effects.",
        "C": "This describes the mean square within groups (MSW), which is computed from variability of individual scores around their own group mean. MSW captures only random, within-group error and is unaffected by treatment effects.",
        "D": "Interaction effects are estimated in factorial (two-way or higher) ANOVA designs that include two or more independent variables. A one-way ANOVA has only one independent variable and therefore cannot produce an interaction term."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-081-vignette-L2",
      "source_question_id": "081",
      "source_summary": "When conducting a one-way ANOVA, the mean square between (MSB) provides an estimate of variability in dependent variable scores due to treatment effects plus error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "ANOVA",
        "F-ratio"
      ],
      "vignette": "A clinical researcher randomly assigns 60 adults with insomnia to one of three sleep intervention programs and measures sleep efficiency at post-treatment. She is 58 years old and notes that her participants vary considerably in baseline health status, which she did not control for. When she examines her ANOVA summary table, she focuses on the numerator of the F-ratio and wonders what it actually measures about the data.",
      "question": "In this one-way ANOVA, what does the numerator of the F-ratio — the mean square between groups — represent?",
      "options": {
        "A": "An estimate of variability attributable to treatment effects and random error combined",
        "B": "An estimate of variability attributable exclusively to individual differences among participants",
        "C": "An estimate of variability attributable only to systematic differences caused by the interventions",
        "D": "An estimate of variability attributable to measurement unreliability and unsystematic error alone"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The MSB (numerator of F) estimates variability among group means, which is influenced by two sources: any true treatment effect that systematically shifts group means, and random sampling error that causes group means to differ even without real effects. The researcher's concern about uncontrolled baseline health is a distraction — it does not change what MSB conceptually estimates.",
        "B": "Individual differences among participants are captured within groups, not between them. Variability attributable purely to individual differences contributes to the MSW (denominator), not MSB.",
        "C": "MSB estimates treatment effects plus error, not treatment effects alone. If MSB captured only systematic treatment variance, the F-ratio would be a perfect signal with no noise, which is not the case — random error always contaminates between-group estimates.",
        "D": "Measurement unreliability and unsystematic error contribute primarily to within-group variability (MSW). MSB is not a pure error estimate; it is inflated above MSW when treatment effects are present, which is the basis for the F-ratio test."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-081-vignette-L3",
      "source_question_id": "081",
      "source_summary": "When conducting a one-way ANOVA, the mean square between (MSB) provides an estimate of variability in dependent variable scores due to treatment effects plus error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A psychologist divides 90 participants into four groups receiving different doses of a cognitive enhancer and measures working memory scores. The summary table shows a notably large difference between the group means compared to the spread of scores within each group. A colleague suggests that the large between-group variance must purely reflect the drug's efficacy, while another colleague argues it is contaminated by random fluctuation. The psychologist must decide which colleague — if either — correctly describes what the between-group variance component estimates.",
      "question": "What does the between-group variance component (mean square between) actually estimate in this one-way analysis?",
      "options": {
        "A": "Exclusively the systematic effect of the drug manipulation on working memory scores",
        "B": "Exclusively the random sampling error unrelated to the dose manipulation",
        "C": "The sum of all individual-difference variability pooled across all four groups",
        "D": "A combination of treatment effects from dose differences and random error"
      },
      "correct_answer": "D",
      "option_explanations": {
        "D": "Correct. The first colleague is wrong and so is the second — MSB estimates both treatment effects and random error simultaneously. Even if the drug has no effect, MSB would still differ from zero due to random sampling variability. When the drug does have an effect, MSB is inflated above MSW, but random error remains a component. This is exactly why F = MSB/MSW: dividing by the pure-error baseline tests whether the between-group variability exceeds what error alone would predict.",
        "A": "This is the red herring supported by the first colleague's statement. While MSB is sensitive to treatment effects, it is not a pure estimate of them. Random error always contributes to MSB, which is why MSB and MSW both estimate the same quantity (error only) when the null hypothesis is true.",
        "B": "This describes what MSW estimates — pure random error unrelated to treatment. MSB under the null hypothesis equals error, but under the alternative it exceeds error because treatment effects push group means apart, inflating the between-group estimate.",
        "C": "Pooled individual-difference variability across groups describes MSW, which is computed by averaging within-group variances. MSB is derived from the deviation of each group mean from the grand mean, not from individual-level deviations within groups."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-081-vignette-L4",
      "source_question_id": "081",
      "source_summary": "When conducting a one-way ANOVA, the mean square between (MSB) provides an estimate of variability in dependent variable scores due to treatment effects plus error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "partition"
      ],
      "vignette": "A researcher studying treatment outcomes for depression randomly assigns 120 participants to one of four psychotherapy modalities and collects a continuous post-treatment symptom score. After analysis, she presents a summary table in which the total sum of squares has been partition ed into two components. She notes that one of those components, when divided by its degrees of freedom, yields a value that is systematically larger than the other component's ratio only when the manipulation has produced genuine differences in outcomes. A graduate student reviewing the table incorrectly concludes that the larger ratio is a clean index of how much the therapies differ.",
      "question": "Why is the graduate student's conclusion that the between-groups mean square is a 'clean index of therapy differences' incorrect?",
      "options": {
        "A": "Because the between-groups mean square also absorbs random error, making it an impure estimate of treatment effects alone",
        "B": "Because the between-groups mean square captures individual differences within each therapy group rather than differences between groups",
        "C": "Because the between-groups mean square is always equal to the within-groups mean square regardless of treatment effects",
        "D": "Because only the within-groups mean square can detect treatment effects when sample sizes are unequal"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The graduate student's error is treating MSB as if it isolates treatment variance. MSB estimates treatment effects plus random error — even under the null hypothesis (no real therapy differences) MSB is nonzero due to sampling error. The F-ratio is necessary precisely because comparing MSB to MSW controls for the random error component that contaminates MSB, revealing whether the between-group variance exceeds what error alone produces.",
        "B": "Individual differences within groups are captured by the within-groups component (MSW), not by MSB. MSB is based on how much group means deviate from the grand mean, a between-group metric, not a within-group metric. This option reverses the logic of variance partitioning.",
        "C": "MSB equals MSW in expectation only when the null hypothesis is true (no treatment effect). When therapies genuinely differ, MSB exceeds MSW in expectation, which is the entire inferential basis of the F-test. Claiming they are always equal would make ANOVA theoretically impossible.",
        "D": "The within-groups mean square (MSW) estimates random error and does not directly detect treatment effects; it serves as the denominator baseline. Unequal sample sizes require adjustments (e.g., Type III sums of squares) but do not change the fundamental logic of what each mean square estimates."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-081-vignette-L5",
      "source_question_id": "081",
      "source_summary": "When conducting a one-way ANOVA, the mean square between (MSB) provides an estimate of variability in dependent variable scores due to treatment effects plus error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators divides study participants into several groups, each receiving a different version of an experimental procedure, and records a single continuous outcome measure for everyone. When they compute a number summarizing how spread out the group averages are from the overall average — weighting each group's contribution by its size — they find this number is noticeably larger than a second number summarizing how spread out individual scores are from their respective group averages. A junior team member argues that because the first number is larger, it directly tells them how powerful the experimental procedure is. A senior team member disagrees, pointing out that the first number is always inflated by something beyond the procedure's influence.",
      "question": "What does the senior team member recognize that the junior member is missing about what the first number actually reflects?",
      "options": {
        "A": "The first number reflects treatment effects combined with random sampling variability, not treatment effects alone",
        "B": "The first number reflects only the random variability among individual participants and not any group-level differences",
        "C": "The first number reflects the degree of measurement error in the outcome instrument and not group separation",
        "D": "The first number reflects the interaction between different participant subgroups and the experimental procedure"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The 'first number' is MSB — the mean square between groups — and the senior member recognizes that it is inflated by random sampling error in addition to any genuine treatment effects. Even with no real procedural differences, the group averages would diverge somewhat by chance, and this chance divergence is embedded in MSB. The junior member's mistake is treating MSB as a pure signal when it is a signal-plus-noise estimate.",
        "B": "Random variability among individual participants defines the second number (MSW), not the first. MSW is calculated from how much individuals deviate from their own group's average, capturing within-group noise. The first number is derived from between-group deviations and cannot be purely individual-level variability.",
        "C": "Measurement error in the outcome instrument contributes to within-group variance (the second number), because it makes individual scores less consistent around their group mean. Measurement error does not selectively inflate between-group averages unless it is systematically biased by group membership, which is not indicated in this scenario.",
        "D": "Interaction effects require at least two independent variables and describe how the effect of one variable differs across levels of another. This study has a single experimental procedure with multiple levels (one independent variable), so no interaction term exists. The between-groups component in this design cannot reflect an interaction."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-070-vignette-L1",
      "source_question_id": "070",
      "source_summary": "To analyze the data obtained in a study evaluating the effects of a two-hour online lecture on statistics for improving the statistics knowledge of 35 psychologists, the researcher will use the t-test for correlated samples.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "repeated measures",
        "pretest-posttest",
        "correlated samples"
      ],
      "vignette": "A researcher wants to evaluate whether a two-hour online statistics lecture improves knowledge among a group of 35 practicing psychologists. She administers a statistics knowledge test before the lecture and again immediately afterward using a pretest-posttest repeated measures design. The same 35 participants provide both scores, meaning the two sets of data are not independent. The researcher plans to compare the mean pretest score to the mean posttest score to determine if a statistically significant change occurred across correlated samples.",
      "question": "Which statistical test is most appropriate for analyzing the data in this study?",
      "options": {
        "A": "Independent samples t-test",
        "B": "One-way ANOVA",
        "C": "t-test for correlated samples",
        "D": "Pearson correlation coefficient"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The independent samples t-test is used when two groups are composed of different, unrelated participants. Here, the same 35 psychologists provide both scores, so the samples are dependent, not independent — making this test inappropriate.",
        "B": "A one-way ANOVA compares means across three or more independent groups. This study has only one group measured twice, so ANOVA does not apply and would not account for the within-subject pairing of scores.",
        "C": "Correct. The t-test for correlated samples (also called the paired-samples t-test or dependent t-test) is designed for exactly this situation: one group measured on two occasions, producing paired data. It controls for individual differences by analyzing the difference scores between pretest and posttest.",
        "D": "The Pearson correlation coefficient assesses the strength and direction of the linear relationship between two continuous variables. It does not test for mean differences, so it cannot answer whether the lecture improved knowledge scores."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-070-vignette-L2",
      "source_question_id": "070",
      "source_summary": "To analyze the data obtained in a study evaluating the effects of a two-hour online lecture on statistics for improving the statistics knowledge of 35 psychologists, the researcher will use the t-test for correlated samples.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "paired",
        "pretest-posttest"
      ],
      "vignette": "A training coordinator at a professional psychology organization recruits 35 licensed psychologists to participate in a continuing education study. Many of the participants report feeling anxious about statistics before the training begins, a detail noted in the demographic survey. Each participant completes a statistics knowledge assessment before and after a two-hour online lecture, yielding paired scores for the same individuals. The coordinator wants to determine whether mean scores changed significantly from before to after the intervention.",
      "question": "Which statistical test should the coordinator use to analyze the pre- and post-lecture knowledge scores?",
      "options": {
        "A": "t-test for correlated samples",
        "B": "Mann-Whitney U test",
        "C": "Independent samples t-test",
        "D": "One-sample t-test"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Because the same 35 psychologists contribute both a pretest and a posttest score, the data are paired and dependent. The t-test for correlated samples is specifically designed to compare two means derived from related observations, making it the appropriate choice here.",
        "B": "The Mann-Whitney U test is the nonparametric equivalent of the independent samples t-test, used when two separate groups are compared and distributional assumptions are not met. There is no indication here that normality is violated, and more importantly the data are paired — not from two independent groups.",
        "C": "The independent samples t-test requires that the two sets of scores come from unrelated groups of participants. Since the same individuals are measured twice, the scores are dependent, violating the independence assumption of this test.",
        "D": "The one-sample t-test compares a single sample mean against a known or hypothesized population value. This study involves two sets of scores from the same group, not a comparison to a fixed standard, so the one-sample t-test is not applicable."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-070-vignette-L3",
      "source_question_id": "070",
      "source_summary": "To analyze the data obtained in a study evaluating the effects of a two-hour online lecture on statistics for improving the statistics knowledge of 35 psychologists, the researcher will use the t-test for correlated samples.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "within-subject"
      ],
      "vignette": "A researcher investigates whether exposure to a structured two-hour online module improves statistics proficiency among 35 licensed psychologists. Notably, the sample is entirely composed of female practitioners between the ages of 40 and 55, which the researcher worries might limit generalizability — though no comparison group of males is included. Each participant's knowledge is assessed at two time points using the same validated instrument, creating a within-subject data structure. The researcher also considered whether she needed more than one group to run her analysis, but ultimately concluded that the single-group, two-measurement design was sufficient for her chosen test.",
      "question": "Which statistical test is most appropriate for this study's design?",
      "options": {
        "A": "Independent samples t-test",
        "B": "t-test for correlated samples",
        "C": "Two-way ANOVA",
        "D": "Wilcoxon signed-rank test"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The independent samples t-test compares means from two separate, unrelated groups. The red herring here is the mention of gender homogeneity — it might suggest a between-groups comparison, but no male group exists. The same participants are measured twice, making this test inappropriate.",
        "B": "Correct. The within-subject design with a single group measured at two time points on a continuous outcome calls for the t-test for correlated samples. The homogeneous demographics are irrelevant to the choice of test; the key features are the paired data structure and a comparison of two means from the same participants.",
        "C": "A two-way ANOVA is used when there are two independent variables and the researcher is examining main effects and interactions across multiple groups. There is only one group and one manipulated factor (time) here, so a two-way ANOVA is not warranted.",
        "D": "The Wilcoxon signed-rank test is the nonparametric alternative to the t-test for correlated samples, used when the assumption of normality is seriously violated or the data are ordinal. With 35 participants and no stated distributional problems, the parametric t-test for correlated samples is the more appropriate and statistically powerful choice."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-070-vignette-L4",
      "source_question_id": "070",
      "source_summary": "To analyze the data obtained in a study evaluating the effects of a two-hour online lecture on statistics for improving the statistics knowledge of 35 psychologists, the researcher will use the t-test for correlated samples.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "difference scores"
      ],
      "vignette": "A psychologist-researcher designs a study in which 35 practicing clinicians complete an online statistics module. To evaluate its impact, she records each participant's score on a validated knowledge measure before and after the module and then computes difference scores for each individual. She initially considers whether a between-subjects approach would be more rigorous, but notes that using each participant as their own baseline actually reduces the error variance attributable to individual differences in baseline knowledge. Despite the relatively small n, she is confident her chosen test is appropriate because the distribution of difference scores appears approximately normal.",
      "question": "Given the study's analytic strategy and the researcher's rationale for test selection, which statistical test is being described?",
      "options": {
        "A": "One-sample t-test",
        "B": "Independent samples t-test",
        "C": "One-way repeated measures ANOVA",
        "D": "t-test for correlated samples"
      },
      "correct_answer": "D",
      "option_explanations": {
        "D": "Correct. The t-test for correlated samples operates precisely by computing a difference score for each participant and then testing whether the mean of those difference scores is significantly different from zero. The researcher's explicit mention of difference scores, within-subject error reduction, and normality of difference scores all point to this test. This is its defining computational and conceptual logic.",
        "A": "The one-sample t-test also involves difference scores if the population mean is set to zero, which makes it a tempting distractor. However, the one-sample t-test compares a single sample mean to a known constant — it does not involve pairing pre- and post-scores from the same individuals across two measurement occasions.",
        "B": "The independent samples t-test is ruled out by the study's explicit use of within-subject measurement and the researcher's rationale about reducing individual-difference error variance. That rationale is the conceptual advantage of paired designs over between-subjects designs, not the logic of the independent samples t-test.",
        "C": "One-way repeated measures ANOVA is also used when the same participants are measured multiple times, making it a highly plausible distractor. However, repeated measures ANOVA is designed for three or more time points. With only two measurement occasions, the t-test for correlated samples is the appropriate and equivalent (and simpler) choice, as F = t² in the two-condition case."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-070-vignette-L5",
      "source_question_id": "070",
      "source_summary": "To analyze the data obtained in a study evaluating the effects of a two-hour online lecture on statistics for improving the statistics knowledge of 35 psychologists, the researcher will use the t-test for correlated samples.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A group of thirty-five licensed professionals in a helping field each complete an evaluation before and after attending a focused two-hour online session on a technical topic relevant to their work. The facilitator is particularly struck by how much the group varies in prior familiarity with the material, which she worries could obscure any real changes. By using each person's own starting point as the comparison, she reasons, that variability essentially cancels out of the equation. She notes that what she ultimately analyzes is a single column of numbers — one per person — and that she is asking whether the average of those numbers differs meaningfully from zero.",
      "question": "Based on the described analytic approach and the facilitator's reasoning, which statistical procedure is being used?",
      "options": {
        "A": "One-sample t-test",
        "B": "t-test for correlated samples",
        "C": "Independent samples t-test",
        "D": "Wilcoxon signed-rank test"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Although the facilitator describes analyzing 'a single column of numbers' — the difference scores — this is precisely how the t-test for correlated samples works. The test pairs each person's before and after score, computes a difference, and tests whether the mean difference equals zero. The reasoning about within-person variability canceling out is the conceptual rationale for choosing a paired over an independent design.",
        "A": "The one-sample t-test is the most seductive distractor because the facilitator describes analyzing one set of numbers (difference scores) and testing whether their mean equals zero — which superficially sounds like a one-sample test against a constant of zero. The critical distinction is that those numbers were derived by pairing two within-person observations, not collected as a single independent sample. The one-sample t-test applies when there is a single group measured once and compared to an external benchmark.",
        "C": "The independent samples t-test requires two separate, unrelated groups. The vignette describes a single group of professionals each measured before and after the session. The facilitator's rationale about using each person's own starting point as a baseline explicitly signals a within-person (dependent) design, which rules out the independent samples t-test.",
        "D": "The Wilcoxon signed-rank test is the nonparametric analog to the t-test for correlated samples and would apply to the same paired design when distributional assumptions are seriously violated. Nothing in the vignette indicates non-normality or an ordinal scale, and with 35 participants the parametric test is generally appropriate. Because the Wilcoxon test and the paired t-test share the same structural logic, it is a strong distractor — but the absence of any indication of assumption violations favors the parametric approach."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-105-vignette-L1",
      "source_question_id": "105",
      "source_summary": "Squaring a correlation coefficient produces a coefficient of determination, which indicates the amount of variability in one variable that is accounted for by variability in another variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "correlation",
        "coefficient of determination",
        "variability"
      ],
      "vignette": "A researcher calculates a Pearson correlation of r = .70 between hours of weekly exercise and self-reported mood scores in a sample of 120 adults. She wants to go beyond the correlation itself and determine how much of the variability in mood scores is statistically accounted for by variability in exercise hours. Her supervisor recommends squaring the correlation coefficient to obtain the coefficient of determination. The resulting value is interpreted as the proportion of shared variance between the two measures.",
      "question": "What value does the researcher obtain when she squares the correlation coefficient, and what does it represent?",
      "options": {
        "A": "r² = .49; the coefficient of determination, indicating that 49% of the variability in mood is accounted for by exercise hours",
        "B": "r² = .49; the effect size index d, indicating a medium practical difference between the two groups",
        "C": "r² = .70; the coefficient of determination, indicating that 70% of the variability in mood is accounted for by exercise hours",
        "D": "r² = .49; the reliability coefficient, indicating that 49% of variance in mood scores is due to true score variance"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Squaring r = .70 yields r² = .49, which is the coefficient of determination. This value indicates that 49% of the variance in mood scores is accounted for by variance in exercise hours, capturing the proportion of shared variability between the two variables.",
        "B": "Incorrect. Cohen's d is an effect size measure for mean differences between groups, not a product of squaring a correlation. While r² = .49 is numerically correct, labeling it as Cohen's d misidentifies both the statistic and its interpretation.",
        "C": "Incorrect. The coefficient of determination is obtained by squaring r, not by using r itself. Reporting r = .70 as the coefficient of determination confuses the correlation coefficient with its square and overstates the shared variance.",
        "D": "Incorrect. A reliability coefficient (e.g., Cronbach's alpha or a test-retest coefficient) reflects consistency of measurement, not the proportion of variance one variable shares with another. While r² = .49 is numerically correct, interpreting it as reliability conflates two distinct statistical concepts."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-105-vignette-L2",
      "source_question_id": "105",
      "source_summary": "Squaring a correlation coefficient produces a coefficient of determination, which indicates the amount of variability in one variable that is accounted for by variability in another variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "regression",
        "variance"
      ],
      "vignette": "In a study examining predictors of academic performance in first-year medical students, a statistician runs a simple linear regression using undergraduate GPA as the sole predictor of first-year medical school GPA. The analysis yields a Pearson r of .55 between the two variables. One reviewer notes that the student sample skewed older than typical, raising concerns about range restriction, but the statistician decides the result is still meaningful. The statistician wants to communicate, in percentage terms, how much of the variance in medical school GPA is explained by undergraduate GPA.",
      "question": "What statistic should the statistician report to convey the proportion of variance in medical school GPA explained by undergraduate GPA?",
      "options": {
        "A": "The unstandardized regression coefficient (b), which directly expresses how many GPA points are gained per unit increase in undergraduate GPA",
        "B": "The standardized regression coefficient (β), which expresses the predictor's weight in standard deviation units and is equivalent to the proportion of explained variance",
        "C": "r² = .30, the coefficient of determination, which indicates that approximately 30% of the variance in medical school GPA is accounted for by undergraduate GPA",
        "D": "The standard error of the estimate, which quantifies how much variance in medical school GPA is unexplained by the regression model"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. Squaring r = .55 yields r² ≈ .30, the coefficient of determination. This statistic directly expresses that approximately 30% of the variance in medical school GPA is accounted for by undergraduate GPA, which is the proportional variance interpretation requested.",
        "A": "Incorrect. The unstandardized regression coefficient (b) indicates the predicted change in the outcome per one-unit change in the predictor, but it does not express proportion of variance explained. It is sensitive to the scales of measurement and cannot be interpreted as a percentage of variance.",
        "B": "Incorrect. The standardized regression coefficient (β) indicates a predictor's relative weight in standard deviation units, and in simple regression it equals r numerically, not r². It does not represent the proportion of variance explained and cannot be interpreted as a percentage of shared variance.",
        "D": "Incorrect. The standard error of the estimate quantifies the average prediction error around the regression line and reflects unexplained variance, but it is expressed in the units of the outcome variable, not as a proportion. It does not directly communicate the percentage of variance accounted for by the predictor."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-105-vignette-L3",
      "source_question_id": "105",
      "source_summary": "Squaring a correlation coefficient produces a coefficient of determination, which indicates the amount of variability in one variable that is accounted for by variability in another variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "r²"
      ],
      "vignette": "A clinical psychologist presents findings at a conference showing that scores on a new anxiety screening tool are strongly associated with subsequent hospitalizations in a sample of outpatients (r = .60, p < .001). An audience member who is skeptical of the tool's practical value points out that the screening tool and hospitalization rates are both influenced by patients' socioeconomic status, suggesting the relationship may be inflated. Despite this concern, the presenter emphasizes the r² value as the most important interpretive metric for her audience. A colleague in the audience, however, argues that the r² is being overstated because the presenter fails to account for shrinkage.",
      "question": "When the presenter emphasizes r² as the most important interpretive metric, what specific information is she conveying to her audience?",
      "options": {
        "A": "That the statistical relationship between the screening tool and hospitalizations is not due to chance, as indicated by the significance level of the r² value",
        "B": "That 36% of the variability in hospitalization rates is accounted for by variability in screening tool scores, regardless of whether causation is implied",
        "C": "That the screening tool is causally responsible for 36% of hospitalizations, after accounting for the confounding effect of socioeconomic status",
        "D": "That the screening tool and hospitalization rates share 60% of their variance, indicating that 60% of the variability in one is explained by the other"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. r² = .60² = .36, indicating that 36% of the variability in hospitalization rates is statistically accounted for by variability in screening tool scores. This interpretation describes shared variance and makes no causal claim — the third-variable concern about socioeconomic status does not change what r² itself communicates about the proportion of accounted-for variance.",
        "A": "Incorrect. Statistical significance (p < .001) speaks to whether the relationship is likely due to chance, but r² itself is an effect size measure expressing proportion of variance, not a significance test. Conflating r² with the p-value misrepresents what this statistic conveys.",
        "C": "Incorrect. r² does not imply causation, and it does not control for third variables such as socioeconomic status. The audience member's concern about a confound is legitimate, but even if the r² is inflated, it still represents accounted-for variance rather than causal attribution — and the confound critique affects the validity of the result, not its mathematical definition.",
        "D": "Incorrect. r = .60 is the correlation coefficient; the proportion of shared variance is r² = .36, not .60. Interpreting r directly as a percentage of shared variance is a common and consequential error because it substantially overstates the degree of overlap between the two variables."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-105-vignette-L4",
      "source_question_id": "105",
      "source_summary": "Squaring a correlation coefficient produces a coefficient of determination, which indicates the amount of variability in one variable that is accounted for by variability in another variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "accounted for"
      ],
      "vignette": "A health psychologist publishes a study reporting that a composite measure of chronic stress is a statistically significant predictor of systolic blood pressure in a community sample (N = 250, p = .002). The reported association between the two measures is described as moderate. When a journalist writes about the study, she states that 'stress explains nearly half of the reason people develop high blood pressure,' a claim the researcher immediately disputes. The researcher clarifies that the metric she actually reported — which requires a specific arithmetic transformation of the reported association — shows that only about 20% of the individual differences in blood pressure are accounted for by individual differences in chronic stress.",
      "question": "Which statistical concept best explains the researcher's clarification and the journalist's error?",
      "options": {
        "A": "Regression to the mean, whereby extreme scores on one variable tend to be associated with less extreme scores on another, inflating apparent predictive power",
        "B": "The coefficient of determination (r²), whereby squaring the correlation coefficient reveals that only 20% of variance in blood pressure is accounted for by variance in chronic stress — not the larger value implied by the correlation itself",
        "C": "Shrinkage, whereby the predictive utility of a regression model decreases when applied to a new sample, reducing the proportion of variance explained from the original estimate",
        "D": "Effect size inflation, whereby statistical significance in large samples (N = 250) causes the reported association to overestimate the true population relationship"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The researcher's metric requiring an 'arithmetic transformation' of the reported association is r², obtained by squaring r. If r ≈ .45, then r² ≈ .20, meaning 20% of variance in blood pressure is accounted for by stress — far less than the 'nearly half' the journalist implied by treating r as if it directly expressed explained variance. This is precisely the distinction between r and r².",
        "A": "Incorrect. Regression to the mean describes the tendency for extreme scores to move closer to the mean upon remeasurement; it is a phenomenon relevant to repeated measurement designs and does not explain why squaring a correlation coefficient yields a smaller value representing explained variance.",
        "C": "Incorrect. Shrinkage refers to the reduction in R² when a regression model is cross-validated on a new sample, because the original model capitalizes on sample-specific variance. While it involves explained variance, it describes a problem of generalizability across samples, not the mathematical relationship between r and r² within a single study.",
        "D": "Incorrect. Effect size inflation in large samples is a concern about the relationship between statistical significance and practical significance — large N studies can produce significant p-values for trivially small effects. However, this does not explain the specific arithmetic transformation the researcher performed or the journalist's error of treating the correlation as a direct percentage of explained variance."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-105-vignette-L5",
      "source_question_id": "105",
      "source_summary": "Squaring a correlation coefficient produces a coefficient of determination, which indicates the amount of variability in one variable that is accounted for by variability in another variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "Two researchers are debating the practical importance of a finding in which a measure of early childhood adversity is shown to be meaningfully linked to adult sleep disturbance in a large national survey. One researcher argues that their finding is 'substantial' because the numerical index describing the strength of the link between the two measures — a value of .45 — represents a meaningful magnitude. The second researcher counters that this framing significantly overstates the finding's practical meaning, and that a more honest communication would involve converting that .45 into a different numerical form before describing how much of the variation in adult sleep disturbance is actually captured by knowing someone's adversity history. After the conversion, the second researcher concludes that the finding, while real, is far more modest in scope than the first researcher implies.",
      "question": "The second researcher's argument and conversion procedure most directly illustrate which statistical principle?",
      "options": {
        "A": "That the p-value associated with the link between early adversity and sleep disturbance should be reported alongside the magnitude index to guard against overstating practical significance in large samples",
        "B": "That the original magnitude index must be squared to yield the proportion of variability in sleep disturbance accounted for by adversity, which at .20 is substantially smaller than the original index of .45 suggests",
        "C": "That a standardized index of group differences would better characterize the practical significance of the finding than the measure of association initially reported",
        "D": "That the predictive value of early adversity for sleep disturbance shrinks when the model is applied to independent samples, making the .45 value an overestimate of the true population relationship"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The second researcher is describing the coefficient of determination, obtained by squaring the correlation coefficient: .45² ≈ .20. This conversion reveals that only about 20% of the variability in adult sleep disturbance is accounted for by variability in early adversity — substantially less than the .45 index alone implies. The debate hinges on the common misinterpretation of r as directly expressing explained variance rather than requiring squaring to yield that value.",
        "A": "Incorrect. Reporting p-values alongside effect sizes is a legitimate practice to distinguish statistical from practical significance, and large samples can yield significant p-values for small effects. However, the second researcher's argument is not about adding a supplementary statistic — it is about transforming the existing index through a specific arithmetic operation to yield a more accurate representation of explained variance.",
        "C": "Incorrect. Standardized mean difference indices (such as Cohen's d) are used to quantify group differences and are a different family of effect size metrics. The scenario describes a link between two continuous measures in a survey context, not a comparison of group means, making a mean-difference index conceptually mismatched to the research question.",
        "D": "Incorrect. Shrinkage describes the reduction in predictive accuracy when a model derived from one sample is applied to another, reflecting overfitting to sample-specific noise. While this is a legitimate concern in predictive modeling, the second researcher's argument involves a mathematical transformation of the reported value within the same study — not a concern about cross-sample generalizability."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-127-vignette-L1",
      "source_question_id": "127",
      "source_summary": "Stepwise multiple regression is a type of multiple regression that's used to identify the fewest number of predictors needed to make an accurate prediction.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "stepwise",
        "regression",
        "predictors"
      ],
      "vignette": "A researcher is studying academic achievement in college students and has access to 12 potential predictors, including GPA, study hours, socioeconomic status, and anxiety scores. She wants to build a multiple regression model but is specifically concerned with parsimony — identifying the fewest predictors needed to maintain strong predictive accuracy. She decides to use a stepwise approach, allowing the statistical program to enter and remove predictors based on preset significance criteria at each step. The final model retains only 4 of the original 12 predictors.",
      "question": "Which statistical method best describes the approach the researcher used?",
      "options": {
        "A": "Hierarchical multiple regression",
        "B": "Stepwise multiple regression",
        "C": "Standard (simultaneous) multiple regression",
        "D": "Simple linear regression"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Hierarchical multiple regression also uses multiple predictors but enters them in theoretically driven blocks determined by the researcher — it does not automatically select or remove variables based on statistical criteria to achieve parsimony.",
        "B": "Stepwise multiple regression is specifically designed to identify the smallest set of predictors that yields adequate prediction accuracy. Variables are entered or removed based on significance thresholds, and the goal is parsimony, matching this scenario precisely.",
        "C": "Standard multiple regression enters all predictors simultaneously in a single step; it does not selectively add or remove predictors to minimize the number needed. The researcher's procedure of iteratively adding and dropping variables rules this out.",
        "D": "Simple linear regression involves only one predictor and one outcome variable. Because the researcher is working with 12 potential predictors and producing a multi-predictor model, simple linear regression does not apply."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-127-vignette-L2",
      "source_question_id": "127",
      "source_summary": "Stepwise multiple regression is a type of multiple regression that's used to identify the fewest number of predictors needed to make an accurate prediction.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "regression",
        "parsimony"
      ],
      "vignette": "A clinical researcher is examining which factors best predict treatment dropout in an outpatient substance use program. She has gathered data on 15 variables — including age, severity of dependence, childhood trauma history, social support, and motivation level — from 240 participants. Despite having a large sample, she is a junior investigator with limited grant funding, and she wants her model to be practical for busy clinicians to use. She runs an analysis that automatically tests each variable's contribution and removes those that do not meet the significance threshold, ultimately retaining a three-variable model that accounts for nearly as much variance as the full 15-variable model.",
      "question": "Which analytic approach did the researcher most likely use?",
      "options": {
        "A": "Hierarchical multiple regression",
        "B": "Canonical correlation",
        "C": "Stepwise multiple regression",
        "D": "Logistic regression"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Hierarchical multiple regression requires the researcher to specify which variables enter in which order based on theory or prior research. It does not automatically test and remove variables; its goal is to examine incremental variance explained by blocks, not to identify the minimum set of predictors.",
        "B": "Canonical correlation examines the relationship between two sets of multiple variables simultaneously — it is used when there are multiple outcome variables, not a single outcome. The researcher has one outcome (dropout) and is seeking the smallest predictor set, which does not fit canonical correlation.",
        "C": "Stepwise multiple regression automatically adds and removes predictors based on significance criteria to produce the most parsimonious model. The retention of three variables that account for nearly as much variance as 15 variables exemplifies this method's goal, and the desire for clinical practicality reflects the parsimony objective.",
        "D": "Logistic regression is used when the outcome variable is categorical (e.g., yes/no dropout), and while it could be appropriate given the dichotomous outcome here, it does not describe the automatic variable-selection process. The key feature — iterative selection for parsimony — identifies this as stepwise rather than standard logistic regression."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-127-vignette-L3",
      "source_question_id": "127",
      "source_summary": "Stepwise multiple regression is a type of multiple regression that's used to identify the fewest number of predictors needed to make an accurate prediction.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "predictors"
      ],
      "vignette": "A health psychologist is developing a brief screener to identify patients at high risk for non-adherence to cardiac rehabilitation. She begins with a pool of 18 candidate variables derived from the literature, including demographic factors, psychological measures, and behavioral indicators. Her analysis sequentially evaluates each variable's statistical contribution and retains only those that add unique explanatory value once prior variables are already in the model, stopping when no remaining variable meets the inclusion criterion. A colleague cautions her that this data-driven selection process can capitalize on chance and may not generalize well to new samples. The final screener contains only 5 items.",
      "question": "Which method best describes the analysis the psychologist conducted?",
      "options": {
        "A": "Standard multiple regression with listwise deletion",
        "B": "Hierarchical multiple regression",
        "C": "Factor analysis",
        "D": "Stepwise multiple regression"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Standard multiple regression with listwise deletion refers to how missing data are handled (omitting cases with any missing value), not to a variable-selection process. Standard multiple regression enters all predictors at once rather than iteratively evaluating and removing them.",
        "B": "Hierarchical multiple regression does involve ordering predictors, but that order is determined theoretically by the researcher in advance. It does not automatically select predictors based on statistical significance at each step, and it does not aim to minimize the number of retained predictors.",
        "C": "Factor analysis is a data reduction technique used to identify latent constructs underlying a set of variables; it does not produce a regression equation predicting an outcome. While it reduces many variables to fewer factors, it serves a different purpose than predicting a criterion and selecting the fewest useful predictors.",
        "D": "Stepwise multiple regression sequentially adds predictors based on their statistical contribution, retaining only those that uniquely explain variance above and beyond previously entered predictors. The colleague's warning about capitalizing on chance and poor generalizability is a well-documented limitation of stepwise selection, confirming this identification."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-127-vignette-L4",
      "source_question_id": "127",
      "source_summary": "Stepwise multiple regression is a type of multiple regression that's used to identify the fewest number of predictors needed to make an accurate prediction.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "parsimonious"
      ],
      "vignette": "A research team studying burnout among emergency physicians initially entered all 20 collected variables into a regression analysis but found the model difficult to interpret and unlikely to be clinically useful due to its complexity. They subsequently reran the analysis using a method in which the software evaluated each candidate variable's incremental F-statistic at each stage, and variables were either admitted to or expelled from the equation depending on whether they met predetermined alpha thresholds of .05 for entry and .10 for removal. The team emphasized that their overriding goal was to produce the most parsimonious predictive model — one that could be feasibly applied in future screening without burdening clinicians with unnecessary variables. A reviewer later noted this approach is criticized because the resulting model is optimized for the current dataset and may overfit.",
      "question": "Which statistical procedure did the research team employ?",
      "options": {
        "A": "Hierarchical multiple regression",
        "B": "Stepwise multiple regression",
        "C": "Ridge regression",
        "D": "Discriminant function analysis"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Hierarchical multiple regression also uses sequential entry of predictors and can accommodate multiple blocks of variables, making it a plausible choice. However, it requires the researcher to specify the entry order based on theory, and it does not automatically test variables for inclusion or exclusion using alpha thresholds — both of which are hallmarks of the described procedure.",
        "B": "Stepwise multiple regression uses predetermined significance thresholds (entry and removal alpha levels) to automatically admit or expel variables from the equation at each step, with the explicit goal of parsimony. The reviewer's criticism about overfitting and sample-specificity is a canonical limitation of stepwise selection, distinguishing it from other methods.",
        "C": "Ridge regression is a regularization technique designed to handle multicollinearity by introducing a penalty term that shrinks regression coefficients; while it can reduce model complexity, it does not use alpha thresholds to include or exclude individual predictors, and it is not described as employing entry and removal criteria.",
        "D": "Discriminant function analysis is used to predict group membership (a categorical criterion) from multiple predictors and also produces a linear combination of variables. However, it is not used when the outcome is a continuous variable like burnout scores, and it does not operate by sequentially testing incremental F-statistics for entering and removing predictors."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-127-vignette-L5",
      "source_question_id": "127",
      "source_summary": "Stepwise multiple regression is a type of multiple regression that's used to identify the fewest number of predictors needed to make an accurate prediction.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators studying job performance in air traffic controllers gathers information on 22 characteristics of each controller, ranging from personal history variables to cognitive test scores to workplace attitudes. They initially notice that many of these characteristics are moderately intercorrelated, which led them to first consider a technique designed to find underlying clusters — but they ultimately decided they needed an equation, not clusters. Instead of running an equation using all 22 characteristics simultaneously, the software was allowed to decide which characteristics to include by testing each one's unique contribution after the ones already selected were accounted for, dropping any characteristic that failed to add meaningfully and stopping when none of the remaining characteristics would improve the equation enough to justify inclusion. The resulting equation used only 6 of the 22 original characteristics to predict performance almost as accurately as using all 22.",
      "question": "Which analytic method did the investigators most likely use?",
      "options": {
        "A": "Factor analysis followed by regression on factor scores",
        "B": "Canonical correlation analysis",
        "C": "Stepwise multiple regression",
        "D": "Standard multiple regression"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Factor analysis followed by regression on factor scores is a two-stage approach that first reduces intercorrelated variables into underlying dimensions and then uses those dimensions as predictors. The vignette explicitly notes the team considered but rejected a clustering/dimensionality technique, and the final analysis used individual original characteristics, not derived factors, making this option inconsistent.",
        "B": "Canonical correlation analysis is designed when there are multiple outcome variables, not a single criterion like job performance. While it can identify a linear combination of predictors, it does not sequentially evaluate individual variables for entry and removal based on their incremental contribution to predicting one outcome.",
        "C": "Stepwise multiple regression is precisely the method in which the software — rather than the researcher — decides which variables to include by iteratively testing each variable's unique contribution given what is already in the model, dropping non-contributors and stopping when no remaining variable improves prediction sufficiently. The retention of 6 of 22 variables while preserving nearly all predictive accuracy exemplifies its parsimony goal.",
        "D": "Standard multiple regression enters all predictors simultaneously in a single step, producing coefficients for all 22 variables at once. It does not involve any automated selection, sequential testing, or removal of variables, and it would not produce a reduced model of only 6 predictors unless variables were pre-selected by the researcher before running the analysis."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-059-vignette-L1",
      "source_question_id": "059",
      "source_summary": "The single-sample chi-square test is the appropriate statistical test to use to determine if there's a significant difference in the number of dog owners who chose each of seven different labels for canned dog food.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "chi-square",
        "frequencies",
        "categories"
      ],
      "vignette": "A consumer psychologist recruits 200 dog owners and asks each participant to choose their preferred label from seven different canned dog food options. The researcher records the frequencies of selections for each of the seven label categories. She wants to determine whether participants show a significant preference for certain labels over others, or whether choices are distributed equally across all seven options. She plans to use an inferential test to compare the observed frequencies with expected frequencies under a null hypothesis of equal preference.",
      "question": "Which statistical test is most appropriate for analyzing whether dog owners' label preferences are significantly different from an equal distribution across all seven categories?",
      "options": {
        "A": "One-way ANOVA",
        "B": "Single-sample chi-square test",
        "C": "Independent-samples t-test",
        "D": "Pearson correlation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "One-way ANOVA is used to compare mean scores across three or more independent groups on a continuous dependent variable. This study does not involve a continuous outcome variable — it involves counts of categorical choices — making ANOVA inappropriate here.",
        "B": "The single-sample chi-square (goodness-of-fit) test is the correct choice because it is designed to compare observed frequencies of choices across categorical options against expected frequencies (here, equal distribution), which is precisely the research question being asked.",
        "C": "An independent-samples t-test compares the means of two independent groups on a continuous variable. This study has seven categorical response options and no continuous dependent variable, so a t-test is not applicable.",
        "D": "Pearson correlation assesses the linear relationship between two continuous variables. Because the outcome here is categorical frequency data (label choices) rather than two continuous variables, correlation is not the appropriate analytic tool."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-059-vignette-L2",
      "source_question_id": "059",
      "source_summary": "The single-sample chi-square test is the appropriate statistical test to use to determine if there's a significant difference in the number of dog owners who chose each of seven different labels for canned dog food.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "categorical",
        "goodness-of-fit"
      ],
      "vignette": "A marketing researcher is hired by a pet food company to evaluate seven newly designed labels for a canned dog food product. She recruits 150 dog owners from a variety of geographic regions and income levels, suspecting that regional preferences might complicate her findings. Each participant selects exactly one label they find most appealing, producing a count for each of the seven labels. She wants to test whether label selections deviate significantly from what would be expected if all seven labels were equally preferred.",
      "question": "Which statistical procedure best addresses the researcher's primary question about whether label choices are equally distributed?",
      "options": {
        "A": "Kruskal-Wallis test",
        "B": "Factorial ANOVA",
        "C": "Single-sample chi-square test",
        "D": "Mann-Whitney U test"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The Kruskal-Wallis test is a nonparametric alternative to one-way ANOVA that compares ranked scores across three or more independent groups. While it handles non-normal distributions, it requires at least an ordinal dependent variable across groups, not a single set of categorical frequency counts.",
        "B": "Factorial ANOVA examines the effects of two or more independent variables on a continuous dependent variable, including interaction effects. This design has no independent groups being compared on a continuous outcome — it involves a single sample of frequency counts across seven nominal categories.",
        "C": "This is correct. The single-sample chi-square goodness-of-fit test is designed exactly for this situation: comparing observed counts in a set of categorical options against an expected distribution (in this case, equal preference across seven labels). Regional variation is a neutral contextual detail that does not change the analytic approach.",
        "D": "The Mann-Whitney U test is a nonparametric test comparing the distributions of two independent groups on an ordinal variable. This study involves one sample making categorical selections across seven options, not two groups being compared — making this test inapplicable."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-059-vignette-L3",
      "source_question_id": "059",
      "source_summary": "The single-sample chi-square test is the appropriate statistical test to use to determine if there's a significant difference in the number of dog owners who chose each of seven different labels for canned dog food.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "frequencies"
      ],
      "vignette": "A researcher studying consumer behavior presents 180 dog owners with seven distinct canned dog food labels and asks each person to select their favorite. The researcher notes that the sample includes both male and female participants, raising the possibility that gender might predict label preference, and she wonders if she should account for this in her analysis. However, her primary research question is simply whether any label is significantly more preferred than the others overall, regardless of participant gender. She plans to compare how often each label was chosen against what would be expected by chance.",
      "question": "Which statistical test best addresses the researcher's primary question?",
      "options": {
        "A": "Two-way chi-square test (chi-square test of independence)",
        "B": "Single-sample chi-square test",
        "C": "Binary logistic regression",
        "D": "One-way ANOVA"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The chi-square test of independence (two-way chi-square) examines whether two categorical variables are related — for example, whether gender and label choice are associated. While the mention of gender in the vignette may suggest this test, the researcher's primary question is about overall distribution of choices across labels, not a relationship between two variables, making this a compelling but incorrect choice.",
        "B": "Correct. The single-sample chi-square goodness-of-fit test compares the observed frequencies of a single categorical variable (label chosen) against an expected distribution (equal probability). The mention of gender is a red herring; the primary question only requires comparing observed versus expected frequencies for one variable.",
        "C": "Binary logistic regression predicts group membership on a binary outcome variable from one or more predictors. Although it can handle categorical outcomes, it is designed for prediction from predictors and requires a binary (not seven-category) criterion variable — it does not address whether observed frequencies differ from chance.",
        "D": "One-way ANOVA compares mean scores on a continuous dependent variable across three or more independent groups. Because the outcome variable here is a nominal category (label chosen), not a continuous score, ANOVA is not appropriate regardless of the number of label options."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-059-vignette-L4",
      "source_question_id": "059",
      "source_summary": "The single-sample chi-square test is the appropriate statistical test to use to determine if there's a significant difference in the number of dog owners who chose each of seven different labels for canned dog food.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "expected"
      ],
      "vignette": "A consumer research firm collects data from a single group of 200 dog owners, each of whom is shown seven candidate labels for a new canned dog food product and asked to select one. The firm's analysts note that the data involve nominal-level selections and that the seven labels were not rated on a scale but simply chosen or not chosen. Because prior industry data suggested that two labels were historically more popular, one analyst proposes modeling the expected selection rates as unequal across the seven options rather than assuming uniform distribution. The team's statistician confirms that the analytic approach can accommodate non-uniform expected frequencies and will test whether the observed pattern of selections departs significantly from those specified expected values.",
      "question": "Which statistical test is the statistician most likely recommending?",
      "options": {
        "A": "Chi-square test of independence",
        "B": "Multinomial logistic regression",
        "C": "Single-sample chi-square test",
        "D": "Friedman test"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The chi-square test of independence assesses whether two categorical variables are associated by comparing observed cell frequencies in a contingency table. This study involves only one categorical variable (label selected) measured in one sample — there is no second variable to cross-tabulate against, so this test is not appropriate despite sharing the chi-square family.",
        "B": "Multinomial logistic regression predicts membership in one of three or more categories based on predictor variables. While it can handle a seven-category outcome, it requires predictor variables and is designed for prediction, not for comparing observed frequencies against hypothesized expected frequencies — making it unsuitable for this descriptive distributional question.",
        "C": "Correct. The single-sample chi-square goodness-of-fit test compares observed frequencies in a single categorical variable against any set of expected frequencies, which need not be equal. The statistician's confirmation that non-uniform expected frequencies can be accommodated is the key distinguishing detail pointing to this test, since many students assume it only works with equal expectations.",
        "D": "The Friedman test is a nonparametric alternative to repeated-measures ANOVA, comparing ranked scores across three or more related conditions measured on the same participants. It requires at least ordinal data across repeated measurements and is not designed to test whether observed category counts match a hypothesized distribution."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-059-vignette-L5",
      "source_question_id": "059",
      "source_summary": "The single-sample chi-square test is the appropriate statistical test to use to determine if there's a significant difference in the number of dog owners who chose each of seven different labels for canned dog food.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of researchers recruits a single group of dog owners from a local veterinary clinic and shows each person seven differently designed product packages for the same food item. Each person picks exactly one package they would most likely buy, and the team records how many people chose each option. Before collecting data, the team had assumed that demand would be split fairly evenly, but they also had access to sales data suggesting two specific designs had historically outsold the others by a measurable margin, so they built those unequal estimates into their predictions. After collection, the team wants to formally assess whether the spread of actual selections matches their pre-specified predictions, rather than comparing two groups or examining relationships among continuous measurements.",
      "question": "Which statistical procedure is the team's analysis most likely to employ?",
      "options": {
        "A": "Repeated-measures ANOVA",
        "B": "Chi-square test of independence",
        "C": "Single-sample chi-square test",
        "D": "Spearman rank-order correlation"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Repeated-measures ANOVA compares mean scores on a continuous dependent variable across multiple time points or conditions within the same participants. Although each participant in this study evaluates multiple packages, the outcome is a single nominal choice per person, not a set of continuous scores measured repeatedly — making this test inapplicable.",
        "B": "The chi-square test of independence examines whether two categorical variables are associated in a contingency table. While the study involves categorical data and chi-square logic, there is only one categorical variable (package chosen) in one sample — there is no second grouping variable to create a cross-tabulation. The mention of multiple packages could mislead students toward thinking two variables are being examined.",
        "C": "Correct. The single-sample chi-square goodness-of-fit test is uniquely designed to compare observed frequencies of a single categorical variable against pre-specified expected frequencies in one sample. The team's use of prior sales data to generate non-uniform expected values is fully accommodated by this test, and the absence of group comparisons or continuous scores rules out all other options.",
        "D": "Spearman rank-order correlation measures the monotonic association between two ordinal or ranked variables. Even though the team used ranked sales data to estimate preferences, the actual analysis involves matching a count distribution to a predicted distribution — not correlating two ordinal variables — so this test does not apply."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-037-vignette-L1",
      "source_question_id": "037",
      "source_summary": "The standard error of the mean increases in size as the population standard deviation increases and the sample size decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "standard error of the mean",
        "sample size",
        "population standard deviation"
      ],
      "vignette": "A researcher is planning a study on depression symptom severity scores, which are known to have a large population standard deviation due to high variability in the clinical population. She is debating whether to recruit 20 or 200 participants. Her advisor explains that the standard error of the mean will be substantially larger with the smaller sample size given the high population standard deviation. The advisor further notes that the standard error of the mean is a key indicator of how precisely the sample mean estimates the population mean.",
      "question": "Which of the following best describes the relationship between the standard error of the mean, population standard deviation, and sample size?",
      "options": {
        "A": "The standard error of the mean decreases as population standard deviation increases and increases as sample size increases.",
        "B": "The standard error of the mean is unaffected by sample size and is determined solely by the population standard deviation.",
        "C": "The standard error of the mean increases as population standard deviation increases and decreases as sample size increases.",
        "D": "The standard error of the mean decreases as both population standard deviation and sample size increase."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This option reverses the direction of the relationship with population standard deviation. A larger population standard deviation means more variability in scores, which produces a larger — not smaller — standard error of the mean.",
        "B": "Sample size is a critical determinant of the standard error of the mean. Specifically, SEM = σ/√n, so both population standard deviation and sample size jointly determine the standard error. Ignoring sample size is incorrect.",
        "C": "This is correct. The formula for the standard error of the mean is σ/√n, meaning it grows proportionally with population standard deviation and shrinks as sample size increases. Larger samples provide more precise estimates of the population mean.",
        "D": "While an increase in sample size does decrease the standard error, an increase in population standard deviation increases it — not decreases it. This option incorrectly states that increasing both produces a decrease."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-037-vignette-L2",
      "source_question_id": "037",
      "source_summary": "The standard error of the mean increases in size as the population standard deviation increases and the sample size decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "standard error",
        "sample size"
      ],
      "vignette": "A health psychologist is comparing two studies examining chronic pain intensity ratings. Study A recruited 15 participants from a single specialty clinic, while Study B recruited 150 participants from multiple community hospitals. Both studies used the same validated pain scale. The researcher notes that Study A produced a much wider confidence interval around the mean than Study B, even though the pain ratings in Study A appeared less scattered within the clinic sample. She suspects the difference in precision between the two studies is primarily explained by one key factor.",
      "question": "What most directly explains why Study A produced a wider confidence interval than Study B?",
      "options": {
        "A": "Study A had lower internal validity due to its restricted clinic sample, which biased the confidence interval upward.",
        "B": "Study A had a larger standard error because its smaller sample size produced less precise estimation of the population mean.",
        "C": "Study A had greater measurement error on the pain scale because clinic patients tend to have more variable true scores.",
        "D": "Study A's confidence interval was wider because its participants were drawn from a population with a larger standard deviation than Study B's participants."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Internal validity refers to the degree to which a study can establish causal relationships by controlling confounds — it does not directly determine the width of confidence intervals. Confidence interval width is driven by statistical precision, not internal validity.",
        "B": "This is correct. The standard error of the mean is σ/√n. With only 15 participants, Study A's standard error is substantially larger than Study B's with 150 participants, directly producing a wider confidence interval regardless of within-sample scatter.",
        "C": "The vignette explicitly notes that Study A's ratings appeared less scattered, making greater measurement error within that sample an unlikely explanation. Measurement error is related to reliability, not directly to the standard error formula based on sample size.",
        "D": "While a larger population standard deviation would increase the standard error, the vignette specifies that both studies used the same scale and the within-sample variability of Study A was actually lower. The most direct explanation is the difference in sample size, not population standard deviation."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-037-vignette-L3",
      "source_question_id": "037",
      "source_summary": "The standard error of the mean increases in size as the population standard deviation increases and the sample size decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "variability"
      ],
      "vignette": "A graduate student conducts two separate surveys measuring job satisfaction among hospital nurses. In the first survey, she recruits 250 nurses from a large metropolitan area where job conditions vary enormously across dozens of institutions; scores on the satisfaction measure span nearly the entire possible range. In the second survey, she recruits only 30 nurses from a single rural hospital where working conditions are remarkably uniform and scores cluster tightly. When she computes her estimates, she is surprised to find that despite having far more participants, her first survey yields estimates with similar imprecision as the second. A faculty mentor reviews the data and immediately identifies the statistical reason for this unexpected outcome.",
      "question": "Which statistical concept most directly explains why the larger first survey did not yield substantially more precise estimates than the smaller second survey?",
      "options": {
        "A": "The first survey had lower external validity because convenience sampling from a metropolitan area introduces systematic bias that inflates the margin of error.",
        "B": "The first survey suffered from restriction of range, which attenuated the true relationship between sample size and estimation precision.",
        "C": "The far greater score variability in the first survey's population produced a much larger standard error, partially counteracting the advantage of the larger sample size.",
        "D": "The second survey's small sample created a positively skewed sampling distribution, which artificially reduced its apparent imprecision relative to the first survey."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "External validity concerns the generalizability of findings to other populations and settings, not the mathematical precision of point estimates. Convenience sampling can affect generalizability but does not directly explain why margin of error remains high despite large n.",
        "B": "Restriction of range refers to a phenomenon where limited score variability in a sample attenuates observed correlations. This concept applies in the opposite direction from what the vignette describes — the first survey had expanded, not restricted, range, and the concept applies to correlations rather than to estimation precision via the standard error.",
        "C": "This is correct. The standard error equals σ/√n. Although the first survey had 250 participants versus 30, its population standard deviation was dramatically larger due to extreme score variability across institutions. These two influences on the standard error can offset each other, yielding similar imprecision in the estimates despite very different sample sizes.",
        "D": "A small sample does create a more leptokurtic and potentially skewed sampling distribution, but with n = 30 the central limit theorem provides reasonable approximation to normality. More importantly, skewness of the sampling distribution does not directly explain similar imprecision between the two surveys — the driving factor is the interplay of population variability and sample size in the standard error formula."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-037-vignette-L4",
      "source_question_id": "037",
      "source_summary": "The standard error of the mean increases in size as the population standard deviation increases and the sample size decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "sampling distribution"
      ],
      "vignette": "Two independent research teams each attempt to estimate the mean reading achievement score for third-graders in their respective states. Team 1 works in a state with a highly heterogeneous school system — urban, suburban, and rural schools vary enormously in resources — and draws a carefully randomized sample of 400 students. Team 2 works in a more educationally homogeneous state and draws a randomized sample of 100 students. Upon comparing notes, both teams are surprised to discover that Team 1's sampling distribution yields less confidence in their point estimate than Team 2's, despite Team 1's sample being four times larger. A statistician consultant is brought in to explain this counterintuitive finding.",
      "question": "What explanation would the statistician most likely offer for why Team 1's larger sample produced a less precise estimate than Team 2's smaller sample?",
      "options": {
        "A": "Team 1's randomization procedure introduced selection bias because drawing from a heterogeneous system makes it harder to achieve true random sampling, thereby increasing estimation error.",
        "B": "Team 1's larger sample size paradoxically increased Type II error risk, reducing the sensitivity of their point estimate to the true population mean.",
        "C": "Team 1's population had sufficiently greater score variability that the resulting standard error exceeded Team 2's, despite Team 1's four-fold advantage in sample size, because standard error is jointly determined by both population spread and sample size.",
        "D": "Team 2 benefited from reduced sampling error because homogeneous populations produce narrower confidence intervals independently of sample size, meaning sample size becomes irrelevant in homogeneous contexts."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Random sampling from a heterogeneous population remains valid random sampling — heterogeneity does not itself introduce selection bias. Selection bias arises from non-random sampling procedures, not from the diversity of the population being sampled. This option conflates population heterogeneity with sampling bias.",
        "B": "Type II error refers to the failure to reject a false null hypothesis and is a concept from hypothesis testing, not from point estimation precision. Larger samples reduce Type II error risk — they do not increase it. This option misapplies hypothesis testing logic to the question of estimation precision.",
        "C": "This is correct. The standard error formula is σ/√n. Team 1's state has a dramatically larger σ due to extreme school heterogeneity. Even with n = 400 versus n = 100, if σ₁ is more than twice σ₂, Team 1's standard error (σ₁/20) can exceed Team 2's (σ₂/10). The key insight is that sample size alone does not determine precision — it interacts multiplicatively with population variability.",
        "D": "While it is true that population homogeneity (lower σ) reduces the standard error, sample size does not become 'irrelevant' — it continues to influence the standard error according to the same formula. This option correctly identifies the role of population variability but overstates the conclusion by dismissing sample size as a factor, which is statistically incorrect."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-037-vignette-L5",
      "source_question_id": "037",
      "source_summary": "The standard error of the mean increases in size as the population standard deviation increases and the sample size decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A consulting firm hires two analysts to independently generate numerical estimates for their respective client projects. Analyst 1 collects information from 500 sources, but those sources come from an industry where outcomes differ wildly from one organization to the next — some reporting figures ten times higher than others. Analyst 2 collects information from only 80 sources in an industry where most organizations operate nearly identically, producing figures that fall within a narrow band. When the firm's director reviews the final reports, she notes that Analyst 2's estimate carries a tighter margin around it than Analyst 1's, even though Analyst 1 drew on far more sources. She concludes that Analyst 1 would need to collect an implausibly large number of additional sources to achieve the same tightness as Analyst 2.",
      "question": "Which of the following statistical principles most directly accounts for the director's observation?",
      "options": {
        "A": "Analyst 1's estimate is less precise because drawing from a larger, more diverse pool of sources introduces systematic measurement error that accumulates with each additional source consulted.",
        "B": "Analyst 2's estimate is more precise because a small, homogeneous group of sources is more internally consistent, reducing the unreliability of each individual observation and thereby increasing the accuracy of the aggregate estimate.",
        "C": "Analyst 1's estimate is less precise because the high spread of values in the underlying population from which sources were drawn produces a larger average deviation of possible sample means from the true mean, which the fourfold advantage in number of sources is insufficient to overcome.",
        "D": "Analyst 2's estimate is more precise because using fewer sources reduces the opportunity for outliers to distort the estimate, and outlier influence is the primary driver of imprecision in aggregate estimates."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Systematic measurement error refers to consistent, directional inaccuracies in how a construct is measured — not to the spread of true values in a population. Consulting more sources does not itself accumulate measurement error; this option misattributes the imprecision to a measurement reliability problem rather than to population variability interacting with sample size in the standard error.",
        "B": "This option correctly identifies that Analyst 2's population is homogeneous, but it frames the explanation in terms of reliability of individual observations rather than the mathematical relationship between population spread, number of observations, and the precision of a mean estimate. Internal consistency and reliability are distinct concepts from the standard error of the mean, which depends on population-level variability divided by the square root of n.",
        "C": "This is correct. Without using technical jargon, this option accurately describes the standard error of the mean: the average deviation of all possible sample means from the true population mean (i.e., the standard error) is larger when the underlying population values are highly spread out. Analyst 1's industry has enormous spread (large σ), so even 500 sources cannot reduce the standard error to the level achieved by Analyst 2's 80 sources in a tight industry (small σ). This is because SEM = σ/√n, and a large σ dominates a moderately large n.",
        "D": "While outliers can distort estimates, the vignette describes a situation where one population simply has greater true variability — not one defined by pathological outliers. Furthermore, reducing the number of sources does not inherently limit outlier influence; the primary driver of Analyst 2's tighter margin is the narrow true spread of the source population, not the number of sources used or their outlier composition."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-026-vignette-L1",
      "source_question_id": "026",
      "source_summary": "Of the three single-subject designs listed (multiple baseline, reversal, and discrete trials), the multiple baseline design would be the most appropriate for evaluating the effects of virtual reality exposure for treating the storm, height, and spider phobias of a 34-year-old woman.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "multiple baseline",
        "single-subject",
        "phobias"
      ],
      "vignette": "A researcher wants to use a single-subject design to evaluate the effectiveness of virtual reality exposure therapy in a 34-year-old woman who has three distinct phobias: storms, heights, and spiders. The researcher plans to introduce the intervention sequentially across the three phobias rather than simultaneously, so that each untreated phobia continues to serve as a control while another is being treated. The researcher notes that because storm phobia involves genuine safety concerns, it would be ethically problematic to treat and then withdraw treatment. This single-subject design allows causal inference without requiring the removal of a presumably beneficial intervention.",
      "question": "Which single-subject design is the researcher using?",
      "options": {
        "A": "Reversal design",
        "B": "Multiple baseline design",
        "C": "Discrete trials design",
        "D": "Changing criterion design"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. A reversal (ABAB) design involves withdrawing the intervention after it has produced an effect and then reintroducing it to demonstrate experimental control. This is inappropriate when withdrawing treatment would be harmful or unethical, as it would be for established phobia treatment.",
        "B": "Correct. The multiple baseline design staggers the introduction of an intervention across behaviors, settings, or individuals without requiring withdrawal of treatment. It is ideal here because the three phobias serve as independent baselines and treatment can be maintained ethically.",
        "C": "Incorrect. A discrete trials design is a structured instructional format drawn from applied behavior analysis used primarily for skill acquisition, not a research design for evaluating treatment across multiple phobia targets in this fashion.",
        "D": "Incorrect. A changing criterion design evaluates an intervention by progressively shifting a performance criterion to demonstrate that behavior tracks the criterion over time. It is suited for behaviors that change gradually and in one direction, not for comparing treatment across multiple independent phobia targets."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-026-vignette-L2",
      "source_question_id": "026",
      "source_summary": "Of the three single-subject designs listed (multiple baseline, reversal, and discrete trials), the multiple baseline design would be the most appropriate for evaluating the effects of virtual reality exposure for treating the storm, height, and spider phobias of a 34-year-old woman.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "single-subject",
        "baseline"
      ],
      "vignette": "A clinical researcher is treating a 34-year-old woman who presents with co-occurring generalized anxiety disorder and three specific phobias involving storms, heights, and spiders. The researcher decides to use a single-subject design in which a stable baseline is first established across all three phobia targets simultaneously before any intervention begins. Virtual reality exposure is then introduced to one phobia at a time in a staggered sequence, with the remaining phobias continuing under baseline observation to serve as controls. The researcher deliberately avoids any design that would require removing the intervention once it has been successfully applied.",
      "question": "Which single-subject research design best describes the approach the researcher is employing?",
      "options": {
        "A": "Multiple baseline design",
        "B": "Reversal design",
        "C": "Alternating treatments design",
        "D": "Discrete trials design"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The multiple baseline design staggers the introduction of an intervention across multiple targets (here, three phobias) while maintaining simultaneous baselines, eliminating the need to withdraw treatment and thereby allowing ethical application of a beneficial intervention.",
        "B": "Incorrect. The reversal design requires withdrawing an effective treatment to re-establish baseline conditions, which would be ethically problematic for a woman receiving a beneficial phobia treatment and inconsistent with the researcher's stated goal of avoiding treatment withdrawal.",
        "C": "Incorrect. The alternating treatments design rapidly alternates two or more interventions within the same phase to compare their effects on a single behavior; it does not involve staggering one treatment across multiple independent targets over time.",
        "D": "Incorrect. Discrete trials is a structured teaching technique used in behavioral interventions (especially ABA), not a research design framework for evaluating treatment efficacy across multiple phobia targets in a sequential manner."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-026-vignette-L3",
      "source_question_id": "026",
      "source_summary": "Of the three single-subject designs listed (multiple baseline, reversal, and discrete trials), the multiple baseline design would be the most appropriate for evaluating the effects of virtual reality exposure for treating the storm, height, and spider phobias of a 34-year-old woman.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "sequential"
      ],
      "vignette": "A researcher is evaluating virtual reality exposure therapy with a single female client who has three distinct fear responses that are functionally independent of one another. To demonstrate experimental control, the researcher introduces the intervention sequentially rather than all at once, so that at any given time some fear responses have not yet received treatment and can be compared against those currently being treated. The client's therapist initially suggested a design that would involve reinstating fear by withdrawing treatment at various points, but the researcher rejected this approach on both ethical and practical grounds. The client's comorbid depression is carefully monitored throughout but does not alter the structural logic of the research design chosen.",
      "question": "Which research design did the researcher most likely select?",
      "options": {
        "A": "Reversal design",
        "B": "Changing criterion design",
        "C": "Multiple baseline design",
        "D": "Alternating treatments design"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. The reversal design was explicitly rejected because it requires withdrawing a beneficial treatment to demonstrate experimental control — exactly the approach the therapist suggested and the researcher refused. The reversal design is not compatible with ethically irreversible or harmful treatment withdrawals.",
        "B": "Incorrect. The changing criterion design demonstrates control by showing that a target behavior shifts in stepwise fashion as the performance criterion is progressively altered. It is suited to behaviors shaped in gradual increments and does not involve staggering an intervention across multiple independent fear targets.",
        "C": "Correct. The multiple baseline design achieves experimental control without treatment withdrawal by introducing the intervention at different times across independent targets. The sequential introduction and ongoing untreated comparators are the defining features that distinguish this design and that the researcher explicitly chose.",
        "D": "Incorrect. The alternating treatments design compares the rapid alternation of two or more conditions within a single participant, which would require concurrently applying and switching between treatments rather than treating fear targets in a staggered, one-at-a-time sequence."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-026-vignette-L4",
      "source_question_id": "026",
      "source_summary": "Of the three single-subject designs listed (multiple baseline, reversal, and discrete trials), the multiple baseline design would be the most appropriate for evaluating the effects of virtual reality exposure for treating the storm, height, and spider phobias of a 34-year-old woman.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "irreversibility"
      ],
      "vignette": "A researcher studying a 34-year-old woman with three independent fear responses uses a within-subject strategy to evaluate a novel exposure-based technology. The researcher is especially concerned about irreversibility — once the fear is extinguished, reinstating the original fear level purely for experimental purposes would be both impractical and ethically indefensible. Accordingly, the researcher structures the study so that simultaneous observation of all three targets begins immediately, but the active experimental condition is introduced to only one target at a time, in staggered fashion, across the study period. On first inspection, the design appears to resemble a repeated-measures group study because all three targets are tracked from the outset, but no group-level randomization is involved and the logic of control relies entirely on the continued absence of the intervention in the as-yet-untreated targets.",
      "question": "Which research design does this study most precisely exemplify?",
      "options": {
        "A": "Reversal design",
        "B": "Multiple baseline design",
        "C": "Within-subjects factorial design",
        "D": "Changing criterion design"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. The reversal design is precisely the design the researcher rejected due to irreversibility concerns. Although reversal designs also achieve within-subject control, they require treatment withdrawal, which is the core problem the researcher sought to avoid.",
        "B": "Correct. The multiple baseline design addresses the irreversibility problem by establishing simultaneous baselines across independent targets and staggering intervention introduction, using the untreated targets as ongoing controls without ever requiring treatment withdrawal.",
        "C": "Incorrect. A within-subjects factorial design is a group-level experimental design in which the same participants experience all levels of one or more independent variables; it involves statistical aggregation across participants and randomization of condition order, neither of which applies in a single-subject design.",
        "D": "Incorrect. The changing criterion design also avoids treatment withdrawal, making it a tempting choice here; however, it demonstrates control by showing behavior tracks progressively shifted performance criteria in a single behavior stream, not by staggering intervention across multiple independent behavioral targets."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-026-vignette-L5",
      "source_question_id": "026",
      "source_summary": "Of the three single-subject designs listed (multiple baseline, reversal, and discrete trials), the multiple baseline design would be the most appropriate for evaluating the effects of virtual reality exposure for treating the storm, height, and spider phobias of a 34-year-old woman.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A woman in her mid-thirties has three separate distressing reactions — one triggered by weather events, one by elevated locations, and one by a particular type of animal — that her clinician considers functionally unrelated to one another. A researcher collaborating with the clinician decides to track all three reactions from the very start of the study, recording how distressed she becomes in each situation week after week without changing anything. After several weeks of stable tracking data, the researcher begins applying a new computerized procedure to address only the first reaction, while still tracking the other two as before. Once that first reaction has clearly diminished, the same computerized procedure is applied to the second reaction, and eventually the third, with each unaddressed reaction continuing to be observed throughout. The researcher never attempts to restore any of the reactions to their original levels after they have been successfully reduced, citing both practical and ethical reasons.",
      "question": "Which research design does this study most precisely represent?",
      "options": {
        "A": "Reversal design",
        "B": "Alternating treatments design",
        "C": "Multiple baseline design",
        "D": "Changing criterion design"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. The reversal design demonstrates experimental control by removing the intervention after it has produced an effect, thereby allowing the target behavior to return toward baseline. The researcher here explicitly avoided reinstating the original reactions, making the reversal design inapplicable even though the study uses careful repeated observation across time.",
        "B": "Incorrect. The alternating treatments design involves rapidly switching between two or more interventions or conditions within a single study to compare their relative effects. This study applies only one procedure, sequentially rather than alternately, across three independent targets — a fundamentally different logical structure.",
        "C": "Correct. All defining features of the multiple baseline design are present: simultaneous tracking of multiple functionally independent targets from the outset, staggered introduction of a single intervention one target at a time, the untreated targets serving as ongoing controls, and deliberate avoidance of any treatment withdrawal. The absence of jargon and the surface resemblance to a longitudinal tracking study are the primary sources of difficulty.",
        "D": "Incorrect. The changing criterion design is also a single-subject design that does not require treatment withdrawal, making it a plausible distractor; however, it works by demonstrating that a behavior progressively approximates a series of shifting performance standards within a single behavior, rather than by staggering an intervention across multiple independent and previously simultaneous baselines."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-048-vignette-L1",
      "source_question_id": "048",
      "source_summary": "Changing the level of significance (alpha) from .05 to .10 increases the probability of making a Type I error and decreases the probability of making a Type II error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "alpha",
        "Type I error",
        "Type II error"
      ],
      "vignette": "A researcher is conducting a study on the effectiveness of a new psychotherapy protocol. After consulting with colleagues, she decides to change her alpha level from .05 to .10 to increase the sensitivity of her statistical tests. Her advisor reminds her that adjusting the alpha level has direct consequences for both Type I error and Type II error rates. The researcher acknowledges that this decision involves a fundamental tradeoff in inferential statistics.",
      "question": "What is the most accurate description of the consequence of changing the alpha level from .05 to .10?",
      "options": {
        "A": "The probability of a Type I error decreases, and the probability of a Type II error increases.",
        "B": "The probability of a Type I error increases, and the probability of a Type II error decreases.",
        "C": "Both the probability of a Type I error and the probability of a Type II error decrease.",
        "D": "The alpha change affects only statistical power and has no direct effect on either error type."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect and reverses the true relationship. Raising alpha from .05 to .10 widens the rejection region, making it easier to reject the null hypothesis — which increases, not decreases, the probability of a Type I error (rejecting a true null).",
        "B": "This is correct. Increasing alpha from .05 to .10 enlarges the critical region, raising the probability of a Type I error (falsely rejecting a true null hypothesis). At the same time, the larger rejection region means the test is less likely to miss a true effect, thereby reducing the probability of a Type II error (failing to reject a false null).",
        "C": "This is incorrect because the two error rates move in opposite directions when alpha changes. Raising alpha increases the Type I error rate; although power increases (reducing Type II error), you cannot simultaneously lower both error rates simply by changing alpha.",
        "D": "This is incorrect. While raising alpha does increase statistical power (1 − β), power is directly defined as 1 minus the Type II error rate. Therefore, changing alpha unavoidably affects the Type II error rate as well as the Type I error rate — it is not limited solely to power."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-048-vignette-L2",
      "source_question_id": "048",
      "source_summary": "Changing the level of significance (alpha) from .05 to .10 increases the probability of making a Type I error and decreases the probability of making a Type II error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "null hypothesis",
        "significance level"
      ],
      "vignette": "A clinical researcher studying treatment outcomes for generalized anxiety disorder initially sets a significance level of .05 for her study, but later revises it upward to .10 because her sample size is smaller than anticipated. The study population is predominantly female adults between 35 and 50 years of age, a demographic detail her IRB notes but considers irrelevant to the statistical decision. Her statistician warns that the revised threshold will affect the balance between two competing risks when evaluating the null hypothesis.",
      "question": "Which of the following best describes the consequence of raising the significance level from .05 to .10 in this study?",
      "options": {
        "A": "The study becomes more conservative, reducing the chance of incorrectly rejecting the null hypothesis while increasing the chance of missing a true effect.",
        "B": "The study gains sensitivity, increasing the chance of incorrectly rejecting the null hypothesis while decreasing the chance of missing a true effect.",
        "C": "The study's internal validity is compromised because a higher significance level introduces systematic bias into the results.",
        "D": "The study's effect size estimate becomes inflated, which independently increases the chance of rejecting the null hypothesis regardless of the true population difference."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This describes the consequence of lowering alpha (e.g., from .05 to .01), which makes a test more conservative. Raising alpha from .05 to .10 does the opposite — it liberalizes the criterion for rejection, increasing Type I error risk and reducing Type II error risk.",
        "B": "This is correct. Raising the significance level from .05 to .10 lowers the threshold needed to reject the null hypothesis, increasing sensitivity but also raising the probability of a Type I error (incorrectly rejecting a true null). Simultaneously, the wider rejection region reduces the probability of a Type II error (missing a true effect).",
        "C": "This is incorrect. Significance level is a decision criterion for interpreting test statistics and does not introduce systematic bias or affect internal validity. Internal validity concerns whether observed effects can be attributed to the independent variable, which is determined by study design, not alpha level.",
        "D": "This is incorrect. Effect size is a measure of the magnitude of a relationship or difference in the data and is calculated independently of alpha. While a larger effect size increases the probability of rejecting the null, simply changing alpha does not alter the observed effect size or its calculation."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-048-vignette-L3",
      "source_question_id": "048",
      "source_summary": "Changing the level of significance (alpha) from .05 to .10 increases the probability of making a Type I error and decreases the probability of making a Type II error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "power"
      ],
      "vignette": "A researcher studying the impact of mindfulness training on depressive symptoms is concerned that her study may lack sufficient power to detect a clinically meaningful difference between groups. Her sample size is fixed due to budget constraints, and she cannot increase the number of participants. A colleague suggests adjusting the decision threshold to improve the study's ability to detect true effects, noting that this adjustment will also affect another form of decision-making error. The researcher is skeptical, worried that the tradeoff might undermine the credibility of her findings.",
      "question": "If the researcher follows her colleague's suggestion by raising the decision threshold, what is the most accurate description of the resulting tradeoff?",
      "options": {
        "A": "The probability of detecting a true effect increases, and the probability of falsely detecting an effect that does not exist also increases.",
        "B": "The probability of detecting a true effect increases, and the probability of falsely detecting an effect that does not exist decreases.",
        "C": "The probability of detecting a true effect decreases, and the probability of a correct rejection increases due to greater stringency.",
        "D": "The probability of detecting a true effect remains unchanged because sample size is the only determinant of statistical power."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. Raising the decision threshold (i.e., increasing alpha) enlarges the rejection region, which increases statistical power — the probability of detecting a true effect (reducing Type II error). However, the same enlarged rejection region also increases the probability of falsely rejecting a true null hypothesis (Type I error). Both changes occur simultaneously as a direct consequence of raising alpha.",
        "B": "This is incorrect because it misidentifies the direction of one consequence. While power does increase (correctly stated), raising alpha increases — not decreases — the probability of falsely detecting a non-existent effect. The two error rates move in opposite directions, but both are affected.",
        "C": "This is incorrect on both counts. Raising the decision threshold does not decrease power; it increases it. Additionally, greater stringency (lower alpha) is associated with reduced Type I errors — raising alpha does the opposite and does not improve stringency.",
        "D": "This is incorrect. While sample size is a major determinant of statistical power, it is not the only one. Alpha level, effect size, and measurement reliability all influence power. Raising alpha directly increases power even without changing sample size."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-048-vignette-L4",
      "source_question_id": "048",
      "source_summary": "Changing the level of significance (alpha) from .05 to .10 increases the probability of making a Type I error and decreases the probability of making a Type II error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "rejection region"
      ],
      "vignette": "A neuropsychologist evaluating a cognitive rehabilitation intervention notes that prior studies in this area have used stringent decision criteria, resulting in several published failures to demonstrate benefit even when preliminary clinical data appeared promising. Believing that the field may be leaving effective treatments undiscovered, she argues for widening the rejection region in her upcoming trial. A biostatistics consultant agrees that this would reduce the number of undiscovered true effects but cautions that the credibility of any positive finding would be reduced. A colleague interprets this tradeoff as increasing the study's precision, but the biostatistician corrects this characterization.",
      "question": "What is the biostatistician most likely describing when cautioning that positive findings would have reduced credibility under the proposed change?",
      "options": {
        "A": "Widening the rejection region lowers the probability of missing a true effect but raises the probability of incorrectly concluding an effect exists when it does not.",
        "B": "Widening the rejection region improves precision by narrowing confidence intervals, but this makes the results more sensitive to sampling error.",
        "C": "Widening the rejection region increases both the probability of detecting true effects and the probability of correct null rejections simultaneously.",
        "D": "Widening the rejection region reduces statistical power by increasing the proportion of results that fall in the non-rejection zone."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. Widening the rejection region is equivalent to increasing alpha, which reduces the Type II error rate (fewer missed true effects, higher power) but raises the Type I error rate (greater probability of rejecting a true null hypothesis). The biostatistician's concern about credibility specifically references this increased risk of false positives — a direct consequence of raising alpha.",
        "B": "This is incorrect. Confidence interval width is primarily a function of sample size and variance, not alpha level per se (though alpha determines confidence level). More importantly, widening the rejection region does not improve precision — precision refers to the consistency of estimates, not the liberality of decision criteria. This option conflates two distinct statistical concepts.",
        "C": "This is incorrect because it asserts that both error-related outcomes improve simultaneously, which is not possible by changing alpha alone. Raising alpha improves detection of true effects (reduces Type II error) but simultaneously increases false positives (raises Type I error). The two cannot both improve through a simple alpha adjustment.",
        "D": "This is incorrect and reverses the actual relationship. Widening the rejection region (raising alpha) increases — not decreases — statistical power by making it easier to reject the null hypothesis. Reducing power would result from narrowing the rejection region (lowering alpha)."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-048-vignette-L5",
      "source_question_id": "048",
      "source_summary": "Changing the level of significance (alpha) from .05 to .10 increases the probability of making a Type I error and decreases the probability of making a Type II error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team has been using a strict standard for declaring that their intervention has an effect, and they have repeatedly concluded that their intervention produces no benefit despite observing moderate improvements in outcome measures. Frustrated by these repeated non-conclusions, a senior investigator proposes relaxing the standard so that smaller observed differences will be enough to declare that the intervention works. A junior researcher enthusiastically agrees, stating that the change will finally allow the team to identify real improvements. However, a methodologist on the team raises a concern: while the change does address the problem of missing real improvements, it simultaneously introduces a new problem — the team will now more frequently declare that the intervention works in situations where it genuinely does not.",
      "question": "What tradeoff is the methodologist describing?",
      "options": {
        "A": "Relaxing the standard increases the risk of concluding an effect exists when none does, while reducing the risk of missing an effect that is real.",
        "B": "Relaxing the standard decreases the risk of concluding an effect exists when none does, because a lower threshold produces more conservative results overall.",
        "C": "Relaxing the standard increases the risk of missing a real effect because smaller observed differences are less likely to reflect genuine population-level changes.",
        "D": "Relaxing the standard improves the accuracy of the effect size estimate, which in turn makes it less likely that any single study conclusion is incorrect."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. The methodologist is describing the tradeoff inherent in raising alpha (relaxing the decision threshold). A more lenient standard reduces the probability of missing true effects (Type II error decreases, power increases) but simultaneously increases the probability of incorrectly declaring an effect when none exists (Type I error increases). This is precisely the concern the methodologist raises.",
        "B": "This is incorrect and reverses the relationship. Relaxing the standard (raising alpha) makes the test more liberal, not more conservative. A lower threshold for declaring an effect increases — not decreases — the probability of false positives. More conservative results are associated with stricter, lower alpha values.",
        "C": "This is incorrect. Relaxing the standard (raising alpha) makes it easier to detect effects, thereby reducing the risk of missing real effects — the opposite of what this option states. Raising alpha increases power (reduces Type II error), which is why the junior researcher correctly identifies it as a solution to the team's pattern of non-conclusions.",
        "D": "This is incorrect. Effect size accuracy is determined by the quality of measurement and study design, not by the decision threshold used to declare significance. Changing the standard for declaring an effect does not alter the magnitude of the observed effect or improve the precision of effect size estimation."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-116-vignette-L1",
      "source_question_id": "116",
      "source_summary": "The appropriate bivariate correlation coefficient to use when the scores to be correlated are both reported as ranks is the Spearman rank-order correlation coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "ranks",
        "bivariate correlation",
        "ordinal"
      ],
      "vignette": "A researcher asks ten clinical supervisors to rank order ten practicum students from best to worst overall clinical performance. She then asks those same supervisors to rank order the same students from best to worst on empathy skills specifically. Both sets of scores are expressed as ranks on an ordinal scale. The researcher wants to compute a bivariate correlation between the two sets of ranked scores to determine whether supervisors' overall performance rankings are related to their empathy rankings.",
      "question": "Which correlation coefficient is most appropriate for this analysis?",
      "options": {
        "A": "Pearson product-moment correlation (r)",
        "B": "Spearman rank-order correlation (rₛ)",
        "C": "Point-biserial correlation (rpb)",
        "D": "Phi coefficient (φ)"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The Pearson r is appropriate when both variables are measured on continuous interval or ratio scales with approximately normal distributions. Because these scores are ordinal ranks rather than continuous measures, Pearson r is not the optimal choice here.",
        "B": "The Spearman rank-order correlation is specifically designed for situations in which both variables are expressed as ranks (ordinal data). It is the correct bivariate correlation when both sets of scores are rank-ordered, making it ideal for this scenario.",
        "C": "The point-biserial correlation is used when one variable is dichotomous (naturally or artificially binary) and the other is continuous. Neither variable here is dichotomous, so this coefficient does not apply.",
        "D": "The phi coefficient is used when both variables are dichotomous. In this study both variables consist of ordinal ranks across ten students, not binary categories, so phi is inappropriate."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-116-vignette-L2",
      "source_question_id": "116",
      "source_summary": "The appropriate bivariate correlation coefficient to use when the scores to be correlated are both reported as ranks is the Spearman rank-order correlation coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "ranked",
        "correlation"
      ],
      "vignette": "A sport psychology researcher is studying whether athletes who rank higher on mental toughness also rank higher in competitive performance outcomes. She collects data from 15 collegiate swimmers, asking three expert coaches to collaboratively assign each swimmer a single mental-toughness rank (1 = highest, 15 = lowest) and a separate competitive-performance rank based on season outcomes. One swimmer is noted to have an unusually high mental-toughness rank but a surprisingly low performance rank, which the researcher flags as a potential outlier. The researcher now wants to determine the appropriate statistical method to examine the association between the two ranked variables.",
      "question": "Which statistical approach is most appropriate for examining the association between the two sets of ranked scores?",
      "options": {
        "A": "Pearson product-moment correlation",
        "B": "Kendall's tau",
        "C": "Spearman rank-order correlation",
        "D": "Simple linear regression"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "When both variables consist of ordinal rank data, the Spearman rank-order correlation is the standard bivariate measure of association. It is robust to the influence of extreme ranks (such as the flagged outlier) and does not require interval-level measurement or normality assumptions.",
        "A": "Pearson r requires continuous, interval- or ratio-level data with approximately normal distributions. Because both variables here are ordinal ranks rather than continuous scores, Pearson r violates its measurement assumptions and is not optimal.",
        "B": "Kendall's tau is also a nonparametric correlation for ranked data and can be appropriate, but Spearman's rₛ is the more widely cited and standard choice for bivariate correlation of ranks on the EPPP. Kendall's tau is preferred in specific situations (e.g., small samples with many tied ranks), but Spearman rₛ is the primary answer taught for this scenario.",
        "D": "Simple linear regression quantifies prediction of one continuous variable from another and requires interval- or ratio-level measurement on both variables. It is not designed to assess association between two sets of ordinal ranks and would be inappropriate here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-116-vignette-L3",
      "source_question_id": "116",
      "source_summary": "The appropriate bivariate correlation coefficient to use when the scores to be correlated are both reported as ranks is the Spearman rank-order correlation coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "ordinal"
      ],
      "vignette": "A hospital psychologist is studying the relationship between nurses' empathy and patient satisfaction. To avoid assumptions about the scale properties of their measurement tools, the research team converts all raw scores to ordered positions within the sample before analysis. The empathy tool yields scores that nurses themselves have questioned regarding equal intervals between scale points, and the patient satisfaction scores are derived from a brief Likert-type survey. The psychologist notes that the data do not appear to follow a normal distribution and considers using a nonparametric approach. A colleague suggests using the most straightforward bivariate association measure for this type of ordinal data.",
      "question": "Which statistical method is the colleague most likely recommending?",
      "options": {
        "A": "Pearson product-moment correlation",
        "B": "Spearman rank-order correlation",
        "C": "Biserial correlation",
        "D": "Kendall's tau-b"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Because both variables have been converted to ordered positions (ranks) and the measurement scales are ordinal at best, the Spearman rank-order correlation is the standard nonparametric bivariate coefficient. It directly operates on ranked data and is the most commonly referenced choice for this situation in research methods coursework and the EPPP.",
        "A": "Pearson r is a parametric coefficient requiring interval- or ratio-level data and approximately normal distributions. The team specifically converted scores to ranks to avoid such assumptions, making Pearson r inappropriate and contrary to the stated analytic rationale.",
        "C": "The biserial correlation is used when one variable is continuous and the other is artificially dichotomized from an underlying continuous distribution. Neither variable here is dichotomous or artificially collapsed, so biserial correlation does not apply to this design.",
        "D": "Kendall's tau-b is also a nonparametric correlation suitable for ordinal data and would not be incorrect in principle. However, Spearman rₛ is the canonical, most frequently taught answer for straightforward bivariate correlation of two sets of ranks, and the colleague's description of the 'most straightforward' measure points specifically to Spearman rₛ."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-116-vignette-L4",
      "source_question_id": "116",
      "source_summary": "The appropriate bivariate correlation coefficient to use when the scores to be correlated are both reported as ranks is the Spearman rank-order correlation coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "monotonic"
      ],
      "vignette": "A researcher in a rehabilitation center wants to assess whether occupational therapists' subjective ordering of patients by functional independence relates to those patients' ordering by the interdisciplinary team on recovery progress. The center's statistician notes that the relationship between the two sets of scores appears monotonic but not linear in the scatterplot, and that the measurement tools used do not guarantee equal-interval properties. The statistician also notes a handful of tied values in both variables and mentions that a correction formula can accommodate them. The research team has continuous raw scores on file but deliberately chooses not to use them.",
      "question": "The statistician's description best supports the use of which statistical method?",
      "options": {
        "A": "Pearson product-moment correlation with a nonlinearity correction",
        "B": "Spearman rank-order correlation",
        "C": "Kendall's tau-b",
        "D": "Eta coefficient (η)"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "The Spearman rank-order correlation captures monotonic (not necessarily linear) associations between two variables measured on ordinal scales. It accommodates tied ranks via a correction formula, and the deliberate decision to use ordered positions rather than raw continuous scores is precisely the situation for which Spearman rₛ was designed.",
        "A": "Although Pearson r can technically be applied to ranked data, doing so yields the Spearman coefficient mathematically. The mention of a nonlinearity 'correction' for Pearson r is not a standard psychometric procedure, and the team's explicit choice to avoid continuous scores further disqualifies a Pearson-based approach.",
        "C": "Kendall's tau-b is a legitimate nonparametric association measure for ranked data with ties and is theoretically defensible here. However, Spearman rₛ is the standard answer for bivariate rank correlation in EPPP contexts, and the statistician's description of ordered positions and a tie-correction formula more directly maps onto Spearman's established framework.",
        "D": "Eta (η) measures the strength of a curvilinear or nonlinear relationship between a categorical independent variable and a continuous dependent variable. Although the scatterplot is described as nonlinear (red herring), the research question involves two ordinal ordered variables — not a categorical and continuous pair — so eta is inappropriate."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-116-vignette-L5",
      "source_question_id": "116",
      "source_summary": "The appropriate bivariate correlation coefficient to use when the scores to be correlated are both reported as ranks is the Spearman rank-order correlation coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "Two independent panels of experts each evaluated the same set of twelve candidates for a competitive fellowship program. Each panel placed the candidates in sequence from most to least promising without assigning numeric scores, producing only an ordered list. A methodologist examining the two lists wants to determine how consistently the panels agreed in their relative judgments. She notes that the distribution of the underlying judgments cannot be assumed to follow any particular shape, that a few candidates received identical placements from both panels, and that equal gaps between successive positions are not guaranteed. She is not interested in predicting one panel's sequence from the other — only in whether the two sequences tend to move together.",
      "question": "Which analytic approach most directly addresses the methodologist's question?",
      "options": {
        "A": "Simple linear regression",
        "B": "Pearson product-moment correlation",
        "C": "Spearman rank-order correlation",
        "D": "Intraclass correlation coefficient"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "The Spearman rank-order correlation is specifically designed to quantify the degree of agreement between two sets of ordinal rankings when no interval-level measurement or normality assumption is warranted. The scenario describes exactly two ordered lists (ranks), no interest in prediction, and conditions (tied positions, non-normal distribution, unequal intervals) that rule out parametric alternatives — the defining conditions for Spearman rₛ.",
        "A": "Simple linear regression addresses prediction of one variable from another and requires continuous, interval-level outcome data. The methodologist explicitly states she is not interested in prediction, and the data are ordinal position lists — not continuous scores — making regression inappropriate on both grounds.",
        "B": "Pearson r requires interval- or ratio-level continuous data with approximately normal distributions and assumes equal intervals between values. The scenario notes that equal gaps between positions are not guaranteed and that normality cannot be assumed, conditions that exclude Pearson r despite the surface similarity to a bivariate association question.",
        "D": "The intraclass correlation coefficient (ICC) measures agreement or consistency among raters when scores are on a continuous, interval-level scale (e.g., multiple raters each assigning numeric scores). Although ICC is used for inter-rater agreement — a concept that might seem relevant here — the data are purely ordinal position lists, not continuous ratings, so ICC's parametric assumptions are violated."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-058-vignette-L1",
      "source_question_id": "058",
      "source_summary": "In a negatively skewed distribution of scores, the mean is the lowest score and the mode is the highest score.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "negatively skewed",
        "mean",
        "mode"
      ],
      "vignette": "A researcher administers a final exam to a group of advanced graduate students who have extensively prepared for the test. When the scores are compiled and analyzed, the distribution is found to be negatively skewed, with most students earning very high scores and only a few scoring much lower. The researcher notes that the mean falls below the mode in the resulting distribution. She wants to describe the relative positions of the central tendency measures to her colleagues.",
      "question": "Which of the following best describes the relationship between the mean and mode in a negatively skewed distribution?",
      "options": {
        "A": "The mean and mode are equal, as skew does not affect central tendency measures.",
        "B": "The mean is higher than the mode because extreme high scores pull the mean upward.",
        "C": "The mean is lower than the mode because extreme low scores pull the mean downward.",
        "D": "The mode is lower than the mean because the bulk of scores are concentrated at the low end."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. Skew directly affects the mean by pulling it toward the tail, but the mode — the most frequently occurring value — remains near the peak of the distribution. Skew therefore creates a measurable difference between these measures of central tendency.",
        "B": "Incorrect. This describes a positively skewed distribution, where a long tail of extreme high scores pulls the mean above both the median and mode. In a negatively skewed distribution, the tail extends in the opposite direction — toward low scores.",
        "C": "Correct. In a negatively skewed distribution, the tail extends toward lower values. These extreme low scores pull the mean downward, making it the lowest of the three measures of central tendency, while the mode — representing the most frequently occurring score — remains at the high end of the distribution.",
        "D": "Incorrect. In a negatively skewed distribution, the bulk of scores are concentrated at the high end, not the low end. It is the mean — not the mode — that is pulled downward by the few extreme low scores in the left tail."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-058-vignette-L2",
      "source_question_id": "058",
      "source_summary": "In a negatively skewed distribution of scores, the mean is the lowest score and the mode is the highest score.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "skewed",
        "distribution"
      ],
      "vignette": "A clinical psychologist administers a well-known cognitive assessment to a sample of highly educated adults over age 65, many of whom are experienced with standardized testing. The resulting score distribution is skewed, with a long tail extending toward the lower end of the scale. The psychologist notices that the most frequently obtained score is noticeably higher than the arithmetic average of all scores. Despite the sample being older — a factor sometimes associated with lower performance — the majority of participants performed near the ceiling of the measure.",
      "question": "Based on the described distribution, which statement most accurately characterizes the relationship between the measures of central tendency?",
      "options": {
        "A": "The median is lower than both the mean and mode, reflecting the influence of the high-performing majority.",
        "B": "The mean is the lowest measure of central tendency, pulled downward by the few very low scores.",
        "C": "The mean and median are approximately equal because the distribution is roughly symmetric despite the tail.",
        "D": "The mode is the lowest measure of central tendency because most scores cluster at the bottom of the distribution."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. In a negatively skewed distribution, the median falls between the mean and mode — it is not lower than both. The median is resistant to extreme scores, so it is pulled less than the mean but more than the mode away from the peak.",
        "B": "Correct. The distribution described has a tail extending toward low scores, making it negatively skewed. The few extreme low scores pull the mean downward more than they affect the median or mode, so the mean becomes the lowest measure of central tendency while the mode — the most frequent score — remains at the high end.",
        "C": "Incorrect. Equal mean and median characterize a symmetric, normal distribution. The scenario explicitly describes a tail extending toward low scores, indicating asymmetry that separates the mean, median, and mode in the order: mean < median < mode.",
        "D": "Incorrect. In a negatively skewed distribution, scores cluster at the high end, meaning the mode — the most frequently occurring value — is the highest, not the lowest, measure of central tendency. The mode is pulled toward the low end only in a positively skewed distribution."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-058-vignette-L3",
      "source_question_id": "058",
      "source_summary": "In a negatively skewed distribution of scores, the mean is the lowest score and the mode is the highest score.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "tail"
      ],
      "vignette": "A researcher studying pain tolerance recruits athletes who train daily and rarely report significant discomfort. After administering a standardized pain threshold assessment, she finds that the vast majority of participants score near the upper boundary of the scale, with only a handful of athletes reporting unusually low thresholds. The distribution's tail extends substantially to the left. Interestingly, the researcher initially focuses on the fact that the sample's average score is lower than she expected given the participants' high fitness levels, and she wonders if the average truly reflects the typical athlete in her sample.",
      "question": "The researcher's concern about the average score being lower than expected is best explained by which of the following statistical properties?",
      "options": {
        "A": "Positive skew, in which a few extreme high scores inflate the mean above the modal value.",
        "B": "Kurtosis, in which the peakedness of the distribution affects how representative the mean is of the central scores.",
        "C": "Negative skew, in which a few extreme low scores depress the mean below the modal value.",
        "D": "Bimodal distribution, in which two distinct subgroups create a mean that falls between the two peaks and is unrepresentative of either group."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. Positive skew features a tail extending toward high scores, which would inflate the mean above the mode — the opposite of what is described. Here, the tail extends to the left (toward low scores), and the mean is lower than expected, not inflated.",
        "B": "Incorrect. Kurtosis refers to the degree of peakedness or flatness of a distribution and can affect how representative the mean is in a general sense. However, it does not specifically explain why a mean is pulled lower than the modal score; that is characteristic of skewness, not kurtosis.",
        "C": "Correct. The scenario describes a distribution whose tail extends to the left (low end), with most participants scoring near the top of the scale and only a few scoring very low. This is negative skew. The few extreme low scores pull the mean downward below the modal value, which explains why the researcher finds the average lower than the typical athlete's performance would suggest.",
        "D": "Incorrect. A bimodal distribution would show two distinct peaks, suggesting two subpopulations. While this could also produce a mean unrepresentative of either group, the scenario describes a single concentration of scores near the ceiling with only a few low outliers — consistent with unimodal negative skew, not bimodality."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-058-vignette-L4",
      "source_question_id": "058",
      "source_summary": "In a negatively skewed distribution of scores, the mean is the lowest score and the mode is the highest score.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "outliers"
      ],
      "vignette": "A health psychologist examines self-reported hours of weekly exercise in a sample of competitive triathletes. Nearly all participants report training between 15 and 20 hours per week, while a small number report fewer than 3 hours due to recent injuries. The researcher computes several summary statistics and finds that one particular measure of central tendency yields a value notably lower than the value that appears most often in the dataset. She is troubled because this discrepancy makes her summary statistic appear to underrepresent the sample's typical behavior, and a colleague suggests the outliers on the low end are responsible.",
      "question": "The colleague's explanation is most consistent with which of the following properties of the dataset?",
      "options": {
        "A": "Positive skew, where outliers on the high end elevate the mean above the median and mode.",
        "B": "Negative skew, where outliers on the low end depress the mean below the mode.",
        "C": "High kurtosis, where a heavy-tailed distribution inflates the standard deviation and makes the mean appear unrepresentative.",
        "D": "Regression to the mean, where participants with extreme initial scores tend to score closer to the group mean on subsequent assessments."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Positive skew would involve outliers at the high end inflating the mean above the mode, making the mean an overestimate of typical performance. In this scenario, the mean is lower than the modal value — the opposite pattern — indicating the distortion is caused by low-end, not high-end, outliers.",
        "B": "Correct. The scenario describes a distribution where most scores cluster at the high end (15–20 hours) and a few outliers fall at the extreme low end (fewer than 3 hours due to injury). This produces negative skew: the tail extends toward low scores, the few low-end outliers depress the mean, and the mean falls below the mode — precisely the discrepancy the researcher observes.",
        "C": "Incorrect. High kurtosis (leptokurtosis) refers to a distribution with heavier tails and a sharper peak than the normal distribution, which inflates the standard deviation and affects variability estimates. While it can make the mean less representative in a general sense, it does not specifically cause the mean to fall below the mode; that directional discrepancy is a feature of skewness.",
        "D": "Incorrect. Regression to the mean is a phenomenon where extreme scores on one measurement occasion tend to be less extreme on a subsequent occasion, due to random measurement error. It describes change across time points and has no bearing on why a single distribution's mean falls below its mode at one measurement occasion."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-058-vignette-L5",
      "source_question_id": "058",
      "source_summary": "In a negatively skewed distribution of scores, the mean is the lowest score and the mode is the highest score.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A university administrator reviews end-of-semester evaluation scores submitted by students rating their instructor. The overwhelming majority of students assign the highest possible rating, and a small cluster of students — later identified as those who failed the course — give the lowest possible ratings. When the administrator's assistant computes a simple arithmetic summary of all ratings, the resulting number is noticeably lower than the value that appeared most often in the data. The assistant, concerned the summary value misrepresents the instructor's actual standing, argues that the handful of very unhappy students is disproportionately pulling the summary figure in one direction. A faculty ombudsperson reviewing the same data suggests the discrepancy actually reflects a well-known and expected property of datasets shaped like this one.",
      "question": "Which of the following properties is the ombudsperson most likely referencing to explain the discrepancy between the arithmetic summary and the most frequently occurring value?",
      "options": {
        "A": "Positive skew, in which a cluster of unusually high values inflates the arithmetic summary above the most common value.",
        "B": "Negative skew, in which a cluster of unusually low values depresses the arithmetic summary below the most common value.",
        "C": "High variability, in which a large spread of scores around the center causes the arithmetic summary to be pulled away from the peak of the distribution.",
        "D": "Bimodal distribution, in which two distinct groups — satisfied and dissatisfied students — create an arithmetic summary that falls between the two peaks and does not represent either group well."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Positive skew occurs when a small number of extremely high values pull the arithmetic summary (mean) upward above the most common value (mode). In this scenario, the distortion is in the opposite direction — the arithmetic summary is lower than the most frequent value — ruling out positive skew.",
        "B": "Correct. The dataset described has the large majority of scores at the high end (most students giving top ratings) and only a few scores at the low end (failing students giving bottom ratings). This produces a negatively skewed distribution, in which the tail extends toward low values and the small cluster of low scores depresses the mean below the mode. This is precisely the 'well-known and expected property' the ombudsperson references.",
        "C": "Incorrect. High variability (large standard deviation or variance) indicates wide spread of scores but does not explain a directional discrepancy between the mean and mode. High variability could occur in a symmetric distribution where the mean and mode are equal; it does not, on its own, cause the mean to fall systematically below the mode.",
        "D": "Incorrect. A bimodal distribution would suggest two distinct peaks — one for satisfied students and one for dissatisfied students — with the mean falling between them. While this scenario does involve two identifiable groups, the description emphasizes that the vast majority cluster at one extreme and only a small minority are at the other, which is characteristic of a unimodal negatively skewed distribution rather than a true bimodal one."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-102-vignette-L1",
      "source_question_id": "102",
      "source_summary": "A two-way ANOVA is the appropriate statistical test to analyze the main and interaction effects of treatment and condition on systolic blood pressure.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "two-way ANOVA",
        "interaction effect",
        "main effect"
      ],
      "vignette": "A researcher designs a study to examine whether two independent variables — medication type (Drug A vs. Drug B) and stress condition (high vs. low) — affect systolic blood pressure in hypertensive patients. She is specifically interested in whether the main effect of each factor is significant and whether there is an interaction effect between them. The study includes equal cell sizes and normally distributed data. The researcher wants to use a single statistical test that can evaluate all of these effects simultaneously using a two-way ANOVA framework.",
      "question": "Which statistical test is most appropriate for analyzing the simultaneous main effects and interaction effect of two independent variables on systolic blood pressure?",
      "options": {
        "A": "One-way ANOVA",
        "B": "Two-way ANOVA",
        "C": "Independent samples t-test",
        "D": "Repeated measures ANOVA"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A one-way ANOVA tests the effect of a single independent variable on a continuous dependent variable. It cannot evaluate two independent variables simultaneously nor detect an interaction effect between them.",
        "B": "A two-way ANOVA is specifically designed to analyze the independent (main) effects of two categorical independent variables as well as their interaction effect on a continuous dependent variable such as systolic blood pressure. This is the correct answer.",
        "C": "An independent samples t-test compares the means of two groups on one dependent variable using one independent variable with only two levels. It cannot handle two independent variables or interaction effects.",
        "D": "A repeated measures ANOVA is used when the same participants are measured across multiple time points or conditions, addressing within-subject variance. This design does not describe repeated measurements on the same participants across both factors."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-102-vignette-L2",
      "source_question_id": "102",
      "source_summary": "A two-way ANOVA is the appropriate statistical test to analyze the main and interaction effects of treatment and condition on systolic blood pressure.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "factorial design",
        "interaction"
      ],
      "vignette": "A clinical researcher recruits 120 adults with hypertension, many of whom also have comorbid anxiety disorder. Participants are randomly assigned to one of two treatment conditions (behavioral intervention vs. pharmacological treatment) and one of two exercise regimens (aerobic vs. resistance). Systolic blood pressure is measured at the end of an eight-week period. The researcher uses a factorial design and wants to determine whether treatment type, exercise regimen, or their interaction explains variance in blood pressure outcomes.",
      "question": "Which statistical test should the researcher use to evaluate both independent effects and their combined interaction on systolic blood pressure?",
      "options": {
        "A": "Pearson correlation",
        "B": "One-way ANOVA",
        "C": "Two-way ANOVA",
        "D": "Multiple regression"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Pearson correlation measures the linear association between two continuous variables. It is not designed to test group mean differences or interaction effects between categorical independent variables.",
        "B": "A one-way ANOVA can evaluate the effect of a single categorical independent variable on a continuous outcome. This study includes two independent variables (treatment and exercise) and a desired test of their interaction, making a one-way ANOVA insufficient.",
        "C": "A two-way ANOVA is appropriate when a researcher has two categorical independent variables in a factorial design and wants to examine their individual main effects and their interaction effect on a continuous dependent variable. This matches the described study exactly.",
        "D": "Multiple regression can model the effects of multiple predictors on a continuous outcome and can include interaction terms, but it is typically used with continuous predictors. While it can handle categorical variables with dummy coding, the standard approach for factorial group designs testing main effects and interactions is the two-way ANOVA."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-102-vignette-L3",
      "source_question_id": "102",
      "source_summary": "A two-way ANOVA is the appropriate statistical test to analyze the main and interaction effects of treatment and condition on systolic blood pressure.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A psychophysiology researcher recruits participants from two clinical sites and assigns them to receive either mindfulness-based stress reduction or a waitlist control. She also categorizes participants by biological sex. She notices that female participants seem to respond more strongly to the mindfulness intervention than male participants, suggesting the effect may differ across groups. She measures systolic blood pressure at post-treatment and plans to analyze the sources of variance in the data, including whether the two grouping variables jointly account for the pattern she observed. Each cell has approximately equal sample sizes and the data meet parametric assumptions.",
      "question": "What is the most appropriate statistical test for the researcher to use to evaluate the independent and combined effects of intervention and biological sex on systolic blood pressure?",
      "options": {
        "A": "Two-way ANOVA",
        "B": "MANOVA",
        "C": "Analysis of covariance (ANCOVA)",
        "D": "Independent samples t-test"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "A two-way ANOVA is the correct choice. It tests the main effects of two categorical independent variables (intervention type and biological sex) and their interaction — precisely what is needed when the researcher suspects the effect of one variable changes across levels of the other.",
        "B": "MANOVA (Multivariate Analysis of Variance) is used when there are two or more dependent variables measured simultaneously. This study has only one dependent variable (systolic blood pressure), so MANOVA is unnecessarily complex and not the appropriate choice.",
        "C": "ANCOVA (Analysis of Covariance) is used to statistically control for a continuous covariate while testing group differences on a dependent variable. No covariate is mentioned in this study, and the researcher's goal is to examine group differences and their interaction, not to control for a confounding variable.",
        "D": "An independent samples t-test compares two group means on a single dependent variable using one binary independent variable. It cannot simultaneously evaluate two independent variables or detect an interaction effect between them."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-102-vignette-L4",
      "source_question_id": "102",
      "source_summary": "A two-way ANOVA is the appropriate statistical test to analyze the main and interaction effects of treatment and condition on systolic blood pressure.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "cell"
      ],
      "vignette": "A researcher studying cardiovascular health randomly assigns 200 participants to one of four groups formed by crossing two variables: a dietary intervention (Mediterranean vs. standard diet) and a psychological treatment (cognitive restructuring vs. no treatment). After 12 weeks, systolic blood pressure is recorded for all participants. Inspection of group means suggests that cognitive restructuring reduces blood pressure more substantially under the Mediterranean diet condition than under the standard diet — a pattern the researcher wants to formally test. She notes that each cell contains 50 participants and that her outcome data are continuous and approximately normally distributed.",
      "question": "Which statistical test will allow the researcher to formally evaluate whether the pattern she observed in the cell means reflects a statistically significant finding?",
      "options": {
        "A": "One-way ANOVA with post-hoc comparisons",
        "B": "Two-way ANOVA",
        "C": "Hierarchical multiple regression",
        "D": "Repeated measures ANOVA"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A one-way ANOVA with post-hoc comparisons can identify which groups differ from one another but treats the four groups as levels of a single variable. It cannot formally partition variance into two main effects and an interaction term, and therefore cannot test the differential pattern across two independent variables that the researcher describes.",
        "B": "A two-way ANOVA is the correct answer. The design crosses two independent variables (diet and psychological treatment), creating four cells. The pattern the researcher observed — the effect of one variable differing across levels of the other — is precisely the definition of an interaction effect, which a two-way ANOVA is designed to formally test alongside the two main effects.",
        "C": "Hierarchical multiple regression models predictors entered in sequential blocks and can test interaction terms via product terms, but it is most appropriate when predictors are continuous or when a researcher wants to control for theoretically ordered blocks of variables. For a fully crossed experimental design with categorical grouping variables, a two-way ANOVA is the standard and more interpretable approach.",
        "D": "Repeated measures ANOVA is used when participants are measured more than once — typically across time or conditions — and the goal is to account for within-subject correlations. Here, participants are assigned to mutually exclusive groups and measured only once, making repeated measures ANOVA inappropriate."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-102-vignette-L5",
      "source_question_id": "102",
      "source_summary": "A two-way ANOVA is the appropriate statistical test to analyze the main and interaction effects of treatment and condition on systolic blood pressure.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A health researcher creates four distinct participant groups by simultaneously varying which of two lifestyle programs people are assigned to and whether they receive an additional weekly counseling session. At the end of three months, she records a single physiological reading from each participant. When she graphs the group averages, she notices that adding counseling appears to benefit one lifestyle program's participants substantially more than the other's — a diverging pattern across the four groups. She wants to determine whether both program membership and counseling status independently account for differences in her physiological measure, and also whether the diverging pattern she observed is statistically reliable. Her data are continuous, roughly bell-shaped, and each of the four groups has the same number of participants.",
      "question": "Which statistical procedure is most appropriate to address all three of the researcher's analytical goals?",
      "options": {
        "A": "One-way analysis comparing all four groups with planned contrasts",
        "B": "Two-way analysis of variance",
        "C": "Multivariate analysis of variance",
        "D": "Analysis of covariance"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A one-way analysis comparing all four groups treats the design as a single grouping factor with four levels. While planned contrasts can compare specific pairs, this approach cannot separately partition the variance attributable to each of the two design factors nor formally test the statistical reliability of the diverging pattern (i.e., the interaction), which requires a factorial framework.",
        "B": "A two-way ANOVA is the correct procedure. The researcher has two categorical independent variables (lifestyle program and counseling status) crossed to create four groups. She wants to assess the independent effect of each variable (two main effects) and whether the effect of one variable differs across levels of the other (interaction effect). These are precisely the three tests provided by a two-way ANOVA, and her data meet its parametric assumptions.",
        "C": "A multivariate analysis of variance is designed for situations in which the researcher has multiple dependent variables measured simultaneously. The researcher records only a single physiological outcome per participant, so there is no multivariate outcome structure to justify MANOVA; applying it here would be unnecessarily complex and analytically mismatched.",
        "D": "An analysis of covariance is used to statistically adjust for a continuous variable that may confound group comparisons, thereby improving precision in estimating group differences. No confounding covariate is described in this study, and the researcher's three goals all concern between-group differences and their interaction rather than covariate adjustment."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-036-vignette-L1",
      "source_question_id": "036",
      "source_summary": "When there are statistically significant main and interaction effects, interpreting the main effects without considering the interaction can lead to erroneous conclusions.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "ANOVA",
        "interaction",
        "main effect"
      ],
      "vignette": "A researcher conducts a 2×2 factorial ANOVA examining the effects of therapy type (CBT vs. medication) and severity (mild vs. severe) on depression scores. Results reveal significant main effects for both therapy type and severity, as well as a significant interaction between the two factors. When a colleague reports only the main effects in the study summary, the principal investigator cautions that doing so will produce misleading conclusions about the data.",
      "question": "Which statistical principle best explains the principal investigator's caution?",
      "options": {
        "A": "When a significant interaction is present, interpreting main effects in isolation can be misleading because the effect of one factor depends on the level of the other factor.",
        "B": "A significant main effect always overrides a significant interaction, so the interaction should be reported separately and given less interpretive weight.",
        "C": "Reporting main effects without the interaction inflates the risk of a Type II error by obscuring true null relationships among variables.",
        "D": "When both main effects are significant, the interaction is redundant and adds no additional interpretive value to the findings."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. In factorial ANOVA, a significant interaction indicates that the effect of one independent variable differs across levels of the other. Interpreting main effects alone glosses over this conditional relationship and can yield erroneous, overly simplified conclusions.",
        "B": "This is incorrect. Statistical convention holds the opposite: when an interaction is significant, interpretation of main effects must be qualified by the interaction, not the reverse. Main effects do not override interactions.",
        "C": "This is incorrect. Type II error refers to failing to reject a false null hypothesis (a false negative), which is a power-related issue. Reporting main effects without acknowledging the interaction does not directly alter the probability of Type II error; it is an interpretive problem, not a power problem.",
        "D": "This is incorrect. When a significant interaction exists, it is not redundant with the main effects — it conveys unique, essential information about how the factors jointly influence the outcome. Omitting it misrepresents the data structure."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-036-vignette-L2",
      "source_question_id": "036",
      "source_summary": "When there are statistically significant main and interaction effects, interpreting the main effects without considering the interaction can lead to erroneous conclusions.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "factorial",
        "interaction effect"
      ],
      "vignette": "A clinical researcher uses a factorial design to study whether a new mindfulness intervention reduces anxiety differently for men and women. Participants are randomly assigned to mindfulness or waitlist control conditions. The researcher, who is also treating some participants clinically, notes that both the intervention main effect and the gender main effect reach statistical significance, and she reports these findings at a conference without discussing the significant interaction effect. A statistician in the audience raises a concern about the validity of her conclusions.",
      "question": "What is the most likely basis for the statistician's concern?",
      "options": {
        "A": "Ignoring the significant interaction effect means the researcher may be drawing inaccurate conclusions from the main effects, since the intervention's impact likely differs by gender.",
        "B": "The researcher's dual role as clinician and investigator introduces a confound that undermines the internal validity of the main effect findings.",
        "C": "Presenting main effects from a factorial design at a conference without replication constitutes a threat to external validity.",
        "D": "The significant main effects are statistically uninterpretable unless the interaction effect is first removed from the model as a source of variance."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. When a significant interaction is present alongside significant main effects, the main effects must be interpreted conditionally. The statistician's concern is that presenting only the main effects implies a uniform effect of the intervention across gender, which the interaction directly contradicts.",
        "B": "This is incorrect. While dual-role conflicts of interest raise ethical concerns and can introduce bias, they are not the basis for a statistical objection about interpreting main effects versus interactions. The statistician's concern is interpretive, not about confounding from the researcher's role.",
        "C": "This is incorrect. External validity concerns generalizability of findings beyond the study sample, not the proper statistical interpretation of factorial results. Presenting main effects without an interaction is an interpretive error, not a generalizability problem.",
        "D": "This is incorrect. Main effects are not statistically uninterpretable when an interaction is present — they remain valid numerical estimates. The issue is that interpreting them as context-free summaries is substantively misleading, not that they are mathematically invalid or need to be removed."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-036-vignette-L3",
      "source_question_id": "036",
      "source_summary": "When there are statistically significant main and interaction effects, interpreting the main effects without considering the interaction can lead to erroneous conclusions.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "ANOVA"
      ],
      "vignette": "A psychologist publishes a study using a 2×2 between-subjects design to examine whether cognitive training improves memory performance across two age groups: younger adults and older adults. The ANOVA output shows that cognitive training yields a large, significant improvement overall, and that older adults score significantly lower on memory overall. The psychologist concludes that cognitive training is broadly effective and recommends it universally. However, the supplemental data table — which most readers overlook — shows that younger adults dramatically improved with training while older adults showed almost no benefit.",
      "question": "What critical error did the psychologist make in drawing conclusions from this study?",
      "options": {
        "A": "The psychologist over-relied on effect size rather than statistical significance, leading to an inflated sense of the training's utility across age groups.",
        "B": "The psychologist interpreted the main effects as if they applied uniformly across groups, failing to account for the significant interaction between training and age group.",
        "C": "The psychologist committed a Type I error by treating two separate significant effects as though they jointly confirmed the same hypothesis.",
        "D": "The psychologist violated the assumption of homogeneity of variance by combining younger and older adults, which invalidates both main effect conclusions."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The supplemental data reveals that the training effect is highly discrepant across age groups — a clear interaction. The psychologist's error was treating the significant main effect for training as a universal finding, when the interaction shows training benefits only younger adults. This is precisely the interpretive error that arises from ignoring a significant interaction.",
        "A": "This is incorrect. The question does not indicate that effect size was used inappropriately or that significance was ignored. The error is not about choosing the wrong metric but about drawing broad conclusions from main effects when an interaction qualifies those conclusions.",
        "C": "This is incorrect. Type I error is a false positive — concluding an effect exists when it does not. The psychologist did not fabricate results; rather, they drew an overly broad interpretation of real results. The error is conceptual and interpretive, not one of inflated alpha.",
        "D": "This is incorrect. Violation of homogeneity of variance is a statistical assumption concern that can affect the validity of F-test results. However, the scenario does not indicate such a violation was present or tested, and the core error described is one of over-interpreting main effects without considering the interaction."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-036-vignette-L4",
      "source_question_id": "036",
      "source_summary": "When there are statistically significant main and interaction effects, interpreting the main effects without considering the interaction can lead to erroneous conclusions.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "moderates"
      ],
      "vignette": "At a research team meeting, a biostatistician reviews a completed two-factor between-subjects study comparing the effectiveness of two antidepressant medications across patients with and without a comorbid anxiety disorder. The team's written report emphasizes that Medication A outperformed Medication B on average and that patients with comorbid anxiety fared worse on average — both well-supported by the data. The biostatistician notes that the report is technically accurate as written, yet still fundamentally misleading, and requests a revision. A closer look at cell means reveals that Medication A was strongly superior for patients without comorbid anxiety but nearly equivalent to Medication B for those with it, while Medication B showed a slight edge for patients with comorbid anxiety.",
      "question": "What does the biostatistician most likely believe the report failed to adequately convey?",
      "options": {
        "A": "The report failed to acknowledge that a variable moderates the relationship between medication type and outcome, rendering the overall comparison between medications misleading without this qualification.",
        "B": "The report failed to control for the comorbid anxiety variable as a covariate, which inflated the apparent main effect for medication type and introduced a confound.",
        "C": "The report failed to distinguish between statistical significance and clinical significance, potentially overstating the practical importance of the medication main effect.",
        "D": "The report failed to use a repeated-measures design, which would have provided greater power to detect true medication differences across the comorbidity groups."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. The cell means reveal a crossover-like pattern indicating that the comorbid anxiety variable moderates the effect of medication on outcome — precisely what an interaction effect captures. Reporting only that Medication A outperformed Medication B on average is misleading because this advantage does not hold across both comorbidity groups. The biostatistician's concern is that the main effect conclusion is conditionally false.",
        "B": "This is incorrect. Using comorbid anxiety as a covariate (ANCOVA) would remove its variance from the model, which is appropriate when it is a nuisance variable — not when it is a theoretically important moderator. Treating it as a covariate would actually suppress the interaction, further obscuring the very pattern the biostatistician wants reported.",
        "C": "This is incorrect. The distinction between statistical and clinical significance is an important methodological concern, but it does not explain why the report is misleading about which medication is better. The problem is not the magnitude of the effect but the direction and consistency of the effect across subgroups.",
        "D": "This is incorrect. A repeated-measures design would be appropriate when the same participants are measured under multiple conditions. This is a between-subjects design with different patients in each cell, and switching to repeated measures is neither justified by the scenario nor relevant to the interpretive error the biostatistician identified."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-036-vignette-L5",
      "source_question_id": "036",
      "source_summary": "When there are statistically significant main and interaction effects, interpreting the main effects without considering the interaction can lead to erroneous conclusions.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team publishes a summary stating that Treatment X, on average, outperforms Treatment Y, and that Subgroup 1 consistently achieves better outcomes than Subgroup 2 regardless of treatment. Both of these summary statements are mathematically accurate when looking at the full dataset. A methodologist reviewing the paper argues that, despite the accuracy of each individual statement, the clinical guidance derived from the summary is dangerously wrong. Upon reviewing the underlying data broken down by subgroup and treatment, it becomes apparent that Treatment Y actually produces better outcomes specifically within Subgroup 2, even though Treatment X's overall average is pulled upward by its dramatic superiority in Subgroup 1.",
      "question": "Which methodological principle most precisely explains why the methodologist considers the clinical guidance in this summary to be erroneous?",
      "options": {
        "A": "The summary conflates correlation with causation by treating average group differences as evidence that Treatment X causes better outcomes for all patients.",
        "B": "The summary draws conclusions from overall averages without recognizing that the relationship between treatment and outcome reverses or substantially changes across subgroups, making unqualified recommendations inappropriate.",
        "C": "The summary over-generalizes findings from a restricted sample by applying results from specific subgroups to a broader population without adequate representativeness.",
        "D": "The summary relies on descriptive statistics rather than inferential tests, so the apparent differences between treatments may reflect sampling variability rather than true effects."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The described pattern — where Treatment X appears superior overall but Treatment Y is actually better for Subgroup 2 — is the hallmark of a significant treatment-by-subgroup interaction. When such an interaction exists, interpreting the main effect (Treatment X beats Treatment Y) as a universal clinical recommendation is precisely the type of erroneous conclusion that arises from ignoring the interaction. The methodologist's objection is that the directionality of the treatment effect is conditional, not uniform.",
        "A": "This is incorrect. Conflating correlation and causation refers to inferring that an observed association implies a causal mechanism, which is a design validity issue. The report's error is not about causal inference but about summarizing a conditional relationship as an unconditional one — a separate and distinct interpretive problem.",
        "C": "This is incorrect. Overgeneralization from a restricted sample is an external validity concern about whether findings extend beyond the study participants. The problem here is not about who the sample represents but about how the internal structure of the data is being summarized and whether average effects apply uniformly within the sample itself.",
        "D": "This is incorrect. The scenario does not indicate that inferential tests were absent; the claim that the summary statements are 'mathematically accurate' implies the data were analyzed. Furthermore, the methodologist's concern is about the logic of interpretation, not about whether the findings could be due to chance — replacing descriptive with inferential statistics would not resolve the interpretive error."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-003-vignette-L1",
      "source_question_id": "003",
      "source_summary": "The most effective way to control extraneous variables is through random assignment of subjects to the different treatment groups.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "random assignment",
        "extraneous variables",
        "confound"
      ],
      "vignette": "A researcher designs a study to test whether a new cognitive-behavioral intervention reduces anxiety in college students. She uses random assignment to place participants into either the treatment group or a waitlist control group. Before the study begins, she notes that participants vary widely in baseline anxiety levels, sleep quality, and prior therapy experience. By using random assignment, she expects these extraneous variables to be roughly equally distributed across both groups. The researcher concludes that this design minimizes the likelihood of a confound influencing her results.",
      "question": "What is the primary methodological advantage of random assignment in this study?",
      "options": {
        "A": "It increases the external validity of the study by making the sample more representative of the general population.",
        "B": "It eliminates measurement error by standardizing the way outcome variables are assessed across groups.",
        "C": "It controls extraneous variables by distributing them roughly equally across treatment conditions.",
        "D": "It reduces participant attrition by ensuring that highly anxious individuals are evenly spread across groups."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "External validity refers to the generalizability of findings to other populations, settings, or times. Random assignment addresses internal validity — the ability to draw causal inferences — by balancing extraneous variables across groups, not by making the sample more representative.",
        "B": "Measurement error relates to the reliability and consistency of assessment instruments, not to group composition. Random assignment does not standardize how variables are measured; it controls pre-existing participant differences between groups.",
        "C": "Correct. Random assignment is the most powerful method for controlling extraneous variables because it probabilistically distributes participant characteristics — such as baseline anxiety, sleep quality, and prior therapy history — equally across conditions, thereby preventing them from systematically biasing the results.",
        "D": "While random assignment may incidentally spread highly anxious participants across groups, reducing attrition is not its primary purpose. Attrition is a separate methodological threat addressed through participant retention strategies, not through randomization at study entry."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-003-vignette-L2",
      "source_question_id": "003",
      "source_summary": "The most effective way to control extraneous variables is through random assignment of subjects to the different treatment groups.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "random assignment",
        "internal validity"
      ],
      "vignette": "A clinical psychologist conducts a randomized controlled trial examining whether mindfulness-based stress reduction (MBSR) lowers blood pressure in adults with hypertension. Participants are randomly assigned to an MBSR group or a psychoeducation control group. The researcher notes that participants in the study vary considerably in age — ranging from 35 to 72 years — which could plausibly influence outcomes independently of the intervention. Despite this demographic spread, the researcher argues that the study's design adequately accounts for this variability. A colleague asks why random assignment, rather than matching participants by age, was chosen as the primary strategy.",
      "question": "Which of the following best explains why random assignment is preferred over matching as a method for managing the age variable in this study?",
      "options": {
        "A": "Random assignment improves internal validity by distributing age and all other participant characteristics across groups without requiring prior knowledge of which variables may be relevant.",
        "B": "Matching on age improves external validity by ensuring the sample proportionally represents the population's age distribution.",
        "C": "Random assignment eliminates individual differences within each group, making participants in the same condition statistically equivalent.",
        "D": "Matching on age controls for known confounds more precisely than random assignment and is therefore the stronger design choice when age is a suspected moderator."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Random assignment is superior to matching because it simultaneously controls for all extraneous variables — both known and unknown — by probabilistically distributing them across conditions. Matching only controls the specific variable(s) on which participants are matched, leaving all other potential confounds unaddressed.",
        "B": "This option conflates internal and external validity. Matching on age does not make the sample more representative of the population (external validity); it simply equates groups on one known variable. Moreover, external validity is not the concern when comparing treatment conditions within a controlled trial.",
        "C": "Random assignment does not eliminate individual differences within groups; participants still differ from one another within each condition. Rather, it ensures that systematic differences between groups are minimized, making the groups equivalent on average at the outset.",
        "D": "While matching can offer precise control over a specific suspected variable, it is generally considered a weaker approach because it only controls variables the researcher thinks to match on. Random assignment is preferred because it controls all extraneous variables — including those not yet identified — making it the gold standard for internal validity."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-003-vignette-L3",
      "source_question_id": "003",
      "source_summary": "The most effective way to control extraneous variables is through random assignment of subjects to the different treatment groups.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "confound"
      ],
      "vignette": "A research team wants to determine whether a novel brief psychotherapy reduces depressive symptoms more effectively than standard care. They recruit 120 participants from a community mental health center and assign them to conditions by alternating enrollment order: the first participant goes to treatment, the second to control, and so on. The team reasons that because the assignment sequence is predetermined and not based on any participant characteristic, it should function similarly to true random assignment. However, a methodologist reviewing the protocol raises a concern: individuals who seek services earlier in a recruitment cycle may differ systematically from those who enroll later in motivation, symptom severity, or social support. She warns that this alternation scheme leaves a specific vulnerability in the design that true randomization would have prevented.",
      "question": "What core methodological problem does the methodologist's concern most directly identify with the alternating assignment scheme?",
      "options": {
        "A": "The scheme reduces statistical power because it does not ensure equal cell sizes when dropout occurs differentially by condition.",
        "B": "The scheme fails to adequately control extraneous variables because enrollment order is not random and may be systematically related to participant characteristics.",
        "C": "The scheme introduces a demand characteristic because participants assigned early may infer their condition based on order and adjust their responding.",
        "D": "The scheme compromises external validity because enrollment-based assignment limits generalizability to other clinical settings."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Alternating assignment is a form of quasi-random or systematic assignment, not true random assignment. Because early enrollees may differ from later enrollees on characteristics like motivation or symptom severity, these extraneous variables could become systematically confounded with condition membership. True random assignment prevents this by ensuring no predictable relationship between enrollment timing and group assignment.",
        "A": "Differential dropout is a legitimate concern in longitudinal studies and can reduce power and introduce bias, but this is not the specific problem the methodologist identifies. She is concerned about pre-existing systematic differences between groups at the point of assignment, not about what happens after assignment due to attrition.",
        "C": "Demand characteristics refer to participants altering behavior based on perceived study hypotheses. While participants assigned by alternating order could potentially infer their condition, the methodologist's concern is specifically about systematic differences in who is assigned to each group — a confound problem — not about participants' behavioral responses to perceived expectations.",
        "D": "External validity concerns the generalizability of findings beyond the study sample, not the internal integrity of group comparisons. The methodologist's critique is about internal validity — specifically the inability to rule out pre-existing group differences as an alternative explanation for any observed treatment effect."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-003-vignette-L4",
      "source_question_id": "003",
      "source_summary": "The most effective way to control extraneous variables is through random assignment of subjects to the different treatment groups.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "equivalence"
      ],
      "vignette": "A researcher publishes a study comparing two types of trauma-focused psychotherapy. Participants were recruited from two different VA medical centers: one site contributed participants to the prolonged exposure condition, and the other site contributed participants to the cognitive processing therapy condition. The researcher reports that the groups showed no statistically significant differences on demographic variables including age, gender, combat exposure, and PTSD severity at baseline, and concludes that the two groups were therefore equivalent and that any differences in outcomes could be attributed to the intervention. A peer reviewer challenges this conclusion, noting that the design contains a fundamental flaw that baseline equivalence on measured variables cannot repair.",
      "question": "What is the most precise reason the peer reviewer's concern is valid, despite the observed baseline equivalence on measured variables?",
      "options": {
        "A": "The study likely has insufficient statistical power because recruiting from only two sites limits the total sample size available for detecting between-group differences.",
        "B": "Baseline equivalence on measured variables does not rule out systematic differences on unmeasured variables, which only random assignment — not site-based assignment — can probabilistically distribute across groups.",
        "C": "The study's results cannot be generalized beyond VA populations because site-based recruitment restricts the representativeness of the sample across veteran subgroups.",
        "D": "Testing multiple baseline variables simultaneously inflates the familywise Type I error rate, meaning that reported equivalence may be a statistical artifact rather than a true absence of group differences."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Statistical power is a legitimate concern in multi-site studies, but the peer reviewer's objection is not about the study being underpowered to detect baseline differences. The problem is structural: no matter how large the sample, assigning entire sites to conditions means unmeasured site-level and participant-level variables are systematically confounded with treatment condition.",
        "B": "Correct. The fundamental flaw is that participants were assigned by site, not randomly. Even if measured baseline variables appear equivalent, this assignment method cannot control for unmeasured extraneous variables that may differ systematically between sites — things like therapist competence, institutional culture, referral patterns, or patient characteristics not assessed. Only random assignment distributes both known and unknown confounds across conditions, which is precisely why it is the gold standard for establishing group equivalence.",
        "C": "External validity and generalizability to other veteran populations are real limitations but are not the flaw the peer reviewer is pointing to. The concern is about internal validity — whether observed outcome differences can be causally attributed to the interventions — not about whether findings apply to other settings.",
        "D": "Multiple comparison inflation of Type I error is a genuine statistical issue when many baseline variables are tested simultaneously, and it could mean that a reported 'no significant difference' on one variable is actually a false negative. However, this is a secondary statistical concern. The primary and more fundamental problem identified by the reviewer is the absence of random assignment, which cannot be compensated for by any post-hoc statistical adjustment."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-003-vignette-L5",
      "source_question_id": "003",
      "source_summary": "The most effective way to control extraneous variables is through random assignment of subjects to the different treatment groups.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators wants to find out whether a structured group activity improves mood among residents of an assisted living facility. Because the facility has two wings, all residents on the east wing are invited to participate in the group activity each afternoon, while residents on the west wing continue with their usual unstructured free time. Before the program begins, the investigators gather information on age, mobility, and current mood ratings from all participants and find no meaningful numerical differences between residents on the two wings. At the end of six weeks, east wing residents report notably better mood. The investigators assert that the results demonstrate the group activity's effectiveness. A consultant brought in to evaluate the study points out that the design, despite the matching of measured baseline characteristics, cannot support this conclusion.",
      "question": "Which of the following most precisely captures the methodological basis for the consultant's skepticism?",
      "options": {
        "A": "The study lacks a standardized outcome measure, making it impossible to determine whether the mood improvements reflect genuine change or response bias among east wing residents.",
        "B": "The investigators failed to account for the possibility that east wing residents may have been exposed to more natural light or engaged in more social interaction independent of the activity, which would also improve mood.",
        "C": "Because group membership was determined by physical location rather than by a process that gives each participant an equal and independent chance of being in either group, pre-existing differences on factors not captured at baseline could systematically favor one wing over the other.",
        "D": "The six-week duration is insufficient to detect sustained mood changes, and the apparent improvement may reflect regression toward the mean rather than a true treatment effect."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The core problem is that assignment to condition was determined by physical location — a non-random, non-independent process — rather than by a mechanism that gives every participant an equal chance of being placed in either condition. This means any unmeasured differences between wings (staffing patterns, social network density, history of wellness programming, etc.) are fully confounded with treatment condition. Baseline equivalence on the few measured variables cannot correct for this, because only random assignment distributes all extraneous variables — measured and unmeasured — probabilistically across conditions.",
        "A": "Lack of a standardized outcome measure is a measurement reliability concern and could introduce bias in how mood is reported. However, this critique addresses the quality of the dependent variable's assessment, not the structural reason the design fails to support causal attribution — which is the assignment mechanism itself. Even a perfectly standardized measure would not resolve the fundamental group non-equivalence problem.",
        "B": "This option identifies a specific plausible confound (light exposure, social interaction) and is a compelling distractor because it correctly reasons about alternative explanations. However, it only names one or two possible extraneous variables rather than identifying the root methodological problem: that non-random location-based assignment leaves all unmeasured variables — not just these specific ones — potentially unequal between groups. The consultant's concern is structural, not about one or two named confounds.",
        "D": "Regression toward the mean is a real threat when groups are selected on the basis of extreme scores, and brief measurement windows can limit sensitivity to change. However, neither applies cleanly here: residents were not selected because of extreme mood scores, and the consultant's concern is not about duration or statistical artifacts. The consultant is questioning whether the design allows causal inference at all, which is a question of assignment validity rather than measurement sensitivity or temporal confounds."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-014-vignette-L1",
      "source_question_id": "014",
      "source_summary": "The Solomon four-group design is used to control the threat to internal validity known as pretest sensitization.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "Solomon four-group design",
        "pretest sensitization",
        "internal validity"
      ],
      "vignette": "A researcher studying the effectiveness of a mindfulness intervention is concerned that administering a pretest may sensitize participants to the purpose of the study, artificially inflating posttest scores regardless of the intervention's true effect. To address this threat to internal validity, the researcher decides to use a design that includes two treatment groups and two control groups, half of which receive a pretest and half of which do not. By comparing groups with and without pretests, the researcher can isolate the effect of pretest sensitization from the effect of the intervention itself. The researcher consults a methodology textbook and identifies the appropriate design, which directly addresses this concern.",
      "question": "Which research design is the researcher employing to control for pretest sensitization as a threat to internal validity?",
      "options": {
        "A": "Randomized pretest-posttest control group design",
        "B": "Solomon four-group design",
        "C": "Counterbalanced design",
        "D": "Posttest-only control group design"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The randomized pretest-posttest control group design includes a pretest and posttest for both a treatment and control group, but it does not include groups that omit the pretest. Without no-pretest conditions, this design cannot isolate or detect the sensitizing effect of the pretest itself.",
        "B": "Correct. The Solomon four-group design uses four groups: two receive the pretest (one treatment, one control) and two do not (one treatment, one control). This structure allows the researcher to directly detect and control for pretest sensitization as a specific threat to internal validity.",
        "C": "A counterbalanced design systematically varies the order in which conditions or treatments are administered across participants or groups. It is used to control for order effects and carryover effects, not for pretest sensitization.",
        "D": "A posttest-only control group design eliminates the pretest entirely for all groups, which avoids sensitization but does not allow for its detection or measurement. Unlike the Solomon four-group design, it cannot quantify the magnitude of the pretest sensitization effect."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-014-vignette-L2",
      "source_question_id": "014",
      "source_summary": "The Solomon four-group design is used to control the threat to internal validity known as pretest sensitization.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "Solomon four-group",
        "sensitization"
      ],
      "vignette": "A clinical psychologist is evaluating whether a psychoeducational program reduces stigma toward mental illness among nursing students. She randomly assigns participants to four groups and notes that two groups complete an attitude survey before the program while two groups do not; within each of those pairs, one group receives the psychoeducational program and one serves as a control. The researcher is particularly concerned that completing the survey itself may increase awareness of stigma-related issues, potentially influencing how participants respond on the posttest regardless of program exposure. Although participants are relatively homogeneous in age and professional background, the researcher is careful to account for this potential confound in the design. The design allows her to compare outcomes across groups with and without prior exposure to the attitude survey.",
      "question": "What is the primary reason this researcher chose the Solomon four-group design for her study?",
      "options": {
        "A": "To control for the possibility that repeated testing creates a practice effect that improves performance independently of the intervention",
        "B": "To ensure that the homogeneity of the sample does not limit the generalizability of findings to more diverse populations",
        "C": "To detect and control for the possibility that initial exposure to the survey instrument itself influences posttest scores",
        "D": "To eliminate the threat of selection bias by ensuring all groups are drawn from the same population of nursing students"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Testing effects (practice effects) refer to performance improvements due to repeated exposure to the same test, and they are a legitimate internal validity concern. However, the Solomon four-group design is specifically designed to detect pretest sensitization — the change in awareness or reactivity caused by the pretest — not merely improved scores from practice.",
        "B": "Concerns about generalizability relate to external validity, not internal validity. While the homogeneity of the sample is mentioned as a neutral detail, the researcher's design choice was motivated by an internal validity concern, not by the desire to improve external validity.",
        "C": "Correct. The researcher's explicit concern is that completing the attitude survey before the program might heighten participants' awareness of stigma, changing how they respond to the posttest regardless of whether they received the intervention. This is pretest sensitization, and the Solomon four-group design directly detects and controls for this threat.",
        "D": "Selection bias refers to systematic differences between groups at baseline due to non-random assignment. While random assignment is part of the Solomon four-group design, selecting this design was motivated by the pretest sensitization concern, not primarily to address selection bias."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-014-vignette-L3",
      "source_question_id": "014",
      "source_summary": "The Solomon four-group design is used to control the threat to internal validity known as pretest sensitization.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "randomized"
      ],
      "vignette": "A researcher studying the impact of a diversity training program randomly assigns employees to one of four groups. Groups 1 and 2 complete a measure of implicit bias before the training; Group 1 then undergoes the training while Group 2 serves as a no-treatment control. Groups 3 and 4 skip the initial measurement entirely; Group 3 receives the training and Group 4 does not. After training, all four groups complete the same implicit bias measure. The researcher suspects that employees who completed the initial measure may have become more conscious of their biases and more motivated to change, even before exposure to any training. When analyzing results, she finds that the groups who took the initial measure showed larger pre-to-post changes than those who did not, independent of training exposure.",
      "question": "Which threat to internal validity does the researcher's four-group structure allow her to detect, as evidenced by the differential outcomes between groups who did and did not receive the initial measure?",
      "options": {
        "A": "Instrumentation, because repeated use of the implicit bias measure across groups may have altered how consistently it was scored or administered",
        "B": "Testing effects, because repeated exposure to the same measure independently improved scores regardless of training",
        "C": "Pretest sensitization, because initial exposure to the measure itself heightened awareness and changed participants' motivation or responses on the posttest",
        "D": "History effects, because external events occurring between initial and final measurement may have differentially affected groups who were more recently exposed to bias-related content"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Instrumentation refers to changes in how an instrument is scored, calibrated, or administered over the course of a study — for example, interrater drift or changes in scoring criteria. This is not what is described here; the concern is not about how the measure functioned but about whether taking it changed participants' awareness and motivation.",
        "B": "Testing effects (practice effects) refer to score improvements from the simple act of being exposed to the same test twice, such as familiarity with question formats or memorization of items. While superficially similar, pretest sensitization is more specific: it involves participants becoming psychologically sensitized to the topic of measurement — becoming more aware, reflective, or motivated — rather than just more familiar with the test format. The vignette explicitly describes increased awareness and motivation, not mere familiarity.",
        "C": "Correct. Pretest sensitization occurs when the act of completing a pretest changes participants psychologically — raising awareness, increasing motivation to change, or altering the construct being measured — independently of the treatment. The researcher's design and findings directly illustrate this: the pretest groups changed more regardless of training, indicating the measurement itself, not the training, drove some of the observed change.",
        "D": "History effects refer to external events that occur during the study period and affect participants, potentially differently across groups. While history is a plausible concern in a longitudinal study, the differential outcomes observed here were specifically tied to whether participants took the initial measure, not to exposure to external events. The design is structured precisely to attribute the difference to the pretest, not to history."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-014-vignette-L4",
      "source_question_id": "014",
      "source_summary": "The Solomon four-group design is used to control the threat to internal validity known as pretest sensitization.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "reactive"
      ],
      "vignette": "A researcher is evaluating a cognitive reappraisal intervention for reducing anxiety in college students. Participants are randomly placed into one of four conditions in a 2 × 2 arrangement that varies both whether they complete an initial self-report anxiety questionnaire and whether they receive the intervention. After a six-week period, all participants complete the same anxiety questionnaire. Interestingly, the researcher finds that students who completed the initial questionnaire showed significantly reduced anxiety at follow-up even in the no-intervention condition, while students in the intervention condition who skipped the initial questionnaire showed more modest gains. The researcher concludes that the questionnaire itself had a reactive quality that independently contributed to change. She argues that if she had used a simpler two-group design, she would have significantly overestimated the intervention's effect.",
      "question": "What specific threat to internal validity does this study's design allow the researcher to identify and disentangle from the treatment effect?",
      "options": {
        "A": "Demand characteristics, because participants who completed the initial questionnaire may have inferred the study's hypothesis and responded accordingly to please the researcher",
        "B": "Regression to the mean, because anxious students who scored high at baseline were statistically likely to score lower at follow-up regardless of intervention",
        "C": "Pretest sensitization, because completing the initial anxiety questionnaire itself heightened participants' awareness of their anxiety and catalyzed change independent of the intervention",
        "D": "Placebo effects, because participants in pretested groups who received no intervention may have believed the questionnaire itself constituted treatment"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Demand characteristics refer to participants altering their responses because they perceive the study's hypothesis and wish to confirm it or appear in a favorable light. While participants who took the initial questionnaire might have inferred the study's focus, the vignette specifically describes a 'reactive quality' of the questionnaire — meaning the questionnaire itself catalyzed psychological change, not mere response bias. Demand characteristics would predict biased self-report, not genuine behavioral or emotional change in no-intervention controls.",
        "B": "Regression to the mean is a statistical phenomenon in which extreme scores at baseline tend to move toward the population mean at subsequent measurement, regardless of any intervention. While this is a real threat when selecting participants based on extreme scores, the vignette describes a randomized design across all anxiety levels — not a selected high-anxiety sample — making regression to the mean an unlikely explanation for the differential pattern observed across pretested and non-pretested groups.",
        "C": "Correct. The scenario describes a reactive pretest: the anxiety questionnaire itself raised participants' awareness of their anxiety symptoms, prompting self-directed change even without the formal intervention. The four-group structure reveals this by showing that no-intervention participants who completed the pretest improved more than no-intervention participants who did not. This is the defining feature of pretest sensitization, and the researcher explicitly uses the four-group design to isolate this effect from the intervention's true impact.",
        "D": "A placebo effect occurs when participants experience real psychological or physiological change because they believe they have received an active treatment. While superficially plausible, this explanation requires participants to perceive the questionnaire as a form of treatment — which is not described in the vignette. Moreover, placebo effects are typically studied in intervention conditions, not in control groups, and would not explain the systematic pattern across pretested versus non-pretested groups in the way pretest sensitization does."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-014-vignette-L5",
      "source_question_id": "014",
      "source_summary": "The Solomon four-group design is used to control the threat to internal validity known as pretest sensitization.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators wants to evaluate whether a new workplace wellness program improves employees' self-reported well-being. They divide a large workforce into four groups. Before the program begins, two of the groups are asked to fill out a detailed questionnaire about their current lifestyle habits, stress levels, and health behaviors; the other two groups are not asked to do this. One of the groups that completed the questionnaire and one that did not then participate in the wellness program; the remaining two groups carry on with their normal workday routines for the same period. After eight weeks, all four groups complete the same lifestyle and well-being questionnaire. When the results are analyzed, the investigators are surprised to find that employees who filled out the initial questionnaire — even those who never participated in the wellness program — reported substantially higher well-being at follow-up than employees in the no-program group who had not filled it out. Employees who both participated in the program and completed the initial questionnaire showed the largest gains, but the program-only group without initial questionnaire completion showed more modest improvement than the investigators initially expected.",
      "question": "What does the pattern of results across the four groups most specifically indicate about the initial questionnaire administration?",
      "options": {
        "A": "The initial questionnaire introduced a social desirability bias that caused participants to over-report improvements at follow-up in order to appear consistent with their initial responses",
        "B": "Completing the initial questionnaire itself produced genuine psychological change in participants, independent of program participation, by heightening their awareness and engagement with health-related behaviors",
        "C": "Participants who completed the initial questionnaire developed expectations about the study's purpose that functioned like a placebo, generating real but spurious well-being gains in the absence of any active treatment",
        "D": "The initial questionnaire functioned as a practice exposure that reduced the novelty and emotional reactivity of the follow-up measure, leading to lower anxiety and higher well-being scores through a simple testing familiarity mechanism"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Social desirability bias would lead participants to present themselves favorably, potentially inflating self-reported well-being. However, this explanation predicts biased responding rather than genuine change, and it would be expected to affect both pretested and non-pretested groups if the follow-up questionnaire alone created social pressure. The key distinguishing feature of the results — that only participants who completed the initial questionnaire showed elevated outcomes — is better explained by the questionnaire itself provoking genuine psychological engagement, not response bias.",
        "B": "Correct. The pattern described — where completing the initial questionnaire alone, without program participation, led to meaningful well-being improvement — is the signature of pretest sensitization. The detailed questionnaire about lifestyle habits, stress, and health behaviors raised participants' awareness of their own behaviors and motivated self-initiated change, independent of the formal wellness program. The four-group structure is precisely what allows investigators to isolate this effect, and this is the defining mechanism of pretest sensitization as a threat to internal validity.",
        "C": "A placebo explanation requires participants to believe they are receiving a treatment or active intervention. While completing a questionnaire about one's health could theoretically be interpreted as a form of intervention, this explanation requires an inferential leap not supported by the vignette. More importantly, placebo effects arise from expectancy and perceived treatment, whereas pretest sensitization arises from heightened awareness and self-reflection catalyzed by the measurement instrument itself — a mechanistically distinct process that more precisely fits the described pattern.",
        "D": "A testing familiarity mechanism would predict that participants who completed the initial questionnaire would score differently at follow-up simply because they had practice with the instrument — for example, reduced anxiety about responding or greater ease with the format. This is a testing effect (or practice effect), which is a distinct concept from pretest sensitization. Testing effects predict improved performance or reduced reactivity due to familiarity with the measure, but they do not predict genuine behavioral change in health practices over eight weeks. The vignette describes real well-being improvements, not merely lower questionnaire reactivity."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-080-vignette-L1",
      "source_question_id": "080",
      "source_summary": "Parametric statistical tests are more powerful than nonparametric tests, meaning that when using a parametric test, you are more likely to reject a false null hypothesis.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "parametric",
        "nonparametric",
        "statistical power"
      ],
      "vignette": "A researcher is planning a study comparing anxiety scores across three groups and is deciding whether to use a parametric or nonparametric test. She notes that her data are normally distributed and measured on an interval scale. Her advisor explains that choosing a parametric over a nonparametric test will increase her statistical power. The advisor emphasizes that this means she is more likely to correctly reject a false null hypothesis.",
      "question": "Which of the following best explains why parametric tests are preferred over nonparametric tests when assumptions are met?",
      "options": {
        "A": "Parametric tests reduce the probability of committing a Type I error compared to nonparametric tests.",
        "B": "Parametric tests are more powerful, making it more likely to reject a false null hypothesis than nonparametric tests.",
        "C": "Parametric tests require larger sample sizes, which inherently increases the validity of the findings.",
        "D": "Parametric tests use ranked data, which captures more information than raw scores used in nonparametric tests."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Statistical power refers to the probability of correctly rejecting a false null hypothesis (avoiding a Type I error). Parametric tests do not reduce Type I error rates relative to nonparametric tests; both are set at the same alpha level. Type I error is controlled by the alpha level, not by the choice between parametric and nonparametric procedures.",
        "B": "Correct. Parametric tests are more powerful than nonparametric tests, meaning they have a higher probability of detecting a true effect when one exists — that is, rejecting a false null hypothesis. This advantage holds when the assumptions of parametric tests (e.g., normality, interval data) are satisfied.",
        "C": "Incorrect. Parametric tests do not require larger sample sizes than nonparametric tests; in fact, nonparametric tests often need larger samples to achieve comparable power. Sample size affects power, but the claim that parametric tests inherently require larger samples is false and reverses the logic.",
        "D": "Incorrect. It is nonparametric tests, not parametric tests, that typically use ranked data. Parametric tests use raw score values directly and make assumptions about the underlying distribution. The use of ranks in nonparametric tests is actually what causes some loss of information and reduced power."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-080-vignette-L2",
      "source_question_id": "080",
      "source_summary": "Parametric statistical tests are more powerful than nonparametric tests, meaning that when using a parametric test, you are more likely to reject a false null hypothesis.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "null hypothesis",
        "power"
      ],
      "vignette": "A clinical researcher is comparing depression scores between two treatment groups using data collected from 40 participants. The scores are approximately normally distributed, and the researcher is 52 years old and has spent 20 years conducting clinical trials. She considers using a Mann-Whitney U test rather than an independent samples t-test because she heard it is more 'robust.' Her statistician consultant cautions her that switching to the Mann-Whitney U would reduce power when assumptions for the t-test are met, making it harder to reject the null hypothesis even if the treatments truly differ.",
      "question": "The statistician's caution most directly reflects which principle about statistical tests?",
      "options": {
        "A": "Nonparametric tests, like the Mann-Whitney U, are less powerful than parametric tests, like the t-test, when parametric assumptions are satisfied.",
        "B": "The Mann-Whitney U test inflates the Type I error rate relative to the independent samples t-test, threatening study validity.",
        "C": "Parametric tests require larger sample sizes to achieve equivalent power, so the current sample of 40 is insufficient.",
        "D": "Nonparametric tests produce wider confidence intervals, which directly reduces the probability of rejecting the null hypothesis."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. When the assumptions of a parametric test are met (e.g., normal distribution), parametric tests such as the independent samples t-test are more powerful than their nonparametric counterparts such as the Mann-Whitney U. Greater power means a higher probability of rejecting a false null hypothesis, which is exactly the statistician's concern.",
        "B": "Incorrect. The Mann-Whitney U test does not inflate the Type I error rate. Type I error is controlled by the alpha level and is not uniquely elevated by using nonparametric tests. The statistician's concern is about power (Type II error), not about inflated false positive rates.",
        "C": "Incorrect. This reverses the relationship between parametric and nonparametric tests. Nonparametric tests generally require larger samples to achieve power equivalent to parametric tests, not the other way around. A sample of 40 is not inherently insufficient for a t-test.",
        "D": "Incorrect. While nonparametric tests may produce less precise estimates in some contexts, the primary reason they are less preferred when assumptions are met is reduced statistical power, not specifically wider confidence intervals. Confidence interval width is related to sample size and variance, not simply to the choice of test family."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-080-vignette-L3",
      "source_question_id": "080",
      "source_summary": "Parametric statistical tests are more powerful than nonparametric tests, meaning that when using a parametric test, you are more likely to reject a false null hypothesis.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "ANOVA"
      ],
      "vignette": "A researcher studying therapy outcomes in a rural clinic collects data from four groups of clients, each receiving a different intervention. The outcome variable — a self-reported well-being scale — produces scores that are somewhat skewed, but the researcher's advisor argues the skewness is mild and that using ANOVA is still appropriate given the sample sizes and robustness of the test. A colleague suggests switching to a Kruskal-Wallis test because the data are 'not perfectly normal,' insisting it is the safer choice. The original advisor counters that doing so could actually reduce the likelihood of detecting a true difference among the four groups if one actually exists.",
      "question": "The advisor's concern about switching to the Kruskal-Wallis test is best understood as a concern about which statistical property?",
      "options": {
        "A": "Internal validity, because using a less appropriate test introduces systematic error that biases the results.",
        "B": "Statistical power, because nonparametric tests are less likely to detect a true effect when parametric assumptions are reasonably met.",
        "C": "Type I error rate, because the Kruskal-Wallis test is more likely to produce false positives than ANOVA in mildly skewed distributions.",
        "D": "Reliability of measurement, because rank-based tests discard the original scale information and reduce score consistency."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The advisor's concern is about statistical power — the probability of rejecting a false null hypothesis. When ANOVA assumptions are reasonably met (as with mild skewness and adequate sample sizes), ANOVA is more powerful than the Kruskal-Wallis test. Switching to the nonparametric alternative would decrease the chance of detecting a true group difference.",
        "A": "Incorrect. Internal validity refers to the degree to which a study can establish causal relationships by controlling for confounds, not to the statistical test selected. Choosing a less powerful test does not introduce systematic bias in the measurement of variables or the assignment of participants to conditions.",
        "C": "Incorrect. The concern is not about elevated Type I error. In fact, the Kruskal-Wallis test does not produce more false positives than ANOVA; if anything, nonparametric tests tend to be conservative. The advisor's concern is the opposite: that the true effect will be missed (a Type II error), not that a false positive will be detected.",
        "D": "Incorrect. Reliability of measurement refers to the consistency of a measure across time, raters, or items, and is a psychometric property of the instrument. While it is true that rank-based tests discard some scale information, this does not affect the reliability of the instrument itself; the concern here is about the statistical power of the test."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-080-vignette-L4",
      "source_question_id": "080",
      "source_summary": "Parametric statistical tests are more powerful than nonparametric tests, meaning that when using a parametric test, you are more likely to reject a false null hypothesis.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "robust"
      ],
      "vignette": "A graduate student is designing a study to compare mean differences in a continuous outcome across two independent groups. She reads that one available test is described as 'robust to violations of distributional assumptions,' and she concludes that this robustness makes it the superior choice in virtually all circumstances. Her methods professor pushes back, noting that when the data do satisfy the distributional conditions required by the more assumption-sensitive test, the robust alternative pays an efficiency cost. Specifically, the professor explains that the assumption-sensitive test is better positioned to use the full informational value of the data under those conditions.",
      "question": "The professor's point about the 'efficiency cost' of the robust test most directly illustrates which concept in inferential statistics?",
      "options": {
        "A": "The robust test produces inflated alpha levels, increasing the likelihood of a Type I error when distributional assumptions are actually met.",
        "B": "The assumption-sensitive test has greater statistical power than the robust alternative when parametric assumptions are satisfied, making it more likely to detect a true effect.",
        "C": "The assumption-sensitive test has higher external validity because it generalizes better to populations where distributional assumptions hold.",
        "D": "The robust test reduces effect size estimates because it compresses score variance when distributional conditions are optimal."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The professor is describing statistical power. Nonparametric (robust) tests pay an 'efficiency cost' — they do not fully use the distributional information in the data — which reduces their ability to detect a true effect relative to parametric tests when assumptions are met. The 'assumption-sensitive' parametric test capitalizes on that information, resulting in greater power (a higher probability of rejecting a false null hypothesis).",
        "A": "Incorrect. Robust or nonparametric tests do not inflate the Type I error rate; both test types control alpha at the researcher-specified level. This option confuses power (Type II error concerns) with Type I error, which is a common but important distinction. The professor's concern is about missing a true effect, not about falsely detecting one.",
        "C": "Incorrect. External validity refers to the generalizability of findings to other populations, settings, or times, and is a function of study design and sampling — not of which statistical test is selected. The professor's comment about efficiency is about detecting effects within the sample, which is an internal statistical property (power), not a generalizability issue.",
        "D": "Incorrect. While nonparametric tests use ranks and may lose some information about the magnitude of differences, this is not accurately described as compressing score variance or directly reducing effect size estimates. The core issue is statistical power, not distortion of effect size metrics. Effect size estimates are typically derived from the raw data, not the rank-based test statistic."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-080-vignette-L5",
      "source_question_id": "080",
      "source_summary": "Parametric statistical tests are more powerful than nonparametric tests, meaning that when using a parametric test, you are more likely to reject a false null hypothesis.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "Two researchers are analyzing the same dataset from a study with 35 participants. The first researcher, who is known for taking a cautious approach, uses a method that makes no assumptions about the shape of the underlying distribution. The second researcher uses a different method that is more sensitive to the actual distribution of scores but works well given the data at hand. Both methods test the same question about group differences. When results are compared, the cautious researcher finds p = .09 and does not conclude that a difference exists, while the second researcher finds p = .03 and concludes a difference does exist. Both researchers set the same decision threshold in advance, and neither made a mathematical error.",
      "question": "Which concept most directly explains why the two researchers reached different conclusions from the same dataset?",
      "options": {
        "A": "The cautious researcher committed a Type I error by setting too lenient a decision threshold, while the second researcher correctly identified the true absence of an effect.",
        "B": "The second researcher's method had greater capacity to detect a true effect in the data, because it made fuller use of the distributional properties of the scores.",
        "C": "The difference in conclusions reflects sampling variability, because the two methods drew upon different subsets of the 35 participants.",
        "D": "The cautious researcher's method is more accurate because it is less likely to be influenced by distributional outliers that may have inflated the second researcher's result."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. This scenario illustrates statistical power. The cautious researcher used a nonparametric method (which makes no distributional assumptions), while the second researcher used a parametric method that capitalizes on the distributional properties of the data. When those properties are favorable, the parametric test is more powerful — it is better able to detect a true effect, resulting in a significant finding where the nonparametric test did not reach the threshold. Both researchers used the same alpha level, so the difference is entirely due to the power of the chosen method.",
        "A": "Incorrect. This option inverts the situation. The researcher who found p = .03 and rejected the null hypothesis is more likely to have correctly identified a true effect given the data; it is the cautious researcher who may have missed a true effect (a potential Type II error). A Type I error would involve falsely rejecting a true null hypothesis, which is not what is described here — both decision thresholds were set identically.",
        "C": "Incorrect. Both researchers analyzed the same dataset of 35 participants; neither drew a different sample. Sampling variability refers to differences that arise from drawing different samples from the same population. The divergent results here stem from the different sensitivity of the two methods to the same data, not from any difference in the data analyzed.",
        "D": "Incorrect. While it is true that nonparametric methods can be more resistant to the influence of outliers, there is no mention of outliers in this scenario, and the vignette specifies that the data were suitable for the second researcher's method. This option would favor a wrong answer by implying the nonparametric method is more accurate. The distinguishing detail is that the second researcher's method was appropriate for the data and therefore more powerful, making option B the correct explanation."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-047-vignette-L1",
      "source_question_id": "047",
      "source_summary": "The A-B-A-B single-subject design has two no-treatment phases and two treatment phases with the same treatment being applied in both treatment phases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "single-subject",
        "baseline",
        "treatment"
      ],
      "vignette": "A researcher studying self-injurious behavior in a child with autism uses a single-subject design. The study begins with a baseline phase (A) in which no intervention is applied and the frequency of self-injury is recorded. A treatment phase (B) follows, during which a differential reinforcement procedure is introduced and behavior decreases. The researcher then withdraws the treatment to return to a second baseline phase (A), and finally reintroduces the same treatment in a second treatment phase (B).",
      "question": "Which single-subject research design is being described in this study?",
      "options": {
        "A": "A-B design",
        "B": "Multiple baseline design",
        "C": "A-B-A-B design",
        "D": "Changing criterion design"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The A-B design is the simplest single-subject design, involving only one baseline phase and one treatment phase, with no withdrawal or reintroduction of treatment. This vignette describes four phases, ruling out the A-B design.",
        "B": "The multiple baseline design applies an intervention across different behaviors, settings, or participants in a staggered fashion without withdrawing treatment. This vignette involves a single behavior with alternating treatment and no-treatment phases, which does not match the multiple baseline structure.",
        "C": "The A-B-A-B design is correct because it involves two no-treatment (baseline) phases and two treatment phases in which the same treatment is applied both times. The withdrawal and reintroduction of treatment strengthens causal inference by demonstrating that behavior changes are functionally related to the intervention.",
        "D": "The changing criterion design uses a single treatment phase in which the performance criterion is gradually shifted in successive steps. Because this vignette involves returning to a no-treatment baseline rather than adjusting a criterion, it does not fit the changing criterion design."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-047-vignette-L2",
      "source_question_id": "047",
      "source_summary": "The A-B-A-B single-subject design has two no-treatment phases and two treatment phases with the same treatment being applied in both treatment phases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "withdrawal",
        "single-subject"
      ],
      "vignette": "A school psychologist wants to evaluate the effectiveness of a token economy on a 9-year-old boy's disruptive classroom behavior. She uses a single-subject methodology and begins by recording the number of disruptions per day without any intervention for three weeks. She then implements the token economy, observes a clear reduction in disruptions, and subsequently withdraws the intervention to assess whether disruptions return toward initial levels. After confirming the behavioral reversal, the psychologist reintroduces the identical token economy program. The boy has a co-occurring diagnosis of ADHD, which the psychologist notes as a potential contextual factor.",
      "question": "Which research design does the school psychologist's methodology most closely represent?",
      "options": {
        "A": "A-B-A-B design",
        "B": "A-B-A design",
        "C": "Multiple baseline across behaviors design",
        "D": "Alternating treatments design"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "The A-B-A-B design is correct. The psychologist records a baseline (A), implements treatment (B), withdraws treatment to return to baseline (A), and then reintroduces the same treatment (B). This four-phase structure with two baseline phases and two identical treatment phases defines the A-B-A-B design.",
        "B": "The A-B-A design also involves a withdrawal phase, which is why it is a tempting distractor. However, the A-B-A design ends after the second baseline and does not include the reintroduction of treatment. Because this vignette describes a second treatment phase, the A-B-A label is incorrect.",
        "C": "The multiple baseline across behaviors design would involve applying the same intervention to two or more different target behaviors in a staggered manner. This vignette focuses on a single behavior (disruptions) with sequential treatment withdrawal and reintroduction, not a staggered multi-behavior approach.",
        "D": "The alternating treatments design rapidly alternates two or more different treatment conditions (sometimes including a no-treatment condition) within the same phase to compare their relative effectiveness. This vignette employs only one treatment with distinct, sequential phases, not rapid alternation."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-047-vignette-L3",
      "source_question_id": "047",
      "source_summary": "The A-B-A-B single-subject design has two no-treatment phases and two treatment phases with the same treatment being applied in both treatment phases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "reversal"
      ],
      "vignette": "A behavior analyst is investigating whether a social skills intervention reduces a teenager's verbal outbursts at a residential facility. She collects observational data across four sequential phases, carefully documenting that outburst frequency is high during the first phase, decreases substantially during the second phase when the intervention is active, and then climbs back toward original levels during the third phase when the intervention is suspended. The analyst notes that the reversal of behavior during the suspension phase provides compelling evidence of experimental control. During the fourth and final phase, she reactivates the exact same social skills program and outbursts again decrease. Some staff members suggest that the teenager simply matured or that the initial data collection itself may have suppressed behavior, but the analyst argues her design guards against these alternative explanations.",
      "question": "Which research design did the behavior analyst employ?",
      "options": {
        "A": "Multiple baseline design",
        "B": "A-B-A-B design",
        "C": "A-B-A design",
        "D": "Changing criterion design"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "The A-B-A-B design is correct. The scenario describes four sequential phases — two without the intervention (baseline) and two with the same intervention — and explicitly mentions the reversal of behavior when treatment is withdrawn, which is the mechanism by which the A-B-A-B design establishes experimental control.",
        "C": "The A-B-A design is a strong distractor because it also involves a withdrawal phase and a behavioral reversal. However, the A-B-A design terminates after the second baseline (leaving the participant without treatment), whereas this vignette clearly describes a fourth phase in which the same intervention is reintroduced. The presence of two treatment phases rules out A-B-A.",
        "A": "The multiple baseline design would require the analyst to introduce the same intervention across different behaviors, settings, or subjects in a staggered sequence without withdrawing it. Because this vignette involves a single behavior with sequential withdrawal and reintroduction, the multiple baseline design does not apply.",
        "D": "The changing criterion design uses one continuous treatment phase with incrementally shifting performance standards to demonstrate control. The distinct no-treatment periods and behavioral reversals described in this vignette are inconsistent with the changing criterion design, which does not involve treatment withdrawal."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-047-vignette-L4",
      "source_question_id": "047",
      "source_summary": "The A-B-A-B single-subject design has two no-treatment phases and two treatment phases with the same treatment being applied in both treatment phases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "replication"
      ],
      "vignette": "A clinical researcher studying obsessive-compulsive handwashing in an adult patient sequences her study so that initial weeks involve only naturalistic observation and data collection. When she introduces a response prevention protocol, handwashing frequency drops markedly. Concerned about confounds such as seasonal illness reducing handwashing naturally, she suspends the protocol and documents that handwashing frequency returns to near-original levels, thus ruling out maturation as an explanation. She then achieves within-subject replication by reapplying the identical protocol, again producing a decrease in handwashing. On first reading, a colleague suggests the study is simply a two-phase pre–post design comparing observation to treatment, but the researcher argues the design's internal validity is far stronger than that characterization implies.",
      "question": "What research design does the researcher's methodology represent?",
      "options": {
        "A": "A-B design with follow-up assessment",
        "B": "Concurrent multiple baseline design",
        "C": "A-B-A-B design",
        "D": "A-B-A design"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "The A-B-A-B design is correct. The scenario describes a baseline observation phase (A), treatment introduction (B), treatment withdrawal returning to baseline levels (A), and reintroduction of the identical treatment (B). The within-subject replication — a hallmark of the A-B-A-B design — is explicitly referenced, and the full four-phase sequence matches the design's definition.",
        "D": "The A-B-A design is a highly compelling distractor because the scenario emphasizes the return to baseline and internal validity gains from the withdrawal. However, the researcher explicitly states she reapplied the identical protocol a second time, constituting a fourth phase. The A-B-A design ends at the second baseline and does not include this second treatment phase.",
        "A": "The A-B design with follow-up assessment involves only one baseline and one treatment phase, followed by a post-treatment assessment. This vignette describes a return to baseline conditions (not merely a follow-up assessment) and a reintroduction of treatment, making this a four-phase design that exceeds the scope of an A-B design.",
        "B": "The concurrent multiple baseline design staggers the introduction of an intervention across multiple individuals, behaviors, or settings simultaneously. This vignette involves a single patient and a single target behavior with sequential phases and withdrawals, which does not match the staggered multi-element structure of the concurrent multiple baseline design."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-047-vignette-L5",
      "source_question_id": "047",
      "source_summary": "The A-B-A-B single-subject design has two no-treatment phases and two treatment phases with the same treatment being applied in both treatment phases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A practitioner working with a child who frequently throws objects in a classroom first spends several weeks simply watching and recording how often the throwing occurs without changing anything about the classroom routine. She then arranges a specific environmental adjustment that quickly results in far fewer throwing episodes. Because a colleague questions whether the improvement was coincidental, the practitioner removes the environmental adjustment and the throwing rate climbs back to where it was at the start of the study. Satisfied that this confirms the relationship, she reinstates the very same environmental adjustment a second time, and the throwing rate drops again. A supervisor reviewing the data initially praises the careful documentation as evidence of a longitudinal observational study, but the practitioner clarifies that her approach was fundamentally experimental and allowed her to establish a functional relationship.",
      "question": "Which research methodology did the practitioner use?",
      "options": {
        "A": "A-B-A design",
        "B": "A-B-A-B design",
        "C": "Changing criterion design",
        "D": "Multiple baseline across settings design"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "The A-B-A-B design is correct. The practitioner moved through four sequential phases: initial observation without intervention (A), introduction of an environmental adjustment (B), removal of the adjustment with a return to elevated throwing (A), and reintroduction of the identical adjustment with a second decrease (B). This four-phase sequence with two no-treatment phases and two identical treatment phases, enabling within-subject replication of the functional relationship, precisely defines the A-B-A-B design.",
        "A": "The A-B-A design is a powerful distractor because the emphasis on ruling out coincidence through the behavioral reversal closely resembles the logic used to justify the A-B-A design. However, the A-B-A design ends after the second observation phase and does not include a second implementation of the adjustment. The practitioner's second reintroduction of the identical adjustment constitutes a fourth phase, which distinguishes this as A-B-A-B rather than A-B-A.",
        "C": "The changing criterion design involves one continuous period of intervention during which the required performance standard is gradually shifted to successive, more stringent levels to demonstrate control. Because the practitioner removes the intervention entirely and returns to naturalistic observation before reinstating it, her methodology is inconsistent with the changing criterion design, which never withdraws treatment.",
        "D": "The multiple baseline across settings design would require applying the same intervention in two or more different settings in a staggered, sequential fashion while maintaining baseline in the other settings. This vignette involves a single classroom setting and a single child, with alternating treatment and no-treatment phases rather than staggered introduction across settings."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-091-vignette-L1",
      "source_question_id": "091",
      "source_summary": "To compare an obtained sample mean to a known population mean, you would use a t-test for a single sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "single sample",
        "population mean",
        "t-test"
      ],
      "vignette": "A researcher wants to determine whether the average depression score of patients at a new clinic differs from the nationally established population mean for depression. She collects scores from 40 patients at the clinic and computes the sample mean. Because the population standard deviation is unknown, she plans to conduct an inferential statistical test comparing this single sample mean to the known population mean. The researcher selects the appropriate t-test for her analysis.",
      "question": "Which statistical test is most appropriate for comparing an obtained single sample mean to a known population mean when the population standard deviation is unknown?",
      "options": {
        "A": "Independent samples t-test",
        "B": "One-sample t-test",
        "C": "Paired samples t-test",
        "D": "One-way ANOVA"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. The independent samples t-test is used to compare means from two separate, unrelated groups to each other — not to compare a single sample mean against a fixed population mean. This scenario involves only one sample, not two independent groups.",
        "B": "Correct. The one-sample t-test (also called the single-sample t-test) is specifically designed to compare the mean of one sample against a known or hypothesized population mean, which is exactly the situation described.",
        "C": "Incorrect. The paired samples t-test (dependent t-test) compares two related sets of scores from the same participants measured at two time points or under two conditions. It requires two matched measurements per participant, which is not the case here.",
        "D": "Incorrect. A one-way ANOVA is used to compare means across three or more groups. Although it tests differences in means, it is not appropriate for comparing a single sample mean to a known population value."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-091-vignette-L2",
      "source_question_id": "091",
      "source_summary": "To compare an obtained sample mean to a known population mean, you would use a t-test for a single sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "sample mean",
        "population"
      ],
      "vignette": "A school psychologist reads that the average IQ score in the general population is 100. She is working with a sample of 35 children identified as gifted in her district and wants to determine whether their mean IQ score is significantly different from the general population value. The children in her sample have a range of scores and varying socioeconomic backgrounds, which she notes but determines are not relevant to her primary research question. She does not know the population standard deviation and proceeds with an inferential test.",
      "question": "Which statistical procedure should the school psychologist use to test whether her sample's mean IQ differs from the known population mean?",
      "options": {
        "A": "One-sample t-test",
        "B": "Independent samples t-test",
        "C": "One-sample z-test",
        "D": "Pearson correlation"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The one-sample t-test is appropriate when comparing a single sample mean to a known population mean and the population standard deviation is unknown. Both conditions apply here: there is one sample being compared to the population value of 100, and the population SD is not provided.",
        "B": "Incorrect. The independent samples t-test compares means from two separate groups of participants. This study has only one sample and does not involve two distinct groups being compared to each other.",
        "C": "Incorrect. A one-sample z-test is also used to compare a sample mean to a known population mean, but it requires that the population standard deviation be known. Since the population SD is not known in this scenario, the t-test is the appropriate choice.",
        "D": "Incorrect. The Pearson correlation measures the strength and direction of the linear relationship between two continuous variables within the same sample. It does not test whether a sample mean differs from a population value."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-091-vignette-L3",
      "source_question_id": "091",
      "source_summary": "To compare an obtained sample mean to a known population mean, you would use a t-test for a single sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "mean"
      ],
      "vignette": "A clinical researcher is studying anxiety levels in combat veterans who have recently returned from deployment. She recruits 28 veterans and administers a validated anxiety scale, obtaining a mean anxiety score for the group. She notes that the Department of Veterans Affairs has published a normative mean score for the general veteran population on this same scale. The researcher is interested in whether her sample's mean differs meaningfully from this established normative figure, and she does not have access to the population variance. Some of her colleagues suggest she should compare the veterans across two subgroups defined by deployment length, but she decides to focus on the whole sample versus the published norm.",
      "question": "What is the most appropriate statistical test for this researcher to use?",
      "options": {
        "A": "Independent samples t-test",
        "B": "One-sample t-test",
        "C": "Wilcoxon signed-rank test",
        "D": "One-sample z-test"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The one-sample t-test compares a single sample mean to a known population mean when the population standard deviation is unknown. The researcher has one sample of veterans and a published normative mean — the defining conditions for this test.",
        "A": "Incorrect. The independent samples t-test would be appropriate if the researcher followed her colleagues' suggestion to compare two subgroups (e.g., short vs. long deployment). However, she chose to compare the full sample mean to the population norm, which requires a one-sample design, not a two-group comparison.",
        "C": "Incorrect. The Wilcoxon signed-rank test is a nonparametric alternative to the one-sample or paired t-test, appropriate when data are ordinal or when parametric assumptions are severely violated. Nothing in the vignette indicates non-normality or violated assumptions that would necessitate a nonparametric approach.",
        "D": "Incorrect. The one-sample z-test compares a sample mean to a known population mean but requires that the population standard deviation be known. The vignette explicitly states the researcher does not have access to the population variance, ruling out the z-test."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-091-vignette-L4",
      "source_question_id": "091",
      "source_summary": "To compare an obtained sample mean to a known population mean, you would use a t-test for a single sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "normative"
      ],
      "vignette": "A neuropsychologist administers a standardized cognitive processing speed test to a group of 22 adults diagnosed with mild traumatic brain injury (mTBI). The test publisher has established normative benchmarks based on large-scale standardization samples, and the neuropsychologist wants to evaluate whether her clinical group performs differently from the normative benchmark. She is aware that the standardization sample was enormous, and she contemplates whether the standard error of the normative mean is essentially zero. She is also considering whether to split her sample by injury severity, but ultimately decides to treat the mTBI group as a single cohort for initial analysis.",
      "question": "The neuropsychologist's contemplation about the standardization sample size is relevant because it relates to choosing between closely similar tests. Which test should she ultimately use?",
      "options": {
        "A": "One-sample z-test",
        "B": "Independent samples t-test",
        "C": "One-sample t-test",
        "D": "Mann-Whitney U test"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. Even though the normative benchmark is based on a very large standardization sample — making its mean highly stable — the neuropsychologist does not know the population standard deviation for her clinical analysis. The one-sample t-test is correct because it compares a single sample mean to a known population mean when the population SD is unknown, which remains the situation for her sample of 22 mTBI patients.",
        "A": "Incorrect. The one-sample z-test is tempting here because the normative benchmark comes from a massive standardization sample, which might suggest the population parameters are known. However, the population standard deviation relevant to the researcher's own analysis is still not known, and the z-test requires a known population SD. The large normative sample does not automatically confer known population variance for the clinical group being studied.",
        "B": "Incorrect. The independent samples t-test would be appropriate only if the neuropsychologist decided to split her sample into two subgroups and compare them, or if she compared her clinical sample to a separate control group. Since she is treating the mTBI patients as a single cohort compared to a fixed population benchmark, this test does not apply.",
        "D": "Incorrect. The Mann-Whitney U test is a nonparametric alternative to the independent samples t-test used when comparing two independent groups. It neither applies to a single-sample versus population comparison nor is it indicated here, as no data suggest the parametric assumption of approximate normality is violated for the sample of 22."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-091-vignette-L5",
      "source_question_id": "091",
      "source_summary": "To compare an obtained sample mean to a known population mean, you would use a t-test for a single sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher collects scores from 30 individuals enrolled in a mindfulness program and computes the group's average score on a widely used outcome measure. A large government health agency previously measured this same outcome across thousands of citizens and published the resulting average figure in a national report, but did not publish the spread of individual scores around that figure. The researcher wants to know if her 30 participants, as a group, perform differently from that published average. Two of her supervisors debate whether to instead compare high-attendance participants against low-attendance participants, but the researcher declines this approach and proceeds with the whole group versus the published figure. She cannot locate any documentation of how variable scores are in the broader citizen population.",
      "question": "Which procedure is most appropriate for the researcher to use?",
      "options": {
        "A": "A procedure used to compare the averages of two unrelated groups formed within her sample",
        "B": "A procedure used to compare a group's average performance to an established reference value when the broader population's variability is unknown",
        "C": "A procedure used to compare a group's average performance to an established reference value when the broader population's variability is known",
        "D": "A procedure that examines whether two sets of scores from the same individuals are related to each other"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. This describes the one-sample t-test: comparing one group's mean to a known population mean when the population standard deviation is not available. The vignette specifies that the national report published only the average, not the spread of scores — meaning the population SD is unknown — and the researcher is comparing her single sample to that published figure.",
        "A": "Incorrect. This describes the independent samples t-test, which would be appropriate if the researcher followed her supervisors' suggestion to divide participants into high-attendance and low-attendance subgroups and compare those two groups. She explicitly declined this approach and is using the entire 30-person group compared to the national figure.",
        "C": "Incorrect. This describes the one-sample z-test, which also compares a single sample mean to a known population mean but requires that the population standard deviation be known. The vignette directly states that the spread of individual scores in the broader population was not published, making this test inappropriate despite its similarity to the correct answer.",
        "D": "Incorrect. This describes a paired samples t-test or a correlation analysis, both of which examine relationships or differences within the same set of participants across two measurement occasions or variables. The researcher is not pairing scores or examining within-person relationships; she is comparing a group average to an external benchmark."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-069-vignette-L1",
      "source_question_id": "069",
      "source_summary": "A researcher retains a false null hypothesis when making a Type II error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "null hypothesis",
        "Type II error",
        "retained"
      ],
      "vignette": "A researcher conducts a study examining whether a new cognitive-behavioral intervention reduces depressive symptoms more effectively than a waitlist control. After running the statistical analysis, the researcher finds a p-value of .18 and concludes there is no significant difference between groups. The researcher retains the null hypothesis and publishes a report stating the intervention has no effect. However, a subsequent meta-analysis with much larger samples reveals that the intervention does produce a clinically meaningful reduction in depressive symptoms. The original researcher's conclusion was therefore mistaken.",
      "question": "The error made by the original researcher is best described as which type of statistical decision error?",
      "options": {
        "A": "Type I error, because the researcher incorrectly accepted a finding that was not real",
        "B": "Type II error, because the researcher retained a false null hypothesis and missed a real effect",
        "C": "A sampling error, because the discrepancy between studies is due to random variation in who was selected",
        "D": "A power error, because the researcher failed to set an adequate alpha level before conducting the study"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A Type I error occurs when a researcher rejects a true null hypothesis — concluding an effect exists when it does not. In this scenario the researcher did the opposite, failing to reject the null, so a Type I error does not apply.",
        "B": "A Type II error (beta error) is defined as retaining — failing to reject — a null hypothesis that is actually false. Because the subsequent meta-analysis confirmed a real treatment effect existed, the null was false, and the original researcher's failure to detect it is precisely a Type II error.",
        "C": "Sampling error refers to chance variation between a sample statistic and the population parameter; it is not itself a decision error category. While sampling error can contribute to a Type II error, it does not name the classification of the researcher's mistaken conclusion.",
        "D": "Although low statistical power — often linked to small sample size rather than alpha level — increases the probability of a Type II error, 'power error' is not a recognized category of statistical decision error. The alpha level defines the Type I error rate, not directly the Type II error rate."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-069-vignette-L2",
      "source_question_id": "069",
      "source_summary": "A researcher retains a false null hypothesis when making a Type II error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "statistical power",
        "null hypothesis"
      ],
      "vignette": "A graduate student designs a pilot study with 12 participants per group to test whether a mindfulness-based intervention reduces cortisol levels in adults diagnosed with generalized anxiety disorder. The student reports a non-significant result (p = .22) and concludes that mindfulness has no effect on cortisol. Several colleagues note that the study had very low statistical power given the small sample. A larger replication study with 80 participants per group subsequently finds a statistically significant reduction in cortisol, confirming that the effect is real.",
      "question": "The graduate student's original conclusion best illustrates which type of error in statistical decision-making?",
      "options": {
        "A": "Type I error, because the non-significant result could have been driven by random fluctuations that inflated the effect in the larger study",
        "B": "Experimenter bias, because the student may have been motivated to confirm the null result given resource limitations in the pilot",
        "C": "Type II error, because the student failed to reject a null hypothesis that was actually false",
        "D": "Overgeneralization error, because a pilot study with a clinical population should not be used to draw causal conclusions"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A Type I error involves incorrectly rejecting a true null hypothesis — concluding an effect exists when it does not. Here the student did not reject the null; they retained it. The larger study confirmed the effect was real, further ruling out a Type I error framework.",
        "B": "Experimenter bias refers to systematic distortion of data or interpretation due to the researcher's expectations or motivations. While motivation is mentioned, the scenario describes a formal statistical decision outcome, not a process of biased data handling, so experimenter bias is not the best classification.",
        "C": "A Type II error occurs when a researcher retains (fails to reject) a null hypothesis that is, in reality, false. The subsequent study confirmed the true effect of mindfulness on cortisol, meaning the null was false, and the student's failure to detect it — likely due to inadequate power from the small sample — is a Type II error.",
        "D": "Overgeneralization is a reasoning concern about external validity, not a formal category of statistical decision error. The issue here is not about inappropriately extending conclusions to other populations but about making a wrong binary decision about the null hypothesis."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-069-vignette-L3",
      "source_question_id": "069",
      "source_summary": "A researcher retains a false null hypothesis when making a Type II error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "beta"
      ],
      "vignette": "A pharmaceutical company funds a study comparing a novel anxiolytic to placebo in a sample of 20 patients with social anxiety disorder. The lead investigator reports a non-significant p-value and concludes the drug is no more effective than placebo, submitting the finding to a regulatory agency. The study's beta was estimated at .45, meaning there was only a 55% chance of detecting an effect if one truly existed. Independent researchers later conduct a large-scale trial with 200 participants and find the drug significantly outperforms placebo with a medium effect size. Critics argue the original study's conclusion was premature.",
      "question": "Which statistical decision error best characterizes the conclusion drawn in the original study?",
      "options": {
        "A": "Type I error, because the original study may have been underpowered in a way that artificially inflated the p-value beyond the alpha threshold",
        "B": "Type II error, because the study failed to reject a null hypothesis that subsequent evidence indicates was false",
        "C": "Incorrect alpha setting, because the investigators should have used a more lenient significance threshold given the small sample size",
        "D": "Incidental error, because the discrepancy between studies reflects normal between-study variability in treatment outcome research"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A Type I error involves rejecting a true null hypothesis, not retaining a false one. Underpowered studies actually increase the probability of a Type II error, not a Type I error, because insufficient sample size reduces the ability to detect true effects. The framing here is mechanistically backwards.",
        "B": "A Type II error is defined as failing to reject a false null hypothesis. The high beta (.45) directly quantifies this risk: there was a 45% probability of a Type II error. The large independent replication confirmed the drug's efficacy, establishing that the null was indeed false and the original conclusion was a Type II error.",
        "C": "While adjusting alpha is one strategy researchers discuss in planning studies, changing alpha does not retroactively reclassify the error type. Moreover, using a more lenient alpha to compensate for small samples is methodologically controversial and would increase Type I error risk rather than eliminate Type II error risk.",
        "D": "'Incidental error' is not a recognized category in statistical decision theory. While between-study variability is real, the scenario provides specific mechanistic information — high beta, subsequent significant replication — that points to a formal Type II error classification rather than random variation alone."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-069-vignette-L4",
      "source_question_id": "069",
      "source_summary": "A researcher retains a false null hypothesis when making a Type II error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "sensitivity"
      ],
      "vignette": "A researcher evaluates a brief screening questionnaire intended to identify subclinical depression in college students, finding that the measure's sensitivity is notably low. In a concurrent validity study comparing questionnaire classifications to structured clinical interviews, many students who were clinically identified as depressed by the interview were classified as non-depressed by the questionnaire. The researcher concludes that the questionnaire is adequate for use because the overall classification accuracy rate appears acceptable at 78%. A biostatistician later points out that the pattern of misclassifications systematically mirrors a specific type of inferential error that the researcher is, in effect, repeatedly committing when using the instrument to make classification decisions.",
      "question": "The systematic pattern of misclassification the biostatistician identifies most closely mirrors which inferential error?",
      "options": {
        "A": "Type I error, because the questionnaire is generating false positives by labeling non-depressed students as depressed at an unacceptable rate",
        "B": "Criterion contamination, because the structured interview and the questionnaire are measuring overlapping constructs, inflating apparent accuracy",
        "C": "Type II error, because the questionnaire repeatedly fails to identify individuals who truly have the condition, retaining a false conclusion of non-depression",
        "D": "Systematic measurement error, because the questionnaire contains items that are consistently biased against detecting depression in college-aged populations"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A Type I error corresponds to false positives — concluding an effect or condition is present when it is not. Low sensitivity produces false negatives, not false positives. The described pattern is the opposite: the questionnaire misses true cases, not falsely identifies non-cases, making Type I error the wrong analog.",
        "B": "Criterion contamination occurs when the criterion measure is influenced by knowledge of the predictor, biasing validity estimates upward. The scenario does not indicate that the interview scores were influenced by knowledge of questionnaire results; the issue is with the questionnaire's detection failure, not with contaminated criterion data.",
        "C": "In inferential statistics, a Type II error means failing to reject a false null hypothesis — concluding no effect or condition exists when it actually does. Low sensitivity means the instrument repeatedly concludes 'not depressed' for individuals who are truly depressed, which is structurally identical to retaining a false null. This is the pattern the biostatistician identifies.",
        "D": "Systematic measurement error refers to consistent, directional bias in scores from a measure — such as items that are culturally insensitive or poorly worded. While this could theoretically cause low sensitivity, the biostatistician specifically frames the problem as mirroring an inferential decision error category, pointing to Type II error rather than a psychometric bias explanation."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-069-vignette-L5",
      "source_question_id": "069",
      "source_summary": "A researcher retains a false null hypothesis when making a Type II error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators is examining whether a workplace wellness program reduces employee burnout. After collecting data from 30 employees, they find that the participants in the program and those who did not participate look nearly identical on their burnout scores, and the team announces that the program makes no difference. The investigators are confident in this conclusion partly because their analysis looked clean and partly because the program administrator — who had personally developed the program — reviewed and approved all study procedures before data were collected. Six months later, an independent organization repeats the assessment using 300 employees and concludes, with a high degree of confidence, that the program meaningfully reduces burnout. The original team's announcement is now widely regarded as incorrect.",
      "question": "The original team's erroneous conclusion is best explained by which of the following?",
      "options": {
        "A": "Type I error, because administrator involvement in approving procedures may have biased the study toward a false-positive conclusion that was later corrected",
        "B": "Demand characteristics, because employees knowing they were being observed may have suppressed genuine burnout responses, masking the program's true effect",
        "C": "Experimenter expectancy effect, because the administrator's approval of procedures introduced directional bias that distorted the original results toward the null",
        "D": "Type II error, because the small sample produced insufficient ability to detect a real difference that a much larger study subsequently confirmed"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "A Type I error involves concluding that a difference exists when it does not — a false positive. The original team's conclusion was that there was no difference, the opposite of a positive finding. Administrator involvement is a red herring here; it might raise concerns about bias, but the outcome described is a missed effect, not a fabricated one.",
        "B": "Demand characteristics refer to participants altering their behavior because they are aware of being studied, potentially inflating or deflating scores. While this is a plausible concern in workplace research, it would most likely suppress apparent differences rather than explain a miss that is reversed only with a larger sample — the key signal is the sample size discrepancy, not behavioral reactivity.",
        "C": "Experimenter expectancy effects occur when a researcher's expectations influence how data are collected or interpreted, typically inflating findings in the expected direction. The administrator approved procedures — a detail designed as a red herring — but the original finding was in the null direction. Expectancy effects would more plausibly push results toward finding an effect, not away from one, making this explanation inconsistent with the described outcome.",
        "D": "The defining feature of a Type II error is concluding no difference exists when a real difference is present — which is precisely what occurred. The original team's small group of 30 provided inadequate ability to detect the effect that 300 participants later confirmed with confidence. The administrator detail and the 'clean analysis' note are deliberate distractions; the core structural feature — missed real effect due to limited detection capacity — maps directly onto a Type II error."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-025-vignette-L1",
      "source_question_id": "025",
      "source_summary": "Research studies with good external validity have results that can be generalized to other people, settings, and conditions.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "generalizability",
        "external validity",
        "sample"
      ],
      "vignette": "A researcher conducts a study on the effectiveness of cognitive-behavioral therapy for depression using a large, diverse sample drawn from multiple geographic regions, age groups, and socioeconomic backgrounds. The study's findings are later successfully replicated in community mental health clinics, university counseling centers, and private practices across different countries. The researcher concludes that the study has high generalizability because the results apply broadly across people, settings, and conditions. A peer reviewer praises the study's external validity, noting that the representative sample and multi-site design strengthen the ability to apply results beyond the original study context.",
      "question": "Which methodological quality is most directly illustrated by this study's design and outcomes?",
      "options": {
        "A": "Internal validity, because the use of CBT as a structured treatment ensures that changes in depression can be causally attributed to the intervention rather than to confounding variables.",
        "B": "External validity, because the study's findings can be meaningfully generalized to other people, settings, and conditions beyond the original sample.",
        "C": "Construct validity, because the operationalization of depression and CBT accurately reflects the theoretical constructs being measured and manipulated.",
        "D": "Statistical conclusion validity, because the large sample size and multi-site design increase the power of the statistical tests used to detect real treatment effects."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Internal validity refers to the degree to which a study's design supports causal conclusions by controlling confounds. While important, the vignette emphasizes replication across diverse populations and settings — a hallmark of external validity, not internal validity.",
        "B": "External validity is the degree to which research findings can be generalized to other people, settings, times, and conditions. The diverse sample, multi-site replication, and successful application across varied contexts directly illustrate this concept.",
        "C": "Construct validity refers to how well a test or manipulation actually measures the theoretical construct it purports to assess. The vignette does not focus on whether depression or CBT were operationalized accurately, but on whether findings transfer to new contexts.",
        "D": "Statistical conclusion validity concerns whether the statistical methods used in a study support accurate inferences about relationships between variables. The vignette's emphasis is on breadth of application to different populations and settings, not on the precision of statistical inference."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-025-vignette-L2",
      "source_question_id": "025",
      "source_summary": "Research studies with good external validity have results that can be generalized to other people, settings, and conditions.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "generalize",
        "convenience sample"
      ],
      "vignette": "A clinical researcher investigates the impact of mindfulness-based stress reduction on anxiety levels in a group of 45 undergraduate psychology students who volunteered through a campus research pool. The intervention produced statistically significant reductions in self-reported anxiety, and the researcher notes that the effect size was large. However, when a colleague attempts to apply the same protocol in a community sample of working adults with generalized anxiety disorder, the effects are substantially weaker and inconsistent. The original researcher acknowledges that reliance on a convenience sample of college students may have limited the study's ability to generalize results to other populations.",
      "question": "Which threat to research quality is most centrally illustrated by this scenario?",
      "options": {
        "A": "Low statistical power, because the modest sample of 45 participants may have been insufficient to detect the true effect of mindfulness training in more heterogeneous populations.",
        "B": "Demand characteristics, because undergraduate volunteers may have guessed the study's hypothesis and responded in ways that inflated the apparent effectiveness of the intervention.",
        "C": "Low external validity, because findings from a homogeneous volunteer convenience sample of college students did not transfer to a different population of working adults with clinical anxiety.",
        "D": "Inadequate internal validity, because the absence of a randomized control group makes it impossible to rule out that anxiety reductions were caused by factors other than the mindfulness intervention."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Statistical power refers to the probability of correctly detecting a true effect when it exists. Although sample size affects power, the vignette's central problem is that results from one population did not replicate in another — a generalizability issue, not a power issue.",
        "B": "Demand characteristics occur when participants alter their behavior based on perceived study expectations, which can threaten internal validity. While possible here, the vignette emphasizes the failure to replicate across different populations as the core limitation, pointing to external validity.",
        "C": "External validity is the extent to which findings can be generalized beyond the original study's sample, setting, and conditions. The failure to replicate results in a working-adult clinical population — after success in a convenience student sample — is a textbook example of limited external validity.",
        "D": "Internal validity concerns the degree to which an observed effect can be causally attributed to the independent variable by ruling out confounds. The scenario does not indicate that the original study lacked controls; rather, it emphasizes that the effect did not transfer to a new population, which is an external validity concern."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-025-vignette-L3",
      "source_question_id": "025",
      "source_summary": "Research studies with good external validity have results that can be generalized to other people, settings, and conditions.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "replication"
      ],
      "vignette": "A well-designed randomized controlled trial examining a new pharmacological treatment for PTSD is conducted at a single Veterans Affairs hospital, enrolling exclusively male combat veterans between the ages of 25 and 40. The study employed rigorous randomization, double-blinding, and careful control of confounding variables, resulting in a highly significant treatment effect with a Cohen's d of 0.85. Encouraged by these results, a civilian hospital attempts to implement the same protocol with a mixed-gender sample of survivors of interpersonal trauma ranging from 18 to 65 years old, but the treatment shows only marginal benefit. Independent replication attempts in pediatric and geriatric populations also yield inconsistent outcomes.",
      "question": "Which aspect of research quality does this pattern of findings most directly call into question?",
      "options": {
        "A": "Internal validity, because the failure to replicate across different hospitals suggests that uncontrolled environmental confounds may have caused the original positive result rather than the treatment itself.",
        "B": "Construct validity, because the inconsistent results across populations suggest that the operationalization of PTSD used in the original study may not have captured the same underlying construct in civilian and non-combat trauma populations.",
        "C": "External validity, because the original study's restrictive sample of male combat veterans in a single specialized setting limited the degree to which results could be extended to other types of trauma survivors across different demographic groups.",
        "D": "Statistical conclusion validity, because the large effect size in the original study may have been inflated by the homogeneous sample, leading to overestimates of the true population effect and failed replications."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Internal validity refers to whether the independent variable truly caused the observed effect within the original study. The original study had strong internal validity features (randomization, blinding, confound control); the problem revealed by failed replication is not that the original effect was spurious, but that it does not apply broadly — an external validity issue.",
        "B": "Construct validity concerns whether the measurement tools and manipulations used actually represent the intended theoretical constructs. While construct validity could be raised as a concern, the vignette's emphasis is on population and setting restrictions preventing generalization, not on whether PTSD was measured accurately.",
        "C": "External validity is the extent to which research findings can be generalized to other people, settings, times, and conditions. The original study's exclusive use of a homogeneous, narrowly defined sample (male, combat, single site) prevented the results from applying to civilian survivors, women, and other age groups — a clear external validity limitation.",
        "D": "Statistical conclusion validity concerns the appropriateness of the statistical inferences drawn from the data. While homogeneous samples can inflate effect sizes, the central narrative in the vignette is about breadth of applicability across populations and settings, which is external validity, not a statistical inference problem."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-025-vignette-L4",
      "source_question_id": "025",
      "source_summary": "Research studies with good external validity have results that can be generalized to other people, settings, and conditions.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "population"
      ],
      "vignette": "A research team designs an exceptionally controlled laboratory study to test whether a novel cognitive training program reduces working memory deficits in children with ADHD. Participants are recruited from a single specialized neurodevelopmental clinic and consist entirely of children aged 8–10 with confirmed ADHD diagnoses, no comorbidities, and parents with college education or higher. The team employs rigorous randomization, blinded outcome assessment, and a matched waitlist control group, yielding a highly significant result (p < .001, d = 0.90). When school psychologists in rural and urban public schools attempt to adapt the program, they find negligible improvements, and the research team is puzzled by the discrepancy given the strength of the original design. Critics argue that the homogeneity of the clinic population and the artificial laboratory conditions were the key factors limiting the study's utility.",
      "question": "Which research quality issue most precisely explains why the critics' concerns are valid and why the study's strong design failed to prevent this limitation?",
      "options": {
        "A": "Low internal validity, because differences in school psychologists' training and implementation fidelity introduced confounds that obscured the treatment effect in the replication settings.",
        "B": "Measurement error, because the outcome tools used in the controlled laboratory may not have been sensitive enough to detect working memory improvements in more heterogeneous school populations.",
        "C": "External validity, because the combination of a homogeneous, clinically selected population and artificial laboratory conditions limited whether the findings could be meaningfully applied to diverse real-world settings.",
        "D": "Regression to the mean, because participants in the original sample were selected at the extreme low end of working memory performance and naturally improved over time, regardless of the intervention."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Internal validity concerns causal attribution within the original study. While implementation fidelity problems in replication settings are a real concern, the critics specifically identify the homogeneous population and artificial conditions of the original study — not procedural variability in replications — as the source of the limitation, pointing to external validity.",
        "B": "Measurement error refers to inaccuracies in how constructs are assessed. The vignette gives no indication that the outcome measures were insensitive; rather, the problem is that the original study's population and setting were too narrow to support generalization, which is an external validity issue rather than a psychometric one.",
        "C": "External validity is precisely the degree to which findings can be generalized to other people, settings, and conditions. Even a methodologically strong study — with randomization, blinding, and rigorous controls — can have low external validity if the sample is too homogeneous and the setting too artificial to reflect real-world populations and contexts. This is exactly what the critics identify.",
        "D": "Regression to the mean occurs when extreme scorers on a measure naturally score closer to the average upon retesting, which could mimic a treatment effect. While this is a plausible-sounding threat, the use of a matched waitlist control group in the original design would have controlled for this artifact, so it does not explain the failure to generalize to school settings."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-025-vignette-L5",
      "source_question_id": "025",
      "source_summary": "Research studies with good external validity have results that can be generalized to other people, settings, and conditions.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators carefully selects participants who represent only a narrow slice of people who might benefit from a new program: all are volunteers, live within commuting distance of a major research university, speak fluent English, and have no history of any other significant life difficulties. The program is delivered in a pristine university facility with highly trained staff, state-of-the-art materials, and consistent weekly scheduling — conditions that practitioners in the field described as unlike anything in their day-to-day work. The program produced dramatic improvements for participants in the study. When community organizations tried to offer a similar program to their clients — many of whom had overlapping life stressors, spoke different languages, and attended sessions inconsistently — the benefits virtually disappeared. The investigators had paid meticulous attention to ruling out alternative explanations within the study itself, yet the program failed to work in the real world.",
      "question": "Which property of the original study was most fundamentally compromised, despite the investigators' careful attention to methodological rigor?",
      "options": {
        "A": "Internal validity, because the community organizations' inconsistent delivery of the program introduced uncontrolled variables that confounded the treatment comparison and obscured any true effect.",
        "B": "External validity, because the artificial conditions, highly selected participants, and idealized delivery setting prevented the findings from applying to the diverse real-world populations and contexts where the program was later attempted.",
        "C": "Reliability, because the failure to reproduce the same outcomes across different implementations indicates that the program itself is unstable and yields different results under slightly different conditions.",
        "D": "Construct validity, because the dramatic improvements in the original study may reflect participants responding to the prestige and novelty of the university setting rather than to the active ingredients of the program itself."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Internal validity concerns whether the independent variable — not extraneous factors — caused the observed effect within the original study. The vignette explicitly notes that the investigators were meticulous about ruling out alternative explanations within their study. The problem arose when results failed to transfer to community settings, which is an external validity concern, not an internal validity failure of the original study.",
        "B": "External validity is the degree to which study findings can be generalized to other people, settings, and conditions. The combination of a narrowly selected volunteer sample, an artificial and highly resourced setting, and trained staff operating under ideal conditions created findings that simply did not hold when the program was applied to diverse clients in real-world organizations — a defining external validity failure, even when internal validity was strong.",
        "C": "Reliability in this context would refer to the consistency of measurement or the stability of the program's effects under standardized conditions. The vignette describes not measurement inconsistency but a systematic failure to produce effects in populations and settings different from the original — a generalizability problem that distinguishes external validity from reliability.",
        "D": "Construct validity would be compromised if participants' improvements reflected a response to the setting's prestige (a form of demand characteristics or placebo effect) rather than the program itself — a plausible concern that could explain the original results. However, the vignette's central and most specific detail is that the program failed to work when offered to clients who were more diverse and in less controlled settings, which is most precisely accounted for by external validity limitations rather than by a construct validity threat."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-124-vignette-L1",
      "source_question_id": "124",
      "source_summary": "Discriminant function analysis is the appropriate multivariate technique to use when categorizing people into one of two or more criterion groups based on their scores on two or more predictors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "discriminant function analysis",
        "criterion groups",
        "predictors"
      ],
      "vignette": "A researcher wants to classify psychiatric inpatients into one of three diagnostic criterion groups — schizophrenia, bipolar disorder, or major depressive disorder — based on their scores on several neuropsychological predictors. She has continuous scores on measures of working memory, processing speed, and executive function for each participant. She wants to derive a linear combination of these predictors that maximally separates the three groups. The statistician recommends a multivariate technique specifically designed to categorize individuals into pre-defined criterion groups using multiple continuous predictors.",
      "question": "Which statistical technique is most appropriate for this research goal?",
      "options": {
        "A": "Multiple regression analysis",
        "B": "Discriminant function analysis",
        "C": "Canonical correlation analysis",
        "D": "Cluster analysis"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Multiple regression analysis predicts a continuous outcome variable from multiple predictors; it is not designed to classify individuals into discrete, pre-defined categorical groups, which is what this researcher requires.",
        "B": "Discriminant function analysis is correct. It is specifically designed to derive linear combinations of continuous predictors that best classify individuals into two or more pre-defined criterion groups, exactly matching the described goal.",
        "C": "Canonical correlation analysis examines the relationship between two sets of continuous variables; it does not classify individuals into pre-defined discrete categories and is not appropriate here.",
        "D": "Cluster analysis groups participants into categories based on their similarity without using pre-defined criterion groups; it is an exploratory, not a classificatory, technique applied to known group membership."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-124-vignette-L2",
      "source_question_id": "124",
      "source_summary": "Discriminant function analysis is the appropriate multivariate technique to use when categorizing people into one of two or more criterion groups based on their scores on two or more predictors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "classify",
        "criterion groups"
      ],
      "vignette": "A clinical neuropsychologist is working with a large sample of elderly adults who have already been diagnosed — through independent clinical evaluation — as having either Alzheimer's disease, vascular dementia, or no cognitive impairment. She collects scores on six cognitive and behavioral measures for each participant and wishes to determine which combination of these scores best classifies participants into the correct diagnostic group. The neuropsychologist notes that the sample skews older and predominantly female, but she proceeds with the analysis because her primary interest is in group membership prediction rather than examining relationships among continuous outcomes.",
      "question": "What multivariate statistical technique should the neuropsychologist use to address her research question?",
      "options": {
        "A": "Multivariate analysis of variance (MANOVA)",
        "B": "Logistic regression",
        "C": "Discriminant function analysis",
        "D": "Factor analysis"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "MANOVA tests whether group membership (as an independent variable) produces differences across multiple continuous dependent variables; it does not derive a classification rule to predict group membership from continuous predictors, which is the neuropsychologist's goal.",
        "B": "Logistic regression can predict categorical group membership from multiple predictors and is closely related to discriminant function analysis, but it is most straightforwardly applied to binary outcomes. While multinomial logistic regression handles more than two categories, discriminant function analysis is the canonical technique when groups are pre-defined and the goal is to derive a maximal linear separation among three or more groups.",
        "C": "Discriminant function analysis is correct. It derives linear combinations of continuous predictor scores designed to maximally separate pre-defined criterion groups, directly matching the neuropsychologist's goal of classifying participants into three known diagnostic categories.",
        "D": "Factor analysis is a data-reduction technique that identifies latent dimensions underlying a set of observed variables; it does not classify individuals into pre-defined criterion groups and is not appropriate for this goal."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-124-vignette-L3",
      "source_question_id": "124",
      "source_summary": "Discriminant function analysis is the appropriate multivariate technique to use when categorizing people into one of two or more criterion groups based on their scores on two or more predictors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "classify"
      ],
      "vignette": "A health psychology researcher collects scores on measures of pain catastrophizing, self-efficacy, depression, and sleep disturbance from patients attending a chronic pain clinic. On intake, each patient has been independently assigned to one of three treatment tracks by a multidisciplinary team. The researcher wants to determine which combination of the psychological measures best predicts which treatment track a patient belongs to, and to assess how accurately a new patient could be sorted into the correct track using those measures. Because the outcome has three categories and multiple continuous inputs are involved, a colleague suggests using a regression-based approach, but another colleague argues that a different multivariate technique is more appropriate given that the outcome is a pre-defined categorical membership rather than a continuous variable.",
      "question": "Which technique is most defensible for the researcher's stated goal of predicting pre-defined categorical group membership from multiple continuous predictors?",
      "options": {
        "A": "Multinomial logistic regression",
        "B": "Multiple discriminant (discriminant function) analysis",
        "C": "Hierarchical multiple regression",
        "D": "Multivariate analysis of covariance (MANCOVA)"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Multinomial logistic regression can model a polytomous categorical outcome from multiple predictors and is a genuine competitor to discriminant function analysis. However, the explicit goal here — deriving a linear classification function to sort new individuals into pre-defined criterion groups — is the defining application of discriminant function analysis, and this technique produces classification functions and hit-rate statistics most naturally suited to that purpose.",
        "B": "Multiple discriminant function analysis is correct. When there are three or more pre-defined criterion groups and the goal is to derive linear combinations of continuous predictors that best classify individuals into those groups, this is the canonical technique. It directly produces discriminant functions and classification accuracy statistics.",
        "C": "Hierarchical multiple regression predicts a continuous outcome from predictors entered in specified blocks; it is not designed for classification into discrete categorical groups and does not address the researcher's goal.",
        "D": "MANCOVA tests group differences on multiple continuous dependent variables while controlling for covariates; like MANOVA, it uses group membership as the independent variable rather than the outcome, which is the reverse of what the researcher needs."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-124-vignette-L4",
      "source_question_id": "124",
      "source_summary": "Discriminant function analysis is the appropriate multivariate technique to use when categorizing people into one of two or more criterion groups based on their scores on two or more predictors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "linear combination"
      ],
      "vignette": "A forensic psychologist is developing a risk-stratification tool for a state correctional system. Each offender in the validation sample has already been independently adjudicated as low-risk, moderate-risk, or high-risk for reoffending by a parole board. The psychologist has continuous scores on eight psychometric instruments for each offender and wants to find the weighted linear combination of those scores that maximally separates the three risk strata, with the ultimate aim of generating a classification rule for incoming offenders. A statistician initially recommends principal components analysis to reduce the eight predictors to a smaller set before modeling, which the psychologist considers but ultimately rejects in favor of a single technique that simultaneously optimizes separation and classification. The psychologist notes that the method she has in mind will yield Wilks' lambda as a measure of how well the groups are separated.",
      "question": "Which technique is the psychologist planning to use?",
      "options": {
        "A": "Principal components analysis followed by MANOVA",
        "B": "Discriminant function analysis",
        "C": "Latent profile analysis",
        "D": "Canonical correlation analysis"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Principal components analysis followed by MANOVA would reduce predictors and then test group differences, but MANOVA uses group as the independent variable and does not produce a classification rule for new individuals. The scenario specifies rejection of a two-step approach in favor of one that simultaneously optimizes separation and classification, ruling this out.",
        "B": "Discriminant function analysis is correct. It derives weighted linear combinations of continuous predictors (discriminant functions) that maximally separate pre-defined categorical groups, yields Wilks' lambda as a separation statistic, and produces classification functions for assigning new cases — all features explicitly described in the scenario.",
        "C": "Latent profile analysis identifies unobserved subgroups (latent classes) from continuous indicators without using pre-defined group membership; it is a person-centered exploratory technique, not a supervised classification method applied to known criterion groups.",
        "D": "Canonical correlation analysis finds linear combinations of two sets of continuous variables that are maximally correlated with each other; it does not classify individuals into pre-defined categorical groups and does not yield Wilks' lambda in the context of group separation."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-124-vignette-L5",
      "source_question_id": "124",
      "source_summary": "Discriminant function analysis is the appropriate multivariate technique to use when categorizing people into one of two or more criterion groups based on their scores on two or more predictors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher has a large dataset in which each person has already been sorted into one of four distinct categories by an independent review panel — a process entirely separate from the data she is analyzing. She has continuous numeric scores from eight different instruments completed by each person, and her goal is to find the mathematically optimal way to combine those scores so that new individuals can be automatically sorted into the same four categories with the highest possible accuracy. She is not interested in understanding the latent structure of the instruments themselves, nor in estimating the strength of a linear trend in a continuous outcome. A colleague who reviewed her proposal notes that two functions will account for most of the group separation, and that she should evaluate fit using a statistic that expresses the proportion of variance in group membership not explained by the derived functions.",
      "question": "Which analytical approach is the researcher using?",
      "options": {
        "A": "Cluster analysis",
        "B": "Canonical correlation analysis",
        "C": "Multinomial logistic regression",
        "D": "Discriminant function analysis"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Cluster analysis groups individuals based on similarity among their scores without reference to pre-defined categories; it is exploratory and does not use known group membership as a criterion. The scenario explicitly states groups were assigned by an independent panel before the analysis, ruling out this unsupervised approach.",
        "B": "Canonical correlation analysis relates two sets of continuous variables to each other and can yield multiple functions, making it superficially plausible. However, it does not use pre-defined categorical group membership as the outcome to be predicted, and it does not produce classification rules for sorting new individuals into known groups.",
        "C": "Multinomial logistic regression also predicts polytomous categorical outcomes from multiple continuous predictors, making it a strong distractor. However, the colleague's specific reference to multiple derived functions accounting for group separation and the use of a statistic representing unexplained group variance (Wilks' lambda) are defining features of discriminant function analysis, not logistic regression, which uses log-odds and chi-square-based fit indices.",
        "D": "Discriminant function analysis is correct. The scenario describes all its hallmarks: pre-defined categorical groups established independently, multiple continuous predictors combined to maximize group separation, a goal of classifying new individuals, multiple derived functions (k−1 functions for k groups), and Wilks' lambda as the fit statistic representing unexplained group variance."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-113-vignette-L1",
      "source_question_id": "113",
      "source_summary": "Eta is the correlation coefficient used to assess the degree of association between two variables measured on an interval or ratio scale when their relationship is nonlinear.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "nonlinear",
        "correlation",
        "interval scale"
      ],
      "vignette": "A researcher is examining the relationship between hours of sleep deprivation and cognitive performance scores measured on an interval scale. When the data are plotted, the relationship appears clearly nonlinear: performance declines steeply at first, then levels off, then drops sharply again. The researcher wants to compute the most appropriate correlation coefficient to capture this curvilinear association. A colleague advises that the standard Pearson r would underestimate the strength of this relationship.",
      "question": "Which correlation coefficient is most appropriate for quantifying the degree of association between these two interval-scale variables given their nonlinear relationship?",
      "options": {
        "A": "Pearson r",
        "B": "Eta",
        "C": "Spearman rho",
        "D": "Point-biserial correlation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Pearson r measures the strength and direction of a linear relationship between two interval- or ratio-scale variables. Because the relationship here is nonlinear, Pearson r would systematically underestimate the true association and is therefore not the best choice.",
        "B": "Eta (η) is specifically designed to measure the degree of association between two interval- or ratio-scale variables when their relationship is nonlinear or curvilinear. It does not assume linearity and will capture the full strength of a curvilinear trend that Pearson r would miss.",
        "C": "Spearman rho is a rank-order correlation coefficient used when variables are measured on an ordinal scale or when the data seriously violate normality assumptions. It does not require linearity, but it is intended for ordinal data — not for capturing the degree of curvilinearity between interval-scale variables the way eta does.",
        "D": "The point-biserial correlation is used when one variable is naturally dichotomous and the other is continuous and interval- or ratio-scaled. The sleep deprivation variable here is continuous, not dichotomous, so point-biserial is not applicable."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-113-vignette-L2",
      "source_question_id": "113",
      "source_summary": "Eta is the correlation coefficient used to assess the degree of association between two variables measured on an interval or ratio scale when their relationship is nonlinear.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "curvilinear",
        "association"
      ],
      "vignette": "A health psychologist is studying the relationship between anxiety level (measured on a validated continuous scale) and task performance scores among a sample of 150 undergraduate students who also varied considerably in age. Initial scatterplot inspection reveals a classic inverted-U pattern: moderate anxiety is associated with peak performance, while both very low and very high anxiety are associated with poorer scores. The researcher notes that a standard linear measure of association would fail to detect the true strength of this relationship. She decides to use a statistic specifically designed to capture curvilinear association between continuous variables.",
      "question": "Which statistical measure should the researcher use to most accurately quantify the strength of the association between anxiety and performance?",
      "options": {
        "A": "Eta",
        "B": "Kendall's tau",
        "C": "Multiple R",
        "D": "Pearson r"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Eta (η) is the appropriate coefficient when the relationship between two interval- or ratio-scale variables is curvilinear rather than linear. The inverted-U pattern described here is a textbook curvilinear association, and eta will accurately reflect its strength where Pearson r would not.",
        "B": "Kendall's tau is a nonparametric rank-order correlation similar to Spearman rho, appropriate for ordinal data or small samples with many tied ranks. It is not designed to capture curvilinear relationships between continuous variables and does not address the specific problem of nonlinearity the researcher identified.",
        "C": "Multiple R is the multiple correlation coefficient reflecting the association between a criterion variable and a combination of two or more predictor variables. While it can be extended to include polynomial terms to model nonlinearity, it is not in itself a coefficient of curvilinear association between two variables as eta is.",
        "D": "Pearson r quantifies the linear component of the relationship between two continuous variables. The researcher explicitly noted that a linear measure would fail to capture the true association; the inverted-U pattern means that positive and negative deviations cancel each other out, causing Pearson r to substantially underestimate the relationship."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-113-vignette-L3",
      "source_question_id": "113",
      "source_summary": "Eta is the correlation coefficient used to assess the degree of association between two variables measured on an interval or ratio scale when their relationship is nonlinear.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "association"
      ],
      "vignette": "A sports scientist measures the relationship between competitive experience (in years, ranging from 0 to 30) and average reaction time (in milliseconds) across 200 athletes. The scatterplot reveals that reaction time improves sharply during the first several years of experience, plateaus in the middle years, and then begins to slow again in athletes with the most experience — a pattern the researcher attributes partly to aging effects in the most experienced group. Because the data are continuous and the pattern is not monotonic, the researcher rules out a simple rank-based approach. She also considers that using Pearson r on these data would produce a near-zero coefficient despite the meaningful pattern visible in the plot.",
      "question": "Given the nature of the data and the observed pattern, which statistical measure would most appropriately quantify the strength of the association between years of experience and reaction time?",
      "options": {
        "A": "Spearman rho",
        "B": "Pearson r",
        "C": "Eta",
        "D": "Phi coefficient"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Spearman rho is a rank-based correlation appropriate for ordinal data or monotonic relationships. The researcher explicitly ruled out rank-based approaches because the relationship is non-monotonic (it rises, plateaus, and reverses), making Spearman rho a poor fit even though it can handle some nonlinearity in ordinal contexts.",
        "B": "Pearson r measures linear association between continuous variables. The scenario specifically notes that Pearson r would produce a near-zero coefficient despite a clear and meaningful pattern — this is precisely the failure mode that eta is designed to address when the true relationship is curvilinear.",
        "C": "Eta (η) is the correlation coefficient designed to measure the strength of association between two interval- or ratio-scale variables when their relationship is nonlinear. The non-monotonic, curvilinear pattern described — improvement, plateau, and decline — is exactly the type of relationship eta captures, and it will not be distorted by canceling directional trends the way Pearson r is.",
        "D": "The phi coefficient is a measure of association between two dichotomous variables, essentially a Pearson r applied to a 2×2 contingency table. Years of experience and reaction time are both continuous, making phi entirely inapplicable to this scenario."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-113-vignette-L4",
      "source_question_id": "113",
      "source_summary": "Eta is the correlation coefficient used to assess the degree of association between two variables measured on an interval or ratio scale when their relationship is nonlinear.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "ratio scale"
      ],
      "vignette": "A psychophysiologist is investigating how stimulus intensity (measured in decibels on a ratio scale ranging from 20 to 120 dB) relates to subjective discomfort ratings collected from 300 participants. The relationship plotted across the full range shows an accelerating, nonmonotonic curve: discomfort increases slowly at lower intensities, sharply at moderate-to-high intensities, and then plateaus near the upper threshold. The researcher is concerned that simply ranking the data would lose metric information inherent in the ratio-scale measurement. A reviewer suggests that, because the predictor has more than two categories when grouped, an ANOVA-based index would be defensible — and indeed, the measure the reviewer has in mind is mathematically derivable from an ANOVA framework.",
      "question": "Which measure of association is the reviewer most likely recommending, and why is it appropriate here?",
      "options": {
        "A": "Omega squared (ω²), because it provides an unbiased estimate of variance explained in an ANOVA framework and is appropriate when the predictor is continuous",
        "B": "Eta (η), because it quantifies the degree of nonlinear association between ratio-scale variables and is mathematically equivalent to the square root of the ratio of between-group to total sum of squares in ANOVA",
        "C": "Epsilon squared (ε²), because it corrects for positive bias in eta squared and is therefore preferred when sample sizes are large and the predictor is continuous",
        "D": "Pearson r, because stimulus intensity is measured on a ratio scale and the relationship, while accelerating, does not reverse direction, satisfying the monotonic assumption required for a linear coefficient"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Omega squared is an effect-size measure derived from ANOVA that provides a less biased estimate of population variance explained than eta squared. However, it is an effect size for group differences, not a correlation coefficient measuring the degree of curvilinear association between two continuously measured variables. The reviewer's point about the ANOVA framework fits eta more precisely.",
        "B": "Eta (η) is mathematically derived from the ANOVA framework: it equals the square root of SSbetween divided by SStotal when the continuous predictor is grouped. This connection is exactly what the reviewer described. Eta is specifically appropriate for quantifying nonlinear association between ratio-scale variables, capturing variance explained by any systematic — not just linear — trend in the data.",
        "C": "Epsilon squared is a bias-corrected alternative to eta squared as an ANOVA effect size, similar in spirit to omega squared. Like omega squared, it is an effect-size index rather than a correlation coefficient designed to represent the strength of curvilinear association between two continuous variables; it does not serve the same conceptual function as eta in this context.",
        "D": "Pearson r measures the linear component of association and requires a linear (not just monotonic) relationship to be fully informative. Even if the relationship does not reverse, the accelerating nonlinear curve described means that Pearson r would underrepresent the true strength of association. Moreover, the scenario specifies the researcher's concern about losing metric information — eta retains this information while accommodating nonlinearity, making it superior to Pearson r here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-113-vignette-L5",
      "source_question_id": "113",
      "source_summary": "Eta is the correlation coefficient used to assess the degree of association between two variables measured on an interval or ratio scale when their relationship is nonlinear.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher collects continuous measurements on two variables from 250 participants. When she plots the data, she observes that as scores on the first variable increase from low to high, scores on the second variable first decrease, then stabilize, then increase again — forming a roughly U-shaped cloud of points. She notes that a widely used index based solely on how scores deviate from the group average in a straight-line fashion produces a value near zero, despite the two variables clearly moving together in a systematic way. Both variables were obtained using instruments that produce scores with a true zero point and equal intervals between units. A colleague suggests using a different index that is actually computable from the ratio of explained variation to total variation when the continuous predictor is divided into ordered segments, and which makes no assumption about the shape of the relationship.",
      "question": "Which index is the colleague most likely recommending?",
      "options": {
        "A": "Spearman rho, because converting both variables to ranks before computing the index removes the distorting influence of the U-shaped pattern and produces an accurate measure of the monotonic trend",
        "B": "Eta, because it measures the strength of association between two variables measured on scales with true zero points and equal units when the relationship between them follows a nonlinear pattern",
        "C": "Pearson r computed on squared values of the first variable, because introducing a quadratic term linearizes the U-shaped relationship and allows a standard linear coefficient to capture the association accurately",
        "D": "Omega squared, because dividing the continuous predictor into segments converts the design into a one-way ANOVA and omega squared provides the least biased estimate of the proportion of variance explained"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Spearman rho converts data to ranks and assesses the strength of a monotonic relationship — one that moves consistently in one direction. A U-shaped relationship is non-monotonic: it decreases then increases, which means rank-ordering the data would not resolve the problem and Spearman rho would still substantially misrepresent the association. The colleague's description of making no assumption about the shape of the relationship does not fit a coefficient limited to monotonic trends.",
        "B": "Eta (η) is designed exactly for this situation: two variables measured on interval or ratio scales (the scenario describes true zero points and equal intervals) with a nonlinear — here, U-shaped — relationship. It is computed from the ratio of between-segment variation to total variation, makes no assumption about the form of the relationship, and will not be distorted by the canceling directional trends that cause a linear index to approach zero. All details in the scenario point to eta.",
        "C": "Computing Pearson r on the squared values of the predictor is a form of polynomial regression preprocessing that can linearize certain curvilinear relationships. However, this approach requires the researcher to correctly specify the shape of the curve in advance, introduces a transformed variable rather than analyzing the original measurements, and does not correspond to a standard, named index of curvilinear association. The colleague described an index computed directly from variation in the original data, not a transformed predictor.",
        "D": "Omega squared is an effect-size statistic used to estimate the proportion of variance in a dependent variable explained by a grouping factor in ANOVA. While the colleague's description of dividing the predictor into segments superficially resembles an ANOVA setup, omega squared is a bias-corrected effect size, not a correlation coefficient for curvilinear association. It also does not have the mathematical property of making no assumption about the shape of the relationship in the way that eta does, and it is not the standard measure recommended for assessing the strength of a nonlinear relationship between two continuously measured variables."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-10-vignette-L1",
      "source_question_id": "10",
      "source_summary": "The multiple-sample chi-square test is used to compare the number of adults living in rural, urban, or suburban communities who have received a diagnosis of a bipolar disorder, depressive disorder, or anxiety disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "chi-square",
        "categorical",
        "frequency"
      ],
      "vignette": "A researcher wants to determine whether the frequency of psychiatric diagnoses — bipolar disorder, depressive disorder, or anxiety disorder — differs across three residential settings: rural, urban, and suburban communities. She collects data by recording the categorical diagnosis type and residential category for 900 adults. The researcher plans to compare observed frequencies against expected frequencies to determine whether diagnosis type and residential setting are independent. She notes that all expected cell counts exceed five, satisfying the assumptions for the planned statistical test.",
      "question": "Which statistical test is most appropriate for the researcher's analysis?",
      "options": {
        "A": "One-way ANOVA",
        "B": "Multiple-sample chi-square test",
        "C": "Independent-samples t-test",
        "D": "Pearson product-moment correlation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "One-way ANOVA is used to compare means of a continuous dependent variable across three or more groups. Because the researcher's data consist of categorical frequencies of diagnosis types rather than continuous scores, ANOVA is not appropriate here.",
        "B": "The multiple-sample chi-square test (chi-square test of independence) is correct. It compares observed versus expected frequencies of categorical variables — here, diagnosis type across three residential settings — which precisely matches the study design described.",
        "C": "The independent-samples t-test compares the means of a continuous variable between exactly two groups. This study involves three residential groups and categorical (frequency) data, so the t-test does not fit.",
        "D": "The Pearson correlation assesses the linear relationship between two continuous variables. Residential setting and diagnosis type are both categorical, making this test inapplicable to the described data."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-10-vignette-L2",
      "source_question_id": "10",
      "source_summary": "The multiple-sample chi-square test is used to compare the number of adults living in rural, urban, or suburban communities who have received a diagnosis of a bipolar disorder, depressive disorder, or anxiety disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "chi-square",
        "proportions"
      ],
      "vignette": "A public health research team surveys 1,200 adults across rural, urban, and suburban counties to examine whether proportions of individuals diagnosed with bipolar disorder, depressive disorder, or anxiety disorder vary by community type. The survey is administered online, and the team notes that rural participants are on average ten years older than urban participants — a demographic difference they plan to address in a follow-up study. For their primary analysis, the team intends to use chi-square to test whether the distribution of diagnosis categories differs significantly across the three community types, with all cell expected frequencies meeting the minimum threshold.",
      "question": "Which statistical test best fits the team's primary analysis as described?",
      "options": {
        "A": "Factorial ANOVA",
        "B": "Kruskal-Wallis test",
        "C": "Multiple-sample chi-square test",
        "D": "Binary logistic regression"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Factorial ANOVA examines the effects of two or more categorical independent variables on a continuous dependent variable. The diagnosis outcome here is categorical (frequency counts), and the team is not measuring a continuous dependent variable, so factorial ANOVA is inappropriate.",
        "B": "The Kruskal-Wallis test is a nonparametric analog to one-way ANOVA used to compare ranked continuous or ordinal data across three or more groups. While it handles non-normal distributions, it is designed for ranked scores, not for categorical frequency data as described here.",
        "C": "The multiple-sample chi-square test is correct. It compares the proportions of categorical outcomes — diagnosis type — across multiple independent groups (rural, urban, suburban), which is exactly what this study is designed to do.",
        "D": "Binary logistic regression predicts a binary categorical outcome from one or more predictors. While it can handle categorical predictors, it is designed to model odds of a two-category outcome, not to compare frequency distributions of a three-category diagnosis variable across groups."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-10-vignette-L3",
      "source_question_id": "10",
      "source_summary": "The multiple-sample chi-square test is used to compare the number of adults living in rural, urban, or suburban communities who have received a diagnosis of a bipolar disorder, depressive disorder, or anxiety disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "contingency"
      ],
      "vignette": "A team of epidemiologists constructs a contingency table cross-tabulating community type (rural, suburban, urban) with type of mental health diagnosis (bipolar disorder, depressive disorder, anxiety disorder) for 750 adults who sought outpatient services in the past year. The team notes that the three diagnostic groups differ substantially in mean age and that anxiety disorder was the most common diagnosis in all three communities, which the researchers suggest could reduce statistical power. Despite these observations, the researchers proceed with a test designed to evaluate whether the pattern of diagnosis frequencies is the same or different across community types, reporting a statistic with 4 degrees of freedom.",
      "question": "Which statistical procedure did the epidemiologists most likely use?",
      "options": {
        "A": "One-way MANOVA",
        "B": "Multiple-sample chi-square test",
        "C": "Repeated-measures ANOVA",
        "D": "Mann-Whitney U test"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "One-way MANOVA tests whether group membership (a categorical independent variable) predicts scores on multiple continuous dependent variables simultaneously. The outcome data here are categorical diagnosis frequencies, not continuous scores, making MANOVA inappropriate despite the presence of multiple diagnostic categories.",
        "B": "The multiple-sample chi-square test is correct. A 3×3 contingency table (3 community types × 3 diagnosis categories) yields (3-1)(3-1) = 4 degrees of freedom, exactly matching the reported value. This test evaluates whether the frequency distributions of a categorical variable differ across independent groups.",
        "C": "Repeated-measures ANOVA is used when the same participants are measured on a continuous variable at multiple time points or conditions. The study involves different individuals across independent community groups, not repeated measurements of the same individuals, ruling out this option.",
        "D": "The Mann-Whitney U test is a nonparametric test comparing ranked distributions between exactly two independent groups. This study involves three independent community groups and categorical frequency data, so the Mann-Whitney U does not apply."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-10-vignette-L4",
      "source_question_id": "10",
      "source_summary": "The multiple-sample chi-square test is used to compare the number of adults living in rural, urban, or suburban communities who have received a diagnosis of a bipolar disorder, depressive disorder, or anxiety disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "independence"
      ],
      "vignette": "A health disparities researcher recruits separate, non-overlapping samples of adults from three distinct geographic settings and records which of three mutually exclusive diagnostic categories each participant falls into. She is primarily interested in whether diagnosis type and geographic setting are independent of one another or whether knowing someone's location provides information about their likely diagnosis. A colleague suggests she should instead use a regression approach because the researcher is effectively predicting group membership from diagnostic categories, and the colleague points out that the sample sizes within each setting differ considerably. The researcher rejects this suggestion, arguing that her data structure calls for a different analytic strategy.",
      "question": "Which statistical test did the researcher most likely choose, and why?",
      "options": {
        "A": "Multinomial logistic regression, because the outcome variable has more than two categories and continuous predictors are involved",
        "B": "Multiple-sample chi-square test, because the analysis compares frequency distributions of a categorical variable across independent groups without modeling predictors",
        "C": "Discriminant function analysis, because the goal is to identify which combination of categorical variables best separates the geographic groups",
        "D": "One-way ANOVA with post hoc tests, because three groups are being compared and unequal sample sizes can be accommodated using weighted means"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Multinomial logistic regression models the probability of membership in each category of a multi-category outcome variable as a function of predictor variables. Although it can handle a three-category outcome, it is a regression approach — precisely what the researcher rejected — and it requires predictors to be entered into the model, which does not fit her data structure of comparing frequency distributions across groups.",
        "B": "The multiple-sample chi-square test is correct. The researcher's goal is to test whether geographic setting and diagnosis category are independent — a question answered by examining frequency distributions in a contingency table. Unlike regression, this approach does not require continuous predictors or a modeling framework, which is consistent with the researcher's rejection of the colleague's suggestion.",
        "C": "Discriminant function analysis identifies linear combinations of continuous predictors that best separate pre-defined groups. The diagnostic categories here are nominal and not continuous, and the researcher is not constructing a discriminant function; she is testing the independence of two categorical variables.",
        "D": "One-way ANOVA compares means of a continuous dependent variable across three or more groups and can accommodate unequal sample sizes. However, the dependent variable in this study is a categorical diagnosis type, not a continuous score, making ANOVA fundamentally unsuitable regardless of sample size differences."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-10-vignette-L5",
      "source_question_id": "10",
      "source_summary": "The multiple-sample chi-square test is used to compare the number of adults living in rural, urban, or suburban communities who have received a diagnosis of a bipolar disorder, depressive disorder, or anxiety disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team collects records from community mental health centers in three geographically distinct areas and tallies how many individuals in each area fall into one of three non-overlapping groups based on the nature of their presenting concerns at intake. The team is not interested in scores or measurements — only in whether the relative sizes of the three groups look similar or different depending on which geographic area is examined. Importantly, no individual appears in more than one area or more than one presenting-concern group, and the team verifies that every cell of their summary table has a sufficiently large count before proceeding. A colleague argues that because the team is comparing three areas, they should use the same test a psychologist would use to compare average scores across three classrooms, but the research team disagrees and selects a different procedure entirely.",
      "question": "Which statistical procedure did the research team most likely select?",
      "options": {
        "A": "One-way ANOVA, because three independent groups are being compared and no participants overlap across groups",
        "B": "Kruskal-Wallis test, because the team verified distributional assumptions and three geographic areas are being compared",
        "C": "Multiple-sample chi-square test, because the data consist of frequency counts for mutually exclusive categories compared across independent groups",
        "D": "McNemar's test, because a two-by-two comparison of related proportions is embedded within the larger design"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "One-way ANOVA is precisely the test the colleague recommended — comparing three groups — and the research team explicitly rejected it. ANOVA requires a continuous dependent variable (mean scores), not frequency counts of categorical group membership. The team's data are counts, not scores, making ANOVA incorrect despite the surface similarity of comparing three independent groups.",
        "B": "The Kruskal-Wallis test is the nonparametric alternative to one-way ANOVA and is used when continuous or ordinal data are ranked across three or more independent groups. The team's data are nominal frequency counts, not ranked ordinal scores, so the Kruskal-Wallis test does not apply even though three areas are being compared and assumptions were checked.",
        "C": "The multiple-sample chi-square test is correct. The team has frequency counts of individuals in mutually exclusive categorical groups (presenting-concern type) across independent geographic areas, with no participant appearing in more than one cell, and all expected cell counts are adequate. This is the defining scenario for a chi-square test of independence across multiple samples.",
        "D": "McNemar's test is used to compare proportions in a two-by-two table of matched or repeated-measures categorical data — for example, the same participants measured before and after an intervention. The present study involves three areas and three presenting-concern groups with no repeated measurement or matching, so McNemar's test is fundamentally inapplicable."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-11-vignette-L1",
      "source_question_id": "11",
      "source_summary": "The t-test for correlated samples is used to compare the scores of subjects in two groups on a measure of symptom severity after receiving one of two brief treatments for social anxiety disorder, where subjects were matched in pairs based on the severity of their symptoms.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "matched pairs",
        "correlated samples",
        "t-test"
      ],
      "vignette": "A researcher wants to compare two brief treatments for social anxiety disorder. To control for pre-existing differences in symptom severity, participants are assigned to matched pairs based on their baseline scores on a social anxiety measure, with one member of each pair receiving Treatment A and the other receiving Treatment B. After treatment, the researcher compares post-treatment symptom scores between the two groups. Because participants were organized into matched pairs and the data consist of two related groups, the researcher must select an appropriate inferential statistical test. The research team discusses which t-test is most appropriate for these correlated samples.",
      "question": "Which statistical test is most appropriate for analyzing these data?",
      "options": {
        "A": "Independent samples t-test",
        "B": "t-test for correlated samples",
        "C": "One-sample t-test",
        "D": "One-way ANOVA"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The independent samples t-test is used when two groups are unrelated and participants are not matched or paired. Here, participants are explicitly organized into matched pairs, creating a dependency between observations that violates the independence assumption of this test.",
        "B": "The t-test for correlated samples (also called the paired or dependent samples t-test) is appropriate when two groups are related through matching or repeated measurement. Because participants are matched on baseline severity, scores within each pair are not independent, and this test accounts for that correlation, increasing statistical power.",
        "C": "The one-sample t-test compares a single group's mean to a known or hypothesized population value. This study involves two treatment groups being compared to each other, not a single group compared to a population parameter.",
        "D": "A one-way ANOVA is used to compare means across three or more independent groups. This study has only two groups, and because those groups are matched rather than independent, ANOVA is not the appropriate choice here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-11-vignette-L2",
      "source_question_id": "11",
      "source_summary": "The t-test for correlated samples is used to compare the scores of subjects in two groups on a measure of symptom severity after receiving one of two brief treatments for social anxiety disorder, where subjects were matched in pairs based on the severity of their symptoms.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "matched",
        "dependent"
      ],
      "vignette": "A clinical researcher recruits adults diagnosed with social anxiety disorder and pairs each participant with another participant of similar symptom severity based on initial screening scores. One member of each pair is assigned to a mindfulness-based intervention and the other to a cognitive restructuring program, both lasting four weeks. The researcher notes that participants in the mindfulness group had slightly higher rates of comorbid depression, though this difference was not statistically significant. At the end of treatment, symptom severity scores are collected for both groups, and the researcher plans a statistical comparison of the two dependent sets of scores.",
      "question": "Given the design of this study, which statistical test should the researcher use to compare post-treatment symptom severity between the two groups?",
      "options": {
        "A": "Independent samples t-test",
        "B": "Wilcoxon signed-rank test",
        "C": "t-test for correlated samples",
        "D": "Mann-Whitney U test"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The independent samples t-test requires that observations in the two groups be statistically independent of one another. Because participants were paired on symptom severity, their scores are correlated within each pair, violating the independence assumption required for this test.",
        "B": "The Wilcoxon signed-rank test is the nonparametric equivalent of the paired samples t-test, used when the assumption of normally distributed difference scores is violated. The vignette does not indicate violated parametric assumptions, so this nonparametric alternative is not the best first choice.",
        "C": "The t-test for correlated samples is appropriate when two groups are linked through matching or repeated measurement, as is the case here. Pairing participants on baseline severity creates a dependency between the two sets of scores, and this test explicitly accounts for that relationship to yield a more powerful comparison. The detail about comorbid depression is a neutral distractor that does not change the analytic choice.",
        "D": "The Mann-Whitney U test is a nonparametric alternative to the independent samples t-test, used when groups are unrelated and parametric assumptions are not met. This study involves matched pairs producing dependent scores, so a test designed for independent groups is inappropriate regardless of its parametric status."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-11-vignette-L3",
      "source_question_id": "11",
      "source_summary": "The t-test for correlated samples is used to compare the scores of subjects in two groups on a measure of symptom severity after receiving one of two brief treatments for social anxiety disorder, where subjects were matched in pairs based on the severity of their symptoms.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "pairs"
      ],
      "vignette": "A research team is evaluating two brief interventions for social anxiety disorder by recruiting 40 adults from community mental health clinics. Before randomization, the team ranks all participants by their scores on a validated anxiety questionnaire and then assigns individuals into pairs, placing the two most symptomatic participants together, the next two together, and so on, until 20 pairs are formed. Within each pair, one participant is randomly assigned to Intervention X and the other to Intervention Y. Because the clinics serve a predominantly urban, lower-income population, the investigators are also considering whether socioeconomic factors might confound the results. After eight weeks, symptom severity is measured for all participants, and the team must choose an inferential test to evaluate whether the two interventions differ in effectiveness.",
      "question": "Which inferential statistical test is most appropriate for this study's comparison of post-treatment symptom severity scores?",
      "options": {
        "A": "Independent samples t-test",
        "B": "t-test for correlated samples",
        "C": "Analysis of covariance (ANCOVA)",
        "D": "Repeated measures ANOVA"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The independent samples t-test is designed for unrelated groups in which no pairing or dependency exists between observations. The explicit creation of matched pairs based on pre-treatment anxiety scores establishes a dependency between scores in each pair, violating this test's independence assumption.",
        "B": "The t-test for correlated samples is the correct choice because participants were deliberately paired prior to randomization based on ranked symptom severity scores, creating correlated data within each pair. This design reduces error variance attributable to individual differences in baseline severity, and this test capitalizes on that reduction. The concern about socioeconomic confounds is a red herring that does not alter the fundamental analytic structure imposed by the matched-pairs design.",
        "C": "ANCOVA is used to statistically control for a covariate when groups are otherwise independent. While it might seem appealing given the investigators' concern about socioeconomic confounds, the fundamental structure of the data is a matched-pairs comparison, not an independent-groups design requiring covariate adjustment. ANCOVA does not account for the within-pair dependency.",
        "D": "Repeated measures ANOVA is appropriate when the same participants provide scores under multiple conditions or time points, creating within-subject dependency. Here, different participants make up each member of a pair — pairing creates between-unit correlation, not within-subject repeated measurement — so repeated measures ANOVA does not accurately reflect the design."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-11-vignette-L4",
      "source_question_id": "11",
      "source_summary": "The t-test for correlated samples is used to compare the scores of subjects in two groups on a measure of symptom severity after receiving one of two brief treatments for social anxiety disorder, where subjects were matched in pairs based on the severity of their symptoms.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A psychotherapy researcher is comparing two manualized four-session protocols for social anxiety in a sample of 36 adults. Recognizing that wide individual variability in baseline symptom severity could obscure true treatment differences, the researcher uses a pre-screening procedure in which participants are ranked on their intake questionnaire scores and grouped sequentially into dyads of two; one member of each dyad is then randomly allocated to Protocol A and the other to Protocol B. An analyst reviewing the study notes that the procedure appears similar to a between-subjects design because each participant receives only one treatment, and suggests using an independent-samples approach. At the end of treatment, post-intervention symptom severity scores are the primary outcome.",
      "question": "Which analytical approach best reflects the structure of this study's data, and why?",
      "options": {
        "A": "Independent samples t-test, because each participant receives only one treatment, producing two separate groups of scores",
        "B": "One-way ANOVA, because the researcher is comparing mean symptom severity across two treatment conditions drawn from a broader population",
        "C": "t-test for correlated samples, because the pre-screening procedure that pairs participants on baseline severity creates statistical dependency within each dyad",
        "D": "Analysis of covariance (ANCOVA), because baseline severity scores were collected and can serve as a covariate to reduce residual variance"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Although it is true that each participant receives only one treatment, the independent samples t-test is inappropriate here because the dyad formation process deliberately links the two members of each pair on pre-treatment severity. This linkage violates the independence of observations assumption and reduces error variance in a way that the independent samples t-test cannot exploit. The analyst's suggestion in the vignette is the red herring to be reasoned past.",
        "B": "One-way ANOVA compares means across two or more independent groups and is equivalent to an independent samples t-test when only two groups are present. Like the independent samples t-test, it does not account for the within-dyad dependency created by the matching procedure, and using it would ignore the variance reduction achieved by pairing, leading to a less powerful and conceptually incorrect analysis.",
        "C": "The t-test for correlated samples is correct because the researcher's pre-screening procedure — ranking all participants and grouping them sequentially into dyads — is precisely the matched-pairs design this test was developed for. The pairing on baseline severity makes scores within each dyad statistically dependent rather than independent. This test accounts for that dependency and capitalizes on the reduced error variance, yielding a more powerful test. The subtle hint is 'variance' in the vignette, referring to the researcher's motivation for pairing.",
        "D": "ANCOVA uses baseline scores as a continuous covariate to statistically adjust post-treatment means, which is a legitimate strategy for independent-groups designs. However, the researcher's design does not merely collect baseline scores as a covariate — it uses those scores structurally to create matched pairs whose within-pair correlation must be modeled directly. ANCOVA applied to an independent-groups framework would misrepresent the dependency inherent in the matched-dyad structure."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-11-vignette-L5",
      "source_question_id": "11",
      "source_summary": "The t-test for correlated samples is used to compare the scores of subjects in two groups on a measure of symptom severity after receiving one of two brief treatments for social anxiety disorder, where subjects were matched in pairs based on the severity of their symptoms.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators recruits adults struggling with intense discomfort in social situations for a study comparing two brief programs delivered at a community clinic. Before assigning anyone to a program, the team ranks all volunteers from most to least affected using a structured interview, then works down the ranked list, placing the first two people into a unit, the next two into a unit, and so on, until every volunteer belongs to exactly one two-person unit. Within each unit, a coin flip determines who enters Program Blue and who enters Program Green. Both programs last one month, and at the end, all participants complete the same self-report checklist measuring how much distress they experience in everyday social situations. Because both programs are new, the clinic director is primarily interested in whether one program produces lower distress scores than the other, and a consultant mentions that collecting scores from the same people at two different time points would have made the analysis straightforward.",
      "question": "Which statistical procedure is most defensible for analyzing the distress scores collected at the end of the study?",
      "options": {
        "A": "A procedure that compares the average scores of two unrelated groups by assuming each observation contributes independently to the overall estimate of variability",
        "B": "A procedure designed for situations in which the same individuals contribute scores under two different conditions, allowing each person to serve as their own reference point",
        "C": "A procedure that accounts for a pre-existing numerical link between each Blue-program participant and a specific Green-program participant when computing the comparison statistic",
        "D": "A procedure that adjusts the end-of-program scores using the initial ranking values as a statistical control, removing their influence before comparing the two programs"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This describes the independent samples t-test, which assumes no dependency among observations. The ranking-and-pairing procedure used before assignment creates a deliberate numerical link between specific Blue and Green participants within each unit. Treating these linked observations as independent would violate a core assumption of this test and forfeit the precision gained by the pairing strategy.",
        "B": "This describes a repeated measures or within-subjects approach — such as a paired t-test applied to pre- and post-scores from the same individual. The consultant's comment about collecting scores at two time points is a deliberate red herring designed to make this option appealing. In fact, different people are in Program Blue versus Program Green; the dependency arises not from repeated measurement of the same individual but from the structural pairing of two different people who were matched on initial severity.",
        "C": "This describes the t-test for correlated samples applied to matched-pairs data. The investigators' procedure of ranking all volunteers and placing consecutive dyads into units creates a precise numerical relationship between the two members of each unit — one goes to Blue, one to Green — making their scores statistically linked rather than independent. The appropriate analysis explicitly uses that within-pair link to compute difference scores for each unit, which is exactly what the paired or correlated-samples t-test does.",
        "D": "This describes analysis of covariance, which retains an independent-groups framework while statistically partialling out the influence of a baseline variable. Although the initial rankings are numerically available, they were not used as a measured covariate — they were used structurally to create matched units. Applying ANCOVA would treat the groups as independent after adjusting for baseline, thereby ignoring the unit-level dependency that the design intentionally introduced and that must be modeled directly."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-01-vignette-L1",
      "source_question_id": "01",
      "source_summary": "The study has 3 independent variables (review program, initial test anxiety level, and gender) and 2 dependent variables (mock SAT exam scores and anxiety level after the programs).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "independent variables",
        "dependent variables",
        "MANOVA"
      ],
      "vignette": "A researcher designs a study to evaluate the effectiveness of two SAT preparation programs. Participants are classified by their initial test anxiety level (high vs. low) and by gender (male vs. female). The study has three independent variables — review program type, initial test anxiety level, and gender — and two dependent variables: mock SAT exam scores and post-program anxiety level. The researcher wants to select the most appropriate statistical analysis to simultaneously examine the effects of all independent variables on both dependent variables.",
      "question": "Which statistical analysis is most appropriate for this research design?",
      "options": {
        "A": "Factorial ANOVA",
        "B": "MANOVA",
        "C": "ANCOVA",
        "D": "Multiple regression"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Factorial ANOVA is appropriate when there are multiple independent variables but only a single dependent variable. Because this study has two dependent variables (mock SAT scores and post-program anxiety), factorial ANOVA cannot simultaneously analyze both outcomes and would require running separate analyses, inflating Type I error.",
        "B": "MANOVA (Multivariate Analysis of Variance) is the correct choice because it is specifically designed to handle designs with multiple independent variables and multiple dependent variables simultaneously. It controls the familywise error rate across the dependent variables and tests whether group membership (defined by the independent variables) predicts a linear combination of the dependent variables.",
        "C": "ANCOVA (Analysis of Covariance) is used when a researcher wishes to statistically control for a continuous covariate while examining group differences on a single dependent variable. This design does not involve a covariate being partialed out; it involves multiple dependent variables, which ANCOVA cannot handle simultaneously.",
        "D": "Multiple regression examines the relationship between multiple predictor variables and a single continuous outcome variable. While it could model each dependent variable separately, it is not designed to analyze multiple dependent variables simultaneously and does not test multivariate group differences as MANOVA does."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-01-vignette-L2",
      "source_question_id": "01",
      "source_summary": "The study has 3 independent variables (review program, initial test anxiety level, and gender) and 2 dependent variables (mock SAT exam scores and anxiety level after the programs).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "factorial",
        "multivariate"
      ],
      "vignette": "A team of educational psychologists recruits 240 high school juniors to examine the impact of SAT preparation on academic performance and emotional well-being. Participants are randomly assigned to one of two preparation programs, and they are also categorized by pre-existing anxiety level (high vs. low) and gender. The study tracks two outcomes — standardized test performance and post-intervention anxiety — collected at the same follow-up session. Several participants also reported a history of ADHD, which the team notes but does not formally incorporate into the factorial design.",
      "question": "What is the most accurate description of the number and type of variables in this study's factorial design?",
      "options": {
        "A": "Two independent variables and one dependent variable",
        "B": "Three independent variables and two dependent variables",
        "C": "Three independent variables and one dependent variable, with one covariate",
        "D": "Two independent variables and two dependent variables"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This option understates both the number of independent and dependent variables. The design includes three independent variables (program type, anxiety level, and gender) and two dependent variables (test performance and post-intervention anxiety), so identifying only two IVs and one DV mischaracterizes the structure of the study.",
        "B": "This is correct. The three independent variables are preparation program type, initial anxiety level, and gender. The two dependent variables are standardized test performance and post-intervention anxiety level. The ADHD history noted in the vignette is not incorporated into the design, so it is a reported characteristic rather than a design variable.",
        "C": "Although the ADHD history might suggest a covariate, the vignette explicitly states it was not incorporated into the design. Furthermore, there are two dependent variables, not one. Selecting this option reflects being misled by the ADHD detail and undercounting the dependent variables.",
        "D": "This option correctly identifies the two dependent variables but undercounts the independent variables. Gender, initial anxiety level, and program type each independently define groups within the factorial structure, making three independent variables — not two."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-01-vignette-L3",
      "source_question_id": "01",
      "source_summary": "The study has 3 independent variables (review program, initial test anxiety level, and gender) and 2 dependent variables (mock SAT exam scores and anxiety level after the programs).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "MANOVA"
      ],
      "vignette": "A researcher studying academic interventions recruits students and assigns them to one of two SAT prep programs. Students are also grouped based on whether they entered the study with high or low anxiety, and data are separately recorded for male and female participants. At the end of the study, two scores are collected from each participant: a simulated SAT score and a self-report anxiety rating. Because anxiety was assessed both before and after the program, a colleague suggests that the pre-intervention anxiety score should be treated as a covariate rather than a grouping variable in the MANOVA framework. The lead researcher disagrees, arguing that the study design specifies pre-intervention anxiety as a categorical classification variable.",
      "question": "If the lead researcher's position is followed, how should the study's variables be classified?",
      "options": {
        "A": "Two independent variables and two dependent variables",
        "B": "Three independent variables and one dependent variable, with anxiety as a covariate",
        "C": "Three independent variables and two dependent variables",
        "D": "Two independent variables and two dependent variables, with gender as a moderator"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This option incorrectly counts only two independent variables, likely omitting initial anxiety level because of the colleague's suggestion to treat it as a covariate. However, the lead researcher explicitly classifies it as a categorical grouping variable, making it a third independent variable alongside program type and gender.",
        "B": "This option follows the colleague's recommendation rather than the lead researcher's. Treating pre-intervention anxiety as a covariate would shift the design to ANCOVA-like framework and reduce the independent variable count, but the lead researcher has specified it as a categorical independent variable — meaning this classification is incorrect per the stated design.",
        "C": "This is correct. If initial anxiety is treated as a categorical classification variable (high vs. low), the design has three independent variables: preparation program, initial anxiety level, and gender. The two dependent variables are the simulated SAT score and the post-program anxiety rating. This is consistent with the lead researcher's position.",
        "D": "This option misidentifies gender as a 'moderator' rather than an independent variable. While gender could theoretically function as a moderator in a moderation analysis, within a factorial design it is classified as an independent variable. Additionally, this option undercounts the independent variables by only naming two."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-01-vignette-L4",
      "source_question_id": "01",
      "source_summary": "The study has 3 independent variables (review program, initial test anxiety level, and gender) and 2 dependent variables (mock SAT exam scores and anxiety level after the programs).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "simultaneous"
      ],
      "vignette": "An educational psychologist publishes a study in which high school students were divided into groups based on the type of preparation course they attended, their pre-study nervousness category, and their self-identified sex. At the conclusion of the study, each student completed two assessments: one measuring academic readiness and one measuring emotional state. The researcher reports a single omnibus test result followed by univariate follow-ups and explicitly notes that the goal was to simultaneously evaluate both outcomes in relation to all grouping factors. A critical reviewer argues that the study would have been better served by running two separate analyses, one for each outcome, to simplify interpretation.",
      "question": "The reviewer's critique most directly challenges which aspect of the study's analytic approach?",
      "options": {
        "A": "The use of a multivariate test with three independent variables and two dependent variables",
        "B": "The use of a factorial design with a covariate partialed from the dependent variables",
        "C": "The reliance on univariate follow-up tests rather than multivariate contrasts",
        "D": "The use of a within-subjects factor alongside between-subjects independent variables"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. The study's design features three between-subjects grouping variables (preparation course type, nervousness category, and sex) and two simultaneously measured outcomes (academic readiness and emotional state), which defines a MANOVA framework. The reviewer's suggestion to run separate analyses directly challenges the rationale for using a multivariate test that analyzes both dependent variables at once — the defining feature of this design.",
        "B": "This option incorrectly introduces the concept of a covariate. No covariate is described in the vignette; pre-study nervousness is used as a categorical grouping variable (independent variable), not a continuous variable being statistically controlled. The reviewer's critique does not pertain to covariate adjustment.",
        "C": "While univariate follow-ups are mentioned in the vignette, the reviewer's critique targets the decision to use a simultaneous multivariate approach at all, not the specific form of the follow-up tests. Challenging the follow-up strategy would be a narrower methodological point that does not address the reviewer's core argument about running separate analyses.",
        "D": "The vignette describes no within-subjects factor; all grouping variables (preparation course, nervousness level, sex) define separate, non-repeated groups. A mixed design would require at least one factor in which the same participants appear in multiple conditions, which is not described here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-01-vignette-L5",
      "source_question_id": "01",
      "source_summary": "The study has 3 independent variables (review program, initial test anxiety level, and gender) and 2 dependent variables (mock SAT exam scores and anxiety level after the programs).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of researchers recruits teenagers from two different tutoring companies and, before any sessions begin, classifies each student as either 'highly worried' or 'minimally worried' about an upcoming high-stakes test, based on a brief screener. Students are further sorted by whether they identify as male or female. After the tutoring period ends, each student completes two written assessments: one measuring how well they performed on a practice version of the test and one capturing how calm or anxious they currently feel. A statistician reviewing the study notes that the design produces a matrix of group comparisons that spans eight distinct cells and that two separate outcomes were measured at the same time point.",
      "question": "Which of the following most precisely characterizes the variable structure of this study?",
      "options": {
        "A": "A 2×2×2 between-subjects design with a single continuous dependent variable and a repeated measure",
        "B": "A 2×2 between-subjects design with two dependent variables and one covariate",
        "C": "A 2×2×2 between-subjects design with two dependent variables measured simultaneously",
        "D": "A 2×2×2 mixed design with two dependent variables, one measured before and one after the intervention"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The 2×2×2 structure is correctly identified — three binary grouping factors produce eight cells — but this option mischaracterizes the dependent variable structure. The vignette describes two outcomes both collected at a single post-intervention time point, not a single continuous DV plus a repeated measure. A repeated measure would require the same construct measured at multiple time points for the same participants.",
        "B": "This option undercounts the grouping factors. The design has three independent classification variables (tutoring company, worry level, and sex), not two, so the correct notation is 2×2×2 rather than 2×2. Additionally, there is no covariate described; both outcomes are dependent variables measured at the same post-intervention point.",
        "C": "This is correct. Three binary between-subjects grouping factors (tutoring company affiliation, initial worry classification, and sex) create a 2×2×2 factorial structure with eight cells, consistent with the statistician's observation. The two assessments (practice test performance and current anxiety) are both post-intervention outcome measures collected simultaneously, making them two dependent variables — the hallmark of a multivariate (MANOVA-appropriate) design.",
        "D": "Although the 2×2×2 structure and two dependent variables are correctly noted, this option incorrectly characterizes the design as mixed. A mixed design requires at least one within-subjects factor, meaning the same participants are measured under multiple conditions of that factor. Here, the two outcomes (test performance and anxiety) are distinct constructs measured at a single time point, not the same construct measured before and after — so no within-subjects factor exists."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-06-vignette-L1",
      "source_question_id": "06",
      "source_summary": "The psychologist is using a mixed research design to compare the effects of mindfulness-based cognitive therapy (MBCT) and mindfulness-based stress reduction (MBSR) on the anxiety symptoms of clinic patients with social anxiety disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "mixed design",
        "between-subjects",
        "within-subjects"
      ],
      "vignette": "A researcher is studying the effects of MBCT versus MBSR on anxiety symptoms in patients with social anxiety disorder. Participants are randomly assigned to either the MBCT or MBSR condition, making treatment group a between-subjects factor. Anxiety symptoms are measured at baseline, post-treatment, and three-month follow-up, making time a within-subjects factor. The researcher uses a mixed design to test both the main effects and the interaction between treatment type and time.",
      "question": "Which statistical approach is most appropriate for analyzing the data from this mixed design study?",
      "options": {
        "A": "Independent samples t-test",
        "B": "One-way between-subjects ANOVA",
        "C": "Mixed-design (split-plot) ANOVA",
        "D": "Repeated measures ANOVA"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "An independent samples t-test compares means between two independent groups at a single time point and cannot accommodate a within-subjects factor like repeated measurement across time. It would not capture the interaction between treatment group and time that this design is meant to detect.",
        "B": "A one-way between-subjects ANOVA compares means across groups without accounting for repeated measurements within participants. It would ignore the repeated measures structure of the data, violating the assumption of independence and failing to model the within-subjects time factor.",
        "C": "This is correct. A mixed-design (split-plot) ANOVA is specifically designed for studies with at least one between-subjects factor (treatment group: MBCT vs. MBSR) and at least one within-subjects factor (time: baseline, post-treatment, follow-up), allowing analysis of main effects and their interaction.",
        "D": "A repeated measures ANOVA models change over time within participants but does not include a between-subjects grouping factor. It would not allow comparison between the MBCT and MBSR groups, making it unsuitable for this two-group, multi-time-point design."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-06-vignette-L2",
      "source_question_id": "06",
      "source_summary": "The psychologist is using a mixed research design to compare the effects of mindfulness-based cognitive therapy (MBCT) and mindfulness-based stress reduction (MBSR) on the anxiety symptoms of clinic patients with social anxiety disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "interaction effect",
        "repeated measures"
      ],
      "vignette": "A clinical psychologist conducts a study comparing MBCT and MBSR for patients with social anxiety disorder. Many participants report comorbid depression, but the researcher proceeds because comorbidity rates are similar across both groups. Anxiety is assessed at baseline and again after eight weeks of treatment. The researcher is particularly interested in whether the two treatments differ in how much they reduce anxiety over time, which is captured by the interaction effect in the analysis of the repeated measures data.",
      "question": "What does a statistically significant interaction effect indicate in this study?",
      "options": {
        "A": "The overall average anxiety scores differ significantly between MBCT and MBSR regardless of time.",
        "B": "The rate of change in anxiety over time differs between the MBCT and MBSR groups.",
        "C": "Anxiety scores decreased significantly from baseline to post-treatment for all participants.",
        "D": "The two groups had significantly different anxiety scores at baseline before treatment began."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This describes the main effect of group (treatment type), not the interaction effect. A significant main effect of group would indicate that one treatment produces lower anxiety overall, irrespective of when anxiety was measured, which is a different inferential question than whether the two treatments differ in their trajectories over time.",
        "B": "This is correct. In a mixed design, a significant interaction effect between group and time means that the pattern of change in the dependent variable over time (anxiety reduction) is not the same across groups — that is, MBCT and MBSR differ in how much they reduce anxiety across the measurement periods.",
        "C": "This describes the main effect of time, which would indicate that anxiety scores changed across measurement points when collapsed across both treatment groups. It does not capture whether the two treatments produced different trajectories, which is what an interaction effect addresses.",
        "D": "Baseline group differences would be a concern for internal validity (e.g., a selection confound) rather than a result of the interaction analysis. A significant interaction effect is not interpreted as a baseline difference but rather as differential change trajectories between groups over time."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-06-vignette-L3",
      "source_question_id": "06",
      "source_summary": "The psychologist is using a mixed research design to compare the effects of mindfulness-based cognitive therapy (MBCT) and mindfulness-based stress reduction (MBSR) on the anxiety symptoms of clinic patients with social anxiety disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "sphericity"
      ],
      "vignette": "A researcher compares MBCT and MBSR on anxiety outcomes in a social anxiety sample, measuring symptoms at four time points: baseline, six weeks, twelve weeks, and six-month follow-up. Prior to interpreting results, the researcher runs Mauchly's test and finds it is statistically significant. Given this result, a colleague suggests applying a correction to the degrees of freedom before interpreting the F-statistic for the within-subjects factor. The researcher notes that the study involves two separate treatment groups that were randomly assigned, and that each participant only received one treatment.",
      "question": "What assumption has been violated, and what is the primary reason for applying a degrees-of-freedom correction in this context?",
      "options": {
        "A": "Homogeneity of variance has been violated; the correction adjusts for unequal variances between the two treatment groups.",
        "B": "Normality has been violated; the correction compensates for non-normal distributions in the repeated measures data.",
        "C": "Sphericity has been violated; the correction (e.g., Greenhouse-Geisser) adjusts for unequal variances and covariances among the repeated measures to prevent inflation of the Type I error rate.",
        "D": "Independence of observations has been violated; the correction accounts for correlated errors between participants across the repeated measurement points."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Homogeneity of variance (Levene's test) concerns equality of variances between independent groups, not the structure of variances among repeated measures. While relevant to the between-subjects factor in a mixed design, a significant Mauchly's test specifically signals a sphericity violation within the repeated measures factor, not a between-group variance problem.",
        "B": "Normality is a separate assumption typically evaluated with tests like Shapiro-Wilk or visual inspection of residuals. Mauchly's test does not assess normality, and degrees-of-freedom corrections like Greenhouse-Geisser or Huynh-Feldt are not remedies for non-normality but are instead designed to address the specific pattern of covariance inequality among repeated measures.",
        "C": "This is correct. Mauchly's test evaluates the sphericity assumption — that the variances of the differences between all pairs of repeated measures are equal. A significant result indicates sphericity has been violated, which inflates the F-statistic and increases Type I error risk. Corrections such as Greenhouse-Geisser reduce the degrees of freedom to produce a more conservative and accurate F-test.",
        "D": "Independence of observations is an assumption that concerns whether different participants' scores are unrelated, which is addressed by study design (e.g., avoiding data contamination). Mauchly's test does not assess independence between participants; it assesses the covariance structure across repeated time points within participants. A degrees-of-freedom correction does not remedy a violation of between-participant independence."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-06-vignette-L4",
      "source_question_id": "06",
      "source_summary": "The psychologist is using a mixed research design to compare the effects of mindfulness-based cognitive therapy (MBCT) and mindfulness-based stress reduction (MBSR) on the anxiety symptoms of clinic patients with social anxiety disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "carryover"
      ],
      "vignette": "A psychologist designs a study in which all patients with social anxiety disorder receive both MBCT and MBSR, with half receiving MBCT first and half receiving MBSR first, and a brief washout period separating the two conditions. Anxiety is measured after each treatment phase. After collecting data, the researcher finds that patients who received MBCT first showed substantially lower anxiety during the subsequent MBSR phase compared to patients who received MBSR first before MBCT. The researcher initially attributes this pattern to MBCT's superior efficacy, but a senior colleague raises a methodological concern suggesting the design itself may be responsible for the observed pattern.",
      "question": "What methodological concern is the senior colleague most likely raising?",
      "options": {
        "A": "Selection bias, because participants were not randomly assigned to a single treatment group, making the two groups systematically different before the study began.",
        "B": "A carryover effect, because the residual influence of the first treatment (MBCT) may have persisted into the second treatment phase, confounding interpretation of MBSR's effects.",
        "C": "A demand characteristics threat, because participants who completed MBCT first may have learned what outcomes the researcher expected and adjusted their self-reported anxiety accordingly during the MBSR phase.",
        "D": "Regression to the mean, because participants with the highest baseline anxiety who received MBCT first would naturally show lower scores in any subsequent measurement regardless of treatment."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Selection bias involves pre-existing systematic differences between groups before treatment. In this crossover design, all participants receive both treatments and are counterbalanced by order, so no systematic group difference at baseline is implied. The concern here is not about who was selected but about what happens across treatment phases within individuals.",
        "B": "This is correct. A carryover effect occurs in crossover or within-subjects designs when the effects of a prior condition persist into subsequent conditions, contaminating measurement of the later treatment. Here, the washout period may have been insufficient to eliminate MBCT's effects, so patients in the MBCT-first sequence brought residual benefit into the MBSR phase — making MBSR appear more effective for that group and confounding the comparison.",
        "C": "Demand characteristics refer to cue-driven changes in participant behavior based on perceived experimental expectations. While possible, this explanation requires an assumption that participants systematically inferred what was expected specifically because of treatment order — a more speculative and less parsimonious explanation than carryover. The differential pattern across counterbalancing orders is the hallmark of a carryover effect, not demand characteristics.",
        "D": "Regression to the mean is a statistical phenomenon where extreme initial scores tend toward the average on subsequent measurement, independent of treatment. It would predict lower scores at the second time point for all high-scoring participants regardless of which treatment they received first — it would not produce a differential pattern based on treatment order (MBCT-first vs. MBSR-first), which is the distinguishing feature of the observed findings."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-06-vignette-L5",
      "source_question_id": "06",
      "source_summary": "The psychologist is using a mixed research design to compare the effects of mindfulness-based cognitive therapy (MBCT) and mindfulness-based stress reduction (MBSR) on the anxiety symptoms of clinic patients with social anxiety disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team recruits clinic patients and divides them into two groups based on which program they are assigned to attend. Each participant's self-reported social discomfort is recorded at the start of the study and again at the end of an eight-week program. After reviewing the results, the team notices that participants in one group improved noticeably more than those in the other, but this difference in improvement was driven almost entirely by a subset of participants who reported the highest levels of distress at the outset. When these participants' scores are tracked, their scores at the end of the program are far closer to the group average than they were at the beginning, even in the group that received less focused attention. A consultant reviewing the data cautions the team against concluding that either program caused the observed improvement in these participants.",
      "question": "What phenomenon is the consultant most likely warning the research team about?",
      "options": {
        "A": "A carryover effect, because participants with high initial distress may have benefited from prior therapeutic contact before the programs began, inflating apparent improvement.",
        "B": "Regression to the mean, because participants selected for high initial scores will tend to score closer to the group average at a later time point due to measurement unreliability, independent of any treatment effect.",
        "C": "A ceiling effect, because participants with the highest initial distress reached the maximum possible improvement that the outcome measure could detect, making their gains appear artificially large.",
        "D": "An interaction effect, because the differential improvement between groups was moderated by initial distress level, indicating that treatment type and symptom severity jointly determined outcomes."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A carryover effect occurs in designs where participants are exposed to more than one condition sequentially, and residual effects of an earlier condition influence performance in a later one. This scenario describes participants in separate, non-overlapping groups — there is no evidence of sequential exposure to multiple conditions — so a carryover explanation is not supported by the described design.",
        "B": "This is correct. Regression to the mean is the statistical tendency for individuals who score at the extremes of a distribution at one time point to score closer to the group mean at a subsequent time point, driven partly by measurement error rather than true change. The consultant's caution is warranted because the observed movement toward the average in high-distress participants could reflect this artifact rather than a genuine treatment effect, and attributing these score changes to program content would be an inferential error.",
        "C": "A ceiling effect occurs when a measure lacks sufficient range to capture high scores, causing apparent clustering at the upper boundary of the scale. The scenario describes participants moving toward the group average from a high starting point — not reaching an upper limit of the scale — and the concern is about whether improvement is genuine, not about the measure's inability to detect further change. A ceiling effect would prevent detection of improvement, not inflate it.",
        "D": "An interaction effect in a research design refers to the finding that the effect of one variable (e.g., treatment type) depends on the level of another variable (e.g., initial distress). While the scenario does describe differential improvement by initial distress level, the consultant's concern is specifically about whether any of the observed improvement is real or artifactual — a methodological caution about a statistical artifact, not a finding about how variables jointly determine outcomes."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-04-vignette-L1",
      "source_question_id": "04",
      "source_summary": "The double-blind technique is used to reduce experimenter expectancy when designing a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "double-blind",
        "experimenter expectancy",
        "confound"
      ],
      "vignette": "A researcher is designing a clinical trial to test whether a new antidepressant reduces depressive symptoms compared to a placebo. She is concerned that experimenter expectancy could confound the results if the research assistants administering assessments know which participants received the active drug. To address this, she designs the study so that neither the participants nor the research assistants know which condition each participant has been assigned to.",
      "question": "Which research design technique is the researcher using to control for experimenter expectancy effects?",
      "options": {
        "A": "Single-blind procedure",
        "B": "Double-blind procedure",
        "C": "Matched-pairs design",
        "D": "Counterbalancing"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A single-blind procedure keeps only one party — typically the participant — unaware of condition assignment. Because the researcher is also keeping the research assistants uninformed, this goes beyond single-blind and therefore this option does not fully capture the design described.",
        "B": "The double-blind procedure ensures that neither participants nor experimenters (including assessors) know condition assignments, directly controlling for experimenter expectancy effects. This is precisely what the scenario describes and is the textbook solution to the problem the researcher identified.",
        "C": "Matched-pairs design involves pairing participants on relevant characteristics before random assignment to conditions. It controls for individual-difference confounds but does not address the problem of experimenters' knowledge influencing outcomes.",
        "D": "Counterbalancing is used in within-subjects designs to offset order and carryover effects by varying the sequence of conditions across participants. It does not address experimenter knowledge or expectancy bias."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-04-vignette-L2",
      "source_question_id": "04",
      "source_summary": "The double-blind technique is used to reduce experimenter expectancy when designing a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "blind",
        "internal validity"
      ],
      "vignette": "A research team is conducting a randomized controlled trial examining the effectiveness of a new cognitive-enhancing drug in older adults diagnosed with mild cognitive impairment. Several team members privately believe the drug will show large benefits, raising concerns about internal validity. To address this, the study coordinator arranges for the capsules — both active and placebo — to be prepared by an independent pharmacy using identical packaging, and ensures that outcome assessors are not informed of which participants received which capsule. However, participants are told they may or may not receive the active drug.",
      "question": "Which methodological strategy has the research team implemented to protect the study's internal validity from systematic bias introduced by the research team's beliefs?",
      "options": {
        "A": "Double-blind procedure",
        "B": "Random assignment",
        "C": "Single-blind procedure",
        "D": "Placebo control only"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "The double-blind procedure is correct because both the participants (who are told they may or may not receive the active drug) and the outcome assessors are kept unaware of condition assignment. This directly guards against experimenter expectancy effects stemming from the team's prior beliefs, protecting internal validity.",
        "B": "Random assignment refers to the process of randomly allocating participants to conditions and is mentioned implicitly in the scenario. While it controls for selection bias and many confounds, it does not on its own prevent the assessors' knowledge of condition from biasing their outcome ratings.",
        "C": "A single-blind procedure would keep only participants unaware of condition assignment. Because the scenario specifically describes outcome assessors also being kept uninformed, the design exceeds single-blind and this answer understates the method used.",
        "D": "A placebo control is a component of the study (identical capsules are used) but is not itself a blinding strategy. Using a placebo alone without blinding the assessors would still leave the study vulnerable to experimenter expectancy effects."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-04-vignette-L3",
      "source_question_id": "04",
      "source_summary": "The double-blind technique is used to reduce experimenter expectancy when designing a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "expectancy"
      ],
      "vignette": "A psychopharmacology researcher suspects that the high enthusiasm among her graduate student assessors for a novel anxiolytic medication may be influencing how they score participants' behavioral anxiety ratings. She notes that assessors who know a participant is in the treatment group tend to rate improvement more generously, even when using structured rating scales. Participants in the study were already randomly assigned and provided written informed consent explaining that they would not be told which pill they received until the study concluded. The researcher decides to extend this same information restriction to her assessment team going forward.",
      "question": "What methodological decision has the researcher made to protect against the bias she has identified?",
      "options": {
        "A": "Implementing an attention control condition",
        "B": "Adopting a placebo-controlled design",
        "C": "Implementing a double-blind procedure",
        "D": "Applying demand characteristics controls"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "The double-blind procedure is correct. The scenario describes participants already being kept unaware of their condition, and the researcher now extending the same information restriction to assessors. This two-sided blinding — both participant and assessor — is the defining feature of the double-blind method and directly targets the expectancy bias she observed.",
        "A": "An attention control condition matches participants on non-specific factors like therapist contact time to isolate the active ingredient of an intervention. While useful for controlling non-specific effects, it does not address the assessors' awareness of condition assignments that is driving the bias described.",
        "B": "A placebo-controlled design uses inert comparators to prevent participants from knowing whether they received an active treatment. This is already implied in the scenario (participants don't know which pill they received), but the additional step of blinding the assessors is what distinguishes the described solution — making placebo control alone insufficient.",
        "D": "Demand characteristics refer to cues in a study that signal how participants are expected to behave, potentially altering participant (not assessor) responses. While related to bias, this concept pertains to participants reading into the study's purpose, not to assessors' enthusiasm influencing their ratings."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-04-vignette-L4",
      "source_question_id": "04",
      "source_summary": "The double-blind technique is used to reduce experimenter expectancy when designing a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "allocation"
      ],
      "vignette": "A multicenter clinical trial is evaluating a new behavioral intervention for chronic pain. The study's statistician performs the allocation of participants to conditions using a centralized computerized system, and the codes linking participant IDs to conditions are held in a sealed file accessible only to the data safety monitoring board. Clinicians delivering the intervention are necessarily aware of what they are providing, but the independent evaluators conducting weekly pain assessments have no access to the allocation codes and are instructed not to discuss treatment details with participants before assessments. Upon reviewing the protocol, an external auditor notes that this design feature specifically guards against a form of systematic bias that arises when evaluators' foreknowledge shapes their measurement of outcomes.",
      "question": "Which specific design feature is the auditor identifying as the mechanism guarding against this form of systematic bias?",
      "options": {
        "A": "Randomized controlled trial design",
        "B": "Blinding of outcome assessors",
        "C": "Use of a centralized allocation system",
        "D": "Inclusion of an independent data safety monitoring board"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Blinding of outcome assessors is the specific mechanism the auditor is describing. Because clinicians cannot be blinded (they must deliver the intervention), this is a partial double-blind or assessor-blind design; the key protective feature is that evaluators conducting pain assessments do not know condition assignments, preventing their foreknowledge from systematically biasing outcome ratings — i.e., controlling experimenter expectancy effects.",
        "A": "The randomized controlled trial (RCT) design refers to the overall framework of random allocation and controlled comparison. While the RCT provides the structure within which blinding operates, the RCT design itself does not constitute the specific mechanism preventing evaluator foreknowledge from distorting assessments.",
        "C": "The centralized allocation system ensures allocation concealment — that those enrolling participants cannot predict or influence assignment. This controls selection bias during enrollment but does not prevent already-allocated evaluators from knowing condition status during outcome measurement, which is the bias the auditor flags.",
        "D": "The independent data safety monitoring board holds the allocation codes for safety and oversight purposes, not to blind outcome assessors. Its role is to monitor for adverse events and study integrity, not to serve as the mechanism preventing evaluator expectancy from influencing pain ratings."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-04-vignette-L5",
      "source_question_id": "04",
      "source_summary": "The double-blind technique is used to reduce experimenter expectancy when designing a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators is studying whether a new pill reduces fatigue in adults with a chronic illness. Everyone in the study receives a capsule each morning — some containing the active compound, others containing an inert filler — and the capsules are manufactured to look, smell, and taste identical. The people taking the capsules are not told which type they are receiving. The weekly interviews assessing fatigue levels are conducted by a separate group of staff members who work in a different building, have no contact with the prescribing clinicians, and are given only participant ID numbers with no accompanying treatment records. When the study concludes, a consultant reviewing the methodology praises this arrangement, noting it was specifically designed to prevent a particular form of systematic distortion that would otherwise occur because the interviewers care deeply about the treatment's success.",
      "question": "Which methodological approach does the consultant's praise specifically refer to?",
      "options": {
        "A": "Placebo control with demand characteristics management",
        "B": "Double-blind procedure",
        "C": "Single-blind procedure with interviewer training",
        "D": "Randomization with allocation concealment"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "The double-blind procedure is correct. The scenario describes two layers of unawareness: participants do not know whether they received the active compound or inert filler, and the interviewers assessing outcomes have no access to treatment records and are physically separated from those who know assignments. This dual blinding directly prevents the interviewers' enthusiasm for the treatment's success — i.e., experimenter expectancy — from systematically distorting the fatigue ratings.",
        "A": "Placebo control with demand characteristics management addresses the concern that participants will guess the study's purpose and alter their self-reports accordingly. While the identical capsules serve a placebo function, the consultant's specific praise targets the interviewers' potential bias, not participants' interpretation of cues — making this option an incomplete and misdirected explanation.",
        "C": "A single-blind procedure keeps only one party (typically participants) unaware of condition assignment. The scenario clearly extends unawareness to the outcome interviewers as well, which moves beyond single-blind. Interviewer training might reduce some forms of interviewer bias but does not constitute the systematic structural mechanism described — the interviewers' ignorance of condition — and therefore this option mischaracterizes the design.",
        "D": "Randomization with allocation concealment controls for selection bias by ensuring that those enrolling participants cannot predict or manipulate group assignments. While possibly present in this study, the consultant's praise is specifically directed at the arrangement preventing the interviewers' foreknowledge from distorting outcome measurement — a concern that allocation concealment addresses at enrollment, not at the assessment stage."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-12-vignette-L1",
      "source_question_id": "12",
      "source_summary": "Multivariate analysis of variance (MANOVA) is used to analyze data from a research study that includes two or more dependent variables measured on an interval or ratio scale.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "MANOVA",
        "dependent variables",
        "interval scale"
      ],
      "vignette": "A researcher designs a study to evaluate the effectiveness of a new cognitive-behavioral intervention for anxiety. She measures three outcomes simultaneously: self-reported anxiety, physiological arousal, and cognitive distortion scores, all collected on interval scales. She wants to determine whether group membership (treatment vs. control) predicts differences across these dependent variables simultaneously rather than analyzing each separately. The researcher notes that MANOVA is the appropriate technique because it accounts for the intercorrelations among the dependent variables and controls the familywise error rate. She proceeds to analyze the data using this multivariate approach.",
      "question": "Which statistical procedure is most appropriate for analyzing this study's data, and why?",
      "options": {
        "A": "Repeated measures ANOVA, because the same participants are providing multiple scores across time points, requiring a within-subjects design.",
        "B": "MANOVA, because the study includes two or more dependent variables measured on interval scales, and the researcher wants to test group differences across those variables simultaneously.",
        "C": "Factorial ANOVA, because the study involves multiple independent variables predicting a single outcome measure.",
        "D": "Multiple regression, because the researcher is using several continuous predictor variables to explain variance in a single criterion variable."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Repeated measures ANOVA handles one dependent variable measured at multiple time points within the same participants. This study does not describe a within-subjects time-series design; rather, it measures three distinct outcome constructs simultaneously.",
        "B": "Correct. MANOVA is specifically designed for research scenarios in which two or more dependent variables measured on interval or ratio scales are analyzed simultaneously. It tests group mean differences on a composite of dependent variables while controlling for intercorrelations among them and reducing familywise Type I error.",
        "C": "Incorrect. Factorial ANOVA involves two or more independent variables (factors) predicting a single dependent variable. This study has one grouping factor (treatment vs. control) and multiple dependent variables, which is the inverse of factorial ANOVA's structure.",
        "D": "Incorrect. Multiple regression uses two or more continuous predictors to explain variance in one outcome variable. This study has one categorical grouping variable and multiple dependent outcomes, which does not match the multiple regression framework."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-12-vignette-L2",
      "source_question_id": "12",
      "source_summary": "Multivariate analysis of variance (MANOVA) is used to analyze data from a research study that includes two or more dependent variables measured on an interval or ratio scale.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "multivariate",
        "dependent variables"
      ],
      "vignette": "A clinical psychologist is conducting a study comparing three treatment groups — cognitive-behavioral therapy, mindfulness-based therapy, and a waitlist control — for adults with comorbid depression and chronic pain. Participants range widely in age and baseline symptom severity, which the researcher notes as potential sources of variability. The researcher collects scores on both a validated depression inventory and a pain interference scale, treating both as continuous outcome measures. She is concerned that conducting separate tests for each outcome would inflate the probability of a Type I error, so she chooses a single multivariate procedure that simultaneously evaluates group differences across both dependent variables.",
      "question": "Which statistical test is the researcher using?",
      "options": {
        "A": "Analysis of covariance (ANCOVA), because the researcher is statistically controlling for baseline symptom severity as a covariate while comparing groups on a single outcome.",
        "B": "Discriminant function analysis, because the researcher is interested in classifying participants into groups based on a set of continuous predictor scores.",
        "C": "MANOVA, because the researcher is simultaneously testing group differences across two or more continuous dependent variables to control familywise error.",
        "D": "Profile analysis, because the researcher wants to compare whether the pattern of scores across multiple measures differs by group over repeated measurement occasions."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. ANCOVA involves one dependent variable and statistically removes variance attributable to a covariate before testing group differences. While baseline severity might appear to call for ANCOVA, the defining feature here is that there are two separate dependent variables being analyzed simultaneously, not one.",
        "B": "Incorrect. Discriminant function analysis is mathematically related to MANOVA but serves a different purpose: it is used to predict group membership from a set of continuous predictors, not to test whether groups differ on a set of dependent variables.",
        "C": "Correct. MANOVA is designed precisely for this scenario — multiple continuous dependent variables, multiple groups, and a concern about inflated Type I error from conducting separate ANOVAs. The wide age range and symptom variability are contextual details that do not change the appropriateness of MANOVA.",
        "D": "Incorrect. Profile analysis is a variant of MANOVA used when the same construct is measured multiple times under different conditions to examine the pattern or 'profile' of scores. This study measures two conceptually distinct outcomes (depression and pain), not repeated assessments of the same construct."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-12-vignette-L3",
      "source_question_id": "12",
      "source_summary": "Multivariate analysis of variance (MANOVA) is used to analyze data from a research study that includes two or more dependent variables measured on an interval or ratio scale.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "intercorrelated"
      ],
      "vignette": "A research team studying school-based interventions randomly assigns classrooms to either a social-emotional learning program or a standard curriculum. At the end of the school year, they collect data on students' empathy ratings, peer-conflict frequency, and academic engagement scores, all measured on continuous scales. The team is aware that these three outcomes are intercorrelated with one another and wants to respect that relationship in the analysis rather than treat each outcome as independent. A statistician advises that performing three separate F-tests would not only ignore the relationships among outcomes but would also increase the likelihood of falsely declaring at least one effect significant. The team decides to use a single omnibus procedure that treats the three outcomes as a unified criterion space.",
      "question": "Which analytical approach is the statistician recommending, and what feature of the data primarily justifies its use over separate ANOVAs?",
      "options": {
        "A": "Structural equation modeling, because the intercorrelations among the outcomes suggest an underlying latent construct that the researcher wants to model explicitly.",
        "B": "Repeated measures ANOVA, because the three continuous outcomes are collected from the same students at the same time point, making them a within-subjects factor.",
        "C": "MANOVA, because it simultaneously evaluates group differences across multiple continuous dependent variables while accounting for their intercorrelations and controlling familywise error.",
        "D": "Multivariate multiple regression, because the team has a continuous independent variable predicting multiple continuous outcomes and wants to quantify the predictive relationships."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. Structural equation modeling (SEM) is used to test theorized causal and correlational structures among latent and observed variables. While SEM can handle intercorrelated outcomes, the scenario describes a straightforward group-comparison question with no mention of modeling latent constructs or causal pathways.",
        "B": "Incorrect. Repeated measures ANOVA is appropriate when the same participants are measured on the same dependent variable across multiple time points or conditions. Having three different outcome measures at one time point does not constitute a within-subjects repeated measures design.",
        "C": "Correct. MANOVA is the appropriate choice when a researcher compares groups on two or more continuous, intercorrelated dependent variables simultaneously. Treating the outcomes as a unified criterion space respects their intercorrelations and prevents inflation of the familywise Type I error rate that would result from separate ANOVAs.",
        "D": "Incorrect. Multivariate multiple regression is used when a researcher has multiple continuous predictors forecasting multiple continuous outcomes and is focused on regression coefficients and prediction. The study uses a categorical grouping variable (intervention vs. control) and is focused on mean group differences, not regression-based prediction."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-12-vignette-L4",
      "source_question_id": "12",
      "source_summary": "Multivariate analysis of variance (MANOVA) is used to analyze data from a research study that includes two or more dependent variables measured on an interval or ratio scale.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "familywise"
      ],
      "vignette": "A neuropsychologist is evaluating the cognitive effects of a newly developed medication for early-stage Alzheimer's disease in a randomized controlled trial. Participants are assigned to the drug or placebo condition, and the researcher records scores on processing speed, working memory capacity, and executive function at six-month follow-up. Mindful that running three independent significance tests could produce at least one false positive by chance alone, the researcher elects instead to conduct a single omnibus test that treats the set of cognitive scores as a composite. Upon obtaining a significant omnibus result, she follows up with separate univariate tests to identify which specific cognitive domains drove the effect. A colleague suggests that because the outcomes are measured on ratio scales and are theoretically related to a common underlying cognitive reserve construct, a latent-variable approach might be preferable.",
      "question": "The researcher's initial omnibus test, which treats the cognitive scores collectively to manage familywise error, is best described as which of the following?",
      "options": {
        "A": "Confirmatory factor analysis, because the researcher is testing whether the three cognitive outcomes load onto a single latent construct of cognitive reserve.",
        "B": "MANOVA, because the researcher is simultaneously comparing two groups on multiple continuous outcome variables while protecting against inflated Type I error across the set of outcomes.",
        "C": "Bonferroni-corrected ANOVA, because the researcher is conducting multiple separate significance tests on individual outcome variables while adjusting the alpha level to control familywise error.",
        "D": "Multivariate profile analysis, because the researcher is comparing a pattern of scores across three repeated cognitive assessments within the same measurement battery."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Confirmatory factor analysis tests whether observed variables load onto a hypothesized latent structure; it does not test mean differences between groups. Although the colleague mentions a latent cognitive reserve construct, the researcher's actual analytic goal is to compare group means across the outcome composite, not to model factor structure.",
        "B": "Correct. Despite the red herring about latent variables introduced by the colleague, the researcher's described procedure — a single omnibus test comparing two groups across multiple continuous ratio-scale outcomes to manage familywise error, followed by univariate follow-ups — is the defining procedure of MANOVA.",
        "C": "Incorrect. Bonferroni correction involves adjusting the per-comparison alpha level when conducting multiple separate tests; it does not constitute a single omnibus test. The scenario explicitly states the researcher performs one unified test rather than multiple individual tests, ruling out this approach.",
        "D": "Incorrect. Profile analysis examines whether the pattern (shape) of scores across measurements differs between groups and typically requires the measures to be on the same scale or be commensurate. The three measures here are conceptually distinct cognitive domains, not a repeated-measures profile of a single construct, and the procedure is described as a single omnibus group-comparison test."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-12-vignette-L5",
      "source_question_id": "12",
      "source_summary": "Multivariate analysis of variance (MANOVA) is used to analyze data from a research study that includes two or more dependent variables measured on an interval or ratio scale.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team divides participants into two groups based on whether they received a structured program or no program. At the conclusion of the study, each participant is scored on three separate continuous measures that are expected to move together because they all reflect aspects of the same broad outcome domain. Before analyzing the data, the team notes that all three scores are measured on scales with equal intervals and a true zero point. Rather than examining each score in isolation — which they fear could produce misleading positive findings by chance — they apply a procedure that combines all three scores into a single mathematical composite and produces one overall significance test evaluating whether the groups differ. A peer reviewer praises this choice, noting that it also capitalizes on the fact that the scores share common variance, making the combined test more sensitive than treating each score independently. The team, however, wonders whether a technique that instead attempts to construct a new variable predicting which group each participant belongs to might have been equally informative.",
      "question": "The statistical procedure the team applied to compare the two groups across the three continuous outcome measures is best described as which of the following?",
      "options": {
        "A": "Canonical correlation analysis, because the team is examining the relationship between a set of predictor variables and a set of outcome variables by identifying maximally correlated linear composites.",
        "B": "Principal components analysis, because the team is reducing three intercorrelated continuous scores into a smaller number of composite dimensions that capture the shared variance among them.",
        "C": "MANOVA, because the team is testing group mean differences across multiple continuous outcome variables simultaneously, combining them into a composite while accounting for their shared variance.",
        "D": "Discriminant function analysis, because the team mentions the possibility of constructing a variable that predicts group membership from the continuous scores, which is the defining goal of that technique."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. Canonical correlation analysis identifies the linear combination of variables in one set (predictors) that maximally correlates with a linear combination in another set (criteria). While the team does form composites, their goal is to test group mean differences, not to quantify the association between two sets of variables — making canonical correlation a misfit despite superficial similarity.",
        "B": "Incorrect. Principal components analysis is a data-reduction technique that transforms a set of intercorrelated variables into uncorrelated components; it does not involve group comparisons or significance testing of mean differences. The mention of shared variance and combining scores is a deliberate red herring pointing toward PCA.",
        "C": "Correct. All defining features of MANOVA are present: two groups (from the structured program manipulation), three continuous outcome measures with equal intervals and a true zero point, scores that share common variance (intercorrelated), a single omnibus significance test that forms a mathematical composite of the outcomes, and the motivation to avoid spurious positive findings from separate tests. The peer reviewer's comment about leveraging shared variance further confirms MANOVA's multivariate advantage.",
        "D": "Incorrect. Discriminant function analysis (DFA) is mathematically the inverse of MANOVA — it uses continuous variables to predict group membership rather than testing group mean differences on continuous outcomes. The team's wondering about DFA is a deliberate red herring; the procedure they actually applied is described as testing whether the groups differ, not predicting which group a person belongs to."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-02-vignette-L1",
      "source_question_id": "02",
      "source_summary": "A test with a mean of 60 and standard deviation of 5, where test scores are normally distributed, has about 95% of scores falling between 50 and 70.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "normally distributed",
        "standard deviation",
        "mean"
      ],
      "vignette": "A psychologist develops a cognitive screening test administered to a large normative sample. The scores are normally distributed with a mean of 60 and a standard deviation of 5. The psychologist reports that approximately 95% of scores in the normative sample fall between 50 and 70. She notes this range spans exactly two standard deviations above and below the mean.",
      "question": "The psychologist's finding that approximately 95% of scores fall between 50 and 70 is BEST explained by which statistical principle?",
      "options": {
        "A": "The 68-95-99.7 empirical rule for normal distributions",
        "B": "The central limit theorem applied to sample means",
        "C": "Standard error of measurement reflecting test reliability",
        "D": "The z-score transformation defining percentile ranks"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. The empirical rule states that in a normal distribution, approximately 68% of scores fall within ±1 SD, 95% within ±2 SD, and 99.7% within ±3 SD of the mean. Here, 50 and 70 are exactly ±2 SDs from the mean of 60, accounting for the 95% figure.",
        "B": "The central limit theorem describes how the distribution of sample means approaches normality as sample size increases, regardless of the population distribution. It does not directly explain the proportion of individual scores falling within a given interval around the population mean.",
        "C": "Standard error of measurement (SEM) quantifies the precision of individual test scores by estimating how much scores would vary across repeated administrations. While related to standard deviation, SEM addresses score reliability rather than the proportion of scores within a distributional range.",
        "D": "Z-score transformation converts raw scores to standard scores indicating distance from the mean in SD units. While z-scores are used to locate scores within a normal distribution, the z-score transformation itself is not the principle explaining why 95% of scores fall within ±2 SDs — that is the empirical rule."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-02-vignette-L2",
      "source_question_id": "02",
      "source_summary": "A test with a mean of 60 and standard deviation of 5, where test scores are normally distributed, has about 95% of scores falling between 50 and 70.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "normal distribution",
        "standard deviation"
      ],
      "vignette": "A research team publishes norms for a new stress appraisal inventory given to 1,200 working adults across several industries. The lead researcher, who is particularly interested in occupational burnout among healthcare workers, notes that the inventory scores form a normal distribution with a mean of 60 and a standard deviation of 5. She reports that the vast majority of respondents — about 95% — scored between 50 and 70, even though healthcare workers as a subgroup tended to score slightly higher on average than other industries.",
      "question": "The researcher's statement that approximately 95% of scores fall between 50 and 70 is based on which statistical property of the data?",
      "options": {
        "A": "Regression toward the mean reducing extreme scores over repeated measurement",
        "B": "The relationship between the mean and standard deviation in a normal distribution, where ±2 SDs capture approximately 95% of scores",
        "C": "The confidence interval around the sample mean, which narrows as sample size increases",
        "D": "Homogeneity of variance across industry subgroups reducing overall score spread"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. In a normal distribution, approximately 95% of all individual scores fall within two standard deviations above and below the mean. With a mean of 60 and SD of 5, the interval from 50 to 70 represents exactly ±2 SDs, which is why the researcher can make this claim.",
        "A": "Regression toward the mean is the phenomenon whereby extreme scores on one measurement tend to be less extreme on a subsequent measurement. This principle describes change across time points, not the proportion of scores within a fixed range on a single administration.",
        "C": "A confidence interval around the sample mean describes the range within which the true population mean likely falls, and it does narrow with larger samples. However, confidence intervals address uncertainty about the mean, not the proportion of individual scores within a range around the mean.",
        "D": "Homogeneity of variance refers to the assumption that different groups share similar variances, which is relevant to statistical tests like ANOVA. Even if variance were homogeneous across subgroups, this would not itself explain why 95% of scores fall within ±2 SDs of the overall mean."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-02-vignette-L3",
      "source_question_id": "02",
      "source_summary": "A test with a mean of 60 and standard deviation of 5, where test scores are normally distributed, has about 95% of scores falling between 50 and 70.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "empirical"
      ],
      "vignette": "A psychometrician reviewing a test manual notes that the developers administered their instrument to a large, representative sample and found an approximately bell-shaped score distribution. The manual states that 'applying the empirical rule, nearly all common scores fall within three intervals of increasing coverage.' The psychometrician focuses specifically on the interval spanning 10 points above and 10 points below the test's central tendency value of 60, which the manual claims contains roughly 95% of examinees. A colleague suggests the claim might instead reflect confidence interval logic used to estimate population parameters, since the sample was so large.",
      "question": "The test manual's claim that approximately 95% of individual scores fall within 10 points of the mean is BEST attributed to which concept?",
      "options": {
        "A": "The 95% confidence interval around the sample mean, made precise by the large sample size",
        "B": "The principle that in a bell-shaped distribution, two standard deviations above and below the mean encompass approximately 95% of individual scores",
        "C": "The standard error of measurement, which quantifies the band of scores expected to contain a true score 95% of the time",
        "D": "Chebyshev's theorem guaranteeing that at least 75% of scores fall within two standard deviations for any distribution shape"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The empirical rule specifies that in a normally distributed (bell-shaped) dataset, approximately 95% of individual scores fall within ±2 standard deviations of the mean. With a mean of 60 and SD of 5, ±2 SDs spans 50 to 70 — exactly the 10-point interval described.",
        "A": "A 95% confidence interval estimates the range within which the true population mean likely falls with 95% probability, and it does become narrower with larger samples. However, it addresses estimation of the mean, not the proportion of individual scores within a fixed distance — making this a plausible but incorrect choice.",
        "C": "The standard error of measurement (SEM) is used to construct a confidence band around an individual's obtained score to estimate their true score. While it does produce an interval involving reliability and SD, the 95% band from SEM is not the same as the ±2 SD rule describing score distributions across a normative group.",
        "D": "Chebyshev's theorem is a non-parametric rule stating that for any distribution, at least 1 − 1/k² of data falls within k standard deviations of the mean. At k=2, this guarantees only 75%, not 95%. The 95% figure for ±2 SDs is specific to normal distributions via the empirical rule, not Chebyshev's theorem."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-02-vignette-L4",
      "source_question_id": "02",
      "source_summary": "A test with a mean of 60 and standard deviation of 5, where test scores are normally distributed, has about 95% of scores falling between 50 and 70.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "bell-shaped"
      ],
      "vignette": "A measurement specialist is evaluating a test whose score distribution, when plotted, produces a symmetric, bell-shaped curve. The test was normed on a large sample with a central value of 60; the spread of scores, calculated as the average squared deviation from the mean and then square-rooted, equals 5. A consultant reviewing the report argues that the claim '95% of scores fall between 50 and 70' is actually a confidence interval statement about the mean, not a statement about individual scores. The measurement specialist disagrees, pointing out that the interval's width relative to the spread is the key to understanding the claim. The consultant, persuaded by the large normative sample size, maintains that the interval must be narrower than stated if it were truly about the mean.",
      "question": "Which concept MOST precisely explains why approximately 95% of individual scores fall between 50 and 70 on this test?",
      "options": {
        "A": "The 95% confidence interval for the population mean narrows with large samples, but still spans ±2 standard errors, explaining the range",
        "B": "The standard error of measurement defines the interval within which 95% of repeated measurements of the same individual would fall",
        "C": "In a normal distribution, the interval spanning two standard deviations on either side of the mean contains approximately 95% of all individual observations",
        "D": "Chebyshev's inequality guarantees that at least 95% of scores fall within ±4.5 standard deviations for any symmetric distribution"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The empirical rule for normal distributions states that ±2 SDs from the mean captures approximately 95% of individual scores. With mean = 60 and SD = 5, ±2 SDs yields the interval 50–70. The consultant's error is confusing this individual-score distribution rule with the logic of confidence intervals for the mean.",
        "A": "A 95% confidence interval for the mean is computed using the standard error (SD/√n), not the standard deviation itself. With a large normative sample, this interval would be far narrower than ±10 points. The consultant's position is actually correct that a CI for the mean would not span 50–70, but the CI concept does not explain the 95% figure for individual scores.",
        "B": "The standard error of measurement (SEM) is used to construct reliability-based score bands around an individual examinee's obtained score. A 95% SEM-based band does involve multiplying SEM by approximately 1.96, but SEM ≠ SD and this concept applies to estimating true scores, not to describing what proportion of normative-group scores fall within a fixed interval.",
        "D": "Chebyshev's inequality does apply to any distribution shape regardless of symmetry, but it provides only a lower bound guarantee — at k=2 it guarantees only ≥75%, and achieving 95% requires k≈4.47. The 95%/±2 SD relationship is an exact property of the normal distribution specifically, not a Chebyshev guarantee."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-02-vignette-L5",
      "source_question_id": "02",
      "source_summary": "A test with a mean of 60 and standard deviation of 5, where test scores are normally distributed, has about 95% of scores falling between 50 and 70.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A test developer presents data showing that when a large group of people completed an assessment, the resulting scores, when tallied and plotted, formed a smooth, symmetric, and tapered shape. The midpoint of all the scores was 60. The developer calculated a single number — derived by finding how far each person's score deviated from that midpoint, squaring those differences, averaging them, and taking the square root — and obtained a value of 5. She then reported that 50 and 70 serve as natural boundaries capturing nearly all typical performance, with only rare individuals falling outside this window on either end. A reviewer challenged her, suggesting that the 50–70 range sounded more like an estimate of where the true group average probably lies rather than a description of where individual scores cluster.",
      "question": "The developer's claim that 50 and 70 are boundaries capturing approximately 95% of all individual scores is BEST explained by which principle?",
      "options": {
        "A": "The logic of interval estimation, in which a range is constructed around the sample mean to infer the population mean with 95% certainty",
        "B": "The reliability-based score band, in which a measurement's precision index is used to define the range capturing 95% of an individual's hypothetical repeated scores",
        "C": "The property of symmetric, tapered score distributions in which the interval from two units of spread below the center to two units above the center contains approximately 95% of all individual values",
        "D": "A distribution-free theorem guaranteeing that for any shape of score distribution, intervals defined by two units of spread above and below the center contain at least 95% of observations"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The developer's summary statistic — the square root of the average squared deviation from the mean — is the standard deviation (SD = 5). In a symmetric, bell-shaped (normal) distribution, approximately 95% of individual scores fall within ±2 SDs of the mean. The 50–70 range represents exactly mean ± 2 SD, which is the empirical rule for normal distributions applied to individual scores.",
        "A": "Interval estimation (confidence intervals) uses the standard error — not the standard deviation — to construct a range around the sample mean, estimating where the population mean likely falls. The reviewer's challenge reflects this logic, but the reviewer is wrong: the developer is describing where individual scores cluster, not where the group mean lies. With a large sample, a CI for the mean would be far narrower than 50–70.",
        "B": "The reliability-based score band (related to the standard error of measurement) estimates the range within which an individual's true score likely falls, accounting for the imprecision of a single test administration. While it also produces an interval around a score, SEM is derived from the SD and the test's reliability coefficient — it is not equal to the SD and does not describe the distribution of scores across a normative group.",
        "D": "Distribution-free (nonparametric) theorems like Chebyshev's inequality do guarantee minimum proportions within intervals defined by standard deviations, but Chebyshev guarantees only ≥75% within ±2 SDs for any distribution — not 95%. The 95% figure specifically requires the assumption that the distribution is normal (bell-shaped), making this option incorrect despite its surface plausibility."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-09-vignette-L1",
      "source_question_id": "09",
      "source_summary": "Increasing the size of alpha, increasing the effect size, and using an appropriate parametric test are methods to increase the statistical power of a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "statistical power",
        "effect size",
        "alpha"
      ],
      "vignette": "A researcher is designing a study on the effectiveness of a new cognitive-behavioral intervention for anxiety. After consulting with a statistician, she is concerned that her study may lack sufficient statistical power to detect the treatment effect. The statistician recommends several strategies, including increasing the alpha level from .01 to .05, ensuring the intervention produces a large effect size by using highly trained therapists, and selecting an appropriate parametric test for the data. The researcher wants to understand which combination of changes will most effectively increase her study's power.",
      "question": "Which of the following correctly identifies a method that would increase the statistical power of this study?",
      "options": {
        "A": "Decreasing the alpha level from .05 to .01 to reduce the probability of a Type I error",
        "B": "Increasing the alpha level, maximizing the effect size, and using an appropriate parametric test",
        "C": "Increasing the sample size while simultaneously decreasing the alpha level",
        "D": "Reducing measurement error by using a non-parametric test instead of a parametric test"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Decreasing alpha (e.g., from .05 to .01) actually reduces statistical power because it makes the criterion for rejecting the null hypothesis more stringent, thereby increasing the likelihood of a Type II error rather than reducing it.",
        "B": "Correct. Increasing alpha raises the threshold for Type II errors, a larger effect size means the true difference is more detectable, and appropriate parametric tests use data more efficiently than non-parametric alternatives — all of which combine to increase statistical power.",
        "C": "Incorrect. While increasing sample size does increase power, simultaneously decreasing alpha works against power by raising the bar for statistical significance. This combination has mixed and potentially counterproductive effects on power.",
        "D": "Incorrect. Switching to a non-parametric test when a parametric test is appropriate actually decreases power, because parametric tests are generally more statistically efficient and sensitive when their assumptions are met."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-09-vignette-L2",
      "source_question_id": "09",
      "source_summary": "Increasing the size of alpha, increasing the effect size, and using an appropriate parametric test are methods to increase the statistical power of a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "power",
        "parametric"
      ],
      "vignette": "A clinical researcher is conducting a randomized controlled trial comparing two depression treatments in a sample of older adults, many of whom have comorbid medical conditions. Midway through the study, the data safety monitoring board notes that the study may be underpowered. The researcher considers several adjustments: relaxing the significance criterion, ensuring the treatment protocol is intensive enough to produce a meaningful difference between groups, and reconsidering whether the chosen statistical test best suits the data's distributional properties. Despite the complexity introduced by the participants' comorbidities, the research team must decide which adjustments will most effectively address the power concern.",
      "question": "Which of the following adjustments would most directly increase the statistical power of this study?",
      "options": {
        "A": "Switching from a parametric to a non-parametric test to better accommodate the comorbid population",
        "B": "Reducing the significance criterion to .01 and increasing the number of outcome measures",
        "C": "Relaxing the significance criterion and intensifying the treatment to maximize the detectable difference between groups",
        "D": "Eliminating participants with comorbid conditions to reduce variance and improve internal validity"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. Switching to a non-parametric test when the data meet parametric assumptions reduces rather than increases power, because parametric tests capitalize more fully on the information in continuous data distributions.",
        "B": "Incorrect. Reducing alpha to .01 makes the significance criterion more stringent, which reduces power by increasing the risk of a Type II error. Adding outcome measures introduces additional complexity without directly addressing the power deficit.",
        "C": "Correct. Relaxing the significance criterion (i.e., increasing alpha, such as from .01 to .05) lowers the bar for rejecting the null hypothesis, and intensifying the treatment to produce a larger effect size makes the true difference more detectable — both are established methods for increasing statistical power.",
        "D": "Incorrect. Eliminating participants reduces sample size, which decreases power. While reducing variance can improve power, simply removing a subgroup without replacing those participants is more likely to harm than help statistical power, and it also threatens external validity."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-09-vignette-L3",
      "source_question_id": "09",
      "source_summary": "Increasing the size of alpha, increasing the effect size, and using an appropriate parametric test are methods to increase the statistical power of a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "Type II error"
      ],
      "vignette": "A researcher studying the effects of mindfulness training on cortisol levels in stressed college students finds that her study failed to reach statistical significance, despite clinical observations suggesting the intervention produced meaningful physiological changes. A consultant reviewing the study notes that the failure to detect an effect is likely due to a methodological limitation that increased the probability of a Type II error. The researcher had used a stringent significance criterion of p < .001, chosen a statistical test that did not assume normality despite the data being approximately normally distributed, and used a relatively small sample. The consultant recommends three specific changes to the study design for replication.",
      "question": "Which combination of changes would most directly reduce the probability of a Type II error and increase the likelihood of detecting a true treatment effect in the replication study?",
      "options": {
        "A": "Increase the sample size, use a more sensitive measurement instrument, and reduce the significance criterion to p < .0001",
        "B": "Relax the significance criterion to p < .05, switch to a parametric test, and use a treatment protocol designed to maximize the physiological response",
        "C": "Adopt a non-parametric test, increase the number of study sessions, and stratify the sample by baseline cortisol levels",
        "D": "Increase the sample size, reduce participant dropout, and use a one-tailed instead of two-tailed significance test"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Reducing the significance criterion to p < .0001 makes rejection of the null hypothesis far more difficult, which increases — not decreases — the risk of a Type II error. While a more sensitive instrument and larger sample are helpful, the alpha change undermines power.",
        "B": "Correct. Relaxing alpha to p < .05 (increasing alpha) lowers the threshold for significance and reduces Type II error risk; switching to a parametric test when data are approximately normal is more statistically powerful than a non-parametric alternative; and maximizing the treatment response increases effect size — all three are established power-enhancing strategies.",
        "C": "Incorrect. Adopting a non-parametric test when the data are approximately normally distributed sacrifices statistical efficiency, thereby reducing power. While stratification can reduce error variance, the non-parametric choice is a key error that would not adequately address the Type II error problem.",
        "D": "Incorrect. Increasing sample size and reducing dropout do increase power, and a one-tailed test can increase power when the direction of the effect is specified. However, this option omits the alpha-level and test-type adjustments that are central to the anchor concept, making it an incomplete answer compared to option B."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-09-vignette-L4",
      "source_question_id": "09",
      "source_summary": "Increasing the size of alpha, increasing the effect size, and using an appropriate parametric test are methods to increase the statistical power of a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "sensitivity"
      ],
      "vignette": "A pharmaceutical research team has completed a clinical trial investigating whether a novel anxiolytic produces a measurable reduction in physiological arousal compared to placebo. The trial used a very conservative significance criterion (p < .001) to protect against false positives, given prior concerns in the field about spurious findings. The team selected a distribution-free statistical procedure because one reviewer had flagged that the arousal data appeared skewed, though a subsequent analysis confirmed the data were in fact approximately normally distributed. The trial enrolled 40 participants per group, and the drug was administered at a dose the pharmacologists privately noted was lower than what would be expected to produce robust separation between conditions. The study failed to reach statistical significance, and the team is now evaluating whether the study's sensitivity was adequate.",
      "question": "Which factor most directly accounts for the study's failure to detect a treatment effect, and what change would best address it?",
      "options": {
        "A": "The conservative significance criterion inflated Type I error risk; relaxing it to p < .05 would eliminate false negatives",
        "B": "The sample size of 40 per group was insufficient; enrolling at least 100 participants per group would resolve the detection failure",
        "C": "Multiple factors reduced sensitivity, but switching to a parametric test, increasing alpha, and using a higher drug dose would most comprehensively restore the study's ability to detect a true effect",
        "D": "The skewed data distribution was the primary problem; using a distribution-free test was actually the correct decision and should be retained in the replication"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. The conservative alpha (p < .001) increased Type II error risk — not Type I error risk. Type I error is the false positive rate, which a conservative alpha actually minimizes. The question asks about failure to detect an effect (Type II error), so relaxing alpha addresses that, but this option misidentifies the mechanism and presents it incompletely.",
        "B": "Incorrect. Increasing sample size does increase power and could help the study detect a true effect. However, this option ignores the two other modifiable factors — the inappropriately chosen non-parametric test and the sub-therapeutic dose — that compound the sensitivity problem. Addressing only sample size would not comprehensively restore power.",
        "C": "Correct. Three factors reduced the study's sensitivity: (1) a conservative alpha increased Type II error risk and should be relaxed, (2) the use of a non-parametric test despite approximately normal data sacrificed statistical efficiency relative to an appropriate parametric test, and (3) the sub-therapeutic dose reduced effect size. Correcting all three directly maps onto the established methods for increasing statistical power.",
        "D": "Incorrect. The subsequent analysis confirmed the data were approximately normally distributed, meaning the distribution-free test was not necessary and in fact reduced statistical efficiency compared to an appropriate parametric test. Retaining the non-parametric approach would perpetuate a power-reducing decision, not resolve it."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-09-vignette-L5",
      "source_question_id": "09",
      "source_summary": "Increasing the size of alpha, increasing the effect size, and using an appropriate parametric test are methods to increase the statistical power of a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team designed a study to determine whether a specific training regimen improves performance on a fine motor coordination task. To ensure the study would withstand intense peer scrutiny, the investigators set an unusually demanding threshold for declaring a result meaningful — one far stricter than the conventional standard used in their field. They also opted for a less common analytical procedure because a single dataset from a pilot study appeared asymmetric, even though that pilot included only eight participants and the current full sample of 60 showed no such irregularity. Finally, to make the training sessions feasible given budgetary constraints, the team shortened the regimen considerably from what the intervention developers had originally specified as necessary for producing a detectable difference in performance. The study concluded without detecting any meaningful difference between the trained and untrained groups.",
      "question": "Which of the following best explains why the study failed to detect a difference, and what changes would most effectively address the underlying problem?",
      "options": {
        "A": "The sample was too small to yield stable estimates; recruiting a substantially larger group would most effectively resolve the detection failure",
        "B": "The demanding threshold, the choice of analytical procedure, and the shortened regimen each independently reduced the study's ability to detect a true difference; correcting all three would most comprehensively restore that ability",
        "C": "Demand characteristics from participants who knew they were being observed likely suppressed genuine performance differences; using blind assessment conditions would most effectively address this",
        "D": "The pilot study's asymmetric data indicated a genuine distributional issue in the population; the chosen analytical procedure was appropriate and the demanding threshold was the sole source of the detection failure"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. While sample size does influence detection ability (analogous to statistical power), the vignette specifies 60 participants — a reasonable number — and does not identify sample size as a limitation. The vignette instead emphasizes three other specific decisions: the threshold level, the analytical procedure choice, and the shortened regimen. Focusing solely on sample size misses the multivariate nature of the problem described.",
        "B": "Correct. The 'unusually demanding threshold' corresponds to a very low alpha level (e.g., p < .001), which increases the risk of missing a true effect (Type II error). The 'less common analytical procedure' chosen due to an artifact of a tiny pilot sample corresponds to an unnecessarily chosen non-parametric test despite approximately normal data in the full sample, reducing statistical efficiency. The 'shortened regimen' corresponds to a reduced effect size by delivering a sub-therapeutic dose of the intervention. Correcting all three maps directly onto the three established strategies for increasing statistical power.",
        "C": "Incorrect. Demand characteristics refer to participants changing their behavior because they perceive the study's purpose, which is a threat to internal validity rather than a power concern. Nothing in the vignette describes behavior indicative of demand characteristics, and blind assessment, while valuable for validity, does not address the three specific methodological decisions described as responsible for the null result.",
        "D": "Incorrect. The vignette explicitly states that the full sample of 60 showed no distributional irregularity — only the eight-person pilot did. Retaining the less common analytical procedure based on that unreliable pilot observation is unjustified and reduces statistical efficiency when the data do not require it. This option also incorrectly narrows the problem to the threshold alone, ignoring the compounding contributions of the analytical procedure and shortened regimen."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-07-vignette-L1",
      "source_question_id": "07",
      "source_summary": "Homoscedasticity describes a situation where the variability of scores on one variable is about the same at different values of the other variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "regression",
        "variability",
        "homoscedasticity"
      ],
      "vignette": "A researcher runs a regression analysis examining the relationship between hours of weekly exercise and depression scores in a large community sample. After fitting the model, she plots the residuals against predicted values to check regression assumptions. She observes that the spread of residuals—that is, the variability of depression scores around the regression line—appears roughly equal across all levels of weekly exercise, from zero hours to fifteen hours. The researcher notes that this pattern reflects an important property of well-behaved regression data.",
      "question": "Which assumption of regression analysis does the researcher's residual plot most clearly demonstrate?",
      "options": {
        "A": "Homoscedasticity",
        "B": "Multicollinearity",
        "C": "Normality of residuals",
        "D": "Independence of errors"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Homoscedasticity is correct. It refers specifically to the condition in which the variability of the outcome variable (or regression residuals) is approximately equal across all levels of the predictor variable. The scenario directly describes equal spread of residuals across all exercise levels, which is the defining feature of homoscedasticity.",
        "B": "Multicollinearity refers to high intercorrelations among predictor variables in a multiple regression model—not to the spread of residuals across predictor values. Nothing in the scenario involves multiple predictors or their intercorrelations.",
        "C": "Normality of residuals is a distinct regression assumption that concerns whether residuals are normally distributed, not whether their spread is constant across predictor levels. A residual plot showing roughly constant spread says nothing directly about the shape of the residual distribution.",
        "D": "Independence of errors is the assumption that residuals are not systematically related to one another across observations, often violated in time-series or clustered data. The scenario describes variability across predictor levels, not the sequential or clustered structure of the residuals."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-07-vignette-L2",
      "source_question_id": "07",
      "source_summary": "Homoscedasticity describes a situation where the variability of scores on one variable is about the same at different values of the other variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "residuals",
        "regression"
      ],
      "vignette": "A clinical researcher is examining whether childhood trauma scores predict adult anxiety levels in a sample of 200 outpatients, many of whom have comorbid depression. She fits a regression model and inspects a scatterplot of the residuals against the fitted values. Despite the added complexity of the comorbid depression in the sample, she notices that the vertical spread of data points around the regression line is consistent whether trauma scores are low, moderate, or high. Her advisor tells her this pattern is important for validating her statistical model.",
      "question": "What property of the regression model does the advisor most likely want the researcher to recognize in this scatterplot?",
      "options": {
        "A": "Absence of influential outliers",
        "B": "Linearity of the regression relationship",
        "C": "Homoscedasticity",
        "D": "Heteroscedasticity"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Homoscedasticity is correct. The scenario describes equal vertical spread of residuals across all levels of the predictor (trauma scores), which is the defining characteristic of homoscedasticity. This property is a key assumption for valid ordinary least squares regression inference.",
        "A": "Absence of influential outliers concerns whether specific data points exert undue leverage on the regression line, which is assessed through statistics like Cook's D or leverage values—not through uniform spread of residuals across predictor levels.",
        "B": "Linearity refers to whether the relationship between the predictor and outcome follows a straight line, which would be assessed by checking whether the residuals show a systematic curved pattern—not whether their spread is consistent across predictor levels.",
        "D": "Heteroscedasticity is actually the violation of the assumption being described here. Heteroscedasticity occurs when residual spread is unequal across predictor levels, which is the opposite of what the scenario depicts. Confusing the two terms is a common error that makes this distractor highly plausible."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-07-vignette-L3",
      "source_question_id": "07",
      "source_summary": "Homoscedasticity describes a situation where the variability of scores on one variable is about the same at different values of the other variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "scatterplot"
      ],
      "vignette": "A researcher investigating the link between socioeconomic status (SES) and academic achievement notices something interesting in her scatterplot: for children in low-SES families, achievement scores cluster tightly around the trend line, but for children in high-SES families, achievement scores fan out widely. She reports to her team that this pattern casts doubt on a key assumption of her planned statistical analysis. A colleague suggests the pattern might simply reflect greater individual differences in enrichment opportunities among higher-SES families, which is a meaningful substantive finding.",
      "question": "Which statistical assumption is most directly violated by the pattern the researcher observes?",
      "options": {
        "A": "Normality of the predictor variable",
        "B": "Absence of multicollinearity",
        "C": "Independence of observations",
        "D": "Homoscedasticity"
      },
      "correct_answer": "D",
      "option_explanations": {
        "D": "Homoscedasticity is correct. The scenario describes the opposite of homoscedasticity—a fan-shaped pattern in which residual variance increases at higher SES levels. This is the defining description of heteroscedasticity, a violation of the homoscedasticity assumption. Recognizing that a violated assumption is still the assumption being tested is the key insight at this level.",
        "A": "Normality of the predictor variable is not actually a required assumption of ordinary least squares regression; regression assumes normality of residuals, not of the predictor itself. Additionally, the pattern described—unequal spread across predictor levels—is not related to the distribution of SES scores.",
        "B": "Absence of multicollinearity concerns redundancy among multiple predictor variables in a regression model. Since the scenario involves only one predictor (SES), multicollinearity is not relevant, even though students familiar with multiple regression may be tempted by this option.",
        "C": "Independence of observations would be violated if students were clustered within schools or families such that their scores are correlated, not if the variability of scores differs across SES levels. The fan-shaped spread is a variance issue, not a dependency issue."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-07-vignette-L4",
      "source_question_id": "07",
      "source_summary": "Homoscedasticity describes a situation where the variability of scores on one variable is about the same at different values of the other variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A biostatistician reviewing a colleague's multiple regression output notes that the standard errors of the regression coefficients appear artificially small, leading to inflated t-statistics and overly narrow confidence intervals. She examines the residual plot and sees that the variance of residuals is noticeably larger among participants with high predicted scores than among those with low predicted scores. The colleague had assumed that the conventional formula for computing standard errors was appropriate for this dataset, but the biostatistician argues this assumption was not warranted. She recommends using robust standard errors as a remedy.",
      "question": "Which violated assumption most directly explains why the conventional standard error formula produces misleading results in this situation?",
      "options": {
        "A": "Homoscedasticity",
        "B": "Absence of autocorrelation",
        "C": "Normality of residuals",
        "D": "Exogeneity of predictors"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Homoscedasticity is correct, though it is named here as the violated assumption. When residual variance is unequal across predicted values (heteroscedasticity), ordinary least squares standard errors are biased, leading to inaccurate t-statistics and confidence intervals. The recommendation of robust standard errors is a well-known corrective for heteroscedasticity. This requires recognizing that discussing the violation of homoscedasticity still centers on the homoscedasticity assumption.",
        "B": "Absence of autocorrelation (independence of errors) is violated when residuals are correlated across observations over time or space, not when their variance differs across predicted values. Autocorrelation also distorts standard errors, making this a very plausible distractor, but the described pattern—increasing variance at higher predicted scores—is specific to heteroscedasticity.",
        "C": "Normality of residuals is a regression assumption whose violation primarily affects the validity of significance tests in small samples; it does not directly cause bias in the standard error formula itself, and it is not described by the pattern of unequal residual spread across predicted values.",
        "D": "Exogeneity of predictors (the assumption that predictors are uncorrelated with the error term) is violated in situations involving omitted variable bias or simultaneity, leading to biased coefficient estimates rather than biased standard errors. The described residual pattern and the recommended remedy (robust standard errors) do not address exogeneity violations."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-07-vignette-L5",
      "source_question_id": "07",
      "source_summary": "Homoscedasticity describes a situation where the variability of scores on one variable is about the same at different values of the other variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A graduate student collects data on how many hours per week people spend outdoors and their reported sense of well-being. When she graphs the data, she notices that people who spend very few hours outdoors are all clustered closely together near the trend line, while people who spend many hours outdoors show scores that are scattered widely above and below it. Her faculty mentor, who is focused on the strong positive trend in the data, tells her the relationship looks promising. However, a visiting methodologist quietly notes that a core requirement for the planned inferential procedure is not met here, and that the most straightforward corrective approach would be to transform one of the variables or use a different method for estimating uncertainty.",
      "question": "Which statistical concept does the visiting methodologist's concern most precisely reflect?",
      "options": {
        "A": "Heteroscedasticity as a violation of the homoscedasticity assumption",
        "B": "Non-linearity of the relationship between the two variables",
        "C": "Restriction of range in the lower end of the outdoor exposure variable",
        "D": "Positive skew in the distribution of well-being scores"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Heteroscedasticity as a violation of homoscedasticity is correct. The scenario describes a fan-shaped pattern in which data points are tightly clustered at low predictor values and widely scattered at high predictor values—the hallmark of unequal error variance across levels of the predictor. The methodologist's suggested remedies (variable transformation or alternative methods of estimating uncertainty, such as robust standard errors) are the standard responses to this specific problem.",
        "B": "Non-linearity refers to a curved rather than straight-line relationship between variables, which would be suggested by a systematic bowing or curving pattern in the data rather than a fanning out of scatter. The scenario explicitly describes a strong positive trend (consistent with linearity) but with increasing spread—a variance issue, not a shape-of-relationship issue.",
        "C": "Restriction of range occurs when the observed range of a variable is truncated compared to its true population range, which attenuates correlation estimates. While the clustering at low outdoor exposure values might superficially suggest this, the issue described is about spread of scores around the trend rather than a limited range of the predictor itself.",
        "D": "Positive skew in the distribution of well-being scores would be a concern about the shape of the marginal distribution of the outcome, not about whether variability around the trend line changes across predictor levels. Skewness and unequal residual variance are distinct problems with different remedies and different implications."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-03-vignette-L1",
      "source_question_id": "03",
      "source_summary": "The biggest threat to the internal validity of a study evaluating the effectiveness of a stress reduction technique for alleviating test anxiety is statistical regression.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "statistical regression",
        "extreme scores",
        "internal validity"
      ],
      "vignette": "A researcher recruits participants for a study on a new stress reduction technique by administering a test anxiety inventory and selecting only those who score in the highest 10% — individuals with extreme scores on the measure. After the intervention, participants are retested and show substantially lower anxiety scores. The researcher concludes that the technique significantly reduced test anxiety. A colleague raises a concern about a threat to internal validity that is particularly relevant whenever participants are selected based on extreme scores.",
      "question": "Which threat to internal validity is the colleague MOST likely identifying?",
      "options": {
        "A": "Maturation",
        "B": "Statistical regression to the mean",
        "C": "History",
        "D": "Instrumentation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Maturation refers to natural biological or psychological changes occurring within participants over time (e.g., growing older, becoming more experienced). While maturation can be a validity concern in longitudinal studies, it does not specifically account for why participants selected for extreme scores would show improvement regardless of treatment.",
        "B": "Statistical regression to the mean is the correct answer. When participants are selected specifically because of extremely high (or low) scores, subsequent scores will tend to move toward the population average on retesting — not because of the intervention, but due to measurement error and the statistical properties of extreme scores. This makes it the primary internal validity threat here.",
        "C": "History refers to external events occurring during the study period that could affect the outcome independent of the treatment. Although it is a genuine internal validity threat, it does not specifically relate to the selection of participants based on extreme scores, which is the defining feature of this scenario.",
        "D": "Instrumentation refers to changes in the measuring instrument or the way measurements are taken over the course of a study. While it can threaten internal validity, it does not explain why individuals selected for extreme scores would systematically improve regardless of any real treatment effect."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-03-vignette-L2",
      "source_question_id": "03",
      "source_summary": "The biggest threat to the internal validity of a study evaluating the effectiveness of a stress reduction technique for alleviating test anxiety is statistical regression.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "extreme scores",
        "pretest"
      ],
      "vignette": "A psychology graduate student designs a study to evaluate a mindfulness-based intervention for test anxiety. Participants are recruited from a large undergraduate pool and must score above the 90th percentile on a standardized anxiety measure at pretest to be enrolled; several participants also report a recent personal stressor unrelated to academics. Following eight weeks of mindfulness training, posttest scores are markedly lower than pretest scores for most participants, and the student attributes these gains entirely to the intervention. A faculty supervisor points out a major methodological flaw that threatens the study's ability to draw valid causal conclusions.",
      "question": "Which threat to internal validity BEST explains the supervisor's concern?",
      "options": {
        "A": "Selection bias",
        "B": "Testing effects",
        "C": "Statistical regression to the mean",
        "D": "History"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Statistical regression to the mean is correct. Because participants were selected on the basis of extreme pretest scores (top 10%), scores on retesting are expected to move toward the group average regardless of any intervention effect. This pattern could entirely or partially account for the observed pre-to-post improvement, invalidating the causal inference.",
        "A": "Selection bias refers to systematic differences between groups at the outset of a study, typically a concern in between-group comparisons. Although participants here were selected in a nonrandom way, the specific threat is not about group differences but about the statistical tendency of extreme scorers to score closer to the mean on retesting — that is regression, not selection bias per se.",
        "B": "Testing effects occur when prior exposure to a test influences performance on a subsequent administration of the same test (e.g., practice effects or sensitization). While plausible in a pretest-posttest design, testing effects do not specifically account for improvement tied to the selection of high-scorers; they would affect all participants similarly regardless of initial score level.",
        "D": "History refers to external events between the pretest and posttest that could affect participants' scores independently of the intervention. The mention of a personal stressor might suggest history, but this would typically increase anxiety scores rather than decrease them, and it does not explain the systematic pattern of improvement specifically seen in high scorers at baseline."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-03-vignette-L3",
      "source_question_id": "03",
      "source_summary": "The biggest threat to the internal validity of a study evaluating the effectiveness of a stress reduction technique for alleviating test anxiety is statistical regression.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "pretest"
      ],
      "vignette": "A clinical researcher evaluates a six-week cognitive-behavioral program intended to reduce test anxiety in college students. Enrollment is restricted to students who visited the university counseling center reporting significant distress about upcoming exams — a naturally occurring, help-seeking sample rather than one drawn randomly from the population. A pretest measure of anxiety is administered at intake, and students' scores at the end of the program are notably lower. The researcher is careful to use the same validated instrument at both time points and ensures no major academic events occurred between assessments, yet a methodological reviewer still flags a likely confound that threatens the causal interpretation of the findings.",
      "question": "Which threat to internal validity is the reviewer MOST likely concerned about?",
      "options": {
        "A": "Instrumentation",
        "B": "Spontaneous remission",
        "C": "Statistical regression to the mean",
        "D": "Demand characteristics"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Statistical regression to the mean is correct. The sample consists of individuals who sought help at a moment of peak distress — meaning they were likely at or near their highest anxiety levels at intake. On any subsequent assessment, scores would be expected to drift toward the population average purely due to natural fluctuation and measurement error, independent of any genuine treatment effect. This is the defining condition for regression to the mean.",
        "A": "Instrumentation refers to changes in the measurement tool or procedure across assessments. The vignette explicitly states that the same validated instrument was used at both time points, making instrumentation an unlikely concern and ruling it out as the primary threat. This detail is included to redirect students away from this option.",
        "B": "Spontaneous remission — the natural improvement of a condition without treatment — is a plausible alternative explanation in treatment outcome studies and shares similarities with regression. However, it describes a sustained natural recovery process, whereas the issue here is specifically about the statistical artifact created by selecting participants at a moment of extreme distress, which is regression to the mean.",
        "D": "Demand characteristics involve participants altering their responses based on perceived expectations of the researcher. While possible in self-report studies, demand characteristics would not specifically account for the systematic pattern of improvement in a group selected during a period of peak distress, making it less precise as an explanation than regression to the mean."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-03-vignette-L4",
      "source_question_id": "03",
      "source_summary": "The biggest threat to the internal validity of a study evaluating the effectiveness of a stress reduction technique for alleviating test anxiety is statistical regression.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "measurement error"
      ],
      "vignette": "A university research team is studying whether a brief relaxation protocol reduces anxiety in students identified through mandatory academic probation screenings as highly distressed. All participants completed a self-report anxiety battery during the screening, and only the top quintile of scorers was invited to participate. After the six-week protocol, the team reports significant pre-to-post improvement and, to address concerns, notes that the study used a well-validated instrument with excellent test-retest reliability coefficients. A statistician reviewing the manuscript argues that even with high reliability, a critical confound remains that the researchers have not adequately ruled out.",
      "question": "What is the confound the statistician is MOST likely identifying?",
      "options": {
        "A": "Attrition bias, because high-distress participants are more likely to drop out before posttest",
        "B": "Statistical regression to the mean, because selecting participants based on extreme scores creates expected score movement toward the average regardless of reliability",
        "C": "A ceiling effect, because the most anxious participants have limited room to score higher and can only show apparent improvement",
        "D": "Instrumentation drift, because even reliable measures can show calibration changes when administered under different motivational conditions"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Statistical regression to the mean is correct. The statistician's key point is that even a highly reliable instrument does not eliminate regression to the mean when participants are selected on the basis of extreme scores. Regression occurs because of the combination of imperfect reliability (measurement error) and the selection of extreme scorers — and even strong reliability coefficients only attenuate, not eliminate, this effect. The extreme-score selection criterion is the essential condition that makes regression the primary threat.",
        "A": "Attrition bias is a genuine internal validity concern, particularly when high-distress participants may disproportionately drop out, leaving a less anxious sample at posttest. This could produce an apparent treatment effect, and is a plausible competing explanation. However, attrition would be reflected in differential loss from the study rather than in a predictable, mathematically expected movement of scores toward the mean that occurs even when retention is complete.",
        "C": "A ceiling effect (or, more precisely here, a floor effect — since scores can only decrease from a high starting point) refers to the restricted range in measurement when scores are at an extreme. This is a related concept and shares surface features with the correct answer, but ceiling/floor effects describe a measurement constraint rather than the statistical expectation that extreme scores will regress. The distinction matters because regression occurs due to error variance and selection, not due to the instrument's range limitations.",
        "D": "Instrumentation drift involves subtle changes in how a measure functions across administrations, such as shifts in observer coding standards or participant motivation. The vignette explicitly references high test-retest reliability, which would mitigate most instrumentation concerns. Furthermore, instrumentation drift does not specifically interact with the selection of extreme scorers, making it a less precise explanation for the pattern described."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-03-vignette-L5",
      "source_question_id": "03",
      "source_summary": "The biggest threat to the internal validity of a study evaluating the effectiveness of a stress reduction technique for alleviating test anxiety is statistical regression.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators wants to demonstrate the benefits of their newly developed study-skills workshop. They advertise specifically to students who report feeling overwhelmed and on the verge of failing their upcoming midterms, and they enroll only those who show the highest levels of distress at the first meeting. Six weeks later — after the midterms have passed — participants complete the same self-report survey they filled out at enrollment and show substantially less distress. The investigators note that no significant outside events occurred during the study period, attendance was near perfect, and participants genuinely reported valuing the workshops. They submit their findings as strong evidence for the program's effectiveness.",
      "question": "Which methodological problem MOST seriously undermines the investigators' causal conclusion?",
      "options": {
        "A": "The passage of the midterm exams between assessments represents an external event that naturally reduced the source of participants' distress, regardless of the intervention",
        "B": "Participants who self-selected into the study based on peak distress were likely to score lower at follow-up due to the natural tendency of extreme initial scores to move toward the average on retesting",
        "C": "Near-perfect attendance and positive participant feedback introduce demand characteristics that inflate self-reported improvement",
        "D": "Recruiting only the most distressed participants creates a nonrepresentative sample whose results cannot be generalized to the broader student population"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Statistical regression to the mean is correct. Participants were enrolled specifically because they were at a peak of distress — an extreme state that, by definition, is unlikely to persist at the same level regardless of any intervention. On retesting, scores are statistically expected to move toward the group mean due to the combination of measurement variability and the selection of extreme initial scorers. This artifact can fully account for the observed improvement and is the most fundamental threat to causal inference here.",
        "A": "The completion of the midterm exams is a compelling distractor because it represents a genuine environmental change (the stressor being removed) that could naturally reduce anxiety. This is a history threat to internal validity. However, the vignette states that no significant outside events occurred, which is meant to redirect attention away from history. More importantly, the core methodological problem is that participants were selected at peak distress — even if midterms had not occurred, regression to the mean would still be expected.",
        "C": "Demand characteristics — the tendency of participants to respond in ways they believe the researchers expect — are a plausible concern in studies where participants report valuing the intervention and attendance is high. These social desirability pressures could inflate self-reported improvement on a subjective survey. However, demand characteristics do not specifically account for the pattern of extreme-to-average score movement; they would affect all participants similarly rather than creating a systematic directional shift tied to initial score extremity.",
        "D": "Recruiting only the most distressed students does limit the generalizability of findings to the broader student population, which is a concern for external validity rather than internal validity. While important, external validity threats do not undermine the causal claim within the studied group — they only limit to whom the findings apply. The investigators' causal conclusion is undermined by an internal validity problem (regression), not by external validity limitations."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-08-vignette-L1",
      "source_question_id": "08",
      "source_summary": "The point biserial correlation coefficient is used to determine the correlation between a true dichotomy (e.g., college graduate or nongraduate) and a continuous variable (e.g., yearly income in dollars).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "point biserial correlation",
        "dichotomy",
        "continuous variable"
      ],
      "vignette": "A researcher wants to examine the relationship between military service status (veteran vs. non-veteran) and annual income measured in dollars. Military service status represents a true dichotomy, while annual income is a continuous variable measured on a ratio scale. The researcher consults a statistician to determine the most appropriate correlation coefficient for this type of data pairing. The statistician notes that the nature of one variable as a true dichotomy — not an artificially split continuous variable — is critical for selecting the correct statistic.",
      "question": "Which correlation coefficient is most appropriate for examining the relationship between a true dichotomy and a continuous variable?",
      "options": {
        "A": "Phi coefficient",
        "B": "Spearman rank-order correlation",
        "C": "Point biserial correlation coefficient",
        "D": "Biserial correlation coefficient"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The phi coefficient is used when BOTH variables are true dichotomies (e.g., pass/fail and male/female), not when one variable is continuous. It does not apply here because annual income is continuous.",
        "B": "The Spearman rank-order correlation is used when both variables are measured on ordinal scales or when the assumptions of Pearson correlation are violated. It is not designed for the pairing of a true dichotomy with a continuous variable.",
        "C": "Correct. The point biserial correlation coefficient is specifically designed to quantify the relationship between a naturally occurring, true dichotomy (veteran vs. non-veteran) and a continuous variable (annual income in dollars).",
        "D": "The biserial correlation coefficient is used when one variable is an artificial or forced dichotomy — that is, an underlying continuous variable that has been split into two categories — paired with a continuous variable. Because military service status is a true (not artificial) dichotomy, the biserial, not point biserial, would be incorrect here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-08-vignette-L2",
      "source_question_id": "08",
      "source_summary": "The point biserial correlation coefficient is used to determine the correlation between a true dichotomy (e.g., college graduate or nongraduate) and a continuous variable (e.g., yearly income in dollars).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "dichotomy",
        "correlation"
      ],
      "vignette": "A health psychologist is studying whether having a chronic illness diagnosis (yes vs. no) is related to scores on a validated 100-point quality-of-life scale. Participants were recruited from a large urban hospital system and ranged widely in age, which the researcher notes may influence both variables. Because chronic illness diagnosis is a naturally occurring, two-category variable rather than an artificially created split, the researcher must choose a correlation statistic appropriate for this specific combination of variable types.",
      "question": "Which statistical procedure is best suited to assess the relationship between a naturally occurring two-category variable and a continuous outcome score?",
      "options": {
        "A": "Biserial correlation coefficient",
        "B": "Pearson product-moment correlation",
        "C": "Phi coefficient",
        "D": "Point biserial correlation coefficient"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "The biserial correlation is used when one variable is an artificially dichotomized continuous variable — for example, splitting a continuous anxiety score at the median into 'high' and 'low.' Because chronic illness diagnosis is a natural, true dichotomy (not an artificially split continuous variable), the biserial is inappropriate.",
        "B": "The Pearson product-moment correlation requires both variables to be continuous (or at least interval-level). Because chronic illness status is a true dichotomy with only two values, Pearson is not the appropriate choice, though it is mathematically equivalent to the point biserial in certain formulations.",
        "C": "The phi coefficient measures the association between two true dichotomies. Because quality-of-life scores are continuous, not dichotomous, the phi coefficient does not apply.",
        "D": "Correct. The point biserial correlation coefficient is specifically designed for examining the relationship between a true, naturally occurring dichotomy (chronic illness: yes/no) and a continuous variable (quality-of-life scores), making it the appropriate choice here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-08-vignette-L3",
      "source_question_id": "08",
      "source_summary": "The point biserial correlation coefficient is used to determine the correlation between a true dichotomy (e.g., college graduate or nongraduate) and a continuous variable (e.g., yearly income in dollars).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "correlation"
      ],
      "vignette": "A workforce psychologist is asked to determine whether passing or failing a licensing exam — a straightforward categorical outcome with no underlying continuum — is associated with subsequent annual salary earned by licensed professionals. The researcher emphasizes that the exam outcome is not a scored variable that was later split; it is inherently binary. A colleague suggests using a statistic commonly applied when researchers arbitrarily divide a continuous measure into two groups, but the psychologist argues this would be methodologically incorrect. The psychologist selects a different correlation index that properly accounts for the true binary nature of exam outcome.",
      "question": "Which statistical method did the psychologist correctly select?",
      "options": {
        "A": "Biserial correlation coefficient",
        "B": "Point biserial correlation coefficient",
        "C": "Eta coefficient",
        "D": "Spearman rank-order correlation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The biserial correlation is precisely the statistic the colleague suggested — it is used when one variable is an artificially dichotomized continuous variable. Because pass/fail on a licensing exam is a true, natural dichotomy (not an artificially split continuous variable), the biserial is methodologically incorrect, as the psychologist argued.",
        "B": "Correct. The point biserial correlation coefficient is the appropriate statistic when one variable is a true dichotomy (pass/fail) with no underlying continuum and the other is a continuous variable (annual salary). The psychologist correctly rejected the biserial in favor of the point biserial.",
        "C": "The eta coefficient (correlation ratio) is used to measure the relationship between a nominal or categorical variable with more than two categories and a continuous variable, especially when the relationship may be nonlinear. It does not specifically apply to a two-category true dichotomy paired with a continuous variable in the same way the point biserial does.",
        "D": "The Spearman rank-order correlation is appropriate when both variables are at least ordinal in nature or when Pearson assumptions are violated. It does not specifically address the pairing of a true dichotomy with a continuous variable and would require ranking the dichotomous variable, which is not its intended application."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-08-vignette-L4",
      "source_question_id": "08",
      "source_summary": "The point biserial correlation coefficient is used to determine the correlation between a true dichotomy (e.g., college graduate or nongraduate) and a continuous variable (e.g., yearly income in dollars).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "binary"
      ],
      "vignette": "An industrial-organizational researcher is examining whether employees who were randomly assigned to receive a one-time structured onboarding session (yes vs. no) differ in their scores on a continuous organizational commitment scale administered six months later. The researcher notes that assignment to onboarding is a binary outcome resulting from random allocation, not from measuring some underlying disposition that was categorized after the fact. A reviewer of the study suggests that because the predictor arose from an experimental manipulation and the outcome is continuous, a simple t-test would suffice and any correlation index is redundant. The researcher disagrees, arguing that a specific correlational statistic is both appropriate and informative.",
      "question": "Which correlation statistic is the researcher most likely defending as appropriate for this design?",
      "options": {
        "A": "Biserial correlation coefficient",
        "B": "Phi coefficient",
        "C": "Pearson product-moment correlation",
        "D": "Point biserial correlation coefficient"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "The biserial correlation coefficient is used when a continuous underlying variable has been artificially split into two categories. Because onboarding assignment resulted from random allocation — not from dichotomizing a continuous measure — the biserial is not appropriate; this is a true binary variable with no assumed underlying continuum.",
        "B": "The phi coefficient is used when both variables are true dichotomies. Organizational commitment scores are continuous, so the phi coefficient does not apply; it would require reducing commitment scores to a binary variable, which would discard information.",
        "C": "Although the Pearson product-moment correlation is mathematically equivalent to the point biserial when one variable is coded as 0 and 1, the Pearson formula technically assumes both variables are continuous. The point biserial is the named, formally appropriate statistic for the specific data structure of a true dichotomy paired with a continuous variable, and it is what the researcher would cite by name in this context.",
        "D": "Correct. The point biserial correlation coefficient is the appropriate statistic when one variable is a true dichotomy (random assignment to onboarding: yes/no) — not artificially created from an underlying continuum — and the other is a continuous variable (organizational commitment scores). The researcher is correctly defending its use as informative beyond what a t-test provides."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-08-vignette-L5",
      "source_question_id": "08",
      "source_summary": "The point biserial correlation coefficient is used to determine the correlation between a true dichotomy (e.g., college graduate or nongraduate) and a continuous variable (e.g., yearly income in dollars).",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher collects data from 200 adults, recording whether each person was born inside or outside the country of residence — a fact about each person that is simply true or false with no gradation — alongside each person's total accumulated savings measured to the nearest dollar. The researcher wants to express the strength and direction of the relationship between these two pieces of information as a single numerical index. A consultant points out that the first piece of information cannot reasonably be thought of as a hidden sliding scale that was merely sliced into two groups, and that both pieces of information would need to be ranks for another commonly considered approach to apply. The researcher selects the index that best fits these constraints.",
      "question": "Which statistical index did the researcher select?",
      "options": {
        "A": "Biserial correlation coefficient",
        "B": "Spearman rank-order correlation",
        "C": "Point biserial correlation coefficient",
        "D": "Phi coefficient"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The biserial correlation coefficient is used when one variable is a continuous underlying trait that has been artificially divided into two categories — for example, splitting anxiety scores into 'high' and 'low.' The consultant's note that the first variable cannot reasonably be thought of as a hidden sliding scale rules out the biserial, because nativity status (born inside vs. outside the country) is a genuine, naturally occurring two-category variable with no assumed underlying continuum.",
        "B": "The Spearman rank-order correlation is used when both variables are ordinal, or when both can be meaningfully converted to ranks. The consultant's note that both variables would need to be ranks for this approach to apply explicitly rules out the Spearman, because nativity status cannot be meaningfully ranked.",
        "C": "Correct. The point biserial correlation coefficient is the appropriate index when one variable is a true, naturally occurring dichotomy with no underlying continuum (born inside vs. outside country) and the other is a continuous variable measured on a meaningful numerical scale (total savings in dollars). The consultant's two clues — not an artificially sliced continuum, and ranks would not work — together uniquely identify the point biserial.",
        "D": "The phi coefficient measures the association between two true dichotomies. Because accumulated savings is a continuous variable measured in dollars — not a two-category variable — the phi coefficient does not apply. Applying it would require artificially dichotomizing savings, which would discard information and is not what the researcher is doing."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-05-vignette-L1",
      "source_question_id": "05",
      "source_summary": "All single-subject designs share the characteristic that the dependent variable is measured multiple times during each phase.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "single-subject",
        "dependent variable",
        "phase"
      ],
      "vignette": "A school psychologist is conducting a single-subject design study to assess the effectiveness of a token economy on a child's on-task behavior. The study includes a baseline phase and an intervention phase. Across both phases, the dependent variable — on-task behavior — is measured repeatedly during each observation session. The psychologist explains to a graduate student that this repeated measurement within each phase is a defining feature of all single-subject designs.",
      "question": "Which of the following best describes the characteristic the psychologist is explaining to the graduate student?",
      "options": {
        "A": "The dependent variable is operationally defined once at the start of the study and not revisited.",
        "B": "The dependent variable is measured multiple times within each phase of the design.",
        "C": "A control group is used to compare outcomes across phases of the single-subject design.",
        "D": "The independent variable is manipulated at least twice within a single phase to establish experimental control."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect. While operational definitions are established early, the hallmark of single-subject designs is ongoing, repeated measurement of the dependent variable throughout every phase — not a single initial measurement.",
        "B": "This is correct. A defining characteristic shared by all single-subject designs is that the dependent variable is measured repeatedly within each phase (e.g., baseline, intervention), allowing trends and patterns in the data to be evaluated over time.",
        "C": "This is incorrect. Single-subject designs do not use separate control groups. Instead, each participant serves as their own control through comparisons across phases, which is a fundamental distinction from between-group designs.",
        "D": "This is incorrect. In single-subject designs, it is the dependent variable — not the independent variable — that is measured multiple times. The independent variable is typically introduced or withdrawn between phases to examine its effect."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-05-vignette-L2",
      "source_question_id": "05",
      "source_summary": "All single-subject designs share the characteristic that the dependent variable is measured multiple times during each phase.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "repeated measurement",
        "baseline"
      ],
      "vignette": "A behavioral researcher is working with a 9-year-old boy diagnosed with ADHD to reduce his frequency of interrupting during classroom instruction. The researcher collects data on the number of interruptions each day across a two-week baseline period, then continues collecting daily data after a behavioral intervention is introduced. The boy's teacher is surprised to learn that data collection does not stop after the baseline is complete. The researcher explains that ongoing, repeated measurement throughout every condition is what allows the data patterns to be meaningfully interpreted.",
      "question": "The researcher's explanation reflects which fundamental characteristic of the research approach being used?",
      "options": {
        "A": "The use of a no-treatment control condition alongside the treatment phase ensures internal validity.",
        "B": "Random assignment of measurement occasions across phases reduces order effects in the data.",
        "C": "The dependent variable is measured multiple times within each phase, enabling detection of behavioral trends.",
        "D": "The intervention effect is evaluated by comparing the participant's outcome to a normative population average."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect. Single-subject designs do not use separate no-treatment control groups. The participant serves as their own control through repeated measurement across phases, not through a parallel control condition.",
        "B": "This is incorrect. Measurement occasions in single-subject designs are not randomly assigned; they occur systematically and continuously within each phase. Random assignment of measurement occasions is not a feature of this approach.",
        "C": "This is correct. In all single-subject designs, the dependent variable is measured repeatedly within each phase (not just at the start or end), which allows the researcher to identify stable trends, assess variability, and draw meaningful conclusions about the effect of intervention.",
        "D": "This is incorrect. Single-subject designs evaluate change by comparing the participant's data patterns across phases, not by comparing the individual's performance to population norms. Norm-referenced interpretation is a feature of standardized assessment, not single-subject research."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-05-vignette-L3",
      "source_question_id": "05",
      "source_summary": "All single-subject designs share the characteristic that the dependent variable is measured multiple times during each phase.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "phase"
      ],
      "vignette": "A clinical researcher studying self-injurious behavior in adults with intellectual disabilities designs a study in which she records the frequency of self-injury every day for three weeks before any treatment begins, then introduces a differential reinforcement procedure and continues observing the participant daily for another four weeks. She notes that having a large number of data points in each condition is not simply for statistical convenience — it serves a purpose specific to the logic of her entire research framework. A colleague reviewing the protocol suggests that the repeated observations within conditions might be redundant once a stable trend is established, but the researcher disagrees.",
      "question": "The researcher's insistence on continued measurement within each condition reflects which essential feature of her research approach?",
      "options": {
        "A": "Counterbalancing of conditions to control for carryover effects that could threaten the validity of within-subject comparisons.",
        "B": "The dependent variable is measured multiple times within each phase, which is a universal characteristic of all designs in this research tradition.",
        "C": "Use of interobserver agreement checks distributed throughout each phase to ensure measurement reliability.",
        "D": "Implementation of a time-series structure in which each data point functions as a replication of the preceding measurement occasion."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect. Counterbalancing is a technique used in within-subjects group designs to manage order effects across conditions. It involves systematically varying condition order across participants and is not the rationale behind continuous measurement within each phase of a single-subject design.",
        "B": "This is correct. The defining shared characteristic of all single-subject designs is that the dependent variable is measured repeatedly throughout each phase — not just once or twice. This allows the researcher to establish behavioral stability, identify trends, and make meaningful phase-to-phase comparisons, which is the logical foundation of the entire approach.",
        "C": "This is incorrect. Interobserver agreement is an important reliability procedure in behavioral research, but it refers to the consistency between two independent observers rating the same behavior. It is not the feature that explains why ongoing measurement within each phase is essential to the research design's logic.",
        "D": "This is incorrect. While single-subject data does have a time-series quality, characterizing each data point as a 'replication of the preceding measurement' conflates the purpose of repeated measurement with replication logic. Replication in single-subject research typically refers to demonstrating the same effect across participants or conditions, not across consecutive data points within a phase."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-05-vignette-L4",
      "source_question_id": "05",
      "source_summary": "All single-subject designs share the characteristic that the dependent variable is measured multiple times during each phase.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "trend"
      ],
      "vignette": "A researcher is evaluating an anxiety management intervention for a single adult participant using a withdrawal design. During each condition, she gathers outcome data at the same time each day. A consultant reviewing the protocol focuses heavily on the stability of the data within conditions, noting that a reliable trend — or the absence of one — within any given condition is what makes the transition to the next condition interpretable. The consultant argues that designs where only pre- and post-measurements are taken cannot make this same interpretive claim, regardless of how well-controlled those designs are. She also notes that this interpretive capacity does not depend on how many phases the design includes.",
      "question": "The consultant's argument is grounded in which characteristic that distinguishes the researcher's design from designs that take only pre- and post-intervention measurements?",
      "options": {
        "A": "The withdrawal design establishes experimental control by demonstrating that behavior changes systematically with the introduction and removal of the independent variable.",
        "B": "The use of a single participant eliminates between-subjects variability, providing a cleaner test of the intervention effect than group designs.",
        "C": "The dependent variable is measured multiple times within each phase, allowing behavioral trends to be identified and making phase-to-phase comparisons meaningful.",
        "D": "The design achieves internal validity through systematic replication of the effect across multiple conditions rather than through statistical inference."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect, though highly plausible. While demonstrating systematic covariation between the independent variable and behavior across phases is indeed how withdrawal designs establish experimental control, this is not the feature that distinguishes single-subject designs from pre-post designs. The consultant's point is specifically about what makes conditions interpretable — which is the repeated measurement within each phase.",
        "B": "This is incorrect. Eliminating between-subjects variability is an advantage of single-subject designs in terms of error reduction, but it is not what the consultant identifies as the key interpretive advantage over pre-post designs. A pre-post study with a single participant would also eliminate between-subjects variability but still lack phase-level trend information.",
        "C": "This is correct. The consultant's argument specifically targets the interpretive power that comes from measuring the dependent variable repeatedly within each phase. This continuous within-phase measurement — a characteristic shared by all single-subject designs — is what allows researchers to identify stable trends and determine whether conditions are ready to be changed, something impossible with only pre- and post-measurements.",
        "D": "This is incorrect. Systematic replication is an important feature of single-subject research for establishing external validity, but it describes replication across participants or settings — not the within-phase measurement feature that distinguishes the design from pre-post measurement approaches. The consultant specifically contrasts against single pre-post measurement, not against unreplicated findings."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-05-vignette-L5",
      "source_question_id": "05",
      "source_summary": "All single-subject designs share the characteristic that the dependent variable is measured multiple times during each phase.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher studying one person's eating behavior gathers information about the number of meals skipped every day for six weeks before making any changes, and then continues gathering the same daily information for six more weeks after introducing a new morning routine. She tells a colleague that the same information must be gathered every day throughout both portions of the study — not just at the beginning and end of each portion — because the day-to-day pattern of the numbers within each portion is what the entire logic of her approach depends on. Her colleague suggests that the approach might actually be stronger if she compared the participant's eating data to normative data from a matched population, but the researcher explains that comparison to others is not what this approach is built on. A third observer notes that the design appears similar to a time-series analysis, but the researcher clarifies that her approach is part of a specific research tradition tied to behavior.",
      "question": "The researcher's insistence on daily measurement throughout both portions of the study reflects which fundamental characteristic of the research tradition she is using?",
      "options": {
        "A": "The study uses an interrupted time-series structure in which dense measurement before and after an intervention allows statistical modeling of change trajectories.",
        "B": "The design controls for maturation and history threats by embedding repeated assessment within each condition rather than relying on a single pre-intervention and post-intervention data point.",
        "C": "The dependent variable is measured multiple times within each phase, which is the defining shared characteristic of all designs in this research tradition.",
        "D": "The study achieves within-subject experimental control by demonstrating behavioral covariation with condition changes, which requires dense data across the full duration of each condition."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect and serves as a strong red herring. The scenario's description of dense daily measurement across sequential conditions closely resembles interrupted time-series analysis. However, the researcher explicitly clarifies her approach is part of a behavioral research tradition distinct from statistical time-series modeling, and the feature she describes — continuous within-condition measurement — is a structural characteristic of single-subject designs, not a statistical modeling strategy.",
        "B": "This is incorrect, though plausible. While repeated measurement within conditions does incidentally help control for threats like maturation and history compared to simple pre-post designs, the researcher's stated rationale is not about controlling these threats. She is describing a fundamental structural requirement of her research tradition — not a validity-preserving strategy layered onto the design.",
        "C": "This is correct. The researcher's explanation that daily measurement must occur throughout both portions of the study — not just at endpoints — describes the defining characteristic shared by all single-subject designs: the dependent variable is measured multiple times within each phase. This continuous within-phase measurement is what allows the researcher to evaluate behavioral patterns and trends, which is the logical foundation of single-subject research.",
        "D": "This is incorrect and is the most tempting distractor. Demonstrating behavioral covariation with condition changes (i.e., behavior improves when intervention is introduced, worsens when withdrawn) is indeed how single-subject withdrawal designs establish experimental control, and it does require dense data. However, this describes the mechanism for establishing experimental control across phases, not the characteristic the researcher is identifying — which is the requirement of repeated measurement within each phase as a universal feature of the tradition, independent of how many phases exist."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-005-vignette-L1",
      "source_question_id": "005",
      "source_summary": "Multicollinearity occurs when scores on one or more explanatory variables are highly correlated with scores on one or more of the other explanatory variables.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "multicollinearity",
        "regression",
        "correlated"
      ],
      "vignette": "A researcher conducts a multiple regression analysis to predict therapy outcome using three explanatory variables: depression severity, anxiety severity, and overall psychological distress. Upon inspecting the data, the researcher notices that depression severity and anxiety severity are highly correlated with each other (r = .92), and both are also strongly correlated with psychological distress (r = .89 and r = .87, respectively). The regression model produces unstable beta coefficients with extremely wide confidence intervals, and small changes in the dataset produce large fluctuations in the individual predictor estimates. The researcher consults a statistician, who flags the pattern of intercorrelations among the predictors as the primary source of the problem.",
      "question": "Which statistical problem is most directly illustrated by this scenario?",
      "options": {
        "A": "Heteroscedasticity",
        "B": "Multicollinearity",
        "C": "Autocorrelation",
        "D": "Overfitting"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Heteroscedasticity refers to non-constant variance of residuals across levels of a predictor, not to intercorrelations among predictors. While it also threatens regression assumptions, it does not cause unstable beta coefficients due to predictors sharing variance with one another.",
        "B": "Multicollinearity occurs when two or more explanatory variables in a regression model are highly correlated with each other, making it difficult to isolate the unique contribution of each predictor. The scenario describes exactly this: redundant predictors producing unstable, fluctuating coefficients — the hallmark consequence of multicollinearity.",
        "C": "Autocorrelation refers to the correlation of a variable's residuals with themselves across time or sequential observations, which is a concern in time-series data. It does not describe intercorrelations among separate predictor variables as depicted here.",
        "D": "Overfitting occurs when a model is too closely tailored to sample data and fails to generalize, typically due to too many parameters relative to sample size. While it can co-occur with multicollinearity, the specific problem described — unstable coefficients driven by intercorrelated predictors — is the defining feature of multicollinearity, not overfitting."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-005-vignette-L2",
      "source_question_id": "005",
      "source_summary": "Multicollinearity occurs when scores on one or more explanatory variables are highly correlated with scores on one or more of the other explanatory variables.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "regression",
        "predictors"
      ],
      "vignette": "A clinical researcher studying chronic pain patients builds a regression model to predict pain-related disability using five predictors: catastrophizing, pain intensity, fear-avoidance beliefs, depression, and sleep quality. The researcher is working with a sample of older adults, many of whom have comorbid medical conditions — a demographic detail noted but deemed unremarkable by the research team. When the model is run, the researcher is surprised to find that none of the individual predictors reach statistical significance, yet the overall model F-test is highly significant (p < .001) with an R² of .71. Inspection of pairwise correlations among the predictors reveals coefficients ranging from .78 to .91.",
      "question": "What methodological problem best explains the discrepancy between the significant overall model and the non-significant individual predictors?",
      "options": {
        "A": "Suppressor variable effects",
        "B": "Multicollinearity",
        "C": "Criterion contamination",
        "D": "Capitalization on chance"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Multicollinearity is the correct answer. When predictors are highly intercorrelated (r = .78–.91), they share so much variance that the regression cannot parse out each predictor's unique contribution, inflating standard errors and making individual predictors appear non-significant even when the combined model accounts for substantial variance in the outcome.",
        "A": "A suppressor variable enhances another predictor's validity by controlling for irrelevant variance; it typically causes a specific predictor's coefficient to be inflated or to change sign, not a global pattern of non-significance across all predictors. This does not explain the scenario described.",
        "C": "Criterion contamination occurs when the outcome measure is influenced by knowledge of predictor scores (e.g., rater bias), which compromises the validity of the criterion variable. It does not produce the pattern of high pairwise predictor intercorrelations paired with non-significant individual betas seen here.",
        "D": "Capitalization on chance refers to inflated Type I error rates when many tests are run on the same data without correction. It would more likely produce false positives (spuriously significant predictors), not the pattern of globally non-significant individual predictors alongside a significant omnibus test shown here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-005-vignette-L3",
      "source_question_id": "005",
      "source_summary": "Multicollinearity occurs when scores on one or more explanatory variables are highly correlated with scores on one or more of the other explanatory variables.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "variance inflation"
      ],
      "vignette": "A health psychology researcher examines predictors of medication adherence in a sample of 200 patients with hypertension. The predictor set includes self-efficacy for medication taking, general health self-efficacy, and perceived behavioral control — all measured using validated scales. The researcher notes that the overall model accounts for 55% of the variance in adherence and is statistically significant, yet none of the three predictors have beta weights that individually approach significance. A colleague points out that the variance inflation factor (VIF) values for all three predictors exceed 10. The researcher had initially suspected the non-significant predictors indicated poor construct validity of the scales.",
      "question": "The colleague's observation most strongly suggests which problem is responsible for the pattern of results?",
      "options": {
        "A": "Construct underrepresentation",
        "B": "Restriction of range",
        "C": "Multicollinearity",
        "D": "Model misspecification"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Variance inflation factors (VIF) above 10 are a widely used diagnostic indicator of severe multicollinearity. When predictor scales like self-efficacy and perceived behavioral control are conceptually and empirically overlapping, they share so much variance that individual regression coefficients become unstable and non-significant, exactly as described.",
        "A": "Construct underrepresentation refers to a validity threat in which a test fails to capture the full breadth of a construct — it is a measurement validity issue. While the researcher initially suspected this, the colleague's reference to VIF values points specifically to intercorrelation among predictors, not inadequate construct coverage.",
        "B": "Restriction of range occurs when the variability in a variable is artificially limited (e.g., selecting only high-scorers), which attenuates correlations and can reduce predictive power. It would not produce the specific pattern of non-significant individual predictors alongside a highly significant omnibus model and elevated VIF values.",
        "D": "Model misspecification involves including irrelevant predictors or omitting relevant ones, which can distort coefficients. While the three predictor scales may be redundant, the problem identified by the VIF values is specifically about intercorrelation among existing predictors — the defining feature of multicollinearity — not about the structure of the model itself."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-005-vignette-L4",
      "source_question_id": "005",
      "source_summary": "Multicollinearity occurs when scores on one or more explanatory variables are highly correlated with scores on one or more of the other explanatory variables.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "redundant"
      ],
      "vignette": "A neuropsychologist is building a predictive model for cognitive decline in older adults using scores from three neuropsychological subtests as explanatory variables. After running the analysis, she observes that the standard errors for all three predictor coefficients are substantially larger than expected based on zero-order correlations with the outcome, and the confidence intervals for the beta weights overlap zero despite the model's strong overall fit. She mentions to a colleague that the three subtests were designed to measure distinct cognitive domains but were drawn from the same battery and appear to be more redundant than she expected. The colleague suggests that eliminating one or two of the subtests — or creating a composite score — might resolve the issue, but does not name the specific problem.",
      "question": "Without the colleague naming the problem directly, what statistical phenomenon most precisely accounts for the inflated standard errors and unstable coefficient estimates in this scenario?",
      "options": {
        "A": "Attenuation due to measurement error",
        "B": "Multicollinearity",
        "C": "Heteroscedasticity",
        "D": "Partial confounding"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Multicollinearity is the correct answer. The scenario describes predictor variables that are more redundant than expected — sharing variance with each other — which inflates standard errors and destabilizes individual regression coefficients. The colleague's suggestion to eliminate redundant predictors or combine them into a composite is a recognized remedy for multicollinearity, even though the term is never used in the vignette.",
        "A": "Attenuation due to measurement error reduces observed correlations (both among predictors and with the outcome) because of unreliability in the measures. It would not produce inflated standard errors or the specific pattern of redundancy across predictors; if anything, it would make coefficients smaller and harder to detect uniformly, not unstable in the manner described.",
        "C": "Heteroscedasticity refers to non-uniform variance in the residuals of a regression model, which can inflate or deflate standard errors but is related to how variance in residuals changes across predictor values — not to intercorrelations among predictors. The colleague's remedy (removing redundant predictors) would not address heteroscedasticity.",
        "D": "Partial confounding occurs when a third variable partly explains the relationship between a predictor and outcome, complicating causal interpretation. While it can complicate coefficient estimates, it does not directly explain the pattern of inflated standard errors stemming from predictor redundancy, nor does it suggest the composite-score remedy referenced by the colleague."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-005-vignette-L5",
      "source_question_id": "005",
      "source_summary": "Multicollinearity occurs when scores on one or more explanatory variables are highly correlated with scores on one or more of the other explanatory variables.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A graduate student is working on a study examining what predicts how quickly adults complete a complex problem-solving task. She gathers three separate measures that she believes capture meaningfully different aspects of mental processing: the first assesses how quickly someone can shift attention between tasks, the second measures how efficiently someone can hold and manipulate information in mind, and the third captures how readily someone ignores irrelevant stimuli. When she enters all three into her model, the overall model explains a large proportion of the outcome's variability, but each individual measure appears to contribute virtually nothing on its own, with wide uncertainty ranges around each estimate. Her advisor reviews the inter-measure relationships and notes that the three measures move together so closely that the analysis cannot tell them apart.",
      "question": "What problem has the advisor identified?",
      "options": {
        "A": "The three measures are all tapping the same latent factor, suggesting a factor structure problem rather than an analytical one",
        "B": "Multicollinearity, because the high intercorrelations among the three predictors prevent the model from isolating each predictor's unique contribution",
        "C": "Overfitting, because the strong overall model fit alongside weak individual predictors suggests the model is capitalizing on sample-specific variance",
        "D": "Suppressor effects, because the measures may be masking each other's true predictive relationships with the outcome"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is the correct answer. The advisor's observation that the three measures 'move together so closely that the analysis cannot tell them apart' precisely describes multicollinearity — high intercorrelation among predictors that inflates standard errors and renders individual coefficients non-significant even when the overall model performs well. The recommended solution (as implied by the advisor's observation) would be to collapse or remove redundant predictors.",
        "A": "The observation that the measures share a latent factor is a plausible interpretation and is closely related, but it describes a measurement or structural issue rather than what is causing the analytical instability the advisor observed. Identifying a shared factor structure does not by itself explain the inflated uncertainty around individual coefficient estimates; multicollinearity is the more precise analytical label for what the advisor noted.",
        "C": "Overfitting refers to a model that performs well in the current sample but poorly on new data because it captures noise rather than signal, typically when the ratio of predictors to observations is high. The vignette does not describe a small sample or generalization failure — the problem is specifically that predictors cannot be individually distinguished, which points to multicollinearity rather than overfitting.",
        "D": "Suppressor effects occur when including one variable in a model increases or reveals the predictive power of another by controlling for irrelevant variance — they typically manifest as one predictor's coefficient increasing or reversing sign, not as a global pattern of non-significance across all predictors. The symmetric pattern of all three measures appearing to contribute nothing, combined with the advisor's comment about them moving together, is more characteristic of multicollinearity than suppression."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-006-vignette-L1",
      "source_question_id": "006",
      "source_summary": "A Cohen's d of .60 indicates a medium effect, and it's interpreted in terms of standard deviation units.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "effect size",
        "Cohen's d",
        "standard deviation"
      ],
      "vignette": "A researcher conducts a randomized controlled trial comparing a new cognitive-behavioral intervention to a waitlist control for reducing depressive symptoms. After running the analysis, the researcher reports an effect size using Cohen's d of .60. The researcher explains that this value reflects a meaningful difference between the two groups expressed in standard deviation units. The study's power analysis had been planned around detecting a medium effect.",
      "question": "How should the researcher interpret a Cohen's d of .60 in this study?",
      "options": {
        "A": "The treatment group's mean is .60 standard deviations above the control group's mean, indicating a medium effect size.",
        "B": "There is a 60% probability that the treatment produced a statistically significant result, indicating a large effect.",
        "C": "The correlation between treatment assignment and outcome is .60, reflecting a strong association between variables.",
        "D": "The treatment accounts for 60% of the variance in the outcome, indicating a large effect size."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Cohen's d of .60 means the treatment group's mean is .60 standard deviations higher than the control group's mean. By Cohen's conventions, values around .50–.79 are considered medium effects, so .60 falls squarely in that range.",
        "B": "Incorrect. Cohen's d does not represent a probability of significance; that would be related to p-values or statistical power. A d of .60 is classified as medium, not large, and it expresses a standardized mean difference, not a probability.",
        "C": "Incorrect. A correlation coefficient (r) expresses the strength of a linear relationship between two continuous variables, not a standardized mean difference. Cohen's d specifically quantifies the distance between two group means in standard deviation units.",
        "D": "Incorrect. The proportion of variance accounted for is expressed by r² or eta-squared (η²), not Cohen's d. A d of .60 does not mean 60% of variance is explained; that interpretation conflates two different effect size metrics."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-006-vignette-L2",
      "source_question_id": "006",
      "source_summary": "A Cohen's d of .60 indicates a medium effect, and it's interpreted in terms of standard deviation units.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "effect size",
        "Cohen's d"
      ],
      "vignette": "A clinical psychologist publishes a study examining whether mindfulness-based stress reduction lowers anxiety in college students with high trait anxiety. The sample included a high proportion of first-generation college students, which the authors note as a demographic feature of interest. The results yielded a Cohen's d of .60 between the treatment and control groups. A reviewer asks the psychologist to clarify what practical meaning this statistic conveys about the intervention's impact.",
      "question": "What is the most accurate interpretation of the Cohen's d value reported in this study?",
      "options": {
        "A": "The intervention explained 60% of the variability in anxiety outcomes across participants.",
        "B": "The two groups' means differed by .60 pooled standard deviation units, representing a medium-sized effect.",
        "C": "The result is statistically significant at the .05 level because .60 exceeds conventional thresholds for significance.",
        "D": "The treatment group's scores were .60 times as large as the control group's scores, indicating a modest proportional difference."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Cohen's d is a standardized mean difference: the numerator is the difference between group means and the denominator is the pooled standard deviation. A value of .60 places this effect in the medium range by Cohen's widely used benchmarks (~.50–.79).",
        "A": "Incorrect. Explaining 60% of variance is the interpretation of r² = .60 or η² = .60, not Cohen's d = .60. These are different effect size indices; Cohen's d does not directly convey variance explained.",
        "C": "Incorrect. Cohen's d is an effect size measure and is entirely independent of statistical significance. A p-value determines significance; d of .60 could accompany either a significant or nonsignificant p-value depending on sample size and power.",
        "D": "Incorrect. Cohen's d does not express a ratio of raw scores. It expresses the mean difference in standard deviation units, not a multiplicative proportion of one group's scores relative to another."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-006-vignette-L3",
      "source_question_id": "006",
      "source_summary": "A Cohen's d of .60 indicates a medium effect, and it's interpreted in terms of standard deviation units.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "standardized"
      ],
      "vignette": "A researcher compares two therapeutic approaches for treating post-traumatic stress disorder in veterans. The sample size is relatively small (n = 30 per group), and the researcher notes a concern about insufficient statistical power. Despite this, the analysis yields a standardized mean difference of .60, which falls just short of conventional thresholds for statistical significance at p = .06. A consultant advises the researcher not to equate the significance result with the magnitude of the finding.",
      "question": "What concept does the consultant most likely want the researcher to recognize when interpreting the .60 value?",
      "options": {
        "A": "Type II error, because the failure to reach significance with a meaningful result suggests the study was underpowered.",
        "B": "Effect size, because the .60 value reflects a medium-sized practical difference between groups independent of statistical significance.",
        "C": "Statistical power, because the .60 value directly quantifies the probability of detecting a true effect given the sample size.",
        "D": "Confidence interval width, because the .60 value reflects the precision of the group mean estimates."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The consultant is drawing attention to effect size as distinct from statistical significance. A Cohen's d of .60 indicates a medium effect regardless of p-value, and the consultant's point is that practical meaningfulness (effect size) should not be dismissed simply because p > .05.",
        "A": "Incorrect. While the scenario does describe conditions consistent with a Type II error (underpowered study failing to detect a true effect), the consultant's specific guidance addresses the magnitude of the .60 value itself — an effect size interpretation — not a decision-error classification.",
        "C": "Incorrect. Statistical power is the probability of correctly rejecting the null hypothesis given a true effect, and it is influenced by sample size, alpha, and effect size. Power does not equal .60 in this scenario; the .60 is a standardized mean difference, not a power estimate.",
        "D": "Incorrect. Confidence interval width reflects estimation precision and is related to sample size and standard error. A value of .60 here is not a confidence interval bound; it is a Cohen's d statistic representing the magnitude of group differences."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-006-vignette-L4",
      "source_question_id": "006",
      "source_summary": "A Cohen's d of .60 indicates a medium effect, and it's interpreted in terms of standard deviation units.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "pooled"
      ],
      "vignette": "A graduate student presents findings from a two-group intervention study comparing structured problem-solving therapy to supportive counseling for reducing hopelessness in outpatient adults. The student reports that after dividing the mean difference between groups by the pooled within-group variability, the resulting quotient was .60. Reviewers at the presentation immediately debate whether the study has practical significance, with some pointing to the small sample size and the marginally nonsignificant p-value of .07 as reasons for skepticism. The student's advisor argues that the practical meaning of the .60 figure should be interpreted separately from the significance test.",
      "question": "What is the advisor most precisely identifying when defending the .60 figure as independently meaningful?",
      "options": {
        "A": "Omega-squared, because adjusting for sample size bias produces a corrected estimate of explained variance around .60.",
        "B": "Cohen's d effect size, because a value of .60 represents a medium standardized mean difference that conveys practical significance independent of p-values.",
        "C": "A confidence interval midpoint, because the .60 value reflects the most probable true mean difference estimate given the data.",
        "D": "Observed statistical power, because the quotient of means over variance approximates the power of the test under the given sample size."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The calculation described — dividing the mean difference by the pooled standard deviation — is precisely the formula for Cohen's d. A result of .60 places it in the medium effect range. The advisor is correctly distinguishing effect size from significance testing, emphasizing that d = .60 has meaningful practical interpretation regardless of the p-value.",
        "A": "Incorrect. Omega-squared (ω²) is a variance-explained metric used primarily with ANOVA designs; it is not computed by dividing a mean difference by pooled standard deviation. While it is a more conservative estimate than eta-squared, a value of .60 for ω² would indicate a very large effect, not the medium effect being described here.",
        "C": "Incorrect. A confidence interval midpoint would be expressed in the original measurement units of the outcome variable, not as a standardized quotient. The computation described (mean difference divided by pooled SD) is Cohen's d, not a confidence interval parameter.",
        "D": "Incorrect. Observed power is estimated via power analysis using alpha, sample size, and effect size as inputs — it is not calculated as the quotient of mean difference divided by pooled variability. Conflating the Cohen's d formula with a power calculation reflects a misunderstanding of how each is derived."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-006-vignette-L5",
      "source_question_id": "006",
      "source_summary": "A Cohen's d of .60 indicates a medium effect, and it's interpreted in terms of standard deviation units.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A practitioner reads a report comparing two groups of adults who received different forms of a wellness program. The report states that the numerical difference between the two groups' average scores, after being adjusted relative to how spread out participants' scores were within each group, came out to .60. The practitioner initially focuses on a footnote indicating the comparison did not reach the threshold for being considered a conclusive result, leading them to assume the program made no meaningful difference. A colleague points out that the .60 figure, standing alone, conveys something important about how much the groups actually differed in practical terms, regardless of what the footnote says.",
      "question": "What does the colleague most precisely want the practitioner to recognize about the .60 figure?",
      "options": {
        "A": "It represents the probability that a randomly selected participant from the treatment group outperformed a randomly selected participant from the control group.",
        "B": "It reflects how consistently each group's scores clustered around their own mean, indicating comparable within-group homogeneity.",
        "C": "It is a medium-sized standardized mean difference that indicates the groups diverged by more than half a standard deviation, carrying practical meaning independent of the significance test.",
        "D": "It approximates the proportion of outcome variance attributable to group membership, suggesting the program accounted for more than half of the outcome's variability."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The description — the mean difference adjusted by within-group spread — is the computation of Cohen's d. A value of .60 is a medium effect by conventional standards, meaning the groups' averages are .60 standard deviations apart. The colleague's point is that this practical magnitude is independent of the p-value noted in the footnote.",
        "A": "Incorrect. The probability that a randomly selected treatment participant outperforms a control participant is captured by the common language effect size (CLES) or the area under the ROC curve, not by Cohen's d directly. While d of .60 corresponds to roughly a 66% probability in this sense, that specific interpretation is not what the description of 'mean difference divided by within-group spread' yields.",
        "B": "Incorrect. Within-group homogeneity or consistency of scores around the mean describes variance or standard deviation within a single group, not a between-group comparison. The .60 figure in this context reflects a relationship between groups, not a within-group characteristic.",
        "D": "Incorrect. Proportion of variance explained is captured by r² or eta-squared (η²), not a Cohen's d value. A d of .60 does not mean the program explains 60% of the variance; that misinterpretation conflates two different effect size metrics that are mathematically related but conceptually distinct."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-010-vignette-L1",
      "source_question_id": "010",
      "source_summary": "When test scores represent an interval or ratio scale and the distribution of scores is skewed, the median is usually the best measure of central tendency for the distribution.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "interval scale",
        "skewed",
        "median"
      ],
      "vignette": "A researcher administers a standardized anxiety inventory that produces scores on an interval scale ranging from 0 to 100. After data collection, the researcher plots the distribution and finds it is positively skewed due to a small number of participants with extremely high scores. The researcher must decide which measure of central tendency best represents the typical score in this sample. She considers whether the skewed distribution warrants a particular statistical choice.",
      "question": "Which measure of central tendency is most appropriate for this researcher to report?",
      "options": {
        "A": "Mean, because the anxiety inventory produces interval-scale scores that permit arithmetic operations.",
        "B": "Median, because the distribution is skewed and the median is resistant to the influence of extreme scores.",
        "C": "Mode, because it identifies the most frequently occurring score and is unaffected by the scale of measurement.",
        "D": "Geometric mean, because it is the preferred statistic when distributions depart from normality."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The mean is appropriate for interval data under normal or near-normal distributions, but when the distribution is skewed the mean is pulled toward the tail by extreme scores, making it a poor representative of the typical value.",
        "B": "Correct. When scores are on an interval or ratio scale but the distribution is skewed, the median is the preferred measure of central tendency because it is not distorted by extreme values and accurately reflects the middle of the distribution.",
        "C": "The mode identifies the most frequent score and is most appropriate for nominal data or for describing the most common category; it conveys no information about the overall center of a continuous interval-scale distribution.",
        "D": "The geometric mean is used for ratio-scale data that are positively skewed multiplicatively (e.g., growth rates, income ratios), but it is not the standard recommended measure of central tendency for interval-scale psychological test scores."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-010-vignette-L2",
      "source_question_id": "010",
      "source_summary": "When test scores represent an interval or ratio scale and the distribution of scores is skewed, the median is usually the best measure of central tendency for the distribution.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "distribution",
        "central tendency"
      ],
      "vignette": "A clinical psychologist is evaluating community mental health data from 200 adults who completed a standardized depression measure. The sample includes a small group of individuals with unusually high depression scores who were recently released from inpatient treatment, creating a pronounced positive tail in the distribution. The psychologist wants to communicate the typical level of depression in the broader community sample, and she is aware that the presence of the inpatient subgroup should not disproportionately influence her summary statistic.",
      "question": "Which measure of central tendency should the psychologist select to best represent the typical depression score in this community sample?",
      "options": {
        "A": "Mean, because the large sample size of 200 adults makes it robust to the influence of outliers.",
        "B": "Mode, because the distribution has a clear peak where most community members cluster, and the mode directly identifies that peak.",
        "C": "Median, because the distribution is skewed by extreme high scores and the median is not pulled toward the tail the way the mean is.",
        "D": "Trimmed mean, because removing the extreme scores before computing the average corrects for skew while retaining interval-scale arithmetic."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Large sample size does not eliminate the distorting effect of extreme scores on the mean; with a skewed distribution the mean will still be pulled toward the tail regardless of N, misrepresenting the typical community score.",
        "B": "The mode identifies the most frequently occurring score value and can describe the peak of a unimodal distribution, but it does not capture the overall center of the distribution and is the preferred statistic for nominal data, not continuous interval-scale measures.",
        "C": "Correct. When the distribution of interval-scale scores is skewed by extreme values, the median is the best measure of central tendency because it marks the exact midpoint of ranked scores and is not influenced by the extreme high scores introduced by the inpatient subgroup.",
        "D": "A trimmed mean is a defensible approach to handling outliers and preserves interval-scale arithmetic, but it is not the standard recommended measure of central tendency for skewed distributions; the median is the conventional and widely accepted choice in this situation."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-010-vignette-L3",
      "source_question_id": "010",
      "source_summary": "When test scores represent an interval or ratio scale and the distribution of scores is skewed, the median is usually the best measure of central tendency for the distribution.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "outliers"
      ],
      "vignette": "A school psychologist administers a reading fluency test — a well-validated instrument with equal intervals between score points — to 85 third-grade students. The vast majority of scores cluster between 60 and 90, but five students with severe reading disabilities score near zero, creating a long lower tail. The psychologist's supervisor suggests reporting the arithmetic average because the test produces 'real numbers' that allow precise calculations, and the sample is large enough to be reliable. The psychologist, however, suspects that outliers at the low end are distorting this recommendation.",
      "question": "Which measure of central tendency will most accurately represent the typical reading fluency performance of the class?",
      "options": {
        "A": "Mean, because the test produces equal-interval scores and the sample size of 85 is sufficient to stabilize the arithmetic average.",
        "B": "Median, because the five low-scoring students create a skewed distribution in which the middle-ranked score better represents the typical student than does the arithmetic average.",
        "C": "Mode, because the majority of scores cluster in a narrow band, and the most common score directly reflects where most students perform.",
        "D": "Variance, because quantifying the spread of the distribution accounts for the extreme low scores without distorting the central estimate."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "While equal-interval scaling does permit arithmetic operations and larger samples are generally more stable, neither feature eliminates the distorting effect of a skewed distribution on the mean; the five near-zero scores will still pull the mean downward, making it unrepresentative of the majority of students.",
        "B": "Correct. Despite the interval-scale properties of the test, the distribution is negatively skewed by five extreme low scores. The median — the middle-ranked value — is not affected by how extreme those low scores are, making it the best indicator of typical performance in this class.",
        "C": "The mode identifies the most frequently occurring score or score range, which can be useful for describing the peak of the distribution, but it does not account for the full range of the distribution and is not the recommended summary statistic for skewed continuous data.",
        "D": "Variance is a measure of dispersion, not central tendency; it describes the spread of scores around the mean rather than identifying a typical or representative score. Reporting variance would not resolve the problem of choosing the best central value."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-010-vignette-L4",
      "source_question_id": "010",
      "source_summary": "When test scores represent an interval or ratio scale and the distribution of scores is skewed, the median is usually the best measure of central tendency for the distribution.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "tail"
      ],
      "vignette": "A research team reports income data collected alongside scores from a validated quality-of-life instrument that has demonstrated equal intervals between its scoring units. The quality-of-life scores for the sample look roughly bell-shaped, but a histogram of annual income shows a long right tail driven by a handful of extremely high earners. The team's statistician argues that because the quality-of-life instrument produces precise numeric scores, the arithmetic average of those scores is always the gold-standard summary, and that the shape of the income distribution is irrelevant to summarizing quality-of-life. A consultant reviewing the report disagrees and recommends a different summary statistic for quality-of-life.",
      "question": "On what statistical basis should the consultant recommend reconsidering the measure of central tendency used for the quality-of-life scores?",
      "options": {
        "A": "The consultant should recommend the median for income, not quality-of-life, because only the income variable has a skewed distribution that warrants a nonparametric summary statistic.",
        "B": "The consultant should recommend the mean for quality-of-life scores because the instrument's equal-interval properties always justify arithmetic operations regardless of distributional shape.",
        "C": "The consultant should first examine whether the quality-of-life distribution itself is skewed; if it is, the median would be preferred even for interval-scale scores because skew — not scale level alone — determines the most representative measure of central tendency.",
        "D": "The consultant should recommend using the mode for quality-of-life because the bimodal tendencies often found in quality-of-life data make the mode more representative than either the mean or median."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This option correctly identifies that the income distribution is skewed and warrants the median, but it incorrectly assumes the quality-of-life distribution is necessarily symmetric. The decision about which measure to use for quality-of-life scores must be based on the shape of that variable's own distribution, not on the other variable's shape.",
        "B": "Equal-interval scaling is a necessary but not sufficient condition for preferring the mean; the mean is the preferred measure only when the distribution is also approximately symmetric. When interval-scale data are skewed, the mean is pulled toward the tail and the median becomes the better measure of central tendency.",
        "C": "Correct. The key principle is that distributional shape — specifically skewness — governs the choice of central tendency measure, even for interval- or ratio-scale data. If the quality-of-life distribution is itself skewed, the median is preferred regardless of the scale's numeric properties. The consultant's first task is to inspect the quality-of-life distribution, not assume it is symmetric.",
        "D": "Bimodality is a distinct distributional characteristic from skewness and does not automatically make the mode the best summary statistic for interval-scale data. Moreover, the vignette does not describe bimodality in the quality-of-life scores; introducing that consideration is a distraction from the actual distributional issue at hand."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-010-vignette-L5",
      "source_question_id": "010",
      "source_summary": "When test scores represent an interval or ratio scale and the distribution of scores is skewed, the median is usually the best measure of central tendency for the distribution.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A consultant is asked to review a summary report describing how a large group of people performed on a carefully constructed measurement tool that assigns numbers in a way that each step between consecutive values is exactly the same size. Most people in the group scored in a relatively narrow band near the upper range of the scale, but a smaller number of people received scores near the very bottom, pulling the overall picture downward. The report's author has summarized the group's typical performance using the arithmetic average of all scores, and a footnote notes that this choice was made because the instrument 'yields precise numeric data.' The consultant believes a different summary value would more honestly describe what is typical for this group.",
      "question": "Which statistical concept best explains the consultant's rationale for recommending a different summary value?",
      "options": {
        "A": "Because the measurement tool assigns equal-sized steps between values, only arithmetic operations are valid, and the arithmetic average is therefore always the correct summary regardless of how scores are distributed.",
        "B": "Because the scores are tightly packed at the upper end with a pull toward the bottom, the single value that splits the group exactly in half would better represent the typical performer than a value dragged downward by the few low scorers.",
        "C": "Because the majority of scores cluster near one end of the scale, the most frequently occurring score directly identifies where most people performed and should replace the arithmetic average.",
        "D": "Because the spread of scores is large, the appropriate correction is to report a range or standard deviation alongside the arithmetic average rather than switching to a different measure of the typical value."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Equal spacing between score units (i.e., interval-scale properties) does permit arithmetic operations, but it does not mandate the use of the mean as the sole legitimate summary. When the distribution of such scores is skewed, the mean is pulled away from where most scores lie, and the median — a rank-based value that requires no arithmetic operations — becomes the more representative summary.",
        "B": "Correct. The scenario describes an interval-scale instrument whose score distribution is negatively skewed: most scores cluster near the top with a tail of low scores pulling the arithmetic average downward. The value that divides the ranked distribution in half — the median — is resistant to this pull and accurately represents the typical performer. This matches the principle that for skewed interval- or ratio-scale data, the median is the preferred measure of central tendency.",
        "C": "The most frequently occurring score (the mode) identifies the peak of the distribution and is the recommended summary for nominal data or for describing the most common category. It does not account for all other values and is not the standard recommendation for summarizing the center of a continuous, skewed distribution on an interval scale.",
        "D": "Reporting dispersion statistics such as the range or standard deviation alongside the mean addresses how spread out the scores are, but it does not fix the fact that a skewed distribution renders the mean unrepresentative of the typical score. The issue is which single value best describes the center, not how to supplement the mean with additional information."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-001-vignette-L1",
      "source_question_id": "001",
      "source_summary": "The research study uses a mixed design, which involves at least two independent variables where one variable is a between groups variable and the other is a within subjects variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "mixed design",
        "between-groups",
        "within-subjects"
      ],
      "vignette": "A researcher is examining the effects of two variables on reading comprehension scores. The first variable is instructional method (traditional vs. technology-based), and participants are randomly assigned to one of these two groups — making it a between-groups variable. The second variable is time of assessment (pre-treatment, mid-treatment, post-treatment), and all participants are measured at each time point — making it a within-subjects variable. The researcher describes this as a mixed design because it incorporates both types of independent variables.",
      "question": "Which research design is the researcher using?",
      "options": {
        "A": "Factorial between-groups design",
        "B": "Mixed design",
        "C": "Repeated measures design",
        "D": "Randomized block design"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "A factorial between-groups design involves multiple independent variables, but all variables involve different participants assigned to separate groups. This study has a within-subjects factor (time), so this design does not apply.",
        "B": "Correct. A mixed design includes at least one between-groups variable (instructional method, with participants randomly assigned to one group) and at least one within-subjects variable (time, with all participants measured at every level). This combination defines a mixed design.",
        "C": "A repeated measures design involves all participants being measured under all conditions for every independent variable, with no between-groups factor. This study has a between-groups variable (instructional method), so a pure repeated measures design is not the correct label.",
        "D": "A randomized block design groups participants into homogeneous 'blocks' to reduce error variance and then randomly assigns treatments within blocks. It does not inherently combine between-groups and within-subjects factors as a mixed design does."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-001-vignette-L2",
      "source_question_id": "001",
      "source_summary": "The research study uses a mixed design, which involves at least two independent variables where one variable is a between groups variable and the other is a within subjects variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "between-groups",
        "within-subjects"
      ],
      "vignette": "A clinical researcher is studying the effectiveness of cognitive-behavioral therapy (CBT) versus pharmacotherapy on depression symptoms among adults with comorbid anxiety. Participants were randomly assigned to receive either CBT or pharmacotherapy, and symptom severity was measured at three time points: baseline, 8 weeks, and 16 weeks. Each participant completed a standardized depression inventory at all three time points. The researcher notes that the design involves one variable separating independent groups and another variable tracking the same individuals across time.",
      "question": "What type of research design best describes this study?",
      "options": {
        "A": "Randomized controlled trial with a parallel group design",
        "B": "Crossover design",
        "C": "Mixed design",
        "D": "Multivariate repeated measures design"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. This study is a mixed design because it combines a between-groups variable (treatment type: CBT vs. pharmacotherapy, with participants randomly assigned to one condition only) with a within-subjects variable (time: baseline, 8 weeks, 16 weeks, assessed for all participants). The comorbid anxiety detail is clinically relevant but does not alter the structural design.",
        "A": "A randomized controlled trial with a parallel group design involves random assignment to groups that are assessed concurrently, but the emphasis is on a single post-treatment comparison rather than the combination of a between-groups and a repeated within-subjects factor across multiple time points.",
        "B": "A crossover design involves participants receiving all treatment conditions in sequence, meaning treatment group membership itself becomes a within-subjects variable. In this study, participants remain in one treatment group, so it is not a crossover design.",
        "D": "A multivariate repeated measures design involves multiple dependent variables measured repeatedly for all participants, but it does not inherently incorporate a separate between-groups factor with independent assignment. This study's defining feature is the combination of one between-groups and one within-subjects independent variable."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-001-vignette-L3",
      "source_question_id": "001",
      "source_summary": "The research study uses a mixed design, which involves at least two independent variables where one variable is a between groups variable and the other is a within subjects variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "factorial"
      ],
      "vignette": "A researcher studying anxiety treatment outcomes randomly assigns participants to one of three intervention groups: mindfulness training, exposure therapy, or a waitlist control. Before the study begins, participants are also classified by trait anxiety level — high or low — based on a pre-existing questionnaire administered to all participants. Anxiety symptom scores are then collected from every participant at four separate time points across the treatment period. Some reviewers initially suggest the design is a simple factorial study, but the researcher clarifies that the structural relationship between the variables and participants is more nuanced.",
      "question": "Which design label most accurately characterizes this study?",
      "options": {
        "A": "Three-way between-groups factorial design",
        "B": "Mixed design",
        "C": "Nested design",
        "D": "Split-plot design with only within-subjects factors"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. This is a mixed design. Intervention group (mindfulness, exposure, waitlist) and trait anxiety level (high vs. low) are both between-groups variables — participants remain in one category only. Time of assessment is a within-subjects variable because all participants are measured at all four time points. The combination of at least one between-groups and one within-subjects factor defines the mixed design, regardless of how many between-groups factors exist.",
        "A": "A three-way between-groups factorial design would require all three independent variables to involve separate, independently assigned groups of participants. Here, the time-of-assessment variable is within-subjects (all participants contribute data at every time point), which disqualifies a purely between-groups factorial label.",
        "C": "A nested design involves levels of one variable being 'nested' within levels of another variable (e.g., students within classrooms within schools), such that the nested variable's levels do not cross with the higher-level variable. There is no nested structure described in this study; all factors cross one another.",
        "D": "A split-plot design is actually another name for a mixed design in some statistical traditions; however, the option incorrectly characterizes it as having only within-subjects factors. A true split-plot or mixed design requires at least one between-groups factor, which is present here. The option's qualifier makes it incorrect."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-001-vignette-L4",
      "source_question_id": "001",
      "source_summary": "The research study uses a mixed design, which involves at least two independent variables where one variable is a between groups variable and the other is a within subjects variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "counterbalancing"
      ],
      "vignette": "A psychophysiology researcher recruits participants diagnosed with PTSD and healthy controls, assigning them to groups based on diagnosis status. All participants complete a stress reactivity battery consisting of three tasks administered in a fixed sequence: resting baseline, a cognitive stressor, and a social stressor. To reduce potential order effects, the two stressor tasks are counterbalanced across participants within each diagnostic group. A colleague reviewing the protocol argues the study should be analyzed as a purely repeated measures ANOVA, since every participant completes all three tasks.",
      "question": "The colleague's suggested analysis is insufficient. Which design framework most accurately captures the structure of this study?",
      "options": {
        "A": "Repeated measures ANOVA",
        "B": "Randomized block ANOVA",
        "C": "Mixed design",
        "D": "Multivariate ANOVA (MANOVA)"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. This study is a mixed design. Diagnostic status (PTSD vs. healthy control) is a between-groups variable — participants belong to only one diagnostic category. Task (resting baseline, cognitive stressor, social stressor) is a within-subjects variable because all participants complete all three tasks. The counterbalancing is used to control order effects within this within-subjects structure, and the colleague's error is failing to account for the independent between-groups factor (diagnosis).",
        "A": "A repeated measures ANOVA accounts for the within-subjects factor (tasks) but ignores the between-groups factor (diagnostic group). Because diagnostic status divides participants into independent groups and is itself an independent variable of interest, a repeated measures ANOVA alone would be structurally incomplete and would fail to test the group-by-task interaction.",
        "B": "A randomized block ANOVA partitions variability by grouping participants into homogeneous blocks to reduce error, but it does not inherently model an independent between-groups factor that is itself a variable of theoretical interest. Diagnostic status is not a nuisance blocking variable — it is a substantive independent variable — making this label inappropriate.",
        "D": "MANOVA handles multiple dependent variables simultaneously to control Type I error inflation and examine multivariate outcomes. While it could theoretically be applied to the three task scores as separate dependent variables, it does not capture the repeated-measures within-subjects structure or the combination of between-groups and within-subjects independent variables that defines this design."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-001-vignette-L5",
      "source_question_id": "001",
      "source_summary": "The research study uses a mixed design, which involves at least two independent variables where one variable is a between groups variable and the other is a within subjects variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators recruits two separate groups of people — one group that has never exercised regularly and one group with a documented history of sustained physical training — and ensures the groups have no overlap in membership. Each person in both groups is brought into the laboratory on five separate occasions spread over a ten-week period, and on each visit the same cognitive performance task is administered under identical conditions. The investigators are primarily interested in whether the pattern of change over the five visits differs between the two groups of people, and they take care to note that no individual ever switches from one group to the other at any point during the study.",
      "question": "Which research design framework best characterizes the structure of this investigation?",
      "options": {
        "A": "Longitudinal cohort design",
        "B": "Randomized repeated measures design",
        "C": "Mixed design",
        "D": "Two-group pre-post quasi-experimental design"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. This is a mixed design. Exercise history (no exercise vs. sustained training) is a between-groups variable — participants were recruited into non-overlapping groups and never switched, meaning each person contributes data to only one level of this variable. Visit number (five occasions over ten weeks) is a within-subjects variable — every participant is assessed at all five time points. The combination of one between-groups and one within-subjects independent variable, along with interest in the interaction between them (differential change patterns), is the defining structure of a mixed design.",
        "A": "A longitudinal cohort design follows a defined group of people over time to observe naturalistic change or outcomes, without necessarily including a comparison group or a structured independent variable that distinguishes non-overlapping groups. The presence of two clearly separate, non-interchangeable groups functioning as an independent variable points beyond a simple longitudinal cohort framework.",
        "B": "A randomized repeated measures design involves random assignment of participants to groups or conditions, and all participants are measured repeatedly. In this study, participants are not randomly assigned — they are selected into groups based on an existing attribute (exercise history), making the grouping variable a quasi-experimental subject variable rather than a randomized manipulation. This distinction is critical for proper design classification.",
        "D": "A two-group pre-post quasi-experimental design typically involves one pre-intervention assessment and one post-intervention assessment to evaluate change following a treatment or event, with two non-randomly assigned groups. This study involves five repeated assessments over ten weeks without a discrete intervention event separating a 'pre' from a 'post' phase, and the primary interest is in the trajectory of change across multiple occasions rather than a single pre-to-post comparison."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-007-vignette-L1",
      "source_question_id": "007",
      "source_summary": "Changing alpha from .01 to .05 increases the probability of making a Type I error when the null hypothesis is true and decreases the probability of making a Type II error when the null hypothesis is false.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "alpha",
        "Type I error",
        "null hypothesis"
      ],
      "vignette": "A researcher is designing a clinical trial to test whether a new antidepressant outperforms placebo. She initially sets alpha at .01 but, concerned about missing a true effect, decides to relax it to .05. Her advisor notes that this change affects the probability of both types of decision errors. Specifically, the researcher must now accept a higher risk of rejecting the null hypothesis when it is actually true.",
      "question": "Which of the following best describes the consequence of the researcher changing alpha from .01 to .05?",
      "options": {
        "A": "The probability of a Type I error increases and the probability of a Type II error decreases.",
        "B": "The probability of a Type I error decreases and the probability of a Type II error increases.",
        "C": "Both the probability of a Type I error and the probability of a Type II error increase.",
        "D": "The probability of a Type II error increases while the probability of a Type I error remains unchanged."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Raising alpha from .01 to .05 widens the rejection region, making it easier to reject the null hypothesis. This increases the probability of falsely rejecting a true null hypothesis (Type I error) and simultaneously decreases the probability of failing to reject a false null hypothesis (Type II error).",
        "B": "Incorrect. This describes what would happen if alpha were made more stringent (lowered). Lowering alpha shrinks the rejection region, reducing Type I error risk but increasing the likelihood of missing a true effect (Type II error).",
        "C": "Incorrect. Type I and Type II errors are inversely related when alpha is the only parameter changed. Raising alpha increases Type I error probability but necessarily decreases, not increases, Type II error probability.",
        "D": "Incorrect. Changing alpha directly changes the Type I error rate by definition — alpha is the pre-set probability of a Type I error. Therefore, moving alpha from .01 to .05 unambiguously raises the Type I error probability."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-007-vignette-L2",
      "source_question_id": "007",
      "source_summary": "Changing alpha from .01 to .05 increases the probability of making a Type I error when the null hypothesis is true and decreases the probability of making a Type II error when the null hypothesis is false.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "alpha level",
        "statistical power"
      ],
      "vignette": "A neuropsychologist conducting research on mild cognitive impairment in older adults is concerned that her small sample size may cause her to miss a clinically meaningful effect. She consults a statistician, who recommends increasing the alpha level from .01 to .05 to improve the study's statistical power. The neuropsychologist, who has a strong preference for avoiding false positives due to the clinical implications of misdiagnosis, wonders what she is giving up by accepting the statistician's recommendation.",
      "question": "What is the most accurate description of the trade-off the neuropsychologist faces when raising the alpha level from .01 to .05?",
      "options": {
        "A": "She gains increased statistical power but accepts a higher probability of incorrectly concluding that a true effect exists when it does not.",
        "B": "She gains a reduced probability of missing a true effect but also reduces her chance of correctly identifying a true effect.",
        "C": "She eliminates the possibility of a Type II error at the cost of reduced effect size estimates.",
        "D": "She improves sensitivity to detect real differences but must accept a reduced probability of a Type II error alongside an increased probability of a Type I error, leaving overall accuracy unchanged."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Raising alpha increases statistical power (1 minus the Type II error rate), meaning it reduces the likelihood of missing a real effect. However, this comes at the cost of a higher probability of a Type I error — falsely rejecting a true null hypothesis, which is precisely the researcher's concern given clinical misdiagnosis risks.",
        "B": "Incorrect. While the first clause is accurate (reduced probability of missing a true effect), the second clause is wrong. Raising alpha increases, not reduces, the probability of correctly identifying a true effect (power). This option confuses the directionality of the power increase.",
        "C": "Incorrect. No alpha manipulation can eliminate the possibility of a Type II error entirely, and alpha adjustments do not directly alter effect size estimates. Effect size is a property of the data, not the significance threshold chosen.",
        "D": "Incorrect. This option contains a logical inconsistency: it correctly states that Type I error increases and Type II error decreases, but incorrectly implies that overall accuracy is unchanged. In practice, the balance of error risks shifts meaningfully depending on the base rate of true effects and the direction of the alpha change."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-007-vignette-L3",
      "source_question_id": "007",
      "source_summary": "Changing alpha from .01 to .05 increases the probability of making a Type I error when the null hypothesis is true and decreases the probability of making a Type II error when the null hypothesis is false.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "false positive"
      ],
      "vignette": "A pharmaceutical company is reviewing two independently conducted studies on the same anxiolytic compound. Study A used a more lenient decision threshold and detected a significant treatment effect; Study B used a stricter threshold and failed to detect the same effect with an identical sample size. A biostatistician notes that Study A was more likely to produce a false positive than Study B, while Study B was more likely to fail to detect a real effect if one existed. The research team debates whether Study A's finding should be trusted or whether its methodology made it too easy to find significance.",
      "question": "Which of the following best explains why Study A was more likely to produce a false positive than Study B, and what Study B sacrificed in exchange for stricter standards?",
      "options": {
        "A": "Study A used a higher alpha, which widens the rejection region and increases the rate of falsely rejecting a true null hypothesis, while Study B's lower alpha reduced this risk at the cost of increased probability of failing to detect a true effect.",
        "B": "Study A's larger rejection region increased effect size estimates, while Study B's narrower region suppressed true effects, making it harder to detect them regardless of the null hypothesis's truth status.",
        "C": "Study A used a lower alpha, which reduced the critical value required for significance, making any obtained result more likely to exceed the threshold, while Study B's higher alpha made the threshold harder to reach.",
        "D": "Study A used a higher alpha, which reduced the probability of a Type II error and simultaneously reduced the probability of a Type I error, whereas Study B balanced both error types equally."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. A higher alpha (e.g., .05 vs. .01) lowers the critical value needed for significance, thereby widening the rejection region and increasing the probability of rejecting a true null hypothesis (false positive, or Type I error). In exchange, this stricter-vs.-lenient comparison shows that the lower-alpha study (Study B) reduces Type I error risk but correspondingly increases Type II error risk — the probability of missing a genuine effect.",
        "B": "Incorrect. Alpha levels do not directly affect effect size estimates; effect sizes are calculated from the data independent of the chosen significance threshold. This option confuses the mechanics of alpha with the computation of effect size statistics such as Cohen's d.",
        "C": "Incorrect. The directionality of alpha is reversed. A lower alpha corresponds to a stricter, higher critical value — not a lower one — making significance harder to reach. It is a higher alpha that lowers the critical value and makes significance easier to obtain, increasing false positive risk.",
        "D": "Incorrect. A higher alpha increases Type I error probability and decreases Type II error probability — these two error rates move in opposite directions, not the same direction. This option incorrectly states that raising alpha reduces both error types simultaneously."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-007-vignette-L4",
      "source_question_id": "007",
      "source_summary": "Changing alpha from .01 to .05 increases the probability of making a Type I error when the null hypothesis is true and decreases the probability of making a Type II error when the null hypothesis is false.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "rejection region"
      ],
      "vignette": "A graduate student presenting at a lab meeting describes two versions of the same randomized controlled trial analyzing a mindfulness intervention for chronic pain. In Version 1, the rejection region encompasses a broader portion of the sampling distribution; in Version 2, the rejection region is narrower. The student reports that Version 1 found a statistically significant result and Version 2 did not, using identical data. A senior researcher points out that Version 1 would be expected to yield more findings consistent with a treatment effect across many replications, but would also more frequently reach conclusions that contradict the actual state of affairs when the treatment truly has no effect.",
      "question": "What is the most precise explanation for why Version 1 is simultaneously more likely to find significant results and more likely to draw incorrect conclusions when no effect exists?",
      "options": {
        "A": "Version 1 has a higher alpha, which increases both the probability of correctly detecting a true effect (power) and the probability of incorrectly rejecting a true null hypothesis (Type I error), reflecting the inherent trade-off of raising the significance threshold.",
        "B": "Version 1 has greater statistical power because its larger sample size reduces the standard error, which widens the rejection region and results in more significant findings regardless of the null hypothesis's truth status.",
        "C": "Version 1 uses a one-tailed test rather than a two-tailed test, which concentrates the rejection region in one direction, simultaneously improving detection of directional effects and inflating the probability of erroneous rejection when no effect exists.",
        "D": "Version 1 has lower alpha, which means its critical values are smaller, requiring less extreme data to achieve significance and thereby producing more findings but also increasing the likelihood of errors when the null hypothesis is false."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. A broader rejection region corresponds to a higher alpha level. Setting alpha higher (e.g., .05 vs. .01) lowers the critical value needed for rejection, increasing the probability of detecting a real effect (power) and the probability of falsely rejecting a true null hypothesis (Type I error). This trade-off — more sensitivity at the cost of more false positives — precisely explains the pattern described.",
        "B": "Incorrect. Sample size affects the standard error and thus statistical power, but the vignette specifies that both versions use identical data. Differences in the rejection region therefore reflect different alpha levels, not sample size differences. Larger samples do not widen the rejection region; they narrow the sampling distribution, indirectly affecting where obtained statistics fall.",
        "C": "Incorrect. One-tailed tests do concentrate the rejection region directionally and can increase power for directional hypotheses, but the vignette describes a broader overall rejection region, not a directional one. Additionally, one-tailed vs. two-tailed testing does not straightforwardly explain more findings in both directions across replications, as the question implies.",
        "D": "Incorrect. This option reverses the alpha directionality. A lower alpha corresponds to a smaller, not larger, rejection region and higher, not lower, critical values. Lower alpha makes significance harder to achieve, not easier. The described pattern — broader rejection region leading to more significant findings — requires a higher, not lower, alpha."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-007-vignette-L5",
      "source_question_id": "007",
      "source_summary": "Changing alpha from .01 to .05 increases the probability of making a Type I error when the null hypothesis is true and decreases the probability of making a Type II error when the null hypothesis is false.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team evaluating an early intervention program for at-risk youth has been running the same study under two different internal protocols for three years. Under Protocol X, evaluators almost never conclude the program works unless the data are extremely compelling; under Protocol Y, evaluators are more willing to conclude the program works when the data show even modest promise. Over time, the team notices that Protocol Y flags more programs as successful and misses fewer genuinely effective ones, but a methodologist consultant warns that Protocol Y more frequently declares programs successful when subsequent large-scale trials reveal the programs have no real benefit. Protocol X, by contrast, occasionally fails to recommend programs that later prove genuinely effective when subjected to more rigorous testing.",
      "question": "Which of the following best accounts for the full pattern of differences between Protocol X and Protocol Y described above?",
      "options": {
        "A": "Protocol Y sets a more lenient decision standard, which simultaneously increases the rate of incorrectly endorsing ineffective programs and reduces the rate of overlooking genuinely effective ones, while Protocol X's stricter standard produces the opposite pattern.",
        "B": "Protocol Y relies on a larger sample to achieve its greater sensitivity, which reduces random error and allows genuinely small effects to reach significance, whereas Protocol X's smaller sample inflates variability and causes it to miss real effects.",
        "C": "Protocol Y uses a one-tailed evaluation strategy that increases sensitivity to effects in the anticipated direction, causing it to endorse more programs overall and miss fewer genuine successes, while Protocol X's bidirectional standard is overly conservative for this context.",
        "D": "Protocol Y benefits from reduced measurement error compared to Protocol X, which makes its estimates more accurate on average and explains why it more frequently identifies effective programs, even if its broader confidence intervals occasionally include ineffective ones."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Protocol Y's 'willingness to conclude the program works with modest promise' describes a higher alpha (more lenient significance threshold). A higher alpha increases power — the probability of correctly identifying genuinely effective programs — but also increases the Type I error rate, leading to more frequent false endorsements when programs have no real effect. Protocol X's greater stringency (lower alpha) reduces false endorsements but increases the rate of missing genuine effects (Type II errors). This precisely maps the described bidirectional pattern.",
        "B": "Incorrect. The vignette gives no indication that Protocol Y uses a larger sample; both protocols are applied to the same study population over the same period. Differences in sample size would affect standard error and power, but the mechanism described — a willingness threshold for concluding effectiveness — points to a decision criterion change, not a sample size difference.",
        "C": "Incorrect. A one-tailed strategy would explain increased sensitivity in one anticipated direction, but it would not explain more frequent false endorsements across a broad range of replications in large-scale trials with diverse programs. The described pattern, where Protocol Y is consistently more likely to endorse programs that prove ineffective at scale, is better explained by a uniformly lenient threshold than by a directional test.",
        "D": "Incorrect. Reduced measurement error would improve the accuracy of estimates and might slightly increase power, but it would not systematically produce more false endorsements of ineffective programs. If Protocol Y had genuinely lower measurement error, its estimates would converge toward the true (null) effect for ineffective programs, not away from it. The described overclaiming pattern is inconsistent with simply having more precise measurement."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-011-vignette-L1",
      "source_question_id": "011",
      "source_summary": "Cluster sampling is a type of random (probability) sampling, while convenience sampling, quota sampling, and snowball sampling are types of nonrandom (non-probability) sampling.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "cluster sampling",
        "probability",
        "random"
      ],
      "vignette": "A researcher wants to study depression rates among high school students across a large metropolitan area. She uses cluster sampling by first randomly selecting 15 schools from a complete list of all schools in the district, then surveying every student within those selected schools. Because each school has a known, nonzero probability of selection, the method qualifies as a probability sampling technique. The researcher notes that this approach preserves the benefits of random selection while reducing logistical costs.",
      "question": "Which type of sampling method is the researcher using, and how is it best classified?",
      "options": {
        "A": "Quota sampling, a nonrandom method that ensures proportional representation of subgroups",
        "B": "Snowball sampling, a nonrandom method that relies on participant referrals to grow the sample",
        "C": "Cluster sampling, a random (probability) method in which naturally occurring groups are randomly selected",
        "D": "Convenience sampling, a nonrandom method based on availability of participants"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Quota sampling is incorrect because it is a nonrandom (non-probability) method in which the researcher sets numerical targets for subgroup representation and fills those targets by any available means, without random selection of individual units or groups.",
        "B": "Snowball sampling is incorrect because it is a nonrandom method in which existing participants recruit future participants from their social networks; it does not involve a randomly selected list of naturally occurring groups.",
        "C": "This is correct. Cluster sampling involves randomly selecting intact, naturally occurring groups (here, schools) from a complete enumerated list, giving each group a known probability of selection. It is therefore classified as a random (probability) sampling method.",
        "D": "Convenience sampling is incorrect because it relies on selecting whoever is most accessible or available to the researcher, without any random selection mechanism; it is a nonrandom (non-probability) method."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-011-vignette-L2",
      "source_question_id": "011",
      "source_summary": "Cluster sampling is a type of random (probability) sampling, while convenience sampling, quota sampling, and snowball sampling are types of nonrandom (non-probability) sampling.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "probability",
        "randomly"
      ],
      "vignette": "A public health researcher is examining anxiety levels among adult residents in a large urban county. The county is divided into 40 geographic neighborhoods, and the researcher randomly selects 10 of those neighborhoods using a lottery draw. All adults residing within the 10 selected neighborhoods are then invited to participate. The researcher is a first-generation immigrant herself and notes that immigrant populations are somewhat overrepresented in the chosen neighborhoods, though this was not by design.",
      "question": "Which sampling method best describes the procedure used in this study?",
      "options": {
        "A": "Stratified random sampling, because the population was divided into subgroups before selection occurred",
        "B": "Cluster sampling, because intact geographic units were randomly selected and all members of chosen units were included",
        "C": "Quota sampling, because certain demographic subgroups ended up being overrepresented in the final sample",
        "D": "Systematic sampling, because a fixed interval procedure was used to select participants from a numbered list"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Stratified random sampling is incorrect because it requires the researcher to deliberately divide the population into strata (e.g., by age, income) and then randomly sample from each stratum. Here, the neighborhoods were randomly selected as whole units, not used as strata from which subsamples were drawn.",
        "B": "This is correct. Cluster sampling involves randomly selecting naturally occurring, intact groups (here, geographic neighborhoods) and then including all members of the selected groups. The incidental overrepresentation of immigrants is a byproduct, not an intentional design feature, and does not change the method's classification.",
        "C": "Quota sampling is incorrect because it is a nonrandom method in which the researcher intentionally sets targets for subgroup representation. The overrepresentation of immigrants here was unintentional and resulted from random group selection, not from deliberately filling quotas.",
        "D": "Systematic sampling is incorrect because it involves selecting every kth individual from an ordered list of individuals, not selecting entire geographic units at once. No fixed-interval individual selection procedure was described here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-011-vignette-L3",
      "source_question_id": "011",
      "source_summary": "Cluster sampling is a type of random (probability) sampling, while convenience sampling, quota sampling, and snowball sampling are types of nonrandom (non-probability) sampling.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "enumerated"
      ],
      "vignette": "A psychologist studying stress in rural elementary school teachers begins by obtaining a fully enumerated list of all 60 elementary schools in a sparsely populated state. She uses a random number generator to select 12 schools from that list and then collects data from every teacher employed at each of the 12 schools. The researcher acknowledges that rural teachers tend to know one another professionally, which might make participants seem like a connected network. She is careful to note that no participant referrals or proportional targets were used in selecting the sample.",
      "question": "Which sampling method did the researcher most likely use?",
      "options": {
        "A": "Snowball sampling, because participants in rural communities are likely to refer colleagues to the study",
        "B": "Quota sampling, because the researcher selected a fixed number (12) of schools to meet a target",
        "C": "Stratified random sampling, because the schools represent distinct subgroups within the population",
        "D": "Cluster sampling, because intact institutional units were selected via a random mechanism from a complete list"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Snowball sampling is incorrect because it is a nonrandom method in which current participants recruit subsequent participants. Although the rural community is socially connected, the researcher explicitly did not use participant referrals; selection was conducted by random number generator from an enumerated list of schools.",
        "B": "Quota sampling is incorrect because it is a nonrandom method in which the researcher sets numerical targets for subgroup representation and recruits to fill those targets. Selecting a fixed number of schools via random draw is not the same as filling a quota; the number 12 reflects a sample size decision, not a proportional subgroup target.",
        "C": "Stratified random sampling is incorrect because it requires the researcher to first divide the population into meaningful strata and then randomly sample individuals from each stratum. Here, the schools were selected as whole units, not used as strata from which subsamples were drawn.",
        "D": "This is correct. The researcher had a complete (enumerated) list of all schools, used a random mechanism to select intact institutional units (schools), and then included all members within those units. This precisely defines cluster sampling, a random (probability) method."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-011-vignette-L4",
      "source_question_id": "011",
      "source_summary": "Cluster sampling is a type of random (probability) sampling, while convenience sampling, quota sampling, and snowball sampling are types of nonrandom (non-probability) sampling.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "units"
      ],
      "vignette": "A researcher investigating burnout among social workers in a large state designs a study by first compiling a registry of all licensed social work agencies statewide. He then uses a table of random numbers to select 20 agencies from the registry and administers surveys to every social worker employed at each selected agency. A colleague suggests the study has strong generalizability because every agency had an equal chance of being chosen. Another colleague points out that the social workers within any single agency are likely to share similar work environments and supervisory styles, potentially reducing the effective independence of responses within each selected unit.",
      "question": "The second colleague's concern about reduced independence of responses within selected units is a known limitation of which sampling method?",
      "options": {
        "A": "Stratified random sampling, because sorting the population into subgroups inflates within-group similarity and reduces independent responding",
        "B": "Systematic sampling, because selecting every kth unit from an ordered list can produce periodicity effects that cluster similar cases together",
        "C": "Quota sampling, because filling fixed numerical targets from a single source produces respondents who share environmental context",
        "D": "Cluster sampling, because members within a randomly selected intact group often share characteristics, creating intraclass correlation that reduces effective sample size"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Stratified random sampling is incorrect here because the concern about within-group similarity in that method is actually a feature that improves precision (strata are designed to be homogeneous to reduce error variance), not a limitation. The scenario describes a limitation arising from the within-unit homogeneity of intact, naturally occurring groups — a different phenomenon.",
        "B": "Systematic sampling is incorrect because its periodicity concern refers to bias introduced when the list itself has a cyclical pattern that aligns with the sampling interval, not to shared environmental characteristics among members of a selected group.",
        "C": "Quota sampling is incorrect because it is a nonrandom method and does not involve randomly selected intact groups. While quota samples can share contextual features, the methodological concern described — intraclass correlation reducing effective independence — is specifically associated with the intact-group selection structure of cluster sampling.",
        "D": "This is correct. Cluster sampling's key limitation is intraclass correlation (ICC): members within a randomly selected intact group (here, an agency) share environmental, supervisory, and organizational features, making their responses more similar to one another than to responses from other clusters. This reduces the effective independence of observations and shrinks the effective sample size, directly matching the second colleague's concern."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-011-vignette-L5",
      "source_question_id": "011",
      "source_summary": "Cluster sampling is a type of random (probability) sampling, while convenience sampling, quota sampling, and snowball sampling are types of nonrandom (non-probability) sampling.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators wants to study loneliness in older adults living across a geographically dispersed region. Rather than contacting individuals one by one, the team obtains a complete official directory of all senior living communities in the region — there are 80 in total. They write each community's name on a separate slip of paper, place all 80 slips in a drum, and draw out 16 slips. Every resident of each drawn community is then contacted and asked to participate. The team acknowledges that residents of the same community tend to share daily schedules and social programming, which may make their loneliness scores more similar to one another than to scores from residents of other communities. A reviewer notes that the approach differs from simply recruiting whoever responds to a flyer posted at a local community center.",
      "question": "Which sampling approach does this study most closely exemplify?",
      "options": {
        "A": "Quota sampling, because the investigators set a fixed target of 16 communities to ensure a representative proportion of the total",
        "B": "Convenience sampling, because all residents within the drawn communities were included without individual-level random selection",
        "C": "Stratified random sampling, because the investigators used an official directory to organize the population before drawing the sample",
        "D": "Cluster sampling, because intact naturally occurring groups were selected through a chance-based mechanism from a complete enumerated list, making it a probability method"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Quota sampling is incorrect because it is a nonrandom method in which the researcher intentionally recruits to fill predetermined numerical targets for demographic or other subgroup categories. Selecting 16 communities by drawing slips from a drum is a random mechanism, not a targeted quota-filling procedure, and no demographic proportionality goals were described.",
        "B": "Convenience sampling is incorrect because it is a nonrandom method that selects participants based on accessibility or willingness without a formal randomization step. The reviewer explicitly notes that the approach differs from flyer-based recruitment (which would be convenience sampling); the investigators used a chance mechanism applied to a complete official directory, establishing a known probability of selection.",
        "C": "Stratified random sampling is incorrect because it requires dividing the population into meaningful strata and then drawing random samples from within each stratum. Using an official directory to list communities is not the same as stratifying; the investigators drew intact communities as whole units rather than sampling individuals from within predefined subgroups.",
        "D": "This is correct. The investigators had a complete enumerated list of all senior communities, used a purely chance-based selection mechanism (drawing slips from a drum) to select intact groups, and then included all members of the selected groups. This is the defining structure of cluster sampling, which is classified as a random (probability) method because every group had a known, equal probability of selection."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-002-vignette-L1",
      "source_question_id": "002",
      "source_summary": "The size of the standard error of the mean increases as the population standard deviation increases and the sample size decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "standard error of the mean",
        "sample size",
        "population standard deviation"
      ],
      "vignette": "A researcher is designing a study to estimate the average anxiety score in a clinical population. She knows that the population standard deviation for anxiety scores is quite large, reflecting high variability among individuals. She is considering whether to collect data from 20 participants or 200 participants, and wants to understand how her choice will affect the standard error of the mean. Her advisor reminds her that the standard error of the mean is calculated as the population standard deviation divided by the square root of the sample size.",
      "question": "Based on these considerations, which of the following most accurately describes the relationship between sample size, population standard deviation, and the standard error of the mean?",
      "options": {
        "A": "The standard error of the mean increases as population standard deviation increases and as sample size decreases.",
        "B": "The standard error of the mean decreases as population standard deviation increases and as sample size increases.",
        "C": "The standard error of the mean is unaffected by sample size and is determined solely by the population standard deviation.",
        "D": "The standard error of the mean increases as sample size increases, regardless of the population standard deviation."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. The standard error of the mean equals the population standard deviation divided by the square root of n. A larger standard deviation in the numerator increases the standard error, while a smaller sample size in the denominator also increases it — together producing the largest possible standard error.",
        "B": "This option incorrectly combines directional effects. While it is true that increasing sample size reduces the standard error, stating that the standard error decreases as standard deviation increases is backwards — a larger population standard deviation inflates the standard error, not reduces it.",
        "C": "This is incorrect because sample size is a critical determinant of the standard error of the mean. The formula SE = σ/√n shows that sample size is in the denominator, meaning it directly and substantially influences the standard error.",
        "D": "This reverses the actual relationship. Increasing sample size places a larger value under the square root in the denominator, which reduces — not increases — the standard error of the mean. This option confuses the direction of the sample size effect."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-002-vignette-L2",
      "source_question_id": "002",
      "source_summary": "The size of the standard error of the mean increases as the population standard deviation increases and the sample size decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "standard error",
        "variability"
      ],
      "vignette": "A clinical psychologist is reviewing two recently completed studies that both measured depression symptom severity in outpatient adults. Study A recruited 15 participants from a single urban clinic and reported high variability in scores across participants. Study B recruited 150 participants from the same clinic and reported similar variability in scores. Despite using the same measurement instrument, the two studies arrived at noticeably different levels of precision in their sample mean estimates. The psychologist notes that the primary difference in precision between the two studies is not due to the measurement instrument or the setting, but rather to a mathematical property of how means are estimated.",
      "question": "Which of the following best explains why Study B produced a more precise estimate of the population mean than Study A?",
      "options": {
        "A": "Study B had greater external validity because it used a larger and more diverse sample of participants.",
        "B": "Study B's larger sample size reduced the standard error, producing a more precise estimate of the population mean despite similar score variability.",
        "C": "Study B's larger sample reduced sampling bias, which is the primary source of imprecision in small-sample studies.",
        "D": "Study B benefited from regression to the mean, which stabilizes extreme scores and produces a more accurate population estimate."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The standard error equals the population standard deviation divided by the square root of sample size. Because both studies had similar score variability, the key difference in precision is entirely attributable to Study B's larger n, which reduces the standard error and tightens the estimate of the population mean.",
        "A": "External validity refers to the generalizability of findings to other populations or settings, not to the mathematical precision of a mean estimate. While a larger sample may sometimes improve external validity, this concept does not explain why the standard error differs between the two studies.",
        "C": "Sampling bias refers to systematic distortions in how participants are selected, which can affect the representativeness of a sample. However, the scenario specifies that both studies used the same setting and instrument, and the precision difference is attributed to a mathematical property, not to bias correction.",
        "D": "Regression to the mean is a phenomenon in which extreme scores on a first measurement tend to move closer to the average on a second measurement. It describes changes in individual scores over repeated assessments, not the mathematical relationship between sample size and the precision of a mean estimate."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-002-vignette-L3",
      "source_question_id": "002",
      "source_summary": "The size of the standard error of the mean increases as the population standard deviation increases and the sample size decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "sampling distribution"
      ],
      "vignette": "A graduate student is running a pilot study on cognitive flexibility in older adults, a population known for wide individual differences in performance. She collects data from only 12 participants due to budget constraints, and constructs a sampling distribution of the mean from repeated simulated draws. Her thesis committee expresses concern that her estimate of the population mean will be imprecise, and she attributes this partly to the wide spread of individual scores in the population. A committee member adds that her concern would be compounded even further if she had used a smaller group of participants.",
      "question": "The imprecision the committee is describing is best captured by which of the following concepts?",
      "options": {
        "A": "Confidence interval width, which increases with greater population variance and smaller samples, reflecting uncertainty in the true parameter estimate.",
        "B": "Effect size, which is reduced when there is high population variability and a small number of participants, limiting statistical power.",
        "C": "Standard error of the mean, which increases with greater population variability and smaller sample sizes, widening the spread of the sampling distribution.",
        "D": "Type II error rate, which rises when population variability is high and sample size is small, reducing the probability of detecting a true effect."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The standard error of the mean, computed as σ/√n, directly quantifies the spread of the sampling distribution of the mean. Greater population variability increases the numerator, and smaller sample size reduces the denominator — both widen the sampling distribution and reduce precision of the mean estimate.",
        "A": "Confidence interval width is closely related to the standard error and does increase with variability and smaller samples; however, the confidence interval is a downstream product of the standard error rather than the foundational concept describing the spread of the sampling distribution itself. The vignette specifically points to the spread of the sampling distribution, which is precisely what the standard error quantifies.",
        "B": "Effect size measures the magnitude of a difference or relationship and is influenced by variability (e.g., Cohen's d uses standard deviation in the denominator). However, effect size does not directly describe the precision of a mean estimate or the spread of the sampling distribution, and it does not increase or decrease as a direct function of sample size in the way described here.",
        "D": "Type II error rate does increase when population variability is high and sample size is small, because these conditions reduce statistical power. However, Type II error is a decision-making concept about failing to reject a false null hypothesis, not a concept that directly captures imprecision in estimating the population mean from a sampling distribution."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-002-vignette-L4",
      "source_question_id": "002",
      "source_summary": "The size of the standard error of the mean increases as the population standard deviation increases and the sample size decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "precision"
      ],
      "vignette": "A researcher comparing two labs' studies on working memory capacity notices a curious pattern: Lab A, which tested 200 participants drawn from a homogeneous university population, produced tighter estimates around its reported mean than Lab B, which tested only 30 participants from a community clinic known for serving individuals with highly diverse cognitive backgrounds. A statistician colleague remarks that Lab A's advantage in precision is not simply because its participants were more similar to each other, but because of how both the spread of individual scores and the number of observations jointly determine how closely any given sample mean is expected to track the true population mean. On first glance, many researchers assume Lab A's superiority comes entirely from its participants' homogeneity.",
      "question": "The statistician's remark most directly refers to which statistical concept?",
      "options": {
        "A": "Reduced measurement error in Lab A, since homogeneous samples yield more consistent scores and therefore more reliable instruments.",
        "B": "Higher statistical power in Lab A, since larger samples and lower population variability both increase the probability of detecting true differences.",
        "C": "Smaller standard error of the mean in Lab A, since both the lower population variability and the larger sample size jointly reduce the spread of the sampling distribution of the mean.",
        "D": "Narrower confidence intervals in Lab A due to reduced within-group variance, reflecting greater certainty about the true population parameter."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The statistician explicitly distinguishes between homogeneity alone and the joint influence of population variability and sample size — the exact two factors in the formula SE = σ/√n. The standard error of the mean is the concept that directly captures how these two factors together determine how closely sample means cluster around the population mean in the sampling distribution.",
        "A": "Measurement error and reliability relate to how consistently an instrument measures a construct across administrations and are properties of the measurement tool itself. While homogeneous samples may yield more consistent scores, this does not describe the mathematical relationship between population variability, sample size, and the precision of a mean estimate, which is what the statistician is explaining.",
        "B": "Statistical power is related to the standard error — larger samples and lower variability both increase power — making this a highly plausible distractor. However, power refers specifically to the probability of correctly rejecting a false null hypothesis. The statistician's remark is about how closely sample means track the population mean, which is the standard error's function, not power's function.",
        "D": "Confidence interval width is directly derived from the standard error and does narrow when variability is lower and sample size is larger, making this option very defensible. However, the statistician's remark is specifically about the joint mechanism that determines how sample means distribute around the true mean — this is the standard error itself, of which the confidence interval is a downstream application. The vignette's framing about the 'spread of individual scores and number of observations jointly' points to the standard error formula more precisely than to confidence intervals."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-002-vignette-L5",
      "source_question_id": "002",
      "source_summary": "The size of the standard error of the mean increases as the population standard deviation increases and the sample size decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "Two teams of investigators are each trying to pin down the typical score for a characteristic that varies enormously from person to person in the group they are studying. The first team tests a small number of individuals — about a dozen — and reports a number meant to represent the whole group. The second team tests many more individuals from the same type of group and reports a similar summary number. When a reviewer compares the two reports side by side, she notices that the first team's summary number would be expected to bounce around much more wildly if the study were repeated many times, while the second team's number would stay much closer to the same place each time. The reviewer also notes that if both teams had studied a group where individuals were more similar to one another, both summary numbers would have been steadier — but the first team's would still have been less steady than the second team's, simply because fewer people were measured.",
      "question": "The concept that best explains the reviewer's observation about why the first team's summary number would be expected to vary more across hypothetical repetitions of the study is:",
      "options": {
        "A": "Sampling bias, because the first team's small group is more likely to systematically misrepresent the population, producing estimates that consistently deviate in one direction.",
        "B": "Standard error of the mean, because the expected variability of a sample mean across repeated studies is jointly determined by how spread out individual scores are in the population and how many individuals were measured.",
        "C": "Measurement unreliability, because fewer observations provide less opportunity to average out random errors in each individual's score, leaving the summary number more susceptible to fluctuation.",
        "D": "Regression to the mean, because small samples are more likely to contain extreme scorers whose scores would move toward the average if the study were repeated, causing the summary number to shift."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The reviewer's observation describes the sampling distribution of the mean — specifically how much sample means vary across hypothetical replications. The standard error of the mean (σ/√n) captures exactly this: it increases when individual scores vary widely (high population spread) and when fewer people are measured (small n). The vignette specifies both factors, and the qualifier that a more homogeneous group would stabilize both teams' estimates but not eliminate the difference between them confirms this is about the joint influence of variability and sample size on the standard error.",
        "A": "Sampling bias refers to a systematic, directional distortion in how a sample is drawn — for example, if one type of person is more likely to be included than another. The reviewer's observation is not about a consistent directional error but about random fluctuation across repetitions. Furthermore, the vignette specifies that both teams are studying 'the same type of group,' making differential bias an unlikely explanation for the difference in stability.",
        "C": "Measurement unreliability is a plausible distractor because fewer observations might seem to offer less averaging of random measurement error. However, measurement reliability concerns random error at the level of individual score assessment and is a property of the instrument, not a function of sample size per se. The vignette describes the expected bounce of the summary number across repeated studies — which is the function of the standard error, not instrument reliability — and explicitly attributes the difference to the number of people measured and the similarity among individuals in the group.",
        "D": "Regression to the mean is a compelling distractor because small samples are indeed more likely to contain extreme scorers, and those scores would tend to moderate on retesting. However, regression to the mean describes changes in individual scores across repeated measurements of the same individuals, not the variability of a summary number computed from independently drawn samples. The reviewer is describing what would happen if entirely new groups were tested repeatedly, which is a sampling distribution concept, not a regression to the mean phenomenon."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-009-vignette-L1",
      "source_question_id": "009",
      "source_summary": "In the context of research, between-methods triangulation involves including both qualitative and quantitative methods to collect data.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "triangulation",
        "qualitative",
        "quantitative"
      ],
      "vignette": "A research team studying treatment outcomes for depression decides to use triangulation to strengthen the validity of their findings. They administer standardized rating scales to 200 participants to generate quantitative scores, and they also conduct in-depth interviews to gather qualitative narratives about participants' lived experiences. The researchers explicitly state that their goal is to use both methods together because each method captures different aspects of the construct. By combining these two approaches, the team hopes to cross-validate their conclusions.",
      "question": "Which specific research strategy is the team employing?",
      "options": {
        "A": "Within-methods triangulation",
        "B": "Between-methods triangulation",
        "C": "Construct validity enhancement",
        "D": "Mixed-level replication"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Within-methods triangulation uses multiple data sources or instruments within the same methodological tradition (e.g., two different quantitative measures). Because the team is crossing methodological traditions — using both qualitative and quantitative methods — this label does not apply.",
        "B": "Between-methods triangulation specifically refers to the practice of combining qualitative and quantitative methods to collect data on the same phenomenon, which is precisely what this team is doing by pairing standardized scales with in-depth interviews.",
        "C": "Construct validity refers to whether a measure accurately captures the theoretical construct it is intended to measure. While using multiple methods may indirectly support construct validity, it is not itself a strategy called 'construct validity enhancement,' and this label does not describe the methodological design choice being made.",
        "D": "Mixed-level replication is not a standard methodological term. Replication involves repeating a study to verify results, but it does not describe the deliberate combination of qualitative and quantitative data collection methods used here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-009-vignette-L2",
      "source_question_id": "009",
      "source_summary": "In the context of research, between-methods triangulation involves including both qualitative and quantitative methods to collect data.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "triangulation",
        "mixed-methods"
      ],
      "vignette": "A psychologist studying PTSD recovery in combat veterans designs a mixed-methods study in which participants complete self-report symptom inventories at three time points over six months. The same participants also take part in semi-structured interviews in which they describe how their daily functioning has changed. The psychologist is a veteran herself and notes her positionality in the methods section, acknowledging potential reflexivity concerns. She argues that using triangulation — combining numerical symptom data with narrative accounts — will produce more credible conclusions than either source alone.",
      "question": "What methodological approach does this study exemplify?",
      "options": {
        "A": "Sequential explanatory design",
        "B": "Within-methods triangulation",
        "C": "Between-methods triangulation",
        "D": "Concurrent nested design"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A sequential explanatory design is a specific mixed-methods design in which quantitative data are collected first and then followed by qualitative data to explain the quantitative results. Although this study uses both methods, it does not describe a sequential priority structure, and the researcher's stated goal is cross-validation rather than explanation of one data set by the other.",
        "B": "Within-methods triangulation involves using multiple data sources within the same methodological tradition, such as multiple quantitative instruments. This study crosses traditions by pairing self-report inventories (quantitative) with narrative interviews (qualitative), making between-methods the correct descriptor.",
        "C": "Between-methods triangulation is defined by the deliberate inclusion of both qualitative and quantitative methods to study the same phenomenon, allowing for cross-validation of findings. The pairing of symptom inventories and semi-structured interviews precisely fits this definition, and the researcher explicitly invokes triangulation for the purpose of credibility.",
        "D": "A concurrent nested design is a mixed-methods design in which one method is embedded within another as the primary data collection strategy. The vignette does not indicate that one method is nested inside or subordinate to the other; both appear to have parallel roles in generating evidence."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-009-vignette-L3",
      "source_question_id": "009",
      "source_summary": "In the context of research, between-methods triangulation involves including both qualitative and quantitative methods to collect data.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "convergence"
      ],
      "vignette": "A school psychologist is evaluating the effectiveness of a new social skills program. She collects structured behavioral observation data using a validated checklist during classroom sessions and simultaneously asks teachers to write open-ended weekly reflections about each student's interpersonal interactions. After data collection ends, she compares the patterns in the observation frequencies with the themes emerging from the teacher narratives, looking for convergence across the two data streams. A colleague suggests the approach is redundant because both data sources address the same construct, but the psychologist argues the design strengthens confidence in her conclusions precisely because the sources are methodologically independent.",
      "question": "The psychologist's rationale for using two methodologically independent data sources best reflects which research strategy?",
      "options": {
        "A": "Concurrent triangulation design",
        "B": "Between-methods triangulation",
        "C": "Within-methods triangulation",
        "D": "Convergent validity testing"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Concurrent triangulation design is a mixed-methods design label referring to the overall study structure in which qualitative and quantitative data are collected simultaneously. While the design here is concurrent, this label describes the architecture of the study rather than the underlying methodological principle that motivates combining two types of methods. Between-methods triangulation is the more precise and conceptually accurate term for the strategy itself.",
        "B": "Between-methods triangulation is the practice of using both qualitative and quantitative methods — here, structured behavioral checklists (quantitative) and open-ended teacher narratives (qualitative) — to study the same phenomenon in order to cross-validate findings. The psychologist's explicit argument that methodological independence strengthens confidence is the defining rationale for this strategy.",
        "C": "Within-methods triangulation involves using multiple data sources that belong to the same methodological tradition, such as two different rating scales or two sets of observations coded numerically. Because one data source here is qualitative (open-ended narratives) and the other is quantitative (checklists), the methods cross traditions rather than remain within one.",
        "D": "Convergent validity testing is a psychometric procedure used to establish that a measure correlates with other measures of the same or related constructs. Although the psychologist is looking for convergence, she is doing so to validate her findings through independent methods, not to establish the psychometric properties of a single instrument."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-009-vignette-L4",
      "source_question_id": "009",
      "source_summary": "In the context of research, between-methods triangulation involves including both qualitative and quantitative methods to collect data.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "corroborate"
      ],
      "vignette": "A researcher investigating burnout among emergency room nurses first conducts a large survey that yields Likert-scale scores on exhaustion, depersonalization, and personal accomplishment subscales. Concerned that the numbers alone may not capture the full picture, she recruits a subset of 15 nurses for hour-long unstructured interviews exploring their day-to-day work experiences and coping strategies. The interview transcripts are coded using thematic analysis, and the emergent themes are compared against the survey subscale results to corroborate the overall pattern of findings. Reviewers of the manuscript initially question whether the small interview sample undermines the credibility of the qualitative component, but the researcher argues that the two data streams serve a fundamentally different epistemological purpose — one generating breadth and the other depth — and that their combination is precisely the point.",
      "question": "The researcher's defense of her study design reflects which methodological principle?",
      "options": {
        "A": "Sequential exploratory triangulation",
        "B": "Construct replication across samples",
        "C": "Between-methods triangulation",
        "D": "Concurrent nested design with qualitative priority"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Sequential exploratory triangulation is not a standard methodological term; it conflates the sequential exploratory mixed-methods design (in which qualitative data are collected first to inform quantitative instrument development) with triangulation. The researcher collected survey data before interviews, but her goal was cross-validation of findings rather than building a new instrument, which distinguishes this from any sequential exploratory framework.",
        "B": "Construct replication across samples refers to verifying that a construct behaves consistently when measured in different populations. While the researcher is using two samples (large survey group and small interview subset), her intent is not to test whether the burnout construct replicates across samples but rather to use two methodologically distinct data streams to converge on the same phenomenon.",
        "C": "Between-methods triangulation involves incorporating both qualitative and quantitative methods to study the same phenomenon, with the explicit goal of cross-validating findings from methodologically independent sources. The researcher's defense — that the two streams serve different epistemological functions (breadth vs. depth) and that their combination is the point — is the defining rationale for between-methods triangulation. The small qualitative sample size is irrelevant to whether the design strategy qualifies.",
        "D": "A concurrent nested design with qualitative priority describes a mixed-methods structure in which a qualitative study is the dominant data collection strand and a quantitative component is embedded within it as subordinate. Here, the large survey is clearly the primary data source and the interviews are supplementary, suggesting quantitative rather than qualitative priority, and the researcher does not describe one strand as nested inside the other."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-009-vignette-L5",
      "source_question_id": "009",
      "source_summary": "In the context of research, between-methods triangulation involves including both qualitative and quantitative methods to collect data.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators wants to understand how adolescents experience academic pressure in high-stakes testing environments. They distribute a standardized checklist to 500 students, recording how frequently each student reports specific stress-related behaviors on a numbered scale. Separately, they invite 20 students to participate in small group conversations where students speak freely about their feelings, routines, and beliefs surrounding exams; these conversations are recorded and later read carefully for recurring ideas. The team acknowledges upfront that neither source of information is sufficient on its own — the numbered scale gives broad coverage but misses personal meaning, while the group conversations are rich but narrow. After analysis, the investigators place findings from both sources side by side and assess whether the picture they paint is consistent. A skeptical colleague argues the study would have been stronger had the team simply administered two different numbered checklists and compared those results instead.",
      "question": "What research strategy do the investigators' design and stated rationale most precisely illustrate?",
      "options": {
        "A": "Within-methods triangulation",
        "B": "Concurrent mixed-methods replication",
        "C": "Sequential explanatory design",
        "D": "Between-methods triangulation"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Within-methods triangulation is the strategy the skeptical colleague is actually advocating: using multiple data sources within the same methodological tradition — here, two numbered checklists. The investigators explicitly rejected this approach in favor of crossing methodological traditions, which is exactly what makes within-methods triangulation the wrong answer and a carefully placed red herring.",
        "B": "Concurrent mixed-methods replication is not a standard methodological concept. While 'concurrent' and 'replication' are both real terms, their combination here does not describe a recognized strategy. The investigators are not attempting to replicate findings across independent studies; they are deliberately combining two distinct types of data collection within a single study to cross-validate a shared phenomenon.",
        "C": "A sequential explanatory design is a recognized mixed-methods framework in which the quantitative phase precedes and the findings guide the subsequent qualitative phase, with the purpose of explaining the numerical results in depth. Although the numbered checklist precedes the group conversations chronologically in the vignette, the investigators' stated purpose — placing both sets of findings side by side to assess consistency — reflects cross-validation rather than explanation, which is the hallmark of between-methods triangulation rather than sequential explanatory design.",
        "D": "Between-methods triangulation is defined by the deliberate use of both qualitative and quantitative methods to study the same phenomenon, motivated by the belief that each method's limitations can be offset by the strengths of the other and that convergence across independent sources increases confidence in conclusions. Every element of this study fits: a numbered behavioral checklist (quantitative) paired with free group conversation analysis (qualitative), explicit acknowledgment that neither alone is sufficient, and a final step of assessing consistency across the two data streams."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-012-vignette-L1",
      "source_question_id": "012",
      "source_summary": "The internal validity of a research study is threatened by statistical regression when participants were chosen for inclusion in the study because they obtained extremely low scores on a pretest, which can cause extremely high and low scores to \"regress to the mean\" on retesting, rather than being due to the effects of the independent variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "pretest",
        "regression to the mean",
        "extreme scores"
      ],
      "vignette": "A school psychologist designs a study to evaluate a new reading intervention. Students who scored in the bottom 10% on a pretest of reading fluency are selected for the intervention group. After eight weeks of the program, the same students are retested and show notable improvement. The researcher concludes the intervention was effective, but a colleague raises concerns about whether the improvement truly reflects the program's impact. The colleague specifically notes that extremely low scores on the pretest may have influenced the outcome in a way unrelated to the treatment.",
      "question": "Which threat to internal validity best explains the colleague's concern about the observed improvement in reading scores?",
      "options": {
        "A": "Maturation, because students naturally improve in reading skills over an eight-week period regardless of intervention.",
        "B": "Selection bias, because only the lowest-scoring students were chosen for the intervention group.",
        "C": "Statistical regression to the mean, because students selected on the basis of extreme scores tend to score closer to average on retesting even without intervention.",
        "D": "Testing effects, because repeated exposure to the same reading fluency measure inflates posttest scores."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Maturation refers to natural developmental or biological changes within participants over time (e.g., physical growth, fatigue). While it is a legitimate internal validity threat, it does not specifically arise from participant selection based on extreme pretest scores, so it does not capture the colleague's precise concern.",
        "B": "Selection bias refers to pre-existing group differences that emerge when participants are not randomly assigned. Although the bottom-10% selection creates a non-representative group, the colleague's concern is not about group comparability per se but about the mathematical tendency of extreme scores to move toward average on retesting.",
        "C": "Statistical regression to the mean occurs when participants are chosen specifically because of extreme pretest scores. Because extreme scores contain random measurement error that pushed them to the tail of the distribution, subsequent scores are likely to be less extreme—closer to the population mean—regardless of any treatment effect. This is exactly the threat the colleague is describing.",
        "D": "Testing effects (also called practice effects) occur when familiarity with a test instrument from prior administration improves performance at retesting. While this is also a concern in pretest-posttest designs, it does not specifically arise from selecting participants based on extreme scores and would affect all participants who take the test twice, not only those at the extremes."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-012-vignette-L2",
      "source_question_id": "012",
      "source_summary": "The internal validity of a research study is threatened by statistical regression when participants were chosen for inclusion in the study because they obtained extremely low scores on a pretest, which can cause extremely high and low scores to \"regress to the mean\" on retesting, rather than being due to the effects of the independent variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "extreme scores",
        "pretest"
      ],
      "vignette": "A clinical researcher recruits adults who scored in the top 5% on a validated anxiety inventory to participate in a mindfulness-based stress reduction program. Participants are predominantly women in their 30s with no prior meditation experience, a detail the researcher notes for demographic reporting. After a 10-week program, participants are readministered the same inventory and show a significant average reduction in anxiety scores. The researcher argues this demonstrates the program's efficacy, but a peer reviewer questions whether the result could be explained by a methodological artifact.",
      "question": "Which internal validity threat most directly accounts for the peer reviewer's skepticism about the observed reduction in anxiety scores?",
      "options": {
        "A": "Instrumentation, because the anxiety inventory may measure constructs differently across administrations when participants become familiar with it.",
        "B": "Statistical regression, because participants recruited specifically because of their extreme pretest scores are likely to score closer to the mean on retesting independent of any treatment effect.",
        "C": "Attrition, because participants with the highest anxiety may be more likely to drop out, leaving a less anxious sample at posttest.",
        "D": "Maturation, because anxiety levels naturally fluctuate and decrease over a 10-week period in adults."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Instrumentation refers to changes in the measurement tool, observer standards, or scoring criteria between pre- and posttest that can produce spurious score changes. This is a real validity threat, but the concern here is specifically tied to participant selection based on extreme scores—not a change in the instrument itself.",
        "B": "Statistical regression to the mean is the most direct threat here. Participants were explicitly recruited because they scored at the extreme high end of the anxiety inventory. Extreme scores contain random measurement error; on retesting, those errors are unlikely to be as extreme, pulling scores back toward the population mean. This can mimic a treatment effect even in the absence of any real change.",
        "C": "Attrition (also called experimental mortality) is a genuine concern when high-distress participants are differentially likely to drop out. However, this threat would reduce the sample's average score by removing the most anxious people, not because scores themselves regress. The vignette does not describe dropout and the key feature is extreme-score selection, not differential dropout.",
        "D": "Maturation involves natural within-person changes over time unrelated to treatment, such as spontaneous remission or developmental changes. While anxiety can naturally fluctuate, this threat does not arise specifically from the practice of selecting participants at score extremes, making it a less precise explanation than statistical regression."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-012-vignette-L3",
      "source_question_id": "012",
      "source_summary": "The internal validity of a research study is threatened by statistical regression when participants were chosen for inclusion in the study because they obtained extremely low scores on a pretest, which can cause extremely high and low scores to \"regress to the mean\" on retesting, rather than being due to the effects of the independent variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "confound"
      ],
      "vignette": "A school district implements a remedial math tutoring program and evaluates it using a pre-post design. Eligible students were identified solely because their scores on the spring district assessment placed them more than two standard deviations below the district average. At the end of the tutoring program the following fall, those students showed a mean improvement of 12 points. The program coordinator is enthusiastic, but an external evaluator warns that the improvement could be explained by a confound inherent to how students were identified. Importantly, no control group was included in the study design.",
      "question": "What is the most likely explanation for the evaluator's concern about the observed score improvement?",
      "options": {
        "A": "History, because uncontrolled events between the spring assessment and the fall posttest—such as summer enrichment programs—may have caused improvement.",
        "B": "Maturation, because students naturally improve in math over several months of normal schooling and development regardless of the tutoring program.",
        "C": "Statistical regression to the mean, because selecting students based on extreme low scores means subsequent scores will tend to be less extreme even absent a real treatment effect.",
        "D": "Selection-maturation interaction, because students who score extremely low may be at a developmental stage where rapid academic gains occur naturally."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "History refers to external events between measurement points that could affect outcomes, and the mention of summer as an intervening period makes this a plausible red herring. However, the evaluator specifically flags the method of student identification—scoring two standard deviations below average—as the source of the confound, which points to a statistical phenomenon tied to extreme-score selection rather than external events.",
        "B": "Maturation is a legitimate concern in a study spanning spring to fall, and the absence of a control group makes it difficult to rule out. However, the evaluator's concern is explicitly tied to how students were identified (extreme scores), not to natural developmental progress. Maturation would apply to any pretest-posttest design regardless of who was selected.",
        "C": "Statistical regression to the mean is the key threat here. When participants are selected precisely because their scores were at least two standard deviations below average, those scores likely included a downward random error component. On retesting, random error is equally likely to be positive or negative, so scores tend to move back toward the mean—creating an apparent improvement that is a methodological artifact, not a treatment effect. The evaluator's reference to the identification method is the critical clue.",
        "D": "Selection-maturation interaction is a sophisticated threat that occurs when different groups mature at different rates, complicating group comparisons. However, this threat is most relevant in studies with comparison groups; without a control group this interaction cannot be evaluated. Moreover, this threat does not specifically arise from extreme-score selection—it requires at least two groups maturing at different rates."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-012-vignette-L4",
      "source_question_id": "012",
      "source_summary": "The internal validity of a research study is threatened by statistical regression when participants were chosen for inclusion in the study because they obtained extremely low scores on a pretest, which can cause extremely high and low scores to \"regress to the mean\" on retesting, rather than being due to the effects of the independent variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "measurement error"
      ],
      "vignette": "A hospital-based pain clinic enrolls chronic pain patients who reported the highest levels of pain intensity during an initial intake screen. All enrolled patients complete an eight-week multimodal pain management program, and their pain ratings are recorded again at program completion. The treatment team is pleased to observe that mean pain scores decreased substantially from intake to discharge. A consulting methodologist cautions that the decrease may be artifactual and questions whether patients with the same characteristics would show a similar decline even without the structured program. The methodologist notes that the intake screening process itself may be the source of the problem rather than anything that happened—or failed to happen—during treatment.",
      "question": "What methodological artifact is the consulting methodologist most likely identifying as the explanation for the observed decrease in pain scores?",
      "options": {
        "A": "Instrumentation, because self-report pain scales are particularly susceptible to measurement error that may shift scores systematically from intake to discharge.",
        "B": "Demand characteristics, because patients who enroll in a structured pain program may consciously or unconsciously report lower pain at discharge to please providers.",
        "C": "Statistical regression to the mean, because the practice of enrolling only patients with the highest intake pain scores guarantees that random measurement error inflating those scores will be less extreme on retesting.",
        "D": "Hawthorne effect, because the attention and monitoring inherent in a structured clinic program, rather than the treatment content, produces symptom improvement."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Instrumentation is a plausible choice because self-report pain scales do contain measurement error, and this option explicitly invokes measurement error—the hint word embedded in the scenario. However, instrumentation as a validity threat refers to changes in the measurement tool or its application between time points (e.g., different raters, recalibrated equipment), not to the mathematical behavior of extreme scores across repeated measurement. The methodologist's concern is about the screening process, not a change in the instrument.",
        "B": "Demand characteristics refer to participants altering their responses based on perceived expectations of the researcher or treatment provider. This is a credible explanation for self-reported symptom reduction in a clinical setting, and it could plausibly explain the decline. However, the methodologist specifically states the problem lies in the intake screening process itself—not in how patients respond to social cues—making demand characteristics an imprecise fit.",
        "C": "Statistical regression to the mean is the correct answer. The methodologist's critical clue is that 'the intake screening process itself may be the source of the problem.' By selecting only the highest-pain patients, the clinic inadvertently selected individuals whose scores were partially driven upward by random measurement error at intake. On retesting, that error is distributed more symmetrically around the true score, producing apparent improvement. This would occur even with no treatment at all, which is exactly what the methodologist implies by asking whether untreated patients would show a similar decline.",
        "D": "The Hawthorne effect proposes that awareness of being observed or participating in a special program—rather than the program content itself—drives improvement. This is a real internal validity concern in clinical studies and could explain the improvement. However, it predicts improvement due to attention and monitoring, not due to selection on the basis of extreme scores. The methodologist's emphasis on the screening process as the locus of the problem is the distinguishing detail that points away from the Hawthorne effect."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-012-vignette-L5",
      "source_question_id": "012",
      "source_summary": "The internal validity of a research study is threatened by statistical regression when participants were chosen for inclusion in the study because they obtained extremely low scores on a pretest, which can cause extremely high and low scores to \"regress to the mean\" on retesting, rather than being due to the effects of the independent variable.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A nonprofit organization offers a free weight-loss coaching service and recruits participants by advertising at community health fairs and asking interested individuals to complete an initial weigh-in. Only those whose recorded weight at the weigh-in placed them in the heaviest 15% of the community sample are invited to participate in the 12-week coaching program. At the conclusion of the program, participants are weighed again, and the average weight across the group has dropped by several pounds. The organization publicizes the result as proof that the coaching approach is effective, but an independent analyst is skeptical that the program deserves credit for the change. The analyst notes that the same pattern of weight reduction would likely emerge even if the coaching sessions had contained no useful guidance whatsoever.",
      "question": "What phenomenon is the independent analyst most likely invoking to explain why the observed weight reduction may not reflect the program's effectiveness?",
      "options": {
        "A": "Natural attrition from the program, because participants who did not lose weight were more likely to drop out before the final weigh-in, leaving a systematically lighter sample at program completion.",
        "B": "Spontaneous behavioral change, because individuals who seek out weight-loss services at health fairs are already motivated to change their behavior, and that motivation—rather than the coaching—drives weight reduction.",
        "C": "The mathematical tendency of scores at the outer edge of a distribution to move toward the center of that distribution upon remeasurement, independent of any intervention.",
        "D": "Practice familiarity with the weigh-in procedure, because participants who have previously been weighed in a formal setting alter their behavior before the second measurement by wearing lighter clothing or manipulating food and water intake."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Attrition as an explanation for the observed reduction is highly plausible because participants who fail to lose weight—or who gain weight—may be more likely to drop out before the final weigh-in. This would leave a lighter sample at follow-up purely through selective dropout, not true weight loss. However, the analyst's specific claim is that the reduction would appear even if the coaching had contained no useful guidance—a statement about the selection process itself, not about who completed the program. Attrition cannot explain improvement when no one drops out, but the extreme-score selection mechanism operates regardless.",
        "B": "Motivated self-selection is a compelling distractor because the recruitment setting—health fairs—does plausibly attract individuals already committed to change. This maps onto selection bias and may predict that the sample would improve even without the program. However, this explanation requires that motivation actually produces behavioral change; the analyst argues the pattern would appear even with worthless coaching, implying the explanation is statistical rather than motivational.",
        "C": "This is the correct answer, describing statistical regression to the mean in plain language. By recruiting only from the heaviest 15% of the community sample, the organization selected individuals whose recorded weight at the initial weigh-in was likely inflated by transient factors—time of day, recent meals, measurement variability. On remeasurement, those transient factors are not systematically biased in the same upward direction, so the group's average weight appears to decrease. This artifact requires no behavior change at all, which is precisely what the analyst asserts.",
        "D": "This option describes a form of testing effects or reactivity specific to the weigh-in context—participants strategically manipulate conditions before the second measurement after learning the protocol. While this could produce apparent weight reduction, it predicts deliberate behavioral adjustment in response to being measured, which requires participant awareness and agency. The analyst's claim that even worthless coaching would produce the same result does not require this strategic behavior; it only requires that the group was selected at the extreme of the distribution."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-008-vignette-L1",
      "source_question_id": "008",
      "source_summary": "Community-based participatory research (CBPR) is a type of action research that encourages full participation of community partners in every aspect of the research process from question identification to analysis and dissemination, and involves formulating a research question, planning the study, collecting and analyzing the data, developing action plans from the data, carrying out the action plan, evaluating the results, and disseminating the results.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "community-based participatory research",
        "action research",
        "dissemination"
      ],
      "vignette": "A university researcher partners with residents of a low-income urban neighborhood to study barriers to mental health service access. Rather than imposing a predetermined protocol, the researcher and community members jointly identify the research question, co-design the data collection instruments, and share responsibility for analyzing findings. After the study, both academic and community partners collaborate on developing intervention plans, implementing them, evaluating their effectiveness, and presenting results at both a national conference and a neighborhood town hall. The researcher explicitly describes this as a form of action research in which community partners hold equal decision-making authority at every stage, including dissemination.",
      "question": "Which research methodology is most accurately illustrated in this vignette?",
      "options": {
        "A": "Participatory action research without community co-analysis",
        "B": "Community-based participatory research (CBPR)",
        "C": "Ethnographic field research",
        "D": "Needs assessment survey design"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Participatory action research (PAR) shares roots with CBPR and emphasizes iterative cycles of action and reflection; however, this option incorrectly specifies that community partners do not participate in co-analysis, which contradicts the vignette's explicit statement that analysis was shared. PAR without co-analysis would not fully align with what is described.",
        "B": "CBPR is the methodology described: it is a form of action research requiring full community partner participation across all stages — from question formulation through analysis, action planning, implementation, evaluation, and dissemination — exactly as depicted in the vignette.",
        "C": "Ethnographic research involves immersive observation of a community's culture and practices, typically by an outside researcher who does not necessarily involve community members as co-investigators. It lacks the co-designed intervention and action-plan cycle described here.",
        "D": "A needs assessment survey identifies gaps or problems within a community but does not inherently involve community members as equal partners in all research phases, nor does it include cycles of action, implementation, and evaluation as described in the vignette."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-008-vignette-L2",
      "source_question_id": "008",
      "source_summary": "Community-based participatory research (CBPR) is a type of action research that encourages full participation of community partners in every aspect of the research process from question identification to analysis and dissemination, and involves formulating a research question, planning the study, collecting and analyzing the data, developing action plans from the data, carrying out the action plan, evaluating the results, and disseminating the results.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "participatory",
        "action plan"
      ],
      "vignette": "A public health psychologist working in a predominantly Latino agricultural community is concerned about high rates of depression among farmworkers. Although she holds a doctorate and has extensive survey methodology training, she recruits farmworker representatives to help decide what questions to ask, how to collect data in culturally appropriate ways, and what to do with the findings. The group faces some initial tension because farmworkers distrust academic institutions, but they eventually co-develop a stress-reduction workshop series as their action plan, implement it, evaluate outcomes together, and jointly publish results in both a journal and a Spanish-language community newsletter. The psychologist describes her approach as one requiring full participatory involvement of community partners at every stage.",
      "question": "The research approach described in this vignette most closely reflects which methodology?",
      "options": {
        "A": "Mixed-methods research with community consultation",
        "B": "Collaborative ethnography",
        "C": "Community-based participatory research (CBPR)",
        "D": "Program evaluation"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "CBPR is characterized by genuine co-ownership of all research phases by community members, including question identification, data collection, analysis, action planning, implementation, evaluation, and dissemination — all of which are explicitly present. The cultural tension detail is a realistic feature of CBPR, not a disqualifier.",
        "A": "Mixed-methods research refers to combining quantitative and qualitative data collection within a study. While community consultation may occur in mixed-methods work, the term does not imply the full co-investigator partnership across all research phases that defines CBPR.",
        "B": "Collaborative ethnography does involve engaging community members in interpreting cultural data, but it is primarily concerned with cultural description and does not characteristically include a structured action-plan cycle, implementation, and evaluation loop as described in the vignette.",
        "D": "Program evaluation assesses the effectiveness of an existing intervention rather than co-generating a research question and co-developing an action plan from scratch alongside community members. Although evaluative elements appear here, the full CBPR cycle from question formulation onward is the defining feature."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-008-vignette-L3",
      "source_question_id": "008",
      "source_summary": "Community-based participatory research (CBPR) is a type of action research that encourages full participation of community partners in every aspect of the research process from question identification to analysis and dissemination, and involves formulating a research question, planning the study, collecting and analyzing the data, developing action plans from the data, carrying out the action plan, evaluating the results, and disseminating the results.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "dissemination"
      ],
      "vignette": "A psychologist embedded in a rural Indigenous community notices high rates of youth substance use and begins meeting regularly with tribal elders, youth counselors, and school staff. Over 18 months, these stakeholders collectively decide what data to collect, how to gather it without violating cultural protocols, and how to interpret findings through both statistical summaries and traditional storytelling frameworks. The group then designs a culturally adapted peer-mentorship program, runs it for one school year, evaluates its impact on substance use rates, and shares findings at a tribal council meeting as well as in a regional health journal — emphasizing that the community retained ownership of all data. The psychologist notes that this approach was chosen specifically because purely researcher-driven designs had previously failed in this community.",
      "question": "Which research approach best describes what the psychologist implemented?",
      "options": {
        "A": "Participatory action research (PAR)",
        "B": "Community-based participatory research (CBPR)",
        "C": "Cultural adaptation research",
        "D": "Implementation science framework"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "CBPR is the most precise fit: it is the form of action research that requires full community co-ownership at every stage — question formulation, study planning, data collection and analysis, action plan development, implementation, evaluation, and dissemination — including community data ownership. All of these elements are explicitly present in the vignette.",
        "A": "PAR is closely related and shares the iterative action-reflection cycle, making it a strong distractor. However, PAR does not specifically mandate community co-ownership of data or full partner involvement from question identification onward in the way CBPR does; it can be more researcher-led in practice. The explicit emphasis on co-ownership and full partner authority across all phases points to CBPR specifically.",
        "C": "Cultural adaptation research focuses on modifying evidence-based interventions to fit a specific cultural group, often with researcher guidance rather than community co-investigation. While cultural sensitivity is prominent here, the full cycle of community-driven research from question generation to dissemination exceeds what cultural adaptation research entails.",
        "D": "Implementation science examines how evidence-based practices are adopted in real-world settings, focusing on fidelity, barriers, and uptake. Although the psychologist is implementing an intervention, the study is not evaluating implementation of a previously validated program; rather, the community is generating the research question and program from the outset."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-008-vignette-L4",
      "source_question_id": "008",
      "source_summary": "Community-based participatory research (CBPR) is a type of action research that encourages full participation of community partners in every aspect of the research process from question identification to analysis and dissemination, and involves formulating a research question, planning the study, collecting and analyzing the data, developing action plans from the data, carrying out the action plan, evaluating the results, and disseminating the results.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "co-ownership"
      ],
      "vignette": "A researcher is brought in by a foundation to study opioid overdose rates in a former coal-mining town after several community-led petitions demanded local input into any research conducted about them. The researcher spends three months building trust before any data are collected, and the study's aims are revised twice based on feedback from local recovery coaches, clergy, and former miners who serve as formal co-investigators. Midway through the project, co-investigators vote to exclude a validated questionnaire because community members found it stigmatizing, and a locally developed instrument is substituted despite its lower psychometric rigor. Results are analyzed jointly, and the team produces a harm-reduction toolkit that is piloted in the community before findings are submitted to a public health journal — with local co-investigators listed as co-authors. The foundation's program officer initially classified the project as participatory action research, but the researcher argued a more specific label applied.",
      "question": "The researcher's argument for a more specific label most likely referred to which methodology?",
      "options": {
        "A": "Participatory action research (PAR)",
        "B": "Action science",
        "C": "Community-based participatory research (CBPR)",
        "D": "Empowerment evaluation"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "CBPR is the more specific label the researcher invoked. Unlike PAR, which is a broader family of approaches, CBPR explicitly specifies that community members are full partners at every phase — including question identification, instrument development, data analysis, action planning, piloting, and dissemination — and includes community co-ownership and co-authorship. Every feature described maps precisely onto CBPR's defining characteristics.",
        "A": "PAR is the label the foundation initially used, making it the primary red herring. PAR does involve iterative cycles of action and reflection with community input, but it does not as specifically mandate co-investigator status, co-authorship, and community data co-ownership across all phases. CBPR is the more precise, defined methodology that the researcher would correctly argue applies here.",
        "B": "Action science, associated with Argyris and Schön, focuses on helping practitioners examine and change their own theories-in-use within organizational contexts. While it involves iterative learning cycles, it does not characteristically involve community co-investigators, co-authorship, or population-level public health research as described.",
        "D": "Empowerment evaluation, developed by Fetterman, uses evaluation processes to help communities improve programs and foster self-determination. Although community involvement is central, it is an evaluation framework applied to existing programs rather than a full research methodology spanning question generation through original data collection, analysis, and publication."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-008-vignette-L5",
      "source_question_id": "008",
      "source_summary": "Community-based participatory research (CBPR) is a type of action research that encourages full participation of community partners in every aspect of the research process from question identification to analysis and dissemination, and involves formulating a research question, planning the study, collecting and analyzing the data, developing action plans from the data, carrying out the action plan, evaluating the results, and disseminating the results.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "After a series of overdose deaths, residents of a small city demand a voice in how their neighborhood is studied rather than being studied without input. A social scientist agrees to hold monthly meetings where locals — including harm-reduction advocates, faith leaders, and formerly incarcerated residents — collectively decide what problems deserve attention, what information to gather, and how to make sense of what they find. When preliminary findings suggest that a planned city intervention would not address the root cause, the group revises the city's proposal, pilots a different approach for six months, measures whether it worked, and then presents their findings simultaneously to city officials and at a neighborhood block meeting — insisting that both audiences receive the same unfiltered information. Throughout the process, the social scientist repeatedly declines to make unilateral decisions, deferring instead to the group even when their choices create methodological complications.",
      "question": "Which research approach most precisely characterizes the framework described in this scenario?",
      "options": {
        "A": "Participatory action research (PAR)",
        "B": "Community-based participatory research (CBPR)",
        "C": "Empowerment evaluation",
        "D": "Action science"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "CBPR is the most precise fit. The scenario describes full community co-ownership at every stage: co-identification of the problem, co-design of data collection, co-analysis of findings, co-development and piloting of an action plan, evaluation of results, and simultaneous dual dissemination to professional and community audiences. The researcher's explicit deference to community authority at every decision point — even when it creates methodological complications — mirrors CBPR's defining principle of full partner participation throughout the entire research process.",
        "A": "PAR is the strongest distractor because it also involves iterative cycles of planning, action, and reflection with community input and is a plausible label for this scenario. However, PAR is a broader umbrella that encompasses many approaches with varying degrees of community authority; it does not as specifically define the full co-investigator role at every stage including question identification, joint analysis, and co-equal dissemination. CBPR is the more precise, bounded methodology that captures all these features together.",
        "C": "Empowerment evaluation focuses on helping a community evaluate an existing program to build capacity and self-determination — it presupposes an already-existing intervention being assessed. Here, the group is generating the original research question, designing the study, and developing the intervention from scratch, which exceeds the scope of empowerment evaluation.",
        "D": "Action science, rooted in organizational learning theory, is concerned with helping individuals and organizations surface and challenge their implicit assumptions to produce better outcomes. It does not involve community co-investigators as equal partners in population-level research, nor does it characteristically include joint data collection, co-analysis, and dual community/professional dissemination as described."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-138-vignette-L1",
      "source_question_id": "138",
      "source_summary": "In a normal distribution, 16% of students obtained scores above 165, as this score is one standard deviation above the mean of 150 with a standard deviation of 15.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "normal distribution",
        "standard deviation",
        "mean"
      ],
      "vignette": "A researcher administers a cognitive ability test to a large representative sample. The scores follow a normal distribution with a mean of 150 and a standard deviation of 15. The researcher reports that a score of 165 falls exactly one standard deviation above the mean. Using properties of the normal distribution, the researcher wants to determine what percentage of students scored above 165.",
      "question": "Based on the properties of the normal distribution, approximately what percentage of students scored above 165?",
      "options": {
        "A": "16%",
        "B": "34%",
        "C": "2%",
        "D": "50%"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. In a normal distribution, 68% of scores fall within one standard deviation of the mean (between 135 and 165). The remaining 32% is split equally in both tails, placing approximately 16% of scores above one standard deviation above the mean (i.e., above 165).",
        "B": "Incorrect. 34% represents the proportion of scores between the mean and one standard deviation above (or below) the mean — that is, the area between 150 and 165 in this scenario, not the area above 165.",
        "C": "Incorrect. Approximately 2% of scores fall beyond two standard deviations above the mean (above 180 in this scenario), not one standard deviation. Confusing one SD with two SD boundaries is a common error.",
        "D": "Incorrect. 50% represents the proportion of scores above the mean itself (above 150), not above a score that is one standard deviation above the mean. This would be correct only if 165 were the median/mean."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-138-vignette-L2",
      "source_question_id": "138",
      "source_summary": "In a normal distribution, 16% of students obtained scores above 165, as this score is one standard deviation above the mean of 150 with a standard deviation of 15.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "normal distribution",
        "standard deviation"
      ],
      "vignette": "A school psychologist is reviewing assessment data from a large urban district where students vary considerably in socioeconomic background and native language. The psychologist notes that scores on the district's reading fluency measure approximate a normal distribution, with a mean score of 150 and a standard deviation of 15. A policy decision requires identifying students who performed exceptionally well, defined as scoring 165 or above. The psychologist needs to estimate what proportion of the student population meets this criterion.",
      "question": "What proportion of students in this district would be expected to score at or above 165?",
      "options": {
        "A": "34%",
        "B": "68%",
        "C": "16%",
        "D": "5%"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. 34% represents the proportion of scores falling between the mean (150) and one standard deviation above the mean (165). This is the area within the upper half of the central 68% band, not the area in the upper tail beyond 165.",
        "B": "Incorrect. 68% is the total proportion of scores falling within one standard deviation on either side of the mean (between 135 and 165). This includes scores both below and above the mean, making it far too large for the upper tail alone.",
        "C": "Correct. A score of 165 is exactly one standard deviation above the mean (150 + 15 = 165). In a normal distribution, 84% of scores fall at or below one SD above the mean, leaving 16% in the upper tail. The socioeconomic and language diversity details are relevant to test validity concerns but do not change this distributional property.",
        "D": "Incorrect. Approximately 5% (2.5% per tail) of scores fall beyond two standard deviations from the mean. Since 165 is only one standard deviation above the mean in this scenario, this value underestimates the proportion of students scoring above 165."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-138-vignette-L3",
      "source_question_id": "138",
      "source_summary": "In a normal distribution, 16% of students obtained scores above 165, as this score is one standard deviation above the mean of 150 with a standard deviation of 15.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "percentile"
      ],
      "vignette": "A researcher studying adult cognitive performance reports that scores on a new neuropsychological battery cluster symmetrically around a central value, with most participants scoring near 150. The test developer notes that a score of 165 corresponds to roughly the 84th percentile. A colleague, reviewing the data, assumes that because the test was administered to a highly educated sample, the score distribution must be skewed, and therefore the tail probabilities cannot be easily estimated. The research team wants to determine what percentage of the broader normative sample would be expected to score above 165.",
      "question": "Assuming the scores follow a symmetric, bell-shaped distribution with a mean of 150 and equal spread on both sides, what percentage of the normative sample would score above 165?",
      "options": {
        "A": "2.5%",
        "B": "16%",
        "C": "34%",
        "D": "50%"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. 2.5% represents the proportion beyond two standard deviations above the mean (i.e., beyond 180 if SD = 15). The colleague's concern about skew is a red herring; a score at the 84th percentile in a symmetric distribution corresponds to one SD above the mean, placing 16% — not 2.5% — in the upper tail.",
        "B": "Correct. The 84th percentile in a normal distribution corresponds precisely to one standard deviation above the mean. If the mean is 150 and the score of 165 is at the 84th percentile, then the standard deviation is 15, and 16% of scores fall above 165. The colleague's claim about skew is a red herring; the symmetry of the distribution is established by the scenario.",
        "C": "Incorrect. 34% is the proportion of scores between the mean and one standard deviation above the mean (between 150 and 165), representing one arm of the central 68% band. It is a plausible distractor because it is derived directly from the same region of the distribution, but it does not represent the upper tail.",
        "D": "Incorrect. 50% would be the proportion above the mean in any symmetric distribution. This would be correct if the cutoff were 150 (the mean itself), not 165, which is shifted one standard deviation above the mean."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-138-vignette-L4",
      "source_question_id": "138",
      "source_summary": "In a normal distribution, 16% of students obtained scores above 165, as this score is one standard deviation above the mean of 150 with a standard deviation of 15.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "symmetry"
      ],
      "vignette": "A clinical researcher publishes results showing that a battery of executive function tasks yields scores clustered around a central tendency of 150, with departures from this center being equally common above and below due to the distributional symmetry of the measure. A consulting statistician notes that a score of 165 divides the upper portion of the distribution such that the area between the central value and 165 equals the area between the central value and an equivalent distance below it. After reviewing the data, the research team claims that the proportion of participants scoring above 165 is the same as the proportion scoring below 135. A trainee reviews this report and, noting the symmetry, incorrectly concludes that the two tails together must account for roughly one-third of all scores, making each tail approximately 17%.",
      "question": "What is the correct percentage of participants expected to score above 165, and what reasoning leads to this conclusion?",
      "options": {
        "A": "34%, because 165 marks the boundary of the central band and 34% of scores fall in each half of that band",
        "B": "50%, because the symmetry of the distribution means scores above and below 165 are equally likely",
        "C": "16%, because 68% of scores fall within one standard deviation of the mean, leaving 32% in both tails combined and 16% in the upper tail alone",
        "D": "2%, because in a symmetric distribution only extreme outliers fall beyond the boundaries defined by deviations from the center"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. 34% is the proportion of scores between the mean (150) and one standard deviation above it (165) — one arm of the central 68% band. This option exploits the common error of conflating the area between the mean and +1 SD with the area above +1 SD, but it describes the wrong region of the distribution.",
        "B": "Incorrect. 50% would represent scores above the mean, not above a value that is one full standard deviation above the mean. The symmetry argument is partially correct — both tails beyond ±1 SD are equal — but it does not mean that half of all scores exceed a value located at +1 SD.",
        "C": "Correct. In a normal distribution, 68% of scores fall within one standard deviation of the mean (between 135 and 165 here), leaving 32% in both tails. By symmetry, each tail contains 16%. The trainee's error was conflating the two-tail total (32%) with one-third (~33%), arriving at ~17% per tail. The correct value is 16% per tail, placing 16% of participants above 165.",
        "D": "Incorrect. Approximately 2% (more precisely, 2.3%) falls beyond two standard deviations above the mean, not one. This option confuses the ±1 SD boundary with the ±2 SD boundary, a plausible error given that both describe 'deviations from the center,' but the scenario specifies that 165 is only one standard deviation above the mean of 150."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-138-vignette-L5",
      "source_question_id": "138",
      "source_summary": "In a normal distribution, 16% of students obtained scores above 165, as this score is one standard deviation above the mean of 150 with a standard deviation of 15.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A testing organization releases a report on a widely used assessment administered to thousands of adults. The report notes that the bulk of participants — roughly two-thirds of the entire group — earned scores falling in the band between 135 and 165 points, with the center of that band at 150. The organization further observes that the number of people earning scores between 150 and 165 is identical to the number earning scores between 135 and 150. A policy analyst uses this report to argue that very few people — perhaps only about one in fifty — would score above 165, suggesting that a proposed 165-point eligibility threshold would be prohibitively exclusive. A second analyst disagrees, proposing instead that a 165-point threshold would be far more inclusive than the first analyst claims.",
      "question": "Based on the distributional properties implied by this report, what proportion of participants would actually be expected to score above 165?",
      "options": {
        "A": "2%, because the 165-point threshold lies at the edge of a region where scores become exceedingly rare",
        "B": "34%, because scores above 165 represent half of the central band described in the report",
        "C": "16%, because the two-thirds band implies that one-third of participants fall outside it, split equally above and below",
        "D": "50%, because the equal frequency of scores on both sides of 150 indicates that exactly half of all participants fall above 165"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. Approximately 2% of scores fall beyond two standard deviations from the mean. The scenario describes the band from 135 to 165 as capturing roughly two-thirds of all scores — a clear marker for the ±1 SD range, not the ±2 SD range. The first analyst's 'one in fifty' claim is the embedded red herring designed to make this option plausible, but the distributional math does not support it.",
        "B": "Incorrect. 34% is the proportion of scores falling between the mean (150) and one standard deviation above it (165). This option misreads the statement that two-thirds of participants fall between 135 and 165 by treating that entire upper half of the central band as representative of what lies above 165. The area above 165 is the tail beyond the band, not a portion within it.",
        "C": "Correct. The report states that roughly two-thirds of participants scored between 135 and 165, which is the defining property of a one-standard-deviation band around the mean in a normal distribution. That leaves one-third outside this band. Because the distribution is symmetric (the equal frequency above and below 150 within the band confirms this), the one-third outside the band is split evenly: approximately 16% below 135 and 16% above 165. The second analyst is correct that 165 is far less restrictive than 1 in 50.",
        "D": "Incorrect. 50% would be the proportion of scores above the mean (150), not above a point that is one standard deviation higher (165). The equal frequency of scores between 135–150 and 150–165 demonstrates symmetry around 150, but this symmetry does not imply that half of all scores exceed 165 — it only confirms that 165 lies at a symmetric distance from the mean."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-052-vignette-L1",
      "source_question_id": "052",
      "source_summary": "Of the single-subject research designs, the ABAB design would be least desirable for evaluating the effectiveness of an intervention to eliminate the head banging of a child with autism spectrum disorder, as removing a successful treatment during the second baseline (A) phase would be unethical.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "ABAB",
        "single-subject",
        "ethics"
      ],
      "vignette": "A researcher is designing a single-subject study to evaluate the effectiveness of a behavioral intervention targeting severe self-injurious head banging in a child with autism spectrum disorder. The research team considers several designs, including the ABAB reversal design, which requires withdrawing the intervention during the second baseline phase. A consultant raises an ethics concern, noting that one design would require deliberately allowing the dangerous behavior to return in order to demonstrate experimental control. The team agrees that choosing this particular design would be inappropriate given the severity and risk of the target behavior.",
      "question": "Which single-subject design is LEAST appropriate for studying this intervention, and why?",
      "options": {
        "A": "Multiple baseline design, because it does not allow comparison across conditions",
        "B": "ABAB reversal design, because withdrawing a successful treatment for a dangerous behavior is unethical",
        "C": "Changing criterion design, because it cannot demonstrate functional control over behavior",
        "D": "Alternating treatments design, because it requires simultaneous exposure to multiple interventions"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. The multiple baseline design is actually well-suited to ethically studying dangerous behaviors because it does not require withdrawal of an effective treatment. It introduces the intervention sequentially across behaviors, settings, or individuals, and allows causal inference without reinstating harm.",
        "B": "Correct. The ABAB reversal design requires the researcher to withdraw the intervention during the second A (baseline) phase to demonstrate that the behavior change is due to the treatment. When the target behavior is dangerous (e.g., head banging), reinstating it deliberately constitutes a serious ethical violation, making this design the least appropriate choice.",
        "C": "Incorrect. The changing criterion design can demonstrate functional control by showing that behavior tracks successive criterion levels; it does not require withdrawal of treatment. While it has limitations in controlling for history and maturation, lack of functional control is not an accurate description of its primary weakness.",
        "D": "Incorrect. The alternating treatments design compares two or more interventions by rapidly alternating between them across sessions. It does not require baseline reinstatement and can be used with dangerous behaviors, though its appropriateness depends on the rapidity of switching. It is not the least appropriate design for this scenario."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-052-vignette-L2",
      "source_question_id": "052",
      "source_summary": "Of the single-subject research designs, the ABAB design would be least desirable for evaluating the effectiveness of an intervention to eliminate the head banging of a child with autism spectrum disorder, as removing a successful treatment during the second baseline (A) phase would be unethical.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "reversal",
        "self-injurious"
      ],
      "vignette": "A school psychologist wants to study whether a sensory-based intervention reduces self-injurious behavior in a 7-year-old nonverbal child with a recent diagnosis of autism spectrum disorder and co-occurring intellectual disability. The psychologist considers several single-subject designs and notes that one popular approach requires the researcher to temporarily suspend the intervention after it has proven effective, so that the target behavior returns to baseline levels before reintroducing the treatment. A colleague points out that this requirement raises serious ethical concerns given the nature of the self-injurious behavior being targeted.",
      "question": "Which design is the colleague most likely cautioning against using?",
      "options": {
        "A": "Multiple baseline design across settings",
        "B": "Alternating treatments design",
        "C": "ABAB reversal design",
        "D": "Changing criterion design"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. The multiple baseline design across settings staggers the introduction of the intervention across different contexts without ever requiring withdrawal of treatment. Because it does not reinstate baseline conditions once the intervention has been introduced, it does not carry the same ethical risks for dangerous behaviors.",
        "B": "Incorrect. The alternating treatments design rapidly switches between two or more conditions (e.g., treatment A and treatment B) across sessions to compare their effects. It does not involve a deliberate return to no-treatment baseline conditions, so it does not pose the same ethical concern described in the vignette.",
        "C": "Correct. The ABAB reversal design is the design being cautioned against. It requires that the researcher withdraw the effective intervention during the second A phase to reestablish baseline rates of behavior, which would be ethically unacceptable when the target behavior is self-injurious and potentially dangerous.",
        "D": "Incorrect. The changing criterion design adjusts the performance criterion incrementally as the participant meets each successive goal. It does not involve removing an effective intervention and allowing a dangerous behavior to return to baseline, so it does not carry the ethical concern described in the vignette."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-052-vignette-L3",
      "source_question_id": "052",
      "source_summary": "Of the single-subject research designs, the ABAB design would be least desirable for evaluating the effectiveness of an intervention to eliminate the head banging of a child with autism spectrum disorder, as removing a successful treatment during the second baseline (A) phase would be unethical.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "withdrawal"
      ],
      "vignette": "A behavioral researcher is planning a rigorous study to establish experimental control over a dangerous repetitive behavior in a young child receiving services at a therapeutic day school. The research team is committed to demonstrating a clear functional relationship between the intervention and the behavior change, rather than relying on correlational evidence. One team member proposes a design that would require a withdrawal phase mid-study, arguing that this is necessary for causal inference. However, a supervising psychologist objects strongly, citing the child's welfare and noting that allowing the behavior to increase again — even temporarily — would expose the child to unnecessary physical harm.",
      "question": "Based on the supervising psychologist's concern, which design is most likely being objected to, and what is the primary reason?",
      "options": {
        "A": "Multiple baseline design across participants, because it requires delaying treatment for some children",
        "B": "ABAB reversal design, because demonstrating experimental control requires reinstating a harmful behavior",
        "C": "Changing criterion design, because the shifting criteria may inadvertently increase behavior variability",
        "D": "Alternating treatments design, because rapidly alternating conditions may temporarily worsen the target behavior"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. While the multiple baseline design across participants does delay treatment for some individuals — which raises its own ethical questions — it does not require deliberately withdrawing an effective treatment after it has been introduced. The concern raised in the vignette is specifically about allowing a dangerous behavior to increase again mid-study, which is not a feature of the multiple baseline design.",
        "B": "Correct. The ABAB reversal design requires the researcher to withdraw the active intervention during the second baseline (A) phase to demonstrate that behavior change was caused by the treatment, not by extraneous factors. When the target behavior is dangerous (e.g., head banging), this withdrawal phase means deliberately allowing harmful behavior to return to elevated levels, which is ethically unjustifiable.",
        "C": "Incorrect. The changing criterion design adjusts performance targets incrementally but does not involve a full return to pretreatment baseline. While there may be variability as criteria shift, this design does not call for the systematic reinstatement of dangerous behavior levels in the way that the reversal design does.",
        "D": "Incorrect. The alternating treatments design compares different conditions by rapidly switching between them, but both conditions in this design typically include some form of active intervention or structure. It does not involve a planned return to an untreated baseline, so it does not match the specific ethical concern described by the supervising psychologist."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-052-vignette-L4",
      "source_question_id": "052",
      "source_summary": "Of the single-subject research designs, the ABAB design would be least desirable for evaluating the effectiveness of an intervention to eliminate the head banging of a child with autism spectrum disorder, as removing a successful treatment during the second baseline (A) phase would be unethical.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "experimental control"
      ],
      "vignette": "A clinical researcher is reviewing a proposed protocol that aims to rigorously demonstrate a causal link between a behavioral treatment and significant reductions in a child's dangerous repetitive behavior. The protocol includes four distinct phases, carefully structured so that changes in the behavior co-vary precisely with the presence and absence of the intervention across the phases. A bioethics board reviewing the proposal objects, arguing that the design's mechanism for achieving experimental control inherently requires placing the child in a condition of documented physical risk during the study. The research team counters that any less rigorous design would reduce the study's internal validity, but the board maintains that scientific rigor cannot override participant welfare in this case.",
      "question": "Which design is the bioethics board most likely objecting to, and what specific feature constitutes the ethical problem?",
      "options": {
        "A": "Multiple baseline design across behaviors, because staggering treatment introduction sacrifices some degree of internal validity",
        "B": "Changing criterion design, because the incremental criterion shifts could temporarily increase variability in the dangerous behavior",
        "C": "ABAB reversal design, because reintroducing baseline conditions after successful treatment deliberately exposes the child to harm",
        "D": "Alternating treatments design, because rapid condition switching prevents stable measurement of the dangerous behavior"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. The multiple baseline design is generally considered ethically preferable for dangerous behaviors precisely because it does not require any withdrawal phase. While staggering treatment does limit experimental control compared to reversal designs, the board's concern in this vignette is not about reduced internal validity but about a design feature that actively exposes the child to harm — which is not characteristic of the multiple baseline design.",
        "B": "Incorrect. The changing criterion design adjusts performance standards progressively and does not involve returning to a no-treatment baseline. Although criterion shifts could introduce some behavioral variability, this design does not systematically reinstate dangerous behavior levels as part of its structure. The board's objection to a four-phase design that deliberately reintroduces harmful conditions does not match this design.",
        "C": "Correct. The ABAB reversal design contains exactly four phases: baseline (A), treatment (B), return to baseline (A), and treatment reinstatement (B). Its mechanism for demonstrating experimental control requires withdrawing the effective intervention during the second A phase, which causes the dangerous behavior to re-emerge. This deliberate reinstatement of harm to prove causality is precisely the ethical problem the bioethics board identifies.",
        "D": "Incorrect. The alternating treatments design does not use four sequential phases in the same way as the ABAB design; instead, it rapidly alternates between two or more conditions within a shorter timeframe. It does not include a planned no-treatment baseline reinstatement phase, and its primary limitation is multitreatment interference rather than deliberate exposure to harm through treatment withdrawal."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-052-vignette-L5",
      "source_question_id": "052",
      "source_summary": "Of the single-subject research designs, the ABAB design would be least desirable for evaluating the effectiveness of an intervention to eliminate the head banging of a child with autism spectrum disorder, as removing a successful treatment during the second baseline (A) phase would be unethical.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher is evaluating the merits of four different structured approaches to studying whether a newly developed procedure reduces a young child's pattern of striking her head against hard surfaces. All four approaches use a single child as the subject, and all four have been shown in the literature to establish clear links between a procedure and a change in behavior. The researcher narrows the field to one approach that is especially persuasive to scientific reviewers because it demonstrates that the child's behavior rises and falls in direct correspondence with the presence and absence of the procedure — across more than one cycle. A child welfare consultant, however, reviews the selected approach and immediately flags it as inappropriate for this particular situation, not because of any flaw in its logic, but because of what would need to happen to the child in order for the approach to work as intended.",
      "question": "Which approach has the child welfare consultant flagged, and what is the specific concern?",
      "options": {
        "A": "The approach that introduces the procedure at different time points across multiple target behaviors, because it withholds help from some harmful behaviors while treating others",
        "B": "The approach that requires the procedure to be removed after it has already reduced the dangerous behavior, so that the harmful behavior can re-emerge before the procedure is reintroduced",
        "C": "The approach that gradually shifts performance expectations upward in small steps, because sudden criterion jumps could provoke an increase in the dangerous behavior",
        "D": "The approach that alternates rapidly between two different procedures across sessions, because the child cannot consistently predict which procedure will be used on any given day"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. This describes the multiple baseline design across behaviors, which staggers the introduction of the procedure so that some behaviors continue in an untreated baseline while others receive the intervention. Although withholding treatment from any dangerous behavior raises ethical questions, the design does not require the deliberate removal of a successful procedure once it has been applied — which is the specific concern the consultant identifies.",
        "B": "Correct. This describes the ABAB reversal design. Its defining feature — and the source of its persuasive scientific power — is that the intervention is removed after it has successfully reduced the behavior, allowing the behavior to return to harmful levels before the intervention is reintroduced. For a child who bangs her head against hard surfaces, this means the researcher must intentionally allow the dangerous behavior to re-emerge as part of the study's procedure, which the child welfare consultant correctly identifies as ethically indefensible.",
        "C": "Incorrect. This describes the changing criterion design, which uses progressively shifting performance standards to shape behavior. While a poorly calibrated criterion shift could theoretically produce a temporary increase in behavior variability, this design does not involve deliberately removing an effective procedure and allowing a dangerous behavior to return to baseline. The child welfare concern in the vignette is specifically about planned reinstatement of harm, not about incidental variability.",
        "D": "Incorrect. This describes the alternating treatments design, which rapidly switches between two active procedures across sessions. While the unpredictability of condition switching may create some instability for the child, this design does not include a phase where all intervention is withdrawn so that the dangerous behavior can return to elevated pretreatment levels. The specific mechanism of concern — deliberate procedure removal followed by behavior reinstatement — is not a feature of this design."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-068-vignette-L1",
      "source_question_id": "068",
      "source_summary": "External validity refers to the generalizability of research results to other people, settings, and times.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "generalizability",
        "external validity",
        "sample"
      ],
      "vignette": "A researcher conducts a randomized controlled trial examining the effectiveness of a new CBT protocol for depression. The study enrolls only White, college-educated women between the ages of 18 and 25 from a single university clinic. The results show a statistically significant reduction in depression symptoms. However, reviewers raise concerns about whether the findings can be applied to older adults, men, and individuals from diverse ethnic backgrounds. They argue that the restricted sample limits the generalizability of the results.",
      "question": "The reviewers' concern about whether findings can be applied to other populations and settings is best described as a threat to which type of validity?",
      "options": {
        "A": "External validity",
        "B": "Internal validity",
        "C": "Construct validity",
        "D": "Statistical conclusion validity"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. External validity refers to the degree to which research findings can be generalized beyond the specific sample, setting, and time of the study. The reviewers' concern that results from a narrow demographic group may not apply to other populations is a classic external validity threat.",
        "B": "Incorrect. Internal validity refers to whether the study accurately establishes a causal relationship between the independent and dependent variables — that is, whether changes in the outcome were actually caused by the intervention and not by confounds. The reviewers are not questioning the causal logic within the study, but rather its applicability outside it.",
        "C": "Incorrect. Construct validity concerns whether the measures or manipulations in a study accurately represent the theoretical constructs they intend to capture (e.g., whether a depression scale truly measures depression). This is not the concern raised here.",
        "D": "Incorrect. Statistical conclusion validity refers to whether the study used appropriate statistical methods and had sufficient power to detect a true effect. The concern here is not about statistical procedures but about who the findings can be extended to."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-068-vignette-L2",
      "source_question_id": "068",
      "source_summary": "External validity refers to the generalizability of research results to other people, settings, and times.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "generalizability",
        "sample"
      ],
      "vignette": "A clinical psychologist reads a published study demonstrating that a mindfulness-based intervention significantly reduced anxiety in a sample of 40 adult outpatients at a university-affiliated urban clinic. The participants were all self-referred and highly motivated, and the study was conducted by the intervention's developer. The psychologist works at a rural community mental health center serving low-income clients who are often court-mandated to treatment, and she wonders whether the published results would hold in her setting.",
      "question": "The psychologist's concern about whether the study's findings would apply to her population and setting reflects a limitation in which aspect of the research?",
      "options": {
        "A": "Statistical conclusion validity",
        "B": "Ecological validity",
        "C": "Internal validity",
        "D": "External validity"
      },
      "correct_answer": "D",
      "option_explanations": {
        "D": "Correct. External validity is the extent to which research findings can be generalized to other people, settings, and times. The psychologist's concern that findings from a self-referred, motivated urban sample may not apply to her court-mandated, rural, low-income clients is a prototypical external validity question.",
        "B": "Incorrect. Ecological validity is a related but narrower concept referring specifically to whether research procedures and environments resemble real-world settings. While the psychologist's concern touches on setting differences, ecological validity does not capture the broader concern about generalizing across populations and demographic characteristics — external validity is the more comprehensive and appropriate term here.",
        "C": "Incorrect. Internal validity concerns whether the study correctly attributes the observed changes in anxiety to the mindfulness intervention rather than to confounding variables. The psychologist is not questioning the causal conclusions within the original study, only whether those conclusions extend to her context.",
        "A": "Incorrect. Statistical conclusion validity addresses whether the statistical analyses are appropriate and powerful enough to detect true effects. The psychologist is not questioning whether the original study's statistics were properly applied; she is questioning whether results translate to a different population and setting."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-068-vignette-L3",
      "source_question_id": "068",
      "source_summary": "External validity refers to the generalizability of research results to other people, settings, and times.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "population"
      ],
      "vignette": "A research team publishes findings showing that a structured problem-solving intervention significantly reduces caregiver burnout in a sample of 120 family caregivers recruited through advertisements at a large urban hospital. The caregivers all completed the study voluntarily, were literate in English, and had access to consistent internet service to complete the online sessions. A colleague reviewing the study notes that the tight recruitment criteria and the convenience-based enrollment method raise an important concern. She argues that the findings may not hold for the broader population of caregivers in the United States, particularly those in rural or non-English-speaking communities.",
      "question": "The colleague's critique is most accurately described as identifying a limitation related to which methodological concept?",
      "options": {
        "A": "Selection bias threatening internal validity",
        "B": "Demand characteristics threatening construct validity",
        "C": "External validity of the study findings",
        "D": "Inadequate statistical power threatening statistical conclusion validity"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. External validity refers to whether study findings can be generalized to other populations, settings, and conditions. The colleague's concern that findings from a narrow, convenience-based sample (English-literate, internet-capable, urban hospital volunteers) may not extend to rural or non-English-speaking caregivers is a direct threat to external validity.",
        "A": "Incorrect. Selection bias is indeed relevant to this scenario because the sample was not randomly selected from the target population, and selection bias can threaten internal validity when differential dropout or assignment occurs. However, the colleague's specific concern is not about whether the intervention effect within this study was confounded, but about whether the findings generalize beyond this particular sample — which is an external validity issue.",
        "B": "Incorrect. Demand characteristics occur when participants respond according to perceived study expectations rather than genuine effects, threatening construct validity. While the voluntary and literate sample might be unusually responsive, this is not the colleague's stated concern; she is focused on who the findings apply to, not whether the measured outcomes truly reflect burnout.",
        "D": "Incorrect. Statistical power concerns the ability of a study to detect a true effect given the sample size and effect size. A sample of 120 is moderate and the study found significant results, so power is not the concern raised. The colleague is not disputing whether the statistical result was reliable within this study."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-068-vignette-L4",
      "source_question_id": "068",
      "source_summary": "External validity refers to the generalizability of research results to other people, settings, and times.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "replication"
      ],
      "vignette": "A well-designed study using random assignment finds that a brief motivational interviewing intervention significantly increases medication adherence among adults with Type 2 diabetes in a managed-care setting in the northeastern United States. The methodology is rigorous, with blinded outcome assessors and careful control of confounds. A policy committee reviewing the study for potential nationwide implementation notes that all participants were insured, spoke English as a first language, and were recruited from a single health system with above-average resources. When three subsequent replication attempts in federally qualified health centers serving uninsured, predominantly Spanish-speaking patients yielded inconsistent results, the committee questions what the original study's limitation was.",
      "question": "The committee's analysis most precisely identifies a limitation in which aspect of the original study?",
      "options": {
        "A": "Internal validity, because the original study's random assignment may have failed to equate groups on insurance and language status",
        "B": "External validity, because the original sample's characteristics restricted the generalizability of findings to other populations and settings",
        "C": "Construct validity, because motivational interviewing may not have been operationalized equivalently across different health system contexts",
        "D": "Ecological validity, because the managed-care environment did not adequately represent the real-world conditions of most diabetes patients"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. External validity concerns the degree to which study findings generalize across persons, settings, and times. The original study's insured, English-speaking, single-system sample produced strong internal results, but the failed replications in demographically distinct populations reveal that the findings do not generalize broadly — the defining characteristic of an external validity limitation.",
        "A": "Incorrect. Internal validity concerns whether the intervention itself caused the observed effect within the study, and random assignment is a primary method for ensuring this. The problem here is not that confounds compromised the original study's causal inference — the rigorous methodology addressed that — but that the findings do not hold in other populations, which is an external validity issue.",
        "C": "Incorrect. Construct validity would be threatened if the operationalization of motivational interviewing across studies differed in ways that measured different constructs. While protocol fidelity differences across sites are worth examining, the committee's concern is fundamentally about who the findings apply to, not whether the intervention was measured or implemented as the same construct.",
        "D": "Incorrect. Ecological validity is a specific subtype of external validity addressing the degree to which study conditions mirror naturalistic settings. While the managed-care environment may differ from community health centers, the committee's concern encompasses not just the setting but also demographic, linguistic, and insurance-related population differences — making the broader concept of external validity the more accurate and complete answer."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-068-vignette-L5",
      "source_question_id": "068",
      "source_summary": "External validity refers to the generalizability of research results to other people, settings, and times.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Internal/External Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators carefully recruits 200 participants who all share similar backgrounds, ensuring no meaningful differences exist between those who receive the new program and those who do not. The program is delivered in a single, tightly controlled facility, and follow-up data show that participants in the program group improved substantially more than those who did not receive it. A separate group of practitioners in a different region attempts to deliver the same program to a broader mix of clients — including older adults, people with limited formal education, and individuals from non-Western cultural backgrounds — and finds that the improvements seen in the original study do not appear. The original investigators acknowledge that, while their study was carefully conducted, it was optimized for one narrow context.",
      "question": "The failure of the findings to transfer to the broader and more diverse group of clients in the second setting most directly reflects which limitation of the original study?",
      "options": {
        "A": "The study failed to control for differences between the two groups within the original study, undermining confidence in the causal conclusion",
        "B": "The measures used in the original study may not have captured the true nature of the improvement, introducing error into the results",
        "C": "The tight controls and narrow, homogeneous sample limited the degree to which the findings could be extended to other people and settings",
        "D": "The single facility in which the study was conducted did not reflect real-world conditions, making the results artificially inflated"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. External validity is the extent to which findings from a study can be generalized to other populations, settings, and conditions. The original study's homogeneous sample and tightly controlled single-site design produced strong internal results, but the failure to replicate across diverse clients in a different region directly reveals that the findings lack broad generalizability — the hallmark of an external validity limitation.",
        "A": "Incorrect. This option describes a threat to internal validity, which concerns whether differences in outcomes within the study were truly caused by the program and not by pre-existing differences between groups. The scenario explicitly states that the investigators carefully ensured no meaningful differences existed between groups, addressing this concern. The problem is not whether the original causal inference was sound, but whether it extends to other populations.",
        "B": "Incorrect. This option describes a construct validity concern — specifically, whether the outcome measures accurately captured what they intended to measure. While this is always a legitimate consideration, nothing in the scenario suggests the measures were flawed or that measurement error explains the results. The problem is that the findings did not transfer to different people and settings, not that the original measurements were inaccurate.",
        "D": "Incorrect. This option describes ecological validity, a specific concern about whether the study's controlled conditions resemble naturalistic settings. While the single-facility design is relevant, the scenario emphasizes that the breakdown occurred because of the diversity of the new client group — including age, education, and cultural background — rather than exclusively because the new setting was more naturalistic. External validity is the broader and more precise concept encompassing all these population and setting differences."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-36-vignette-L1",
      "source_question_id": "36",
      "source_summary": "Statistical power is increased by greater sample size, larger effect size, and higher alpha level, but not by population heterogeneity on the dependent variable, as greater population homogeneity is associated with greater statistical power.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "statistical power",
        "sample size",
        "effect size"
      ],
      "vignette": "A researcher is designing a study to detect a treatment effect for a new anxiety intervention. She is concerned about statistical power and wants to maximize the probability of detecting a true effect. She considers several design modifications: increasing the sample size, selecting participants from a highly heterogeneous population on the dependent variable, and choosing a larger effect size intervention. Her advisor reminds her that one of these modifications will actually reduce power rather than increase it.",
      "question": "Which of the following modifications would DECREASE statistical power in this study?",
      "options": {
        "A": "Increasing the alpha level from .01 to .05",
        "B": "Selecting participants from a population with high variability on the dependent variable",
        "C": "Increasing the sample size from 30 to 120",
        "D": "Selecting a treatment known to produce a large effect size"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Increasing alpha from .01 to .05 widens the rejection region, making it easier to reject the null hypothesis and thereby increasing statistical power. This is incorrect as a power-reducing strategy.",
        "B": "Greater population heterogeneity on the dependent variable increases within-group variance (error variance), which reduces the signal-to-noise ratio and therefore decreases statistical power. This is the correct answer.",
        "C": "Increasing sample size directly reduces standard error, narrows confidence intervals, and increases the ability to detect true effects — all of which increase statistical power. This is incorrect as a power-reducing strategy.",
        "D": "A larger effect size means the true difference between conditions is greater, making it easier to detect with any given sample, which increases statistical power. This is incorrect as a power-reducing strategy."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-36-vignette-L2",
      "source_question_id": "36",
      "source_summary": "Statistical power is increased by greater sample size, larger effect size, and higher alpha level, but not by population heterogeneity on the dependent variable, as greater population homogeneity is associated with greater statistical power.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "null hypothesis",
        "sample size"
      ],
      "vignette": "A clinical researcher is conducting a pilot study on the effectiveness of a brief CBT intervention for insomnia among college students. The study uses only 18 participants, and the researcher fails to reject the null hypothesis despite reporting that participants showed meaningful improvements in sleep diary scores. A statistical consultant reviewing the study notes that the participant pool was drawn from a highly diverse university clinic that serves students with a wide range of co-occurring conditions, comorbidities, and demographic backgrounds, contributing to substantial variability in sleep outcomes across the sample. The consultant suggests that the failure to reject the null hypothesis was influenced by a feature of the sample that undermined the study's ability to detect a true effect.",
      "question": "Which of the following best explains the primary reason the study lacked sufficient ability to detect a true treatment effect?",
      "options": {
        "A": "The study had a high rate of Type I error due to a lenient alpha level",
        "B": "The study used a one-tailed test when a two-tailed test was more appropriate",
        "C": "Both the small sample and high population heterogeneity on the outcome variable reduced statistical power",
        "D": "The effect size was inflated by the use of self-report measures rather than objective sleep monitoring"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Type I error refers to falsely rejecting a true null hypothesis, and a lenient alpha would increase rather than decrease power. The scenario describes a failure to detect an effect, which is related to Type II error and low power — not inflated Type I error.",
        "B": "The choice between one-tailed and two-tailed tests affects the placement of the rejection region, but nothing in the scenario indicates a directional hypothesis error. This distractor incorrectly focuses on test directionality rather than the power factors described.",
        "C": "Both a small sample size (n=18) and high variability in the population on the dependent variable (due to diverse comorbidities) reduce statistical power by increasing error variance and limiting sensitivity to detect true effects. This correctly identifies both contributing factors.",
        "D": "While self-report measures can introduce measurement error, the scenario does not indicate that effect size was inflated. The issue described — diverse comorbidities inflating variability — is a power problem, not a measurement validity problem specific to self-report instruments."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-36-vignette-L3",
      "source_question_id": "36",
      "source_summary": "Statistical power is increased by greater sample size, larger effect size, and higher alpha level, but not by population heterogeneity on the dependent variable, as greater population homogeneity is associated with greater statistical power.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A research team studying medication adherence in patients with chronic pain recruits participants from a large, geographically diverse registry spanning rural and urban settings, multiple socioeconomic strata, and varying pain conditions. After collecting data from 200 participants, the team runs their primary analysis and finds a non-significant result despite a clinically meaningful difference in adherence rates between groups. The lead investigator argues that the study was designed with adequate resources, that the intervention was derived from a meta-analysis suggesting a moderate effect, and that the chosen alpha level was conventional. A methodologist on the team suspects the problem lies in something that was overlooked during the sampling design phase.",
      "question": "What aspect of the study design most likely undermined the team's ability to detect the treatment effect?",
      "options": {
        "A": "The alpha level was set too conservatively, reducing the probability of rejecting the null hypothesis",
        "B": "The sample of 200 was insufficient to achieve adequate power for a moderate effect",
        "C": "The high variance on the dependent variable introduced by the heterogeneous sampling strategy reduced statistical power",
        "D": "Relying on a meta-analytic effect size estimate led to an overestimate of the expected effect in this specific population"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A conventional alpha level (typically .05) is not conservative, and there is no indication in the scenario that alpha was set more stringently. Reducing alpha does lower power, but this does not match the information given about the study design.",
        "B": "While small samples reduce power, a sample of 200 is generally considered adequate or more than adequate for detecting a moderate effect size. The scenario specifies that resources were adequate, pointing away from sample size as the primary limitation.",
        "C": "By recruiting from a geographically and clinically diverse registry, the team introduced substantial heterogeneity in the dependent variable (adherence). Greater variance within groups increases error variance and reduces the signal-to-noise ratio, thereby reducing statistical power. This is the correct answer.",
        "D": "Using a meta-analytic effect size as a planning estimate is a reasonable practice and may result in an overestimate for specific populations, but the scenario does not provide evidence that the effect was actually smaller than expected — it emphasizes the diversity of the sample as the methodologist's concern."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-36-vignette-L4",
      "source_question_id": "36",
      "source_summary": "Statistical power is increased by greater sample size, larger effect size, and higher alpha level, but not by population heterogeneity on the dependent variable, as greater population homogeneity is associated with greater statistical power.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "homogeneity"
      ],
      "vignette": "A graduate student replicates a well-known study on stress reduction techniques using a carefully recruited sample of 40 participants who are all full-time employees at a single mid-sized technology firm, all reporting similar levels of workplace stress at baseline. The student expects that using such a tightly recruited, uniform sample will hurt the study because the narrow range of baseline scores will compress the outcome distribution and make it harder to show change. To compensate, she chooses to recruit from a much broader population in a follow-up study, adding participants from hospitals, schools, and retail environments with widely varying occupational stress profiles. She is surprised when the follow-up study with 95 participants fails to reach significance while the original study with 40 did.",
      "question": "Which of the following best explains why the original, smaller study detected an effect while the larger follow-up study did not?",
      "options": {
        "A": "The larger follow-up study had reduced internal validity due to the inclusion of participants from heterogeneous settings",
        "B": "The follow-up study's broader recruitment introduced greater population heterogeneity on the dependent variable, reducing statistical power despite the larger sample",
        "C": "The original study benefited from a higher alpha level that the follow-up study corrected by applying a more stringent threshold",
        "D": "The follow-up study suffered from a restriction of range on the independent variable that attenuated the observed correlation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Internal validity refers to the degree to which a study supports causal inference, typically threatened by confounds, not by participant heterogeneity per se. While heterogeneous sampling can introduce confounds, this option mislabels the statistical mechanism at play — the issue is reduced power, not a validity threat to causal inference.",
        "B": "The original study recruited from a homogeneous population (same employer, similar baseline stress), which reduced within-group variance on the dependent variable and increased statistical power. The follow-up study's heterogeneous recruitment substantially increased error variance, undermining the ability to detect a true effect even with a larger sample. This is the correct answer.",
        "C": "The scenario gives no indication that alpha levels differed between studies. Alpha level adjustments are a plausible power-related consideration but are not supported by the information provided, making this a tempting but ultimately unsupported explanation.",
        "D": "Restriction of range on the independent variable would attenuate the observed effect by limiting variability in the predictor, but the scenario describes broader sampling on the independent variable in the follow-up study — the opposite of restriction of range. The problem described is excess variance on the dependent variable outcome, not range restriction on the predictor."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-36-vignette-L5",
      "source_question_id": "36",
      "source_summary": "Statistical power is increased by greater sample size, larger effect size, and higher alpha level, but not by population heterogeneity on the dependent variable, as greater population homogeneity is associated with greater statistical power.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators runs two sequential studies examining the same intervention. In the first study, they recruit 50 volunteers from a single neighborhood health clinic whose patients share similar demographics, income levels, and health histories. Despite the modest size of this group, the first study yields a clear positive result. In the second study, the team, encouraged by the success and aiming to demonstrate broad applicability, deliberately draws participants from across five regions with widely different backgrounds and health profiles, ultimately enrolling 130 participants. To their frustration, the second study fails to produce a meaningful result. A senior colleague points out that the team made a fundamental error in assuming that a larger and more diverse group would only strengthen what they could demonstrate.",
      "question": "What principle most directly explains why the larger, more diverse second study produced a weaker result than the smaller first study?",
      "options": {
        "A": "The second study's diverse sample undermined the consistency of the intervention delivery, introducing procedural confounds that inflated error",
        "B": "The broader recruitment in the second study reduced the degree to which the sample resembled the original population, threatening the generalizability of the statistical conclusion",
        "C": "Increased variability in outcomes across the more diverse second sample raised within-group error, reducing the study's ability to detect a true effect",
        "D": "The second study's larger sample size increased the probability of including outliers that disproportionately skewed the outcome distribution"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Procedural confounds from inconsistent delivery are a plausible concern in multi-site or diverse studies, and this option is designed to appeal to those familiar with implementation fidelity issues. However, the scenario makes no mention of delivery inconsistency — the explicit factor described is participant heterogeneity, not procedural variability. This is incorrect.",
        "B": "This option invokes a real concept — that a sample dissimilar from the original population may not replicate findings — but describes an external validity or generalizability concern rather than a statistical power issue. The scenario is asking about why the study failed to detect an effect, which is a power problem, not a problem with what population the findings apply to.",
        "C": "The homogeneous first sample reduced within-group variability on the outcome, producing a cleaner signal relative to noise and enabling detection of the true effect with only 50 participants. The second study's deliberately diverse recruitment increased outcome variability within groups, raising the error term and reducing the ability to distinguish the treatment signal from background noise — a direct consequence of population heterogeneity on the dependent variable reducing statistical power. This is the correct answer.",
        "D": "While outliers can distort distributions, their influence is not systematic in the way described, and larger samples are generally more robust to individual outliers, not more susceptible. This option appeals to a misunderstanding of how sample size interacts with outlier sensitivity, but the described mechanism — diverse backgrounds generating variable outcomes — is fundamentally a within-group variance problem, not an outlier problem."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-220-vignette-L1",
      "source_question_id": "220",
      "source_summary": "The reliable change index (RCI) is useful for determining if a change in a client's scores on an outcome measure administered before and after the client receives treatment is attributable to measurement error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "reliable change index",
        "measurement error",
        "pre- and post-treatment"
      ],
      "vignette": "A researcher administers a depression inventory to a client before and after a 12-week cognitive-behavioral intervention. The client's score drops by 8 points, and the researcher wants to know whether this change exceeds what could be attributed to measurement error rather than genuine improvement. The researcher consults the instrument's test-retest reliability coefficient and the standard deviation of the normative sample to evaluate the score change. Using a formula that incorporates these psychometric properties, the researcher computes a ratio and compares it to a critical value of 1.96 to determine if the pre- and post-treatment change is statistically meaningful. The researcher concludes that the client's improvement is unlikely to be a product of measurement error alone.",
      "question": "Which statistical method is the researcher using to evaluate whether the client's score change reflects genuine improvement rather than measurement error?",
      "options": {
        "A": "Effect size (Cohen's d)",
        "B": "Reliable change index (RCI)",
        "C": "Standard error of measurement (SEM)",
        "D": "Minimal detectable change (MDC)"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Cohen's d is an effect size statistic that quantifies the magnitude of group differences or pre-post changes in standard deviation units at the group level. It does not use a critical value of 1.96 to evaluate whether an individual client's change exceeds measurement error, which is the defining feature described here.",
        "B": "The reliable change index (RCI) is correct. It divides an observed pre-post score difference by the standard error of the difference (derived from the instrument's reliability and SD), producing a ratio compared to ±1.96. This is precisely the method described for determining whether an individual's change exceeds what measurement error alone would predict.",
        "C": "The standard error of measurement (SEM) estimates the expected spread of scores around a true score due to measurement error and is an input used in calculating the RCI. However, the SEM alone is not a method for evaluating whether a pre-post change is statistically meaningful; it must be incorporated into the RCI formula to serve that function.",
        "D": "Minimal detectable change (MDC) is a related concept that also uses the SEM to define a threshold of change beyond measurement error, but it is typically applied in rehabilitation research and expresses a raw-score threshold rather than a standardized ratio compared to 1.96. The vignette's use of a ratio and z-score critical value specifically describes the RCI procedure."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-220-vignette-L2",
      "source_question_id": "220",
      "source_summary": "The reliable change index (RCI) is useful for determining if a change in a client's scores on an outcome measure administered before and after the client receives treatment is attributable to measurement error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "measurement error",
        "pre-post"
      ],
      "vignette": "A psychologist is evaluating a 45-year-old client with generalized anxiety disorder who completed a validated anxiety questionnaire at intake and again after 16 sessions of therapy. The client's score improved by 12 points, and the psychologist wants to determine whether this pre-post change is large enough to be considered a true change rather than a fluctuation due to measurement error in the instrument. The psychologist uses the questionnaire's published test-retest reliability coefficient and normative standard deviation to compute a ratio of the observed change to the expected variability in scores under conditions of no true change. This ratio is then compared to a z-score threshold to render a judgment about whether the change is statistically reliable at the individual level.",
      "question": "Which analytical approach is the psychologist applying to evaluate the client's score change?",
      "options": {
        "A": "Clinically significant change methodology",
        "B": "Standard error of measurement comparison",
        "C": "Reliable change index",
        "D": "Regression to the mean correction"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Clinically significant change methodology, as developed by Jacobson and Truax, involves determining whether a client's post-treatment score crosses a normative cutoff to indicate functional recovery — it is conceptually related to the RCI but addresses a different question (recovery vs. meaningful change). The vignette describes computing a ratio compared to a z-score threshold, which is the RCI component of the Jacobson-Truax framework, not the clinical significance cutoff.",
        "B": "The standard error of measurement (SEM) is a psychometric index that quantifies the average error in a single score, and it is an input used in the RCI formula. However, comparing a score change to the SEM alone does not constitute the full procedure described, which involves constructing a ratio and comparing it to a critical z-value.",
        "C": "The reliable change index (RCI) is correct. The procedure described — dividing the observed pre-post change by the standard error of the difference (derived from reliability and SD) and comparing the result to a z-score threshold such as 1.96 — is the defining formula and application of the RCI for individual-level change evaluation.",
        "D": "Regression to the mean refers to the statistical tendency for extreme scores to move closer to the average on retesting, independent of any treatment effect. While it is a concern in pre-post designs, it is not an analytical method involving the computation of a ratio from reliability data, and no correction procedure for it matches the description in the vignette."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-220-vignette-L3",
      "source_question_id": "220",
      "source_summary": "The reliable change index (RCI) is useful for determining if a change in a client's scores on an outcome measure administered before and after the client receives treatment is attributable to measurement error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "reliability"
      ],
      "vignette": "A clinical researcher is conducting a single-case study of a client receiving exposure therapy for specific phobia. The client completed a fear survey before and after treatment, scoring 72 at baseline and 54 at post-treatment. The researcher notes that the survey has a test-retest reliability of .80 and a normative standard deviation of 10, and argues that even though the 18-point drop looks impressive, the instrument's imperfect reliability means some of that change could simply reflect noise in the measurement process. To address this concern, the researcher computes a standardized index using the observed change and the instrument's psychometric properties to determine whether the drop exceeds what would be expected by chance alone at the individual level. Notably, the researcher is not comparing the client to a normative cutoff or to a treatment group, but is instead asking whether the change itself is trustworthy given the instrument's psychometric limitations.",
      "question": "Which procedure is the researcher using to evaluate the trustworthiness of the observed score change?",
      "options": {
        "A": "Clinically significant change index",
        "B": "Reliable change index",
        "C": "Standardized response mean",
        "D": "Confidence interval around the pre-treatment score"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The clinically significant change index assesses whether a client's post-treatment score has moved into a functional normative range, addressing the question of recovery rather than the trustworthiness of the change itself. The vignette explicitly states the researcher is not comparing to a normative cutoff, which rules out the clinically significant change approach.",
        "B": "The reliable change index (RCI) is correct. It directly addresses whether an observed pre-post change exceeds the variability expected from measurement error given the instrument's reliability and SD. The vignette's emphasis on individual-level analysis, psychometric properties (reliability = .80, SD = 10), and ruling out noise as an explanation precisely describes the RCI application.",
        "C": "The standardized response mean (SRM) is a responsiveness statistic used at the group level to evaluate how sensitive an instrument is to change across a sample, computed as the mean change divided by the SD of change scores. It is not designed for individual-level evaluation of whether a single client's change exceeds measurement error.",
        "D": "Constructing a confidence interval around the pre-treatment score uses the standard error of measurement to define a band of plausible true scores and is sometimes used to evaluate change. However, this approach assesses overlap between pre- and post-treatment score confidence intervals and is a distinct procedure from computing a single standardized ratio — which is the defining feature of the RCI described here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-220-vignette-L4",
      "source_question_id": "220",
      "source_summary": "The reliable change index (RCI) is useful for determining if a change in a client's scores on an outcome measure administered before and after the client receives treatment is attributable to measurement error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "test-retest"
      ],
      "vignette": "A researcher studying outcomes of a mindfulness-based intervention administers a well-validated self-report inventory of rumination to participants before and after the 8-week program. One participant's score decreases from 65 to 50, and the researcher wants to make a confident claim about this individual's improvement before considering them a treatment responder. The researcher notes that the inventory's test-retest coefficient, obtained from a separate normative sample over a two-week interval, is .78, and the normative standard deviation is 12. After performing a computation that accounts for both the magnitude of the observed change and the imprecision inherent in repeated administrations of the instrument, the researcher concludes that the 15-point drop is unlikely to be a product of chance fluctuation in scores. Importantly, the researcher makes no reference to whether the participant's final score falls within a healthy normative range.",
      "question": "Which statistical procedure best describes what the researcher performed to evaluate this individual participant's score change?",
      "options": {
        "A": "Minimal detectable change calculation",
        "B": "Jacobson-Truax clinically significant change determination",
        "C": "Effect size estimation using Cohen's d",
        "D": "Reliable change index computation"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Minimal detectable change (MDC) is a threshold derived from the standard error of measurement, often used in rehabilitation contexts, that defines the smallest change score exceeding measurement error with 90% or 95% confidence. While MDC and RCI share conceptual overlap and both use reliability data, MDC yields a fixed raw-score threshold applied across individuals, whereas the procedure described involves computing a standardized ratio for a specific individual's change — which is the RCI.",
        "B": "The Jacobson-Truax framework encompasses two components: the RCI (which is described here) and a clinically significant change criterion (which evaluates whether the post-treatment score crosses into a normative functional range). The vignette explicitly states no judgment was made about whether the final score falls in a healthy range, ruling out the clinically significant change component and identifying only the RCI portion of the framework.",
        "C": "Cohen's d estimates effect size by expressing mean change in standard deviation units, applied at the group level to characterize the magnitude of treatment effects. It does not use the test-retest reliability coefficient as an input, does not produce a ratio compared to a critical z-value, and is not designed for evaluating whether a single individual's change is attributable to measurement error.",
        "D": "The reliable change index (RCI) is correct. The vignette describes using the test-retest reliability coefficient and normative SD to compute a ratio that accounts for the imprecision of repeated measurement and evaluates whether an individual's observed change exceeds expected chance fluctuation. The explicit exclusion of normative cutoff comparison further confirms this is the RCI rather than the full Jacobson-Truax clinically significant change procedure."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-220-vignette-L5",
      "source_question_id": "220",
      "source_summary": "The reliable change index (RCI) is useful for determining if a change in a client's scores on an outcome measure administered before and after the client receives treatment is attributable to measurement error.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A practitioner administers the same paper-and-pencil questionnaire to a client at two time points separated by three months of weekly sessions. The client's total score dropped by 14 points, which the practitioner considers an encouraging sign. Before reporting this as a meaningful outcome in a case summary, the practitioner retrieves a published technical manual that contains data from a large group of individuals who completed the questionnaire twice within two weeks without any intervening treatment. Using the average spread of scores in that sample and a number describing how consistently the questionnaire produces similar results over time, the practitioner performs a brief calculation and obtains a single number that is then compared to 1.96. The practitioner is specifically trying to rule out the possibility that the 14-point drop simply reflects the kind of up-and-down variation in scores that would occur even if nothing in the client's life had changed.",
      "question": "Which procedure did the practitioner perform?",
      "options": {
        "A": "Computation of a confidence interval around the observed change score",
        "B": "Application of a normative comparison to determine clinical recovery",
        "C": "Reliable change index computation",
        "D": "Estimation of the standard error of measurement for a single score"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Computing a confidence interval around a change score is conceptually related and also uses the standard error of the difference to bracket the likely true change. However, this procedure yields a range of values rather than a single ratio compared to a fixed critical value of 1.96. The vignette specifies that the practitioner obtained one number and compared it to 1.96, which is the defining output format of the RCI, not a confidence interval.",
        "B": "Normative comparison for clinical recovery involves evaluating whether a post-treatment score falls within a healthy population range, which requires identifying a cutoff score from normative data. The vignette makes no mention of comparing the final score to a cutoff or evaluating the client's position relative to a normative population — the practitioner's concern is solely whether the observed change could be due to score fluctuation, which is the RCI question.",
        "C": "The reliable change index (RCI) is correct. The practitioner uses a normative group's standard deviation and the questionnaire's test-retest consistency to compute a single ratio of the observed change to expected measurement-based variability, then compares it to 1.96. Every element of the vignette — the two data sources from the technical manual, the single computed value, the 1.96 threshold, and the specific goal of ruling out chance fluctuation — maps precisely onto the RCI formula and its purpose.",
        "D": "The standard error of measurement (SEM) for a single score is computed from reliability data and the normative SD, and it quantifies the expected error around any one observed score. However, evaluating whether a change between two scores exceeds chance requires the standard error of the difference (which combines two SEMs), not the SEM for a single score. The SEM alone does not yield a ratio compared to 1.96 for evaluating pre-post change."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-184-vignette-L1",
      "source_question_id": "184",
      "source_summary": "Structural equation modeling (SEM) is used to test models of the relationships among observed and latent variables.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "structural equation modeling",
        "latent variables",
        "observed variables"
      ],
      "vignette": "A researcher is studying the relationship between depression, anxiety, and academic performance in college students. She proposes that an underlying latent variable she calls 'psychological distress' drives both depression and anxiety scores, and that this latent variable in turn predicts GPA as an observed variable. To test her hypothesized model of these relationships among latent variables and observed variables, she uses structural equation modeling. She reports fit indices including CFI, RMSEA, and SRMR to evaluate how well the model fits the data.",
      "question": "Which statistical technique is this researcher using to test her theoretical model?",
      "options": {
        "A": "Confirmatory factor analysis",
        "B": "Structural equation modeling",
        "C": "Hierarchical multiple regression",
        "D": "Path analysis"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Confirmatory factor analysis (CFA) is a component often embedded within SEM that tests whether observed variables load onto hypothesized latent factors. However, CFA alone does not model the directional structural paths between latent variables and outcomes — the defining feature described in this scenario.",
        "B": "Structural equation modeling (SEM) simultaneously models relationships among latent variables and observed variables, tests an overall proposed theoretical model, and provides global fit indices such as CFI and RMSEA. This matches all elements of the described scenario.",
        "C": "Hierarchical multiple regression predicts an observed outcome from multiple observed predictors entered in theoretically specified blocks. It does not incorporate latent variables or assess overall model fit with indices like CFI or RMSEA.",
        "D": "Path analysis tests directional relationships among observed (not latent) variables. While path analysis is structurally similar to SEM, it cannot model latent variables — a central feature explicitly described in this scenario."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-184-vignette-L2",
      "source_question_id": "184",
      "source_summary": "Structural equation modeling (SEM) is used to test models of the relationships among observed and latent variables.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "latent variables",
        "model fit"
      ],
      "vignette": "A research team investigating workplace burnout recruited 450 nurses across five hospitals. The team hypothesized that burnout — conceptualized as an unmeasured construct — is predicted by both workload and social support, and that burnout in turn predicts intent to leave the profession. Although the nurses varied substantially in age and years of experience, the researchers controlled for these demographics by including them as covariates in the analysis. The team evaluated their proposed model using several model fit indices and reported that the hypothesized structure of latent variables was well-supported by the data.",
      "question": "Which analytical approach best describes what this research team used?",
      "options": {
        "A": "Exploratory factor analysis",
        "B": "Hierarchical multiple regression",
        "C": "Path analysis",
        "D": "Structural equation modeling"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Exploratory factor analysis (EFA) is used to discover the underlying factor structure of a set of observed variables when no prior theory specifies the structure. It does not test directional relationships among constructs or produce overall model fit indices like those described here.",
        "B": "Hierarchical multiple regression can test the incremental predictive value of blocks of observed predictors on an observed outcome and can incorporate covariates, making it superficially appealing here. However, it does not model unmeasured latent constructs or evaluate overall structural model fit with indices such as CFI or RMSEA.",
        "C": "Path analysis tests directional relationships among variables and could handle a similar causal chain, but it works exclusively with observed (measured) variables. The scenario explicitly involves an unmeasured burnout construct, which rules out path analysis.",
        "D": "Structural equation modeling is correct because it combines a measurement model (linking observed indicators to latent variables like burnout) with a structural model (testing directional paths among constructs), and evaluates overall model fit — all features described in this scenario."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-184-vignette-L3",
      "source_question_id": "184",
      "source_summary": "Structural equation modeling (SEM) is used to test models of the relationships among observed and latent variables.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "measurement model"
      ],
      "vignette": "A psychologist is examining whether childhood adversity predicts adult relationship satisfaction through the mediating role of attachment security. Each key construct is assessed with multiple questionnaire items that serve as indicators, and a measurement model is first specified and tested to confirm that each set of indicators adequately reflects its underlying construct before the directional hypotheses are evaluated. The researcher notes that this two-step approach — first confirming the measurement structure, then testing structural paths — is a key advantage of the method. She also reports a chi-square difference test comparing her hypothesized mediation model to a direct-effects-only alternative, finding that the mediation model fits significantly better.",
      "question": "Which analytical method is the psychologist using?",
      "options": {
        "A": "Confirmatory factor analysis with bootstrapped mediation",
        "B": "Structural equation modeling",
        "C": "Path analysis with mediation testing",
        "D": "Latent profile analysis"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Confirmatory factor analysis (CFA) tests the measurement model (how indicators load onto factors) and can be combined with bootstrapping to test mediation among observed composites. However, CFA alone does not simultaneously model structural paths among latent constructs or compare nested models with a chi-square difference test in the way described — that integrated capability belongs to SEM.",
        "B": "Structural equation modeling is correct. It uniquely combines the two-step approach of first establishing a measurement model (confirming indicator-to-construct relationships) and then evaluating structural paths (including mediation), and supports chi-square difference tests comparing nested models — all features explicitly described in the scenario.",
        "C": "Path analysis can test mediation among observed variables and can perform chi-square difference tests when using maximum likelihood estimation. The critical distinction is that path analysis uses observed (not latent) variables, so it cannot employ the measurement model step described in the scenario.",
        "D": "Latent profile analysis (LPA) is a person-centered technique used to identify unobserved subgroups within a population based on response patterns. It does not test directional structural paths, mediation, or involve a measurement model in the sense described here."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-184-vignette-L4",
      "source_question_id": "184",
      "source_summary": "Structural equation modeling (SEM) is used to test models of the relationships among observed and latent variables.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "covariance"
      ],
      "vignette": "A team of organizational psychologists set out to understand how leadership quality shapes employee well-being. They specified a theoretical diagram in which leadership quality and well-being are each indexed by multiple survey items, and their analysis simultaneously estimated the degree to which each item reflected its intended construct and the directional influence of leadership quality on well-being. The researchers judged model adequacy by comparing the covariance matrix implied by their theoretical diagram to the covariance matrix actually observed in the sample, reporting indices that quantified the degree of correspondence. Notably, the method allowed them to correct for measurement error in each construct simultaneously, which they argued was a key advantage over simpler regression-based approaches.",
      "question": "Which analytical method does this description most precisely represent?",
      "options": {
        "A": "Multilevel modeling",
        "B": "Confirmatory factor analysis",
        "C": "Structural equation modeling",
        "D": "Partial least squares path modeling"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Multilevel modeling (MLM) accounts for nested data structures (e.g., employees within organizations) and can examine predictors at multiple levels. While it can handle some covariance structures, it does not simultaneously model measurement error across multiple indicators for each construct or compare implied to observed covariance matrices in the way described.",
        "B": "Confirmatory factor analysis (CFA) does compare implied to observed covariance matrices and does estimate how well items reflect their latent constructs — making it a very tempting answer. However, CFA only establishes the measurement model; it does not estimate directional structural paths between constructs, which is a key feature explicitly described in this scenario.",
        "C": "Structural equation modeling is correct. It uniquely integrates both a measurement model (estimating item-to-construct relationships while correcting for measurement error) and a structural model (estimating directional paths between constructs), and it evaluates fit by comparing the model-implied covariance matrix to the observed covariance matrix — precisely what is described.",
        "D": "Partial least squares path modeling (PLS-PM) is an alternative to SEM that also models latent constructs and structural paths, but it is variance-based rather than covariance-based and does not compare implied to observed covariance matrices. The scenario's explicit reference to covariance matrix comparison distinguishes SEM from PLS-PM."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-184-vignette-L5",
      "source_question_id": "184",
      "source_summary": "Structural equation modeling (SEM) is used to test models of the relationships among observed and latent variables.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Correlation and Regression",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team studying emotional regulation and health outcomes proposed that certain invisible personal qualities — qualities that cannot be directly recorded on any single instrument — drive patterns across several different questionnaire scores, and that these underlying qualities also causally influence each other in a sequence. The team built a diagram depicting these proposed influences and then used a computer program to generate the pattern of relationships among all the questionnaire scores that their diagram would logically predict. They compared this generated pattern to the pattern actually found in their collected data from 600 participants, making adjustments to the diagram when the two patterns diverged substantially. A key feature of their method was that it simultaneously accounted for the imprecision inherent in each questionnaire score while estimating the hypothesized causal chain.",
      "question": "Which analytical method does this research team's approach most precisely represent?",
      "options": {
        "A": "Confirmatory factor analysis",
        "B": "Path analysis",
        "C": "Structural equation modeling",
        "D": "Bayesian network analysis"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Confirmatory factor analysis (CFA) also involves invisible underlying qualities (latent factors), uses a proposed diagram, and compares a generated pattern of questionnaire relationships to observed data — making it a strong distractor. However, CFA only models how questionnaire scores reflect underlying qualities; it does not model causal sequences among those underlying qualities, which is explicitly described here.",
        "B": "Path analysis also uses a diagram of proposed causal sequences and compares predicted to observed relationships among variables. However, path analysis works with directly recorded scores, not invisible underlying qualities, and cannot simultaneously account for imprecision in each score. The combination of invisible underlying qualities and causal sequencing among them rules out standard path analysis.",
        "C": "Structural equation modeling is correct. It is the only technique that simultaneously: (1) models invisible underlying qualities (latent variables) inferred from multiple questionnaire scores, (2) estimates causal sequences among those underlying qualities, (3) compares the pattern of relationships the diagram would logically predict to the pattern actually observed, and (4) accounts for measurement imprecision in each indicator while doing so.",
        "D": "Bayesian network analysis also uses directed diagrams to represent causal or probabilistic relationships among variables and can compare predicted to observed data patterns. However, Bayesian networks do not typically incorporate the concept of invisible underlying qualities inferred from multiple imprecise indicators in the psychometric sense described, and the simultaneous correction for measurement imprecision across indicators is not a defining feature of Bayesian network analysis."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-16-vignette-L1",
      "source_question_id": "16",
      "source_summary": "A bar graph can be used to visually summarize the nominal data collected in a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "nominal data",
        "bar graph",
        "categories"
      ],
      "vignette": "A researcher conducts a study examining the preferred therapeutic orientation (psychodynamic, CBT, humanistic, or behavioral) among licensed psychologists in a metropolitan area. She collects nominal data by asking each participant to select one orientation from the list. To visually summarize the frequency of responses across the four categories, she wants to select the most appropriate graph. A colleague recommends using a bar graph because the variable has no inherent numerical order.",
      "question": "Which type of visual display is most appropriate for summarizing the nominal data collected in this study?",
      "options": {
        "A": "Histogram",
        "B": "Scatter plot",
        "C": "Bar graph",
        "D": "Line graph"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "A histogram is used to display the frequency distribution of continuous or interval/ratio data by grouping scores into adjacent bins. Because therapeutic orientation is a nominal variable with discrete, unordered categories rather than continuous numeric values, a histogram is inappropriate here.",
        "B": "A scatter plot is used to display the relationship between two continuous variables, plotting paired data points on an x- and y-axis. This study involves one categorical variable and frequency counts, not two continuous variables, so a scatter plot does not apply.",
        "C": "A bar graph is the correct choice for displaying nominal data. It represents discrete, unordered categories on the x-axis with separate, non-touching bars whose heights indicate frequency or proportion, making it ideal for summarizing categorical variables like therapeutic orientation.",
        "D": "A line graph is typically used to display trends over time or relationships between continuous variables, where connecting data points with a line is meaningful. Because therapeutic orientation categories have no inherent order or continuity, connecting them with a line would imply a false quantitative relationship."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-16-vignette-L2",
      "source_question_id": "16",
      "source_summary": "A bar graph can be used to visually summarize the nominal data collected in a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "categorical",
        "frequency"
      ],
      "vignette": "A psychology graduate student is conducting a survey study examining which coping strategy (problem-focused, emotion-focused, avoidance, or social support-seeking) adults most commonly use following a stressful life event. Participants range widely in age from 18 to 72, and the student notes that older adults in the sample appear to favor social support-seeking more than younger adults do. After collecting categorical responses from 200 participants, the student needs to select a visual display that accurately represents the frequency of each coping strategy across the sample as a whole.",
      "question": "Which visual display is most appropriate for summarizing the distribution of coping strategy choices in this study?",
      "options": {
        "A": "Bar graph",
        "B": "Line graph",
        "C": "Box plot",
        "D": "Histogram"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "A bar graph is correct because coping strategy is a categorical variable with discrete, unordered groups. The bar graph represents each category with a separate bar and frequency height, which accurately displays how often each strategy was chosen without implying any numerical order or continuous relationship.",
        "B": "A line graph is used to display trends across ordered or continuous data, such as changes over time or across ordered conditions. Connecting coping strategy categories with a line would falsely imply an ordered, continuous relationship among the categories, which does not exist for this nominal variable.",
        "C": "A box plot (box-and-whisker plot) summarizes the distribution of a continuous variable by displaying the median, quartiles, and outliers. Because coping strategy is a categorical, not continuous, variable, a box plot cannot meaningfully represent the frequency of category choices.",
        "D": "A histogram is designed for interval or ratio data grouped into ordered numeric bins, where adjacent bars touch to indicate continuous data. Coping strategy categories are not numeric or ordered, so a histogram would misrepresent the nature of this categorical variable."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-16-vignette-L3",
      "source_question_id": "16",
      "source_summary": "A bar graph can be used to visually summarize the nominal data collected in a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "unordered"
      ],
      "vignette": "A clinical researcher surveying 350 outpatients asks each person to indicate their primary diagnosis from a list: major depressive disorder, generalized anxiety disorder, PTSD, or adjustment disorder. The researcher carefully records the number of patients in each diagnostic group and notes that the sample skews heavily toward MDD, with roughly half of participants selecting that option. Because the researcher wants to present these findings at a conference, she consults a statistician about the best way to visually display the distribution. The statistician emphasizes that the variable is unordered and that the display should not imply any numeric relationship between the diagnostic groups.",
      "question": "Based on the statistician's guidance, which visual display should the researcher use to present the distribution of primary diagnoses?",
      "options": {
        "A": "Histogram",
        "B": "Line graph",
        "C": "Ogive (cumulative frequency curve)",
        "D": "Bar graph"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "A histogram is superficially appealing here because the data involve frequency counts across groups, which can look similar to a bar graph. However, histograms are reserved for continuous or interval/ratio data grouped into ordered numeric bins; using one for an unordered categorical variable like diagnostic group would misrepresent the data's nature.",
        "B": "A line graph implies a continuous, ordered relationship between the points connected by the line. Because the diagnostic categories have no inherent numeric order, connecting them with a line would create a false impression of a trend or progression across groups.",
        "C": "An ogive displays cumulative frequencies or proportions across ordered numeric values and is used with interval or ratio data to show how frequency accumulates. Diagnostic category is an unordered nominal variable, so cumulative display across categories is meaningless and misleading.",
        "D": "A bar graph is correct because it uses separated bars to represent discrete, unordered categories, with bar height indicating frequency. This display does not imply order or numeric continuity among the diagnostic groups, exactly meeting the statistician's requirement for a display appropriate for a nominal variable."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-16-vignette-L4",
      "source_question_id": "16",
      "source_summary": "A bar graph can be used to visually summarize the nominal data collected in a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "mutually exclusive"
      ],
      "vignette": "A research team studying help-seeking behavior asks 500 community members to identify which single type of mental health professional they consulted most recently: psychiatrist, psychologist, licensed counselor, or social worker. Each participant selects exactly one option, and the selections are mutually exclusive. The data are entered into a spreadsheet showing only the label for each participant's choice and a tally of how many participants chose each label. A senior methodologist reviewing the study advises that the visual summary chosen for the final report must respect the underlying measurement properties of the variable and avoid suggesting that any one professional type is numerically greater than or intermediate to another in any meaningful quantitative sense.",
      "question": "Given the methodologist's caution, which visual summary is most appropriate for the data collected in this study?",
      "options": {
        "A": "Frequency polygon",
        "B": "Bar graph",
        "C": "Stem-and-leaf plot",
        "D": "Histogram"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "A bar graph is correct because it displays discrete, unordered categories as separate bars, with no implied continuity or quantitative ordering between them. This directly respects the nominal level of measurement — the labels identify group membership only, with no numeric meaning attached to the categories themselves.",
        "A": "A frequency polygon connects plotted frequency points with lines across an ordered x-axis and is appropriate for continuous or ordinal data where the ordering of values is meaningful. Applying it to mutually exclusive, unordered professional-type categories would falsely imply a quantitative progression across the groups.",
        "C": "A stem-and-leaf plot is used to display the distribution of numeric data by preserving individual values, grouping them by a leading digit (stem) and displaying trailing digits (leaves). Because the variable here consists of category labels rather than numeric values, a stem-and-leaf plot cannot be constructed or interpreted.",
        "D": "A histogram displays continuous numeric data grouped into adjacent ordered bins and is suitable for interval or ratio variables. Although it visually resembles a bar graph, applying a histogram to these mutually exclusive categorical labels would incorrectly impose a numeric, continuous interpretation on data that have no such properties."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-16-vignette-L5",
      "source_question_id": "16",
      "source_summary": "A bar graph can be used to visually summarize the nominal data collected in a research study.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Types of Variables and Data",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of investigators asks a large group of adults to name a single color — red, blue, green, or yellow — that they associate most strongly with feeling calm. Each person picks one and only one color, and the investigators record only which color was named, not anything else about the person. After collecting all responses, the team tallies how many people named each color. One investigator suggests displaying the tallies in a way that connects the four colors with a continuous line to show apparent trends, but a senior team member objects, noting that this approach would misrepresent something fundamental about what the colors actually are in relation to one another. The senior member insists on a display that uses separate, unconnected visual elements — one for each color — whose heights correspond to the tallies.",
      "question": "Which type of visual display is the senior team member recommending?",
      "options": {
        "A": "Line graph",
        "B": "Histogram",
        "C": "Bar graph",
        "D": "Scatter plot"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "A bar graph is correct. The senior member's insistence on separate, unconnected elements — one per color, with height representing frequency — precisely describes a bar graph. This display is appropriate because the colors are discrete, unordered categories (a nominal variable); the gaps between bars signal that no continuous or quantitative relationship exists among the categories.",
        "A": "A line graph connects data points with a continuous line, which the senior member explicitly rejects. Using a line across the four color categories would falsely suggest a trend or ordered, quantitative relationship, which does not exist for an unordered categorical variable like color preference.",
        "B": "A histogram superficially resembles a bar graph and also uses rectangular bars, making it a strong distractor. However, histograms are designed for continuous numeric data grouped into ordered bins, and their adjacent touching bars imply continuity. The color categories here are discrete and unordered, making a histogram conceptually inappropriate despite the visual similarity.",
        "D": "A scatter plot displays the relationship between two continuous variables by plotting individual paired data points on two numeric axes. This study involves only one categorical variable and frequency tallies, with no paired continuous measurements, so a scatter plot cannot represent these data."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-163-vignette-L1",
      "source_question_id": "163",
      "source_summary": "Community members who take part in community-based participatory research (CBPR) act as equal research partners who participate in all phases of the research.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "community-based participatory research",
        "equal partners",
        "all phases"
      ],
      "vignette": "A university research team is studying diabetes prevention in a low-income urban neighborhood. Rather than designing the study independently, the researchers invite neighborhood residents, local health workers, and community organization leaders to serve as equal partners throughout the entire project. These community members help formulate the research questions, select appropriate measures, collect and interpret data, and disseminate findings back to the community. The team explicitly frames this as community-based participatory research, ensuring no phase of the study is conducted without meaningful community input.",
      "question": "Which research design best characterizes the approach described in this scenario?",
      "options": {
        "A": "Community-based participatory research (CBPR)",
        "B": "Participatory action research with a consultant model",
        "C": "Mixed-methods community needs assessment",
        "D": "Field research with community advisory boards"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. CBPR is defined by community members serving as genuine equal partners across all phases of the research process — from question formulation through dissemination — which precisely matches the scenario described.",
        "B": "Participatory action research (PAR) shares collaborative values with CBPR, but a consultant model positions community members as advisors rather than equal partners, which contradicts the scenario's explicit framing of full co-ownership of all phases.",
        "C": "A community needs assessment is a descriptive tool used to identify gaps in services or health outcomes; it does not inherently establish community members as equal research partners across all phases of an ongoing study.",
        "D": "Community advisory boards are a common mechanism for community input in research, but they typically serve in an advisory rather than partnership capacity, meaning researchers retain primary decision-making authority — unlike CBPR's equal-partner model."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-163-vignette-L2",
      "source_question_id": "163",
      "source_summary": "Community members who take part in community-based participatory research (CBPR) act as equal research partners who participate in all phases of the research.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "participatory",
        "equal partners"
      ],
      "vignette": "A team of public health psychologists wants to understand barriers to mental health service use among Somali refugee families resettled in a Midwestern city. Because the community has historically distrusted outside researchers — a factor the team acknowledges as a potential threat to validity — they recruit Somali community health workers and elders to co-design the study instruments, co-conduct interviews, and jointly analyze and interpret the findings. At each stage of the project, final decisions are made jointly between the academic researchers and the community representatives, who are compensated as co-investigators. The team describes their goal as building long-term research capacity within the community itself.",
      "question": "The research approach described in this vignette is most consistent with which of the following methodologies?",
      "options": {
        "A": "Ethnographic field research with key informant interviews",
        "B": "Community-based participatory research (CBPR)",
        "C": "Culturally adapted randomized controlled trial",
        "D": "Stakeholder-engaged dissemination and implementation research"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The defining features of CBPR are present: community members serve as equal partners (co-investigators) across all phases — design, data collection, analysis, and dissemination — with an explicit goal of building community capacity, exactly as described.",
        "A": "Ethnographic field research uses immersive observation and key informant interviews to understand cultural phenomena, but the researcher typically retains primary control of the study design and analysis; community members are informants, not equal partners co-owning every phase.",
        "C": "A culturally adapted RCT involves modifying an intervention for cultural fit but follows a standard experimental design controlled by the research team; community members may provide input but do not function as equal co-investigators across all phases.",
        "D": "Dissemination and implementation (D&I) research focuses on how evidence-based practices are adopted into real-world settings and does engage stakeholders, but stakeholder engagement in D&I does not require equal co-investigator status across all research phases as CBPR demands."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-163-vignette-L3",
      "source_question_id": "163",
      "source_summary": "Community members who take part in community-based participatory research (CBPR) act as equal research partners who participate in all phases of the research.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "co-investigators"
      ],
      "vignette": "A psychology research team studying opioid overdose prevention in a rural Appalachian county holds a series of meetings with local recovery coaches, family members of overdose survivors, and county health officials before the study begins. These individuals are listed as co-investigators on the grant, attend all team meetings, and have voting authority on decisions such as which outcomes to prioritize and how findings should be shared locally. The team notes that previous university-led interventions in the region had poor uptake, which they attributed to a lack of community ownership. Although the academic researchers provide technical expertise in statistics and measure development, no data are collected or reported without the co-investigators' agreement, and the long-term goal includes training local members to lead future studies independently.",
      "question": "Which research methodology most accurately describes the approach being used?",
      "options": {
        "A": "Participatory action research (PAR)",
        "B": "Community-based participatory research (CBPR)",
        "C": "Implementation science with community engagement",
        "D": "Mixed-methods research with embedded community consultation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. CBPR requires community members to function as genuine equal partners — with decision-making authority — across every phase of research, and includes an explicit goal of building community research capacity. All of these features are present in the vignette.",
        "A": "PAR is closely related to CBPR and also emphasizes community collaboration and action, but PAR typically centers on a cycle of action and reflection aimed at social change and may not require formal co-investigator status or capacity-building goals as explicitly as CBPR does; the formal equal-partner structure with voting authority and capacity-building mandate is the distinguishing hallmark of CBPR.",
        "C": "Implementation science with community engagement studies how evidence-based interventions are adopted in practice and often involves community stakeholders, but the engagement is typically advisory rather than constituting full equal partnership with voting authority over all research decisions.",
        "D": "Mixed-methods research with embedded community consultation combines quantitative and qualitative methods and may include community input, but 'consultation' implies a subordinate advisory role for community members rather than the equal co-investigator partnership described in the vignette."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-163-vignette-L4",
      "source_question_id": "163",
      "source_summary": "Community members who take part in community-based participatory research (CBPR) act as equal research partners who participate in all phases of the research.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "capacity-building"
      ],
      "vignette": "A federally funded study on HIV risk reduction among Black gay and bisexual men in a Southern city is structured so that the principal investigators hold academic appointments, yet the grant narrative states that 'the community sets the agenda.' Neighborhood residents affected by HIV, local barbers who serve as cultural liaisons, and representatives from a Black LGBTQ advocacy organization attend every project meeting and have formally ratified each major protocol revision. Midway through the project, the academic team wanted to add a biological outcome measure; the community representatives voted against it, and the measure was dropped. The grant also funds a two-year training program so that community members can eventually lead their own IRB-approved studies. On first read, the heavy involvement of established advocacy organizations might suggest this is a standard stakeholder-engaged trial, but a closer look reveals something more foundational about power sharing.",
      "question": "The research model underlying this study is best described as which of the following?",
      "options": {
        "A": "Stakeholder-engaged effectiveness trial",
        "B": "Participatory action research (PAR) with an advocacy orientation",
        "C": "Community-based participatory research (CBPR)",
        "D": "Patient-centered outcomes research (PCOR)"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The scenario contains the defining features of CBPR: community members have genuine decision-making authority equal to that of academic researchers (they voted down a protocol addition), they are involved across all phases, and the project includes an explicit capacity-building component to train community members to lead future research independently.",
        "A": "Stakeholder-engaged effectiveness trials involve community stakeholders and advocacy groups as advisors or participants but do not grant them co-equal decision-making authority over all protocol decisions; the community's ability to override an academic researcher's proposed measure change is inconsistent with this model.",
        "B": "PAR shares many values with CBPR — including action orientation and community collaboration — and the presence of an advocacy organization may suggest PAR. However, PAR does not uniformly require formal equal-partner status across all research phases or a structured capacity-building training program as described; these features are more specifically characteristic of CBPR.",
        "D": "Patient-centered outcomes research prioritizes outcomes important to patients and may involve patients in setting research priorities, but it is not structured around community members serving as equal co-investigators with veto power over protocol decisions or around building community-led research capacity."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-163-vignette-L5",
      "source_question_id": "163",
      "source_summary": "Community members who take part in community-based participatory research (CBPR) act as equal research partners who participate in all phases of the research.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A group of outside experts working on a health-related project in a small rural town noticed that their previous attempts to introduce programs had repeatedly failed because local people neither trusted them nor felt connected to the outcomes. This time, they invited a range of local residents — including a retired schoolteacher, a pastor, two mothers whose children had been affected by the health issue, and a local grocery store owner — to join their team from the very beginning. These residents did not merely review the experts' plans; they helped decide what questions mattered most, shaped how information would be gathered, reviewed and debated the meaning of results, and chose how to share conclusions with their neighbors. When one expert proposed a data collection strategy the residents felt was invasive, the residents' objection was sufficient to remove it. A portion of the project's funding was also set aside so the residents could eventually run similar projects without the outside experts present.",
      "question": "The approach described in this scenario most closely exemplifies which of the following research methodologies?",
      "options": {
        "A": "Participatory action research (PAR)",
        "B": "Community-based participatory research (CBPR)",
        "C": "Qualitative field research with purposive sampling",
        "D": "Stakeholder-engaged implementation research"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. CBPR is distinguished from closely related approaches by three features that are all present here: community members function as genuine equal partners with decision-making authority across every phase of the research (not just some phases), the partnership explicitly addresses historical mistrust, and there is a formal mechanism for building community capacity to conduct future research independently — the reserved funding for self-led projects.",
        "A": "PAR is the strongest distractor because it also involves community collaboration, action orientation, and challenging existing power structures. However, PAR does not uniformly require that community members hold equal formal authority over every research decision or that independent capacity-building funding be structured into the project; the veto power over data collection strategy and the independence-building fund are more specifically CBPR features.",
        "C": "Qualitative field research with purposive sampling involves selecting community members as informants based on specific characteristics and gathering rich descriptive data, but community members in this model are sources of data rather than co-equal decision-makers who can override expert judgments and who receive funding to run their own future research.",
        "D": "Stakeholder-engaged implementation research does involve community partners and emphasizes real-world relevance, and the historical failure of prior programs (suggesting an implementation problem) makes this option plausible. However, stakeholder engagement in implementation research is typically advisory or consultative, not constitutive of equal co-ownership with veto power over design decisions and structured capacity transfer to community members."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-201-vignette-L1",
      "source_question_id": "201",
      "source_summary": "Bayes' theorem combines the prior probability distribution for the target parameter and the probability distribution for the parameter derived from current data (the likelihood function) to obtain a posterior (updated) probability distribution for the parameter.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "prior probability",
        "likelihood function",
        "posterior distribution"
      ],
      "vignette": "A clinical researcher is studying the prevalence of a rare anxiety disorder and begins with a prior probability distribution based on existing epidemiological data. After collecting new data from a clinical sample, she computes a likelihood function representing the probability of observing those data given different parameter values. She then mathematically combines the prior probability distribution with the likelihood function to produce a posterior distribution that reflects her updated beliefs about the disorder's prevalence. The researcher notes that her posterior distribution shifts meaningfully from the prior because the new data were highly informative.",
      "question": "Which statistical framework is the researcher using to update her beliefs about the prevalence parameter?",
      "options": {
        "A": "Null hypothesis significance testing (NHST)",
        "B": "Bayesian inference",
        "C": "Maximum likelihood estimation (MLE)",
        "D": "Meta-analytic synthesis"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "NHST involves testing a null hypothesis and computing a p-value to decide whether to reject the null; it does not formally incorporate a prior probability distribution or produce a posterior distribution.",
        "B": "Bayesian inference is the framework that uses Bayes' theorem to combine a prior probability distribution with a likelihood function derived from new data to produce an updated posterior distribution — precisely what the researcher is doing.",
        "C": "Maximum likelihood estimation identifies the parameter value that maximizes the probability of the observed data (the likelihood function) but does not incorporate a prior distribution or produce a posterior distribution; it is a frequentist technique.",
        "D": "Meta-analytic synthesis statistically aggregates effect sizes or findings across multiple studies, but does not formally combine a prior probability distribution with a likelihood function to produce a posterior distribution in the Bayesian sense."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-201-vignette-L2",
      "source_question_id": "201",
      "source_summary": "Bayes' theorem combines the prior probability distribution for the target parameter and the probability distribution for the parameter derived from current data (the likelihood function) to obtain a posterior (updated) probability distribution for the parameter.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "prior beliefs",
        "posterior"
      ],
      "vignette": "A neuropsychologist is evaluating the probability that a 70-year-old patient's cognitive decline is due to Alzheimer's disease. She starts by quantifying her prior beliefs about the likelihood of Alzheimer's in this demographic based on published base rates, noting that the patient also carries a diagnosis of type 2 diabetes, which she acknowledges is a complicating factor. After administering a battery of cognitive tests, she uses the resulting data to update her prior beliefs and arrives at a posterior estimate of the probability that Alzheimer's is the correct diagnosis. She finds the posterior probability is substantially higher than her initial estimate.",
      "question": "The neuropsychologist's process of moving from an initial probability estimate to an updated estimate after incorporating new data is best described by which statistical approach?",
      "options": {
        "A": "Frequentist hypothesis testing",
        "B": "Structural equation modeling",
        "C": "Bayesian inference",
        "D": "Receiver operating characteristic (ROC) analysis"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Frequentist hypothesis testing evaluates the probability of observing data as extreme as those obtained under a null hypothesis; it does not formally encode or update prior beliefs about parameter probabilities in the way described.",
        "B": "Structural equation modeling tests relationships among latent and observed variables within a proposed causal structure; it does not involve combining prior beliefs with new data to produce a posterior probability estimate.",
        "C": "Bayesian inference is the correct answer because it is the framework in which prior beliefs (encoded as a prior distribution) are combined with data (via the likelihood function) using Bayes' theorem to produce a posterior probability — exactly the process described.",
        "D": "ROC analysis evaluates the diagnostic accuracy of a test by examining the trade-off between sensitivity and specificity across thresholds; while relevant to clinical diagnosis, it does not involve formally updating a prior probability with new data to obtain a posterior."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-201-vignette-L3",
      "source_question_id": "201",
      "source_summary": "Bayes' theorem combines the prior probability distribution for the target parameter and the probability distribution for the parameter derived from current data (the likelihood function) to obtain a posterior (updated) probability distribution for the parameter.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "updated"
      ],
      "vignette": "A research team studying treatment response in major depressive disorder has conducted three previous small trials and summarizes the accumulated evidence as a probability distribution over likely effect sizes. When a new, larger randomized controlled trial is completed, the team incorporates the new trial's data into their existing probability distribution to obtain an updated probability distribution, which they then use to make predictions about treatment response in future patients. A colleague suggests the team is simply pooling studies the same way that meta-analysis does, but the lead statistician disagrees. The lead statistician emphasizes that the distinguishing feature is the formal probabilistic integration of previous evidence with new data at the level of the parameter distribution rather than aggregating point estimates.",
      "question": "The lead statistician's description of the team's method most closely reflects which statistical approach?",
      "options": {
        "A": "Fixed-effects meta-analysis",
        "B": "Bayesian inference",
        "C": "Sequential analysis",
        "D": "Hierarchical linear modeling"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Fixed-effects meta-analysis pools effect size estimates across studies under the assumption of a single true population effect, weighting studies by their precision; it aggregates point estimates and standard errors rather than formally combining prior probability distributions with likelihood functions at the parameter level.",
        "B": "Bayesian inference is correct because the team is formally encoding previous evidence as a prior probability distribution over effect sizes and using Bayes' theorem to combine it with the likelihood from the new trial, yielding a posterior distribution — the defining feature the statistician describes.",
        "C": "Sequential analysis involves monitoring accumulating data and making decisions about stopping or continuing a study at predetermined interim looks, often using adjusted significance thresholds; it does not involve combining a probability distribution over parameters with new data in the Bayesian sense.",
        "D": "Hierarchical linear modeling (multilevel modeling) accounts for the nested structure of data (e.g., patients within studies) by partitioning variance across levels; although it can be implemented in a Bayesian framework, the distinguishing feature described — formal integration of prior and likelihood to produce a posterior — is specifically Bayesian inference."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-201-vignette-L4",
      "source_question_id": "201",
      "source_summary": "Bayes' theorem combines the prior probability distribution for the target parameter and the probability distribution for the parameter derived from current data (the likelihood function) to obtain a posterior (updated) probability distribution for the parameter.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "parameter"
      ],
      "vignette": "A statistician consulting on a psychiatric genetics study is asked to characterize uncertainty about a heritability parameter for schizophrenia. She notes that the research team had already reviewed 15 twin studies before the current genome-wide study began, and she encodes that accumulated knowledge as a probability distribution. After the genome-wide study yields new data, she multiplies that encoded knowledge by a function representing how probable the observed genetic data would be under each possible value of the heritability parameter, then normalizes the result. Her collaborators initially assume she is computing a confidence interval, but she corrects them: unlike a confidence interval, her method explicitly treats the parameter itself as having a probability distribution rather than being a fixed unknown quantity. The resulting output is a single probability distribution that the team will use to guide future research decisions.",
      "question": "The statistician's approach to characterizing uncertainty about the heritability parameter is best described as which of the following?",
      "options": {
        "A": "Bootstrap resampling",
        "B": "Maximum likelihood estimation",
        "C": "Bayesian inference",
        "D": "Credible interval construction via frequentist methods"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Bootstrap resampling generates empirical sampling distributions of a statistic by repeatedly resampling with replacement from the observed data; it does not involve encoding prior knowledge as a probability distribution or using Bayes' theorem to combine it with a likelihood function.",
        "B": "Maximum likelihood estimation finds the parameter value that maximizes the likelihood of the observed data and can produce point estimates and standard errors, but it treats the parameter as a fixed unknown and does not incorporate a prior probability distribution or yield a posterior distribution over the parameter.",
        "C": "Bayesian inference is correct. The statistician encodes prior knowledge as a probability distribution over the heritability parameter, multiplies it by the likelihood function (the probability of observed data under each parameter value), normalizes to obtain a posterior distribution, and explicitly treats the parameter as having its own probability distribution — all defining features of Bayesian inference via Bayes' theorem.",
        "D": "Credible intervals are specifically a Bayesian concept, not a frequentist one; frequentist methods produce confidence intervals, which describe properties of the estimation procedure over repeated sampling rather than the probability distribution of the parameter itself. The phrase 'credible interval construction via frequentist methods' is self-contradictory, and the broader approach described in the vignette is Bayesian inference, not just interval construction."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-201-vignette-L5",
      "source_question_id": "201",
      "source_summary": "Bayes' theorem combines the prior probability distribution for the target parameter and the probability distribution for the parameter derived from current data (the likelihood function) to obtain a posterior (updated) probability distribution for the parameter.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Overview of Inferential Statistics",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A scientist studying how quickly a particular psychological tendency stabilizes over time begins by summarizing everything known from previous work as a mathematical curve that assigns different degrees of plausibility to different possible values of a single unknown quantity she is trying to measure. She then conducts a new study, and for each possible value of the unknown quantity, she calculates how probable it would have been to obtain exactly the results she observed. She combines these two mathematical objects — the summary of previous knowledge and the study-specific calculation — by multiplying them together and then rescaling the result so that all the plausibilities add up to one. Her colleagues initially assume she is computing what researchers typically report at the end of a study to summarize the most likely value and its margin of error, but she explains that her output is fundamentally different: it is a full curve of plausibilities across all possible values of the unknown quantity, not a single point or an interval around it.",
      "question": "The scientist's procedure for characterizing the unknown quantity is best described by which of the following statistical frameworks?",
      "options": {
        "A": "Empirical curve fitting using nonlinear regression",
        "B": "Bayesian inference",
        "C": "Meta-analytic pooling of effect size distributions",
        "D": "Likelihood ratio testing"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Nonlinear regression fits a specified mathematical curve to observed data by minimizing residuals; while it involves working with curves and can estimate unknown parameters, it does not involve encoding pre-existing knowledge as a plausibility distribution and multiplying it by a study-specific probability calculation to produce an updated plausibility curve.",
        "B": "Bayesian inference is correct. The scientist encodes previous knowledge as a prior probability distribution, computes the likelihood of her observed data at each possible parameter value, multiplies these together, and normalizes the product to obtain a posterior probability distribution — a full distribution rather than a point estimate or interval. This is exactly the process described, and the distinction she draws from a conventional summary statistic mirrors the Bayesian versus frequentist distinction.",
        "C": "Meta-analytic pooling combines effect size estimates across studies, often weighting by sample size or precision, to produce a summary estimate and confidence interval; although it uses previous studies and new data, it does not involve encoding prior knowledge as a probability distribution and multiplying by a likelihood function to obtain a posterior distribution.",
        "D": "Likelihood ratio testing compares the fit of two competing models or parameter values using the ratio of their likelihoods to make a decision about which is better supported; it uses the likelihood function but does not incorporate a prior distribution or produce a posterior distribution across all possible parameter values."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-118-vignette-L1",
      "source_question_id": "118",
      "source_summary": "When a one-way ANOVA produces a statistically significant F-ratio and the independent variable has three or more levels, a post-hoc test would be conducted to determine which group means are significantly different.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "ANOVA",
        "F-ratio",
        "post-hoc"
      ],
      "vignette": "A researcher conducts a study comparing anxiety scores across four different therapy conditions: CBT, DBT, psychodynamic therapy, and a waitlist control. She runs a one-way ANOVA and obtains a statistically significant F-ratio (p < .01). Because the independent variable has four levels, she knows the omnibus test cannot tell her which specific group means differ from one another. She plans to conduct a post-hoc test to identify the source of the significant effect.",
      "question": "What is the primary purpose of conducting a post-hoc test following a significant one-way ANOVA with four group levels?",
      "options": {
        "A": "To determine whether the assumption of homogeneity of variance was met before interpreting the F-ratio",
        "B": "To identify which specific pairs of group means are significantly different from one another",
        "C": "To recalculate the F-ratio using a more conservative alpha level",
        "D": "To convert the omnibus test result into an effect size estimate"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Testing homogeneity of variance (e.g., Levene's test) is an assumption check that occurs before or alongside the ANOVA, not after a significant result. It evaluates whether group variances are equal, not where mean differences lie.",
        "B": "This is correct. A significant F-ratio in a one-way ANOVA with three or more levels only indicates that at least one group mean differs; post-hoc tests (e.g., Tukey's HSD, Scheffé) are used to conduct pairwise comparisons and pinpoint which specific groups differ.",
        "C": "Adjusting the alpha level is associated with corrections for multiple comparisons (e.g., Bonferroni), but the purpose of post-hoc tests is not to recalculate the original F-ratio. Post-hoc tests perform new pairwise comparisons with appropriate error rate control.",
        "D": "Effect size estimates (e.g., eta-squared, omega-squared) quantify the magnitude of the overall relationship between the independent and dependent variables but do not identify which group pairs differ. They are typically calculated from the ANOVA output, not from post-hoc procedures."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-118-vignette-L2",
      "source_question_id": "118",
      "source_summary": "When a one-way ANOVA produces a statistically significant F-ratio and the independent variable has three or more levels, a post-hoc test would be conducted to determine which group means are significantly different.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "ANOVA",
        "pairwise"
      ],
      "vignette": "A clinical researcher examines whether depression severity differs among three medication groups: an SSRI, an SNRI, and a placebo. Participants in all three groups were matched on age and baseline symptom severity. After running the analysis, the overall test yields a statistically significant result (F(2, 87) = 6.43, p = .002), indicating that group membership is associated with depression outcomes. The researcher, noting that all three groups had roughly equal sample sizes, proceeds to examine pairwise comparisons among the groups.",
      "question": "Given the statistically significant omnibus result with three groups, which next step is most appropriate for the researcher?",
      "options": {
        "A": "Conduct a Pearson correlation to evaluate the linear relationship between medication type and depression scores",
        "B": "Run an independent samples t-test between each of the three possible group pairs without any correction",
        "C": "Perform a post-hoc test to determine which specific group means are significantly different while controlling the familywise error rate",
        "D": "Rerun the ANOVA using a within-subjects design to account for the matched groups"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Pearson correlation measures the strength and direction of a linear relationship between two continuous variables. Medication type is a categorical independent variable, so correlation is not appropriate here, and it would not identify where group mean differences lie.",
        "B": "Running multiple independent-samples t-tests without correction inflates the familywise Type I error rate. For three groups there are three pairwise comparisons, each at α = .05, making the cumulative error rate substantially higher than .05. Post-hoc tests are designed to handle this problem.",
        "C": "This is correct. Following a significant ANOVA with three or more levels, post-hoc tests (such as Tukey's HSD) are the appropriate next step. They perform all pairwise comparisons while controlling the familywise error rate, identifying exactly which group pairs drive the significant omnibus result.",
        "D": "Converting to a within-subjects (repeated measures) ANOVA would be appropriate only if the same participants appeared in all conditions. The study describes matched but distinct groups, and rerunning the analysis with a different design would not address the question of which groups differ after a significant omnibus result."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-118-vignette-L3",
      "source_question_id": "118",
      "source_summary": "When a one-way ANOVA produces a statistically significant F-ratio and the independent variable has three or more levels, a post-hoc test would be conducted to determine which group means are significantly different.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "omnibus"
      ],
      "vignette": "A school psychologist evaluates reading fluency scores in students assigned to one of three instructional interventions delivered over a semester. The omnibus test comparing all three groups reaches statistical significance, and the psychologist notes that the effect size is moderate (η² = .09). One colleague suggests the psychologist simply inspect the group means visually to identify which intervention worked best, arguing that the significant overall test already proves the groups differ. A second colleague recommends an additional inferential procedure before drawing conclusions about specific group comparisons.",
      "question": "Which position is methodologically correct, and what procedure should the psychologist use?",
      "options": {
        "A": "The first colleague is correct; a significant overall test with a moderate effect size is sufficient to conclude which specific groups differ",
        "B": "The second colleague is correct; the psychologist should conduct a post-hoc test to determine which specific pairs of group means differ significantly",
        "C": "The second colleague is correct; the psychologist should conduct a planned contrast analysis using a t-distribution to compare all possible pairs",
        "D": "The first colleague is correct; visual inspection of means combined with effect size is an accepted alternative to formal pairwise comparisons after a significant result"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect. A significant omnibus F-test establishes only that at least one group mean differs from another; it does not specify which pair or pairs are responsible. Effect size (η²) quantifies the magnitude of the overall effect but similarly cannot identify where specific differences lie. Concluding which groups differ without pairwise testing is an inferential error.",
        "B": "This is correct. The second colleague is right: following a significant omnibus result with three or more levels, post-hoc tests (e.g., Tukey's HSD, Bonferroni, Scheffé) are required to determine which specific group mean pairs are significantly different, while controlling the familywise error rate.",
        "C": "Planned contrasts are appropriate when specific, theoretically motivated hypotheses about group comparisons are formed before data collection. They differ from post-hoc tests in that they are specified a priori and typically do not require a significant omnibus test as a prerequisite. Using planned contrasts after the fact because the omnibus is significant conflates a priori and post-hoc approaches.",
        "D": "Visual inspection of means is a descriptive tool, not an inferential one. No matter how large the apparent difference between means appears graphically, formal statistical testing is required to determine whether observed differences exceed what would be expected by chance. Effect size alone does not serve this function."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-118-vignette-L4",
      "source_question_id": "118",
      "source_summary": "When a one-way ANOVA produces a statistically significant F-ratio and the independent variable has three or more levels, a post-hoc test would be conducted to determine which group means are significantly different.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "familywise"
      ],
      "vignette": "A researcher studying pain management compares five treatment protocols administered to distinct patient groups. After obtaining a statistically significant overall result, she consults a statistician about next steps. The statistician warns that simply repeating the same basic inferential test for every possible pair of groups would cause the familywise error rate to balloon well above the nominal .05 threshold — across ten possible comparisons in a five-group design, the cumulative probability of at least one false positive approaches .40. The statistician recommends a procedure specifically engineered for this situation that maintains error control across all comparisons simultaneously. On first reading, a colleague assumes this warning is simply an argument for using a more stringent per-comparison alpha, not for a fundamentally different analytical approach.",
      "question": "What is the appropriate next step the statistician is recommending, and why does it differ from merely lowering the per-comparison alpha?",
      "options": {
        "A": "Conduct a Bonferroni correction by dividing the alpha level by the number of comparisons, which is mathematically equivalent to and fully substitutable for post-hoc testing procedures",
        "B": "Perform a post-hoc test (e.g., Tukey's HSD), which simultaneously controls the familywise error rate across all pairwise comparisons using decision criteria derived from the distribution of the studentized range statistic",
        "C": "Rerun the overall model using a MANOVA framework, which inherently controls error rates across multiple dependent comparisons",
        "D": "Apply a sequential Bonferroni (Holm) procedure to the planned contrasts that were specified before data collection, converting post-hoc comparisons into a priori tests"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Post-hoc tests such as Tukey's HSD are specifically designed for the situation following a significant omnibus ANOVA with multiple groups. Tukey's HSD uses the studentized range distribution to set critical values that simultaneously control the familywise error rate across all pairwise comparisons — a more elegant and statistically precise solution than simple alpha division, particularly when group sizes are equal.",
        "A": "The Bonferroni correction (dividing α by the number of comparisons) does control the familywise error rate, but it is not fully substitutable for post-hoc tests. Bonferroni is generally more conservative (less powerful) than Tukey's HSD when all pairwise comparisons are made. Calling them mathematically equivalent is imprecise; they use different distributional logic and produce different critical values.",
        "C": "MANOVA (Multivariate Analysis of Variance) is used when there are multiple dependent variables, not multiple levels of a single independent variable. It controls Type I error across simultaneous tests of different outcome measures. In this scenario, the issue is multiple pairwise comparisons among groups on a single dependent variable, which is a different problem that MANOVA does not address.",
        "D": "Sequential Bonferroni (Holm) procedures are applied to planned (a priori) contrasts, not to post-hoc comparisons formed after seeing the data. If the researcher had not specified these comparisons in advance, relabeling them as a priori to use this procedure would be methodologically inappropriate. Furthermore, planned contrasts and post-hoc tests serve different inferential purposes and have different requirements."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-118-vignette-L5",
      "source_question_id": "118",
      "source_summary": "When a one-way ANOVA produces a statistically significant F-ratio and the independent variable has three or more levels, a post-hoc test would be conducted to determine which group means are significantly different.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Inferential Statistical Tests",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher administers a standardized measure of cognitive flexibility to adults enrolled in one of three distinct training programs and a group that receives no training. After an initial calculation comparing all groups simultaneously, she finds that the result clearly exceeds the threshold she set in advance for concluding the groups differ overall. A graduate student reviewing the work argues that this single calculation already proves the third training program — which had the highest average score — outperformed the others, and that no further analysis is needed. A senior methodologist disagrees, noting that the initial calculation was not designed to answer questions about specific group pairs, and that drawing such conclusions without an additional step risks making at least one incorrect claim across all the comparisons being considered.",
      "question": "What does the senior methodologist most likely recommend, and what is the core justification?",
      "options": {
        "A": "Conduct independent inferential tests between each pair of groups, using the same decision threshold as the initial calculation, because the overall significant result licenses unrestricted pairwise comparisons",
        "B": "Calculate a measure of the overall relationship magnitude between group membership and scores to determine whether the effect is large enough to be practically meaningful before examining specific groups",
        "C": "Perform a set of follow-up pairwise comparisons using a procedure that controls the probability of making at least one incorrect conclusion across all comparisons being considered simultaneously",
        "D": "Run a separate overall test for only the three training program groups, excluding the no-training group, because the significant result was driven by the no-training group pulling down the overall mean"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The scenario describes a significant omnibus test (a one-way ANOVA) across four groups. The senior methodologist is recommending post-hoc testing — follow-up pairwise comparisons that control the familywise error rate (the probability of at least one false positive across all simultaneous comparisons). This is the appropriate and necessary next step when the omnibus result is significant and there are more than two groups.",
        "A": "Conducting uncorrected pairwise inferential tests after a significant omnibus result is a common but methodologically flawed approach. Each individual test at the same threshold as the original calculation does not account for the inflation of error probability across multiple simultaneous tests. The senior methodologist's concern about making 'at least one incorrect claim across all comparisons' is precisely the problem this approach fails to address.",
        "B": "Calculating the magnitude of the overall association (i.e., an effect size measure like eta-squared) provides information about practical significance but does not identify which specific group pairs differ. The methodologist's concern is about inferential conclusions regarding specific comparisons, not about the overall effect magnitude. This step, while sometimes useful, does not resolve the question at hand.",
        "D": "Excluding the no-training group and rerunning the omnibus test would be a post-hoc data manipulation that introduces its own inferential problems and does not address the question of which specific groups differ. The concern is not about the overall test's validity but about how to correctly make specific pairwise inferences following the already-obtained significant result."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-089-vignette-L1",
      "source_question_id": "089",
      "source_summary": "When using purposive sampling, researchers rely on their own judgment to determine which individuals to include as subjects in their studies.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "purposive sampling",
        "judgment",
        "subjects"
      ],
      "vignette": "A researcher studying expert clinical decision-making wants to recruit participants who are particularly informative for her research question. Rather than drawing names randomly from a registry, she uses her own professional judgment to hand-select ten senior psychologists she believes best represent expert clinical reasoning. She identifies these subjects based on their published work, reputation, and years of experience. This approach to purposive sampling allows her to target individuals most likely to yield rich, relevant data.",
      "question": "Which sampling method is the researcher using?",
      "options": {
        "A": "Systematic sampling",
        "B": "Purposive sampling",
        "C": "Stratified random sampling",
        "D": "Snowball sampling"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Systematic sampling involves selecting every nth individual from a list and does not rely on the researcher's judgment about who would be most informative. This researcher is making deliberate, knowledge-based selections rather than following a fixed interval rule.",
        "B": "Purposive sampling is correct. The researcher is using her own professional judgment to deliberately select participants she believes best represent the characteristic of interest — a defining feature of this approach.",
        "C": "Stratified random sampling divides the population into subgroups and then randomly selects from each stratum. Although it involves deliberate categorization, selection within strata is random, not based on the researcher's individual judgment about specific persons.",
        "D": "Snowball sampling relies on existing participants to recruit future participants through their social networks. This researcher is selecting participants based on her own knowledge and expertise, not through referral chains from existing subjects."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-089-vignette-L2",
      "source_question_id": "089",
      "source_summary": "When using purposive sampling, researchers rely on their own judgment to determine which individuals to include as subjects in their studies.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "judgment",
        "recruited"
      ],
      "vignette": "A clinical researcher is investigating resilience factors among survivors of complex trauma. Because trauma survivors are difficult to reach through standard advertising, she personally recruited individuals who she determined — based on her clinical knowledge — were most likely to have experienced prolonged adversity and demonstrated adaptive coping. Several of her selected participants had also been referred by colleagues who knew the researcher's work, though the researcher made the final inclusion decision herself. The researcher acknowledges that her selections were guided entirely by her professional judgment about who would best represent the phenomenon under study.",
      "question": "What type of sampling strategy did this researcher employ?",
      "options": {
        "A": "Convenience sampling",
        "B": "Quota sampling",
        "C": "Purposive sampling",
        "D": "Cluster sampling"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Convenience sampling involves selecting participants based on their availability or accessibility, not based on the researcher's judgment about their representativeness of a particular characteristic. While convenience may have played a minor role here, the defining feature is the researcher's deliberate, knowledge-based selection.",
        "B": "Quota sampling involves identifying subgroups and filling predetermined numerical quotas from each group, which can involve researcher judgment but is structured by pre-set proportional targets. This researcher applied no such quota structure — she selected based solely on her judgment of relevance.",
        "C": "Purposive sampling is correct. The researcher used her own professional judgment to identify and select the individuals she believed would best illuminate the phenomenon of interest, which is the hallmark of purposive sampling.",
        "D": "Cluster sampling involves randomly selecting naturally occurring groups (clusters) and then sampling from within those groups. This researcher did not select pre-existing groups and did not use any randomization step in her selection process."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-089-vignette-L3",
      "source_question_id": "089",
      "source_summary": "When using purposive sampling, researchers rely on their own judgment to determine which individuals to include as subjects in their studies.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "deliberate"
      ],
      "vignette": "A researcher studying treatment-resistant depression wants to ensure her sample captures clinically meaningful variation in the condition. She makes deliberate choices about which outpatient clinics to contact, which clinicians to approach, and ultimately which patients to enroll, basing every decision on her theoretical understanding of what constitutes the most informative cases. Some participants were enrolled because a treating clinician flagged them as unusual cases, but the researcher — not the clinician — retained final authority over all inclusion decisions. The researcher acknowledged that her sample may not generalize well to all people with treatment-resistant depression, but argued this was acceptable given her study's goal of theoretical development rather than population estimation.",
      "question": "Which sampling approach best characterizes the method described?",
      "options": {
        "A": "Snowball sampling",
        "B": "Purposive sampling",
        "C": "Theoretical sampling",
        "D": "Volunteer sampling"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Snowball sampling relies on enrolled participants referring or recruiting others from their networks, creating a chain-referral dynamic. Although a clinician flagged some patients, referral was not the mechanism driving inclusion — the researcher's personal judgment was the determining factor at every stage.",
        "B": "Purposive sampling is correct. The researcher made all inclusion decisions based on her own expert judgment about which individuals would be most informative, which is the defining characteristic of purposive sampling, regardless of whether some candidates were initially identified through clinician contact.",
        "C": "Theoretical sampling is a related but distinct concept from grounded theory methodology, in which sampling decisions are driven iteratively by emerging theoretical categories during data collection. While the researcher mentions theoretical development, she describes making selection decisions based on prior knowledge rather than iteratively adjusting sampling as theory develops.",
        "D": "Volunteer sampling occurs when participants self-select into a study in response to a call for participants. Here, the researcher — not the participants — controlled who was included, ruling out volunteer sampling as the primary mechanism."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-089-vignette-L4",
      "source_question_id": "089",
      "source_summary": "When using purposive sampling, researchers rely on their own judgment to determine which individuals to include as subjects in their studies.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "criterion"
      ],
      "vignette": "A researcher investigating psychotherapy outcomes in adolescents with co-occurring anxiety and ADHD contacts five specialized school-based mental health programs and asks program directors to identify students who meet a specific criterion: those who have received at least 12 sessions of manualized CBT and show documented symptom change. The researcher reviews these referrals and makes the final decision about enrollment, excluding several referred students whose records she judges to be incomplete or atypical. She acknowledges that findings from this sample may not generalize broadly but notes the group was selected for its expected informativeness. The researcher did not use randomization at any stage.",
      "question": "The researcher's enrollment strategy is best characterized as which type of sampling?",
      "options": {
        "A": "Stratified sampling",
        "B": "Snowball sampling",
        "C": "Quota sampling",
        "D": "Purposive sampling"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Stratified sampling divides a population into mutually exclusive subgroups defined by a variable of interest and then randomly selects from each stratum. While the researcher specified a criterion that resembles a stratum definition, she did not randomly select within any subgroup — all decisions were made by her own judgment, disqualifying this as stratified sampling.",
        "B": "Snowball sampling relies on existing participants nominating or referring future participants from their own social or professional networks. Here, program directors flagged candidates, but this is an institutional referral, not a participant-driven chain of social connection. Moreover, the researcher — not participants — made all final inclusion decisions.",
        "C": "Quota sampling involves pre-specifying the number of participants needed from defined subgroups and then filling those quotas, often non-randomly. The researcher does use a criterion for selection and makes non-random choices, which superficially resembles quota sampling. However, there is no evidence she pre-determined numerical targets for subgroups; her selections were guided by judgment about informativeness, not by filling predetermined proportional cells.",
        "D": "Purposive sampling is correct. Despite the institutional referral mechanism used to surface candidates, the researcher exercised independent professional judgment at every critical decision point — which programs to contact, which referrals to accept or reject, and who ultimately qualifies. This researcher-driven, judgment-based selection process is the defining feature of purposive sampling."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-RMS-089-vignette-L5",
      "source_question_id": "089",
      "source_summary": "When using purposive sampling, researchers rely on their own judgment to determine which individuals to include as subjects in their studies.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Research - Single-Subject and Group Designs",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher studying how people cope with chronic pain in underserved communities reaches out to several community health workers she knows personally from years of working in the field. She asks each worker to recommend three to five individuals from their caseloads who seem to embody different ways of managing pain. After receiving over forty names, the researcher reviews brief case notes and personally selects twenty individuals, explaining to a colleague that she chose people based on what they would add to the story she was trying to build. She does not explain any formula or rule she followed, and she acknowledges that someone else making the same selections might have chosen differently. The colleague notices that all twenty selected individuals happen to have been flagged as particularly communicative by the health workers.",
      "question": "Which approach to selecting study participants most precisely characterizes what this researcher did?",
      "options": {
        "A": "Snowball sampling",
        "B": "Purposive sampling",
        "C": "Convenience sampling",
        "D": "Quota sampling"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Snowball sampling is tempting here because the researcher used personal connections to community health workers who then generated names — a structure that resembles a referral chain. However, snowball sampling requires that enrolled participants themselves refer future participants; here, health workers (not enrolled participants) provided names, and the researcher made all final selections through her own judgment. The referral mechanism was institutional, not participant-driven.",
        "B": "Purposive sampling is correct. Despite the layered appearance of the selection process, the decisive and defining act was the researcher's personal, non-formulaic judgment about who would best serve the theoretical aims of her study. Her explicit statement that she selected based on what individuals would 'add to the story' and her acknowledgment that her choices were not replicable by formula are hallmarks of purposive sampling.",
        "C": "Convenience sampling is appealing because the researcher used personal contacts and selected from an accessible pool of candidates. However, convenience sampling is defined by selecting whoever is most available or easiest to reach, not by exercising substantive judgment about who is most theoretically informative. This researcher expended deliberate effort evaluating candidates against a conceptual standard, which moves the method beyond mere convenience.",
        "D": "Quota sampling is plausible because the researcher selected a fixed number (twenty participants) and was guided by a desire for variation in coping styles, which resembles filling cells in a quota matrix. However, quota sampling requires pre-specified numerical targets for defined subgroups that are filled systematically; this researcher followed no such pre-specified structure and made idiosyncratic judgments she herself acknowledged were not rule-governed."
      },
      "legacy_domain_code": "RMS",
      "legacy_domain_name": "Research Methods and Statistics"
    },
    {
      "id": "JQ-TES-049-vignette-L1",
      "source_question_id": "049",
      "source_summary": "When a test has a standard deviation of 10, the test's standard error of measurement ranges from 0 to 10.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "standard error of measurement",
        "reliability coefficient",
        "standard deviation"
      ],
      "vignette": "A psychometrician is evaluating a new cognitive ability test with a standard deviation of 10. She calculates the standard error of measurement using the formula SEM = SD × √(1 – r), where r is the reliability coefficient. She notes that when reliability is perfect (r = 1.00), the SEM equals 0, and when reliability is zero (r = 0.00), the SEM equals the test's standard deviation of 10.",
      "question": "Which of the following best describes the possible range of the standard error of measurement for this test?",
      "options": {
        "A": "The SEM can range from 0 to 10.",
        "B": "The SEM can range from 0 to 1.",
        "C": "The SEM can range from –10 to +10.",
        "D": "The SEM can range from 0 to 100."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The SEM = SD × √(1 – r). When r = 1.00 (perfect reliability), SEM = 0; when r = 0.00 (no reliability), SEM equals the SD, which is 10. Therefore, the SEM ranges from 0 to 10 for this test.",
        "B": "Incorrect. A range of 0 to 1 would apply to the reliability coefficient (r) itself, not the SEM. The SEM is scaled in the same units as the test scores, not bounded by 0 and 1.",
        "C": "Incorrect. The SEM cannot be negative because it is derived from a square root and represents measurement error magnitude, which has no direction. A range of –10 to +10 confuses SEM with a signed deviation score.",
        "D": "Incorrect. A maximum SEM of 100 would apply if the test's standard deviation were 100, not 10. The SEM's upper bound equals the test's SD, which in this case is 10, not 100."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-049-vignette-L2",
      "source_question_id": "049",
      "source_summary": "When a test has a standard deviation of 10, the test's standard error of measurement ranges from 0 to 10.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "reliability",
        "standard deviation"
      ],
      "vignette": "A test developer constructs a new personality inventory for use with adult outpatients. The inventory has a standard deviation of 10 in the normative sample. After pilot testing, she finds the test's reliability is essentially zero — items appear to be measuring different, unrelated constructs. She wonders what implications this finding has for the precision of the scores.",
      "question": "Given a reliability of zero and a standard deviation of 10, what is the standard error of measurement for this inventory, and what does it indicate?",
      "options": {
        "A": "The SEM is 0, indicating the test scores contain no measurement error.",
        "B": "The SEM is 5, indicating moderate measurement error because reliability and error share equal variance.",
        "C": "The SEM is 10, indicating that the test scores are entirely measurement error and provide no reliable information.",
        "D": "The SEM is 1, indicating minimal measurement error because the reliability coefficient is anchored at zero."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. When reliability = 0, SEM = SD × √(1 – 0) = 10 × 1 = 10. This means the observed score variance is entirely error variance, so the scores carry no reliable signal about the true score.",
        "A": "Incorrect. A SEM of 0 occurs when reliability is perfect (r = 1.00), not when it is zero. A SEM of 0 would mean scores perfectly reflect true scores with no error — the opposite of what zero reliability implies.",
        "B": "Incorrect. A SEM of 5 would correspond to a reliability coefficient of 0.75 (since SEM = 10 × √(1 – 0.75) = 10 × 0.5 = 5), not a reliability of zero. Splitting the variance equally does not occur when reliability is at its minimum.",
        "D": "Incorrect. The SEM formula yields 10 × √(1 – 0) = 10 when reliability is zero; the result is never 1 in this context. Anchoring the coefficient at zero does not produce a SEM of 1 — that would require a specific non-zero reliability value near 0.99."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-049-vignette-L3",
      "source_question_id": "049",
      "source_summary": "When a test has a standard deviation of 10, the test's standard error of measurement ranges from 0 to 10.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "reliability"
      ],
      "vignette": "A researcher administers a vocational interest inventory to a large sample of college students. The test has a standard deviation of 10, and the developer reports a high internal consistency coefficient of 0.91. A colleague argues that because the inventory has high reliability, the scores must be extremely precise. However, the researcher points out that even with this strong reliability, there is still a non-trivial degree of imprecision in the scores. She calculates the exact value of this imprecision index and notes that it falls well below 10 but is clearly above 0.",
      "question": "Which of the following values most accurately represents the index of score imprecision for this inventory?",
      "options": {
        "A": "SEM ≈ 0.91, because the imprecision index equals the reliability coefficient directly.",
        "B": "SEM ≈ 3, because when reliability is 0.91 and SD is 10, the SEM = 10 × √(1 – 0.91) ≈ 3.",
        "C": "SEM ≈ 9.1, because the imprecision index equals the SD weighted by the reliability coefficient.",
        "D": "SEM ≈ 1, because high internal consistency near 0.91 compresses measurement error to approximately 1 score point."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. SEM = SD × √(1 – r) = 10 × √(1 – 0.91) = 10 × √0.09 = 10 × 0.30 = 3. Even with reliability of 0.91, there remains about 3 score-point imprecision, consistent with the researcher's observation that the value is above 0 but well below 10.",
        "A": "Incorrect. The SEM is not equal to the reliability coefficient. The reliability coefficient (0.91 here) is a dimensionless ratio bounded between 0 and 1, whereas the SEM is expressed in the metric of the test scores. Equating them conflates two distinct statistics.",
        "C": "Incorrect. Multiplying SD × r (10 × 0.91 = 9.1) reflects the proportion of the SD associated with true score variance, not measurement error. The SEM formula uses √(1 – r), which captures error variance, not true-score variance.",
        "D": "Incorrect. While high reliability does reduce the SEM, a value of approximately 1 would require reliability near 0.99 (SEM = 10 × √0.01 = 1). At reliability = 0.91, the SEM is approximately 3, not 1."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-049-vignette-L4",
      "source_question_id": "049",
      "source_summary": "When a test has a standard deviation of 10, the test's standard error of measurement ranges from 0 to 10.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A test developer is comparing two versions of an achievement test, both normed on the same population with a standard deviation of 10. Version A has items that are highly homogeneous and closely intercorrelated, yielding an error term of 4.36 score points. Version B was assembled from items spanning a broader range of content, resulting in an error term of 7.07 score points. A consultant reviewing the project notes that Version B's error term is closer to the theoretical ceiling for a test with this SD, while Version A's error term reflects substantially better measurement quality. Both error terms fall squarely within the permissible range for a test with these distributional properties.",
      "question": "The consultant's statement that both error terms fall within the permissible range reflects which psychometric principle?",
      "options": {
        "A": "The standard error of estimate ranges from 0 to 1, and both values fall within that interval after appropriate scaling.",
        "B": "The standard error of measurement ranges from 0 to the test's standard deviation of 10, and both values (4.36 and 7.07) fall within that interval.",
        "C": "Internal consistency coefficients range from 0 to 1, and the implied reliability values for both error terms fall within that interval.",
        "D": "The coefficient of determination ranges from 0 to 1, and the proportion of error variance for each version falls within that interval."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The SEM ranges from 0 (when reliability = 1.00) to the test's SD (when reliability = 0.00). With SD = 10, permissible SEM values span 0 to 10. Version A's SEM of 4.36 implies r ≈ 0.81, and Version B's SEM of 7.07 implies r ≈ 0.50 — both realistic reliabilities whose SEMs fall within 0–10.",
        "A": "Incorrect. The standard error of estimate (SEest) is used in regression-based prediction contexts, not to describe measurement precision on a single test. Moreover, its range is not simply 0 to 1 — it is bounded by the criterion's SD, not 0 to 1.",
        "C": "Incorrect. While it is true that internal consistency coefficients range from 0 to 1 and can be derived from the SEM, the consultant's statement specifically concerns the permissible range of the error terms themselves (4.36 and 7.07 score points), not the reliability coefficients. The reliability values are implied, not directly stated.",
        "D": "Incorrect. The coefficient of determination (r²) represents proportion of shared variance between two variables and ranges from 0 to 1. While related to reliability, this concept applies to predictive relationships rather than directly bounding the SEM on a single test with a known SD."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-049-vignette-L5",
      "source_question_id": "049",
      "source_summary": "When a test has a standard deviation of 10, the test's standard error of measurement ranges from 0 to 10.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A test construction team is debating the upper and lower boundaries of a specific numerical index that describes how much an individual's observed score might deviate from their true standing on a given attribute. One team member argues that the index can never exceed a particular value tied to how spread out scores are across the entire group of test-takers. Another member counters that the index could theoretically reach zero, but only if the instrument achieved something no real-world assessment ever accomplishes. The first member then notes that a test composed entirely of random noise would push the index to its absolute maximum, while a test of identical items would push it toward its minimum. All parties agree that in practice, the index always lands somewhere strictly between these two theoretical poles.",
      "question": "What psychometric principle is the team describing?",
      "options": {
        "A": "The confidence interval around a predicted criterion score is bounded by 0 and the criterion variable's standard deviation, depending on the strength of the predictor-criterion relationship.",
        "B": "The standard error of measurement for a test is bounded below by 0 and above by the test's standard deviation, with those endpoints approached as reliability moves from perfect to absent.",
        "C": "The split-half reliability coefficient is bounded between –1 and +1, with the absolute magnitude reflecting the degree of overlap between two halves of a test.",
        "D": "The item discrimination index ranges from –1 to +1, reaching its floor when items are answered incorrectly by high scorers and correctly by low scorers."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The team is describing the SEM = SD × √(1 – r). The 'something no real-world assessment ever accomplishes' is perfect reliability (r = 1.00), which yields SEM = 0. 'Random noise' means zero reliability (r = 0.00), yielding SEM = SD. The upper boundary is thus the test's own standard deviation, and the lower boundary is zero — exactly what the team describes.",
        "A": "Incorrect. The standard error of estimate in regression is bounded by 0 and the criterion's SD, and it shrinks as the predictor-criterion correlation strengthens. This matches some surface features — 'deviation from true standing,' bounded by a group-level spread statistic — but the scenario describes a single test's internal precision, not a prediction from one variable to another.",
        "C": "Incorrect. Split-half reliability is a method for estimating internal consistency and does produce a coefficient between 0 and 1 (or theoretically –1 to +1). However, the scenario describes an index that reaches its maximum when items are random and its minimum when items are identical, which is the SEM's behavior as a function of reliability — not the reliability coefficient itself, which would behave inversely.",
        "D": "Incorrect. The item discrimination index does range from –1 to +1 and reaches its negative extreme when high scorers fail items that low scorers pass. However, the scenario describes a test-level index bounded by the overall group's score spread, not an item-level statistic. The reference to 'identical items' pushing the index toward its minimum also fits SEM behavior (homogeneous items yield high internal consistency and low SEM), but the item discrimination index does not have an upper bound tied to the score distribution's spread."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-106-vignette-L1",
      "source_question_id": "106",
      "source_summary": "A test's criterion-related validity coefficient can be no greater than the square root of its reliability coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "reliability coefficient",
        "validity coefficient",
        "criterion-related validity"
      ],
      "vignette": "A psychometrician is evaluating a newly developed cognitive screening instrument. She computes the test's reliability coefficient and obtains a value of .64. When she attempts to establish criterion-related validity by correlating the test scores with a well-validated external criterion, a colleague suggests the validity coefficient cannot exceed a specific mathematical limit. The psychometrician recalls from her graduate training that the upper bound of any validity coefficient is directly constrained by the test's reliability.",
      "question": "According to psychometric theory, what is the maximum possible criterion-related validity coefficient for this screening instrument?",
      "options": {
        "A": ".64, because the validity coefficient cannot exceed the reliability coefficient itself",
        "B": ".80, because the validity coefficient cannot exceed the square root of the reliability coefficient",
        "C": ".41, because the validity coefficient cannot exceed the reliability coefficient squared",
        "D": "1.00, because validity is theoretically independent of reliability and has no upper bound set by it"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. The validity coefficient is bounded by the square root of the reliability coefficient, not the reliability coefficient itself. If reliability is .64, the validity ceiling is √.64 = .80, not .64.",
        "B": "Correct. The maximum possible criterion-related validity coefficient equals the square root of the reliability coefficient. With a reliability of .64, √.64 = .80, so no observed validity coefficient can exceed .80.",
        "C": "Incorrect. Squaring the reliability coefficient (.64² = .41) is the formula for the coefficient of determination, which describes shared variance, not the upper bound of the validity coefficient.",
        "D": "Incorrect. Validity is not independent of reliability; a test cannot correlate with an external criterion more than it correlates with itself. Reliability sets a definitive ceiling on validity."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-106-vignette-L2",
      "source_question_id": "106",
      "source_summary": "A test's criterion-related validity coefficient can be no greater than the square root of its reliability coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "coefficient alpha",
        "predictive validity"
      ],
      "vignette": "A researcher develops a 40-item personality scale intended to predict employee job performance ratings one year after hire. The instrument is administered to 200 newly hired employees at a mid-sized firm, many of whom vary widely in age and educational background. Coefficient alpha for the scale is computed as .49. When performance ratings are collected twelve months later and correlated with initial test scores, the researcher is surprised to find that the predictive validity coefficient reaches only .55, prompting a supervisor to question whether this value is even theoretically possible.",
      "question": "Given the psychometric relationship between reliability and validity, which conclusion best explains the supervisor's concern about the obtained validity coefficient?",
      "options": {
        "A": "The validity coefficient of .55 is impossible because it exceeds the square root of the reliability coefficient of .49",
        "B": "The validity coefficient of .55 is plausible because predictive validity studies with longitudinal designs routinely produce values higher than the reliability coefficient",
        "C": "The validity coefficient of .55 is suspect because restriction of range in the employee sample would inflate, not deflate, obtained validity estimates",
        "D": "The validity coefficient of .55 is impossible because the coefficient alpha of .49 indicates the scale's items are measuring multiple distinct constructs"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The theoretical upper bound of a validity coefficient is the square root of the reliability coefficient. Here √.49 = .70; since .55 is below .70, the value is technically possible, but the supervisor's concern is grounded in this principle — any coefficient above .70 would be impossible, and checking this boundary is the appropriate psychometric response.",
        "B": "Incorrect. Longitudinal predictive validity designs do not exempt a test from the psychometric constraint that validity cannot exceed the square root of reliability. The design type does not alter the mathematical ceiling imposed by reliability.",
        "C": "Incorrect. Restriction of range in a selected sample typically attenuates (decreases) validity coefficients rather than inflating them, because reduced score variability lowers observed correlations. This is the opposite of what is described.",
        "D": "Incorrect. A low coefficient alpha does suggest multidimensionality or poor internal consistency, but this does not by itself make a specific validity coefficient impossible. The impossibility of a validity coefficient above a given value is determined by the square root of reliability, not by item structure per se."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-106-vignette-L3",
      "source_question_id": "106",
      "source_summary": "A test's criterion-related validity coefficient can be no greater than the square root of its reliability coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "reliability"
      ],
      "vignette": "A test development team creates a structured interview protocol designed to predict therapist-rated treatment outcome at six months post-admission. During validation, a consultant reviews the data and notes that the interview's reliability estimate—obtained by correlating scores from two independent raters—is .36. When the team proudly reports an observed correlation of .68 between interview scores and treatment outcome ratings, the consultant immediately challenges the result as mathematically untenable. The team argues that the high predictive utility of their instrument justifies the finding, noting that the criterion itself was assessed with high fidelity.",
      "question": "Why does the consultant regard the reported correlation of .68 as mathematically impossible?",
      "options": {
        "A": "Because the correlation between interview scores and outcome exceeds the inter-rater reliability estimate, violating the principle that a test cannot correlate with an external criterion more than it correlates with itself",
        "B": "Because a reliability of .36 indicates the protocol is essentially unscored, rendering any correlation with an outcome measure uninterpretable",
        "C": "Because the square root of the reliability coefficient (.36) is .60, and no validity coefficient can exceed this upper bound",
        "D": "Because using treatment outcome as a criterion introduces criterion contamination, which systematically inflates validity estimates beyond achievable limits"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The maximum possible validity coefficient is the square root of the reliability coefficient. √.36 = .60, so a validity coefficient of .68 exceeds this ceiling and is therefore mathematically impossible regardless of the quality of the criterion.",
        "A": "Incorrect. While it is true that a test's correlation with itself bounds its correlation with external variables, the specific ceiling is set by the square root of the reliability coefficient—not by the raw reliability value. The raw reliability is .36, and .68 exceeds even that, but the precise psychometric principle invoked is the square root, making option C the more precise and correct explanation.",
        "B": "Incorrect. A reliability of .36, while low, does not render the instrument entirely unscored or uninterpretable. Low reliability reduces the ceiling for validity but does not make correlations uninterpretable; the problem is that .68 specifically exceeds √.36 = .60.",
        "D": "Incorrect. Criterion contamination occurs when information about the criterion influences test scores (or vice versa), which can artificially inflate validity. While this is a legitimate threat, it is not the mathematical principle the consultant is invoking; the violation here is a hard psychometric boundary set by reliability."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-106-vignette-L4",
      "source_question_id": "106",
      "source_summary": "A test's criterion-related validity coefficient can be no greater than the square root of its reliability coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "attenuation"
      ],
      "vignette": "A senior researcher is auditing validity studies submitted by junior colleagues for a grant application. One study reports a correlation of .75 between a newly developed risk appraisal tool and a composite outcome index. The researcher is immediately skeptical, noting that the tool's scores showed substantial inconsistency across two testing occasions separated by two weeks under identical conditions. She does not question the quality of the criterion measure or the sample's representativeness, but cites a fundamental constraint rooted in attenuation that makes the reported figure untenable. A junior colleague counters that the study used a large community sample with good variability and that the outcome index was independently scored, so there is no reason to doubt the obtained coefficient.",
      "question": "On what precise psychometric basis does the senior researcher conclude that the validity coefficient of .75 is impossible?",
      "options": {
        "A": "Because the inconsistency across testing occasions indicates that restriction of range has suppressed true score variance, making the observed correlation an overestimate of the population value",
        "B": "Because a test-retest reliability estimate derived from the inconsistent scores would fall below .5625, making .75 exceed the square root of that reliability value and thus the theoretical validity ceiling",
        "C": "Because a validity coefficient obtained from a community sample with unrestricted range should be corrected downward using the correction for attenuation formula before conclusions are drawn",
        "D": "Because substantial inconsistency across administrations indicates poor content validity, and content validity is a prerequisite for criterion-related validity to be meaningful"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The substantial inconsistency across testing occasions implies a low test-retest reliability coefficient. If reliability (rxx) is less than .5625, then √rxx < .75, meaning the reported validity coefficient of .75 would exceed the theoretical maximum. The senior researcher's reasoning rests precisely on this psychometric ceiling: a test cannot correlate with a criterion more than the square root of its reliability.",
        "A": "Incorrect. Restriction of range reduces observed correlations; it does not inflate them. A large, diverse community sample (as described) would not produce restriction of range. Furthermore, the researcher's concern is a mathematical ceiling imposed by reliability, not a sampling artifact.",
        "C": "Incorrect. The correction for attenuation formula is used to estimate the correlation that would be obtained if measurement error were removed — it adjusts upward toward a theoretical true-score correlation, not downward. Applying this correction would increase, not decrease, the validity estimate.",
        "D": "Incorrect. Content validity refers to the degree to which a test's items adequately sample a defined domain and is evaluated through expert review rather than statistical consistency. While low test-retest consistency is concerning, it reflects reliability, not content validity, and it is the reliability ceiling—not content validity—that makes the reported coefficient impossible."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-106-vignette-L5",
      "source_question_id": "106",
      "source_summary": "A test's criterion-related validity coefficient can be no greater than the square root of its reliability coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of researchers develops a brief questionnaire intended to forecast how well adults will cope with a major medical diagnosis over the following year. During an initial study, they administer the questionnaire to 300 adults recently diagnosed with a chronic illness and collect detailed coping outcome data twelve months later from independent clinicians who have no knowledge of the questionnaire scores. The questionnaire items, however, produce noticeably different total scores for the same individuals when readministered just two weeks after the initial testing under unchanged circumstances. When the team reports a strong positive association of .78 between the questionnaire and the twelve-month coping outcomes, a senior methodologist states flatly that this number cannot be correct, even before examining the raw data. The team points out that the outcome assessors were fully blind to the questionnaire scores, eliminating any possibility of rater bias or shared-method contamination.",
      "question": "What is the most precise reason the senior methodologist concludes that an association of .78 is impossible for this questionnaire?",
      "options": {
        "A": "Because the twelve-month gap between the questionnaire and outcome assessment is too long for the association to remain as high as .78 without an unrealistically stable underlying construct",
        "B": "Because the blind outcome assessment procedure, while eliminating rater bias, also reduces shared variance between the predictor and criterion, mathematically capping obtainable associations below .78",
        "C": "Because the questionnaire's inconsistency across the two administrations implies a level of measurement precision that sets a mathematical upper limit on any possible association with an external outcome, and .78 exceeds that limit",
        "D": "Because with 300 participants the sample provides enough power to detect only moderate associations, and .78 would represent a spuriously large effect driven by outliers in a sample of this size"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The questionnaire's inconsistency across two administrations indicates low test-retest reliability. In psychometric theory, the maximum possible correlation between a test and any external criterion cannot exceed the square root of the test's reliability coefficient. If inconsistency is pronounced, the reliability is low, its square root is even lower, and a validity coefficient as high as .78 would surpass this mathematical ceiling — making the reported value impossible regardless of outcome-assessment quality.",
        "A": "Incorrect. While temporal distance can attenuate predictive correlations, there is no fixed psychometric law preventing a twelve-month predictive association from reaching .78 if the underlying construct is genuinely stable. The methodologist's objection is not about the time interval but about a hard mathematical boundary imposed by the instrument's internal consistency across administrations.",
        "B": "Incorrect. Blind outcome assessment eliminates rater bias and criterion contamination, which are threats that would inflate validity estimates. Removing these threats would not mathematically cap the association below a specific value; in fact, a clean, uncontaminated criterion would allow a more accurate (not ceiling-constrained) estimate. The cap the methodologist invokes is imposed by the predictor's reliability, not the criterion's assessment procedure.",
        "D": "Incorrect. A sample of 300 participants provides substantial statistical power and is large enough to detect even small to moderate effects reliably. Statistical power concerns the ability to detect a true effect; it does not impose a mathematical ceiling on the size of obtainable correlations. The methodologist's objection is a psychometric impossibility argument, not a power or sampling argument."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-016-vignette-L1",
      "source_question_id": "016",
      "source_summary": "When a test has an alternate forms reliability coefficient of .80, 80% of variability in test scores is due to true score variability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "alternate forms reliability",
        "true score variability",
        "reliability coefficient"
      ],
      "vignette": "A psychometrician develops two versions of a cognitive assessment intended to be used interchangeably in clinical settings. After administering both forms to a large normative sample, the alternate forms reliability coefficient is computed and found to be .80. A colleague asks what this value indicates about the proportion of true score variability in the obtained scores. The psychometrician must explain the relationship between the reliability coefficient and the variance components underlying the test scores.",
      "question": "Based on an alternate forms reliability coefficient of .80, what percentage of the variability in observed test scores is attributable to true score variability?",
      "options": {
        "A": "64%, because reliability coefficients must be squared to yield the proportion of true score variance",
        "B": "80%, because the reliability coefficient directly represents the proportion of observed score variance that is due to true score variance",
        "C": "20%, because the reliability coefficient reflects only error variance, leaving the remainder as true score variance",
        "D": "89%, because the reliability coefficient must be converted by taking its square root to obtain the proportion of true score variance"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Squaring a reliability coefficient to obtain the proportion of true score variance confuses the logic of classical test theory with that of the coefficient of determination in regression. The reliability coefficient itself — not its square — equals the ratio of true score variance to total observed score variance.",
        "B": "Correct. In classical test theory, the reliability coefficient (r) is defined as the ratio of true score variance to total observed score variance. Therefore, an alternate forms reliability of .80 means that 80% of the variability in observed scores reflects true score variability, and the remaining 20% is attributable to measurement error.",
        "C": "Incorrect. This option inverts the interpretation. The reliability coefficient represents true score variance as a proportion of total variance, not error variance. An r of .80 means 80% true score variance and 20% error variance — not the reverse.",
        "D": "Incorrect. Taking the square root of a reliability coefficient yields the correlation between observed scores and true scores (the index of reliability), not the proportion of variance due to true scores. The reliability coefficient itself directly provides the variance proportion."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-016-vignette-L2",
      "source_question_id": "016",
      "source_summary": "When a test has an alternate forms reliability coefficient of .80, 80% of variability in test scores is due to true score variability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "parallel forms",
        "observed score variance"
      ],
      "vignette": "A test development team at a large hospital system creates two versions of a neuropsychological screening battery designed to reduce practice effects in longitudinal assessments. Both forms are administered to 300 adult outpatients ranging widely in age and education level, and the correlation between the two sets of scores is computed to be .80. A senior psychologist notes that the heterogeneity of the sample may have inflated this estimate compared to what would be observed in a more restricted clinical population. The team now wants to determine what the obtained coefficient tells them about the composition of variability within their obtained scores.",
      "question": "Setting aside the concern about sample heterogeneity, what does the obtained coefficient of .80 indicate about the variability of the test scores?",
      "options": {
        "A": "That 80% of observed score variance is attributable to true score variance, because the reliability coefficient in classical test theory is defined as the proportion of total variance that reflects true differences among examinees",
        "B": "That the standard error of measurement equals 20% of the score scale, because the unreliability portion (.20) directly converts to score-unit error",
        "C": "That 64% of observed score variance is attributable to true score variance, because the reliability coefficient must be squared when interpreting it as a proportion of shared variance",
        "D": "That 80% of the variance in Form A scores is predictable from Form B scores, which reflects criterion-related validity rather than reliability"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The reliability coefficient is formally defined in classical test theory as the ratio of true score variance to total observed score variance. A coefficient of .80 therefore means 80% of the variability in obtained scores is due to real differences among individuals, regardless of sample heterogeneity concerns.",
        "B": "Incorrect. The standard error of measurement (SEM) is computed as SD × √(1 − r), not as a simple percentage of the scale derived from the error proportion. While 1 − .80 = .20 does represent the error variance proportion, converting that directly into score units requires the standard deviation and the square root transformation.",
        "C": "Incorrect. Squaring the reliability coefficient (.80² = .64) is appropriate when interpreting a correlation as shared variance in regression contexts, but the reliability coefficient itself already represents the variance ratio in classical test theory. No squaring is needed or appropriate here.",
        "D": "Incorrect. Criterion-related validity refers to the relationship between test scores and an external criterion (e.g., diagnosis, job performance), not between two versions of the same instrument. Parallel forms correlation estimates reliability, not validity, because both forms purportedly measure the same construct."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-016-vignette-L3",
      "source_question_id": "016",
      "source_summary": "When a test has an alternate forms reliability coefficient of .80, 80% of variability in test scores is due to true score variability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "coefficient"
      ],
      "vignette": "A researcher publishes a study evaluating a new employment selection battery for which two equivalent versions were developed so that re-testing of applicants would not be confounded by memory of previous items. The correlation between the two versions, obtained from a representative sample of job applicants, is reported as .80 in the technical manual. A reviewer notes approvingly that this means a substantial portion of score differences between examinees reflects genuine differences in their underlying standing on the measured attribute, rather than random fluctuations in measurement. A skeptical reader wonders whether this interpretation requires any transformation of the reported value before it can be applied to score variance.",
      "question": "What does the reported coefficient of .80 indicate about the sources of variability in this battery's scores, and does any mathematical transformation need to be applied to make this interpretation?",
      "options": {
        "A": "It indicates that 80% of observed score variance is true score variance, and no transformation is needed, because reliability coefficients are defined as variance ratios in classical test theory",
        "B": "It indicates that the correlation between observed scores and true scores is .80, and the proportion of true score variance therefore requires squaring (.64), because reliability coefficients are indices of reliability rather than variance ratios",
        "C": "It indicates that 89% of observed score variance is true score variance, because the square root of .80 transforms the coefficient into the appropriate variance ratio",
        "D": "It indicates that 80% of variance is shared between the two forms due to common method variance, which represents construct-irrelevant variance rather than true score variance and thus overstates reliability"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. In classical test theory, the reliability coefficient is formally defined as the ratio of true score variance to total observed score variance. Therefore, a reliability coefficient of .80 directly and without further transformation tells us that 80% of observed score variability reflects true score variability. No squaring or square-rooting is needed.",
        "B": "Incorrect. This option conflates the reliability coefficient with the index of reliability. The index of reliability is the square root of the reliability coefficient (√r) and represents the correlation between observed and true scores. The reliability coefficient itself (r) is the variance ratio, not the index of reliability. Squaring .80 is therefore unnecessary and produces a distorted value.",
        "C": "Incorrect. Taking the square root of the reliability coefficient (√.80 ≈ .894) yields the index of reliability — the theoretical correlation between observed and true scores — not a variance proportion. Interpreting this value as a percentage of true score variance incorrectly applies a transformation that moves away from, not toward, the variance ratio.",
        "D": "Incorrect. Common method variance is a validity threat (a form of construct-irrelevant variance) that can inflate correlations when both measures share the same response format or administration context. While this is a legitimate psychometric concern, it does not change the formal definition of what a reliability coefficient represents. The question asks what the coefficient indicates by definition, not whether that estimate may be inflated."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-016-vignette-L4",
      "source_question_id": "016",
      "source_summary": "When a test has an alternate forms reliability coefficient of .80, 80% of variability in test scores is due to true score variability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "During a staff training seminar on test interpretation, a psychologist presents data from a widely used reading achievement battery that was normed on a large, nationally representative sample. She reports that scores on two independently developed but content-equivalent versions of the battery correlate at .80 with each other in the norming sample. A trainee suggests that this means the test is 'only 64% reliable' because, in the trainee's regression coursework, correlations must be squared before they can be interpreted as meaningful proportions. A second trainee counters that no transformation is needed at all, but cannot articulate why. The psychologist must clarify the apparent contradiction between the two frameworks.",
      "question": "Which of the following best explains why the first trainee's reasoning is incorrect and what the coefficient of .80 actually tells us about score variance?",
      "options": {
        "A": "The first trainee is incorrect because reliability coefficients represent the squared correlation between observed and true scores, meaning .80 already reflects a squared value and squaring it again to .64 would be redundant",
        "B": "The first trainee is incorrect because the reliability coefficient is definitionally equal to the proportion of observed score variance attributable to true score variance, so .80 already represents 80% true score variance without further transformation",
        "C": "The first trainee is incorrect because squaring is appropriate only for split-half reliability coefficients after Spearman-Brown correction, not for alternate forms coefficients, which must be interpreted as raw proportions",
        "D": "The first trainee is incorrect because the .80 coefficient reflects criterion-related validity between the two forms, and validity coefficients — unlike reliability coefficients — are interpreted without squaring"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. In classical test theory, the reliability coefficient is mathematically defined as the ratio of true score variance to total observed score variance (σ²T / σ²X). This is not an arbitrary convention but a derivable result from the model. The trainee's error is applying regression logic (where r² = shared variance) to a quantity that already represents a variance ratio by definition. No squaring is needed or appropriate.",
        "A": "Incorrect. While it is true that the reliability coefficient can be expressed as the square of the index of reliability (r_xx = r²_xT), this does not mean the .80 is 'already squared' in a way that makes squaring redundant. The option is superficially plausible because it involves the squaring relationship, but it misrepresents the logic. The coefficient .80 is a variance ratio, and explaining it as a 'squared value that shouldn't be squared again' is a circular and technically imprecise rationale.",
        "C": "Incorrect. The Spearman-Brown formula is used to correct split-half reliability estimates for the fact that only half the test length was used in the correlation — it is a correction for test length, not a transformation of variance interpretation. Alternate forms reliability and split-half reliability both yield coefficients interpreted identically as variance ratios. There is no differential rule about squaring based on reliability method.",
        "D": "Incorrect. The correlation between alternate forms of the same test is a reliability estimate, not a validity coefficient. Criterion-related validity involves correlating test scores with an external criterion that is conceptually distinct from the test itself. The two forms here are measuring the same construct, making their correlation a reliability estimate. Conflating this with validity misidentifies the nature of the coefficient."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-016-vignette-L5",
      "source_question_id": "016",
      "source_summary": "When a test has an alternate forms reliability coefficient of .80, 80% of variability in test scores is due to true score variability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A psychologist is reviewing a technical report for a new assessment instrument. The report states that two independently constructed but content-equivalent versions of the instrument were administered to 500 adults drawn from the general population, and the scores from the two administrations correlated at .80. A consultant reviewing the same report argues that this number, taken alone, is essentially meaningless without further mathematical processing — specifically, that it must be squared before it can tell us anything useful about how much of the spread in scores is 'real.' The consultant cites as precedent the standard practice in regression analysis of squaring a correlation to determine how much variance in one variable is accounted for by another. The psychologist, however, disagrees with the consultant and believes the value can be interpreted directly.",
      "question": "Who is correct, and what does the .80 value directly indicate about the composition of score variability?",
      "options": {
        "A": "The consultant is correct: squaring the .80 is necessary, and the resulting value of .64 represents the proportion of score variability attributable to stable individual differences, consistent with how shared variance is calculated in regression",
        "B": "The psychologist is correct: the .80 can be interpreted directly as indicating that 80% of the spread in scores reflects genuine differences among individuals, because this type of correlation is by definition equal to a variance ratio, not a bivariate correlation requiring squaring",
        "C": "The psychologist is correct, but only because the sample size of 500 is large enough that the squared value and the unsquared value converge sufficiently to make the distinction trivial for practical interpretation purposes",
        "D": "Both are partially correct: the .80 represents the degree of consistency across administrations, while the squared value (.64) represents the proportion of true score variance, and each answers a different interpretive question about the instrument"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. The consultant's error is applying regression logic to a statistic that is not a bivariate correlation in the ordinary sense. While in regression, r² captures shared variance between two distinct constructs, the reliability coefficient is derived from classical test theory as the ratio of true score variance to total observed variance. Squaring it would produce a distorted, overly conservative figure (.64) that does not correspond to any formally defined quantity in classical test theory.",
        "B": "Correct. The psychologist is correct. The correlation between two content-equivalent forms of the same instrument is an estimate of the reliability coefficient, which is formally defined in classical test theory as the ratio of true score variance to total observed score variance. This means .80 directly and without transformation tells us that 80% of the variability in scores reflects genuine differences among individuals. The consultant's reasoning, though intuitive from a regression perspective, misapplies a different analytical framework.",
        "C": "Incorrect. Sample size affects the precision and stability of a correlation estimate but does not change the mathematical or conceptual framework by which that estimate is interpreted. The psychologist's reasoning is correct because of the definitional properties of reliability coefficients in classical test theory — not because the sample is large enough to make a distinction negligible. This option introduces a plausible-sounding but irrelevant statistical rationale.",
        "D": "Incorrect. This option is seductive because it appears to offer a sophisticated reconciliation of both positions, but it is substantively wrong. The reliability coefficient itself (r = .80) is already the variance ratio — the proportion of true score variance. The squared value (.64) does not represent any standard quantity in classical test theory and has no established interpretive role in reliability analysis. Treating the two figures as answering 'different questions' creates a false equivalence that misrepresents classical test theory."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-005-vignette-L1",
      "source_question_id": "005",
      "source_summary": "To make a statistics test easier, Dr. Haar should remove items with an item difficulty index (p) of .15 and lower and add items with an item difficulty index of .85 and higher.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "item difficulty index",
        "p-value",
        "easier"
      ],
      "vignette": "Dr. Haar has developed a statistics examination for her graduate seminar. After administering it, she calculates the item difficulty index (p-value) for each item and finds that the overall test is too challenging for her students. She consults a colleague about how to revise the test to make it easier. Her colleague advises her to examine which items have very low p-values and which items have very high p-values, then modify the item pool accordingly.",
      "question": "Based on classical item analysis principles, which revision strategy should Dr. Haar follow to make her statistics test easier?",
      "options": {
        "A": "Remove items with p-values of .15 or lower and add items with p-values of .85 or higher.",
        "B": "Remove items with p-values of .85 or higher and add items with p-values of .15 or lower.",
        "C": "Remove items with p-values near .50 and add items with p-values at the extremes of the distribution.",
        "D": "Remove items with negative discrimination indices and replace them with items that have p-values near .50."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. In classical test theory, the item difficulty index (p) represents the proportion of examinees who answered the item correctly; a low p-value (e.g., .15) means few people got the item right (very hard), while a high p-value (e.g., .85) means most people got it right (very easy). Removing hard items and adding easy items will raise the overall test difficulty level in the direction of being easier.",
        "B": "This is incorrect. Removing items with high p-values (e.g., .85) would eliminate the easiest items from the test, and adding items with low p-values (e.g., .15) would introduce very hard items — both changes would make the test more difficult, not easier.",
        "C": "This is incorrect. Items with p-values near .50 are considered moderately difficult and maximally discriminating in classical test theory; removing them would reduce the test's ability to differentiate examinees and would not systematically make the test easier. This strategy conflates optimal discrimination with difficulty adjustment.",
        "D": "This is incorrect. While removing items with negative discrimination indices (which indicate that lower-scoring examinees outperform higher-scoring ones) is good psychometric practice, it does not directly address the overall difficulty level of the test. Items with p-values near .50 are optimal for discrimination but represent moderate difficulty, not the easy items needed to lower the test's difficulty."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-005-vignette-L2",
      "source_question_id": "005",
      "source_summary": "To make a statistics test easier, Dr. Haar should remove items with an item difficulty index (p) of .15 and lower and add items with an item difficulty index of .85 and higher.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "item difficulty",
        "proportion correct"
      ],
      "vignette": "Dr. Haar teaches a first-year graduate statistics course with 30 students, many of whom are international students who report high test anxiety. After scoring the midterm examination, she notes that the class average is only 58%, substantially below her target of 75%. She performs an item analysis and finds that several items show a proportion correct of .12 or lower, while a handful of other items show a proportion correct of .91 or higher. She wants to redesign the item pool for the final exam so that it better matches her students' actual performance level.",
      "question": "Which change to her item pool would most directly address Dr. Haar's goal of producing a less difficult final examination?",
      "options": {
        "A": "Retain only items with proportion correct values near .50, since these items contribute most to test score variance.",
        "B": "Eliminate items with proportion correct values of .12 or lower and introduce new items with proportion correct values of .91 or higher.",
        "C": "Eliminate items with proportion correct values of .91 or higher and introduce new items with proportion correct values of .12 or lower.",
        "D": "Retain the existing items but increase the number of answer choices per item to reduce the probability of guessing correctly."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Removing items that very few students answer correctly (proportion correct ≤ .12) eliminates the most difficult items, and adding items that most students answer correctly (proportion correct ≥ .91) shifts the overall difficulty distribution downward, making the test easier and more aligned with the students' demonstrated knowledge level.",
        "A": "This is incorrect. Items with proportion correct values near .50 provide the most discrimination between high and low scorers, but retaining only those items addresses test reliability and discrimination rather than overall difficulty. The test would remain moderately difficult — not easier — under this strategy.",
        "C": "This is incorrect. Removing high p-value items eliminates the easiest questions, and adding low p-value items introduces the hardest questions. This strategy would make the examination substantially more difficult, the opposite of Dr. Haar's goal.",
        "D": "This is incorrect. Increasing the number of answer choices per item reduces the probability that guessing alone produces a correct answer, which effectively makes items harder rather than easier. This approach addresses guessing behavior rather than actual item difficulty as measured by the proportion of students answering correctly."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-005-vignette-L3",
      "source_question_id": "005",
      "source_summary": "To make a statistics test easier, Dr. Haar should remove items with an item difficulty index (p) of .15 and lower and add items with an item difficulty index of .85 and higher.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "proportion"
      ],
      "vignette": "Dr. Haar reviews the results from her graduate statistics midterm and is troubled by two findings: the class mean was considerably lower than expected, and several of her best students performed inconsistently across item subsets. A measurement consultant reviews her data and notes that many items show a very low proportion of students answering them correctly, which she attributes partly to ambiguous wording. The consultant also observes that a few items were answered correctly by nearly the entire class. Concerned primarily about the overall difficulty level of the test, Dr. Haar asks the consultant what item-level changes would make the exam more accessible to students.",
      "question": "Which recommendation most directly addresses Dr. Haar's concern about overall test difficulty?",
      "options": {
        "A": "Remove items with very low proportions of correct responses and add items with very high proportions of correct responses.",
        "B": "Remove items with ambiguous wording and replace them with items that show high item-total correlations.",
        "C": "Remove items with very high proportions of correct responses because they do not contribute meaningfully to test score variance.",
        "D": "Improve item wording for poorly performing items so that their item-total correlations increase before making any changes to the item pool."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. Removing items that very few students answer correctly and replacing them with items that most students answer correctly directly reduces overall test difficulty. The proportion of students answering an item correctly is the item difficulty index (p), and this strategy shifts the p-value distribution upward, making the exam easier.",
        "B": "This is incorrect. While improving item wording to reduce ambiguity is good test construction practice, focusing on item-total correlations addresses item discrimination rather than overall test difficulty. High item-total correlations indicate that an item differentiates high and low scorers, but this is independent of whether the test is too hard.",
        "C": "This is incorrect. Removing items with very high proportions of correct responses would eliminate the easiest items from the test, making the exam harder rather than easier. Although these items contribute little to variance, removing them is the opposite of the strategy needed to lower difficulty.",
        "D": "This is incorrect. Revising item wording to improve item-total correlations is a strategy for enhancing internal consistency and discrimination, not for directly reducing test difficulty. Even if rewritten items become clearer, their p-values may not shift sufficiently to make the test noticeably easier, and the strategy does not address adding easier content."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-005-vignette-L4",
      "source_question_id": "005",
      "source_summary": "To make a statistics test easier, Dr. Haar should remove items with an item difficulty index (p) of .15 and lower and add items with an item difficulty index of .85 and higher.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "classical test theory"
      ],
      "vignette": "After reviewing her graduate statistics midterm data, Dr. Haar notices that most students clustered near the bottom of the score distribution, and the items that generated the most inter-student variability tended to be ones that roughly half the class answered correctly. A classical test theory analysis reveals that many items in the test have indices in the range of .10–.18, while a small number of items have indices above .80. Her department chair suggests she focus revision efforts on items that are not doing a good job of separating high and low performers. Dr. Haar, however, is less interested in improving discrimination and more interested in ensuring that the overall test better matches her students' current competence level.",
      "question": "Given Dr. Haar's goal of producing a less difficult examination, which revision strategy is most consistent with item analysis principles?",
      "options": {
        "A": "Focus revisions on items with indices in the .10–.18 range by rewriting them so they better discriminate between high and low performers.",
        "B": "Remove items with indices in the .10–.18 range and add items with indices above .80, prioritizing difficulty adjustment over discrimination.",
        "C": "Retain items with indices near .50, as these maximize score variance and therefore support the most meaningful score interpretation.",
        "D": "Remove items with indices above .80 because near-universal correct responses indicate those items are flawed or trivially easy and do not contribute psychometric value."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The indices described — in the .10–.18 range and above .80 — are item difficulty indices (p-values) in classical test theory. Items with very low p-values are very hard (few correct), and items with very high p-values are very easy (most correct). Removing the hardest items and adding the easiest items directly lowers overall test difficulty, which is Dr. Haar's explicit goal.",
        "A": "This is incorrect. Rewriting hard items to improve their discrimination between high and low scorers addresses item-total correlations or discrimination indices, not item difficulty. Items can discriminate well while remaining very difficult; this strategy would not necessarily make the test easier.",
        "C": "This is incorrect. Retaining only items with indices near .50 would preserve items of moderate difficulty, which neither lowers nor raises the overall difficulty substantially. While these items maximize score variance and are preferred for norm-referenced discrimination, this strategy does not achieve the goal of making the test easier.",
        "D": "This is incorrect. Removing items with very high p-values (above .80) eliminates the easiest items from the examination, which would increase rather than decrease overall difficulty. Although items with p-values approaching 1.0 contribute minimally to score variance, removing them is counterproductive when the goal is to make the test less difficult."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-005-vignette-L5",
      "source_question_id": "005",
      "source_summary": "To make a statistics test easier, Dr. Haar should remove items with an item difficulty index (p) of .15 and lower and add items with an item difficulty index of .85 and higher.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "After her graduate students completed a midterm examination, Dr. Haar calculated a number between 0 and 1 for every question by dividing the number of students who got that question right by the total number of students who took the test. She found that a large cluster of questions produced values in the range of .08 to .17, and a smaller group of questions produced values between .83 and .94. Her department chair, concerned about student morale, encouraged her to investigate whether the exam's internal consistency could account for the poor class performance. Dr. Haar appreciated the suggestion but explained that she was focused on a different issue: she wanted to revise the question pool so that future students would be more likely to demonstrate what they had learned rather than to be stumped by the examination itself.",
      "question": "Which revision strategy is most consistent with Dr. Haar's stated goal?",
      "options": {
        "A": "Calculate the correlation between each question's scores and the total test score, then remove questions with correlations below .30.",
        "B": "Remove questions with values in the .08–.17 range and add new questions likely to produce values in the .83–.94 range.",
        "C": "Administer the exam twice to the same group and compare question-level performance across administrations to identify unstable items.",
        "D": "Remove questions with values in the .83–.94 range because these questions provide little information about differences among students."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The values Dr. Haar calculated — the proportion of students answering each question correctly — are item difficulty indices (p-values). Questions with p-values in the .08–.17 range are extremely difficult (few students succeeded), and questions with p-values in the .83–.94 range are very easy (almost all students succeeded). Removing the hardest questions and adding the easiest ones shifts the overall difficulty distribution downward, making the exam more accessible — exactly what Dr. Haar wants.",
        "A": "This is incorrect. Correlating each question's scores with total test scores produces item-total correlations, which reflect item discrimination rather than item difficulty. Removing low-discrimination items would improve internal consistency and the test's ability to differentiate high and low scorers but would not systematically make the test easier for students who are struggling.",
        "C": "This is incorrect. Comparing question-level performance across two administrations is a method for assessing test-retest reliability or item stability over time. While useful for identifying inconsistent items, this approach does not address the proportion of students answering questions correctly and would not directly result in a less difficult examination.",
        "D": "This is incorrect. While questions that nearly everyone answers correctly (high p-values) do contribute minimally to score variance and discrimination, removing them would eliminate the easiest questions from the test, making it harder rather than easier. This reasoning applies to maximizing discrimination, not to achieving Dr. Haar's goal of better matching the exam to students' competence level."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-117-vignette-L1",
      "source_question_id": "117",
      "source_summary": "Shrinkage is associated with cross-validation and refers to the fact that a validity coefficient is likely to be smaller than the original coefficient when the predictor(s) and criterion are administered to another sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "shrinkage",
        "cross-validation",
        "validity coefficient"
      ],
      "vignette": "A research team develops a battery of cognitive and personality tests to predict managerial performance ratings. In their initial development sample, the battery yields an impressive validity coefficient of .65. To evaluate how well this value will generalize, they administer the same battery and criterion measure to a new, independent sample and compute the correlation again. The team observes that the validity coefficient in the new sample drops to .41 and notes that this outcome was expected given the statistical properties of the original solution.",
      "question": "Which psychometric phenomenon best explains the drop in the validity coefficient observed when the battery was applied to the new sample?",
      "options": {
        "A": "Restriction of range, which occurs when the variability of scores in the new sample is narrower than in the development sample, artificially lowering the observed correlation.",
        "B": "Shrinkage, which refers to the expected decrease in a validity coefficient when a regression-based prediction equation or predictor composite is applied to a new, independent sample during cross-validation.",
        "C": "Criterion contamination, which occurs when the criterion measure is influenced by knowledge of predictor scores, inflating the original coefficient and making replication impossible.",
        "D": "Regression to the mean, which describes the tendency for extreme scores on a first measurement to move closer to the group mean on a second measurement, reducing observed correlations."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Restriction of range is a genuine threat to validity coefficients and can reduce correlations in replication samples, but it refers specifically to limited score variance in the sample rather than to the statistical capitalization on chance inherent in the original development process. The vignette emphasizes the expected, predictable nature of the drop, which is the hallmark of shrinkage.",
        "B": "Shrinkage is the correct answer. When a regression equation or composite weights are derived from a development sample, they capitalize on chance variation specific to that sample. When the equation is applied to a new sample through cross-validation, the coefficient predictably decreases because those chance elements do not replicate, a phenomenon definitionally called shrinkage.",
        "C": "Criterion contamination refers to the criterion being influenced by awareness of predictor scores, which artificially inflates the original coefficient. While this could make replication difficult, it is a bias in the criterion rather than a statistical property of generalizing regression weights, so it does not fit the scenario as described.",
        "D": "Regression to the mean describes a statistical tendency for extreme scorers to score closer to the average on a subsequent occasion. Although it can affect observed scores across administrations, it is not the term used to describe the expected decline in a validity coefficient when cross-validated on a new sample, making it an incorrect fit here."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-117-vignette-L2",
      "source_question_id": "117",
      "source_summary": "Shrinkage is associated with cross-validation and refers to the fact that a validity coefficient is likely to be smaller than the original coefficient when the predictor(s) and criterion are administered to another sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "cross-validation",
        "validity"
      ],
      "vignette": "A graduate student at a mid-sized university develops a structured interview protocol designed to predict first-year GPA among doctoral applicants. Using a sample of 80 recently admitted students, she derives optimal scoring weights for each interview dimension and obtains a correlation of .58 between total interview scores and first-year GPA. Notably, the development sample included a high proportion of students from competitive research institutions. When she administers the same protocol to the next incoming cohort of 75 students and recomputes the correlation, it is only .37, and she is surprised by the magnitude of the decline.",
      "question": "The decline in the validity correlation from the development sample to the new cohort is most likely a direct illustration of which phenomenon?",
      "options": {
        "A": "Predictive bias, which occurs when a test systematically over- or underpredicts criterion performance for a particular subgroup, producing different regression slopes or intercepts across groups.",
        "B": "Shrinkage, which refers to the expected reduction in a validity coefficient that occurs when a prediction equation developed in one sample is applied to a new, independent sample.",
        "C": "Restriction of range, which occurs when the range of scores on the predictor or criterion in the new cohort is narrower than in the development sample, suppressing the observed correlation.",
        "D": "Concurrent validity attenuation, which occurs when a short time interval between predictor and criterion administration reduces the observed correlation by conflating prediction with simultaneous measurement."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Predictive bias refers to differential prediction across subgroups, typically examined by comparing regression lines for different demographic or background groups. Although the development sample skewed toward competitive-institution graduates — a detail that could suggest a subgroup issue — the vignette does not compare prediction accuracy across subgroups, making predictive bias an incorrect fit.",
        "B": "Shrinkage is correct. The development of optimal scoring weights in one sample capitalizes on chance covariance specific to that sample. When these weights are applied to a new, independent sample, the correlation predictably decreases. The student's surprise is a narrative cue emphasizing that the drop was not anticipated, yet from a psychometric standpoint shrinkage is the expected outcome.",
        "C": "Restriction of range is a plausible alternative because the vignette mentions the development sample disproportionately included students from competitive institutions, which could suggest range differences. However, restriction of range refers to limited score variance in a sample, not to the general decline in coefficients resulting from applying regression weights to a new sample. The vignette points to a cross-sample generalization issue rather than a variance problem.",
        "D": "Concurrent validity attenuation is not a standard psychometric term. Attenuation of validity coefficients due to measurement error is a real concept, but it does not specifically address the decrease in a coefficient when the same prediction equation is applied to a new sample. This option conflates time-of-administration issues with the cross-validation process described."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-117-vignette-L3",
      "source_question_id": "117",
      "source_summary": "Shrinkage is associated with cross-validation and refers to the fact that a validity coefficient is likely to be smaller than the original coefficient when the predictor(s) and criterion are administered to another sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "cross-validation"
      ],
      "vignette": "A consulting firm develops a selection battery consisting of a cognitive ability test and a conscientiousness inventory to predict sales performance ratings for a large retail chain. After administering the battery to 200 current employees who also received performance ratings, the firm derives a multiple regression equation that yields a multiple R of .61. The psychometrician notes that the development sample happened to include an unusually high number of top performers, a fact that some colleagues believe could account for future discrepancies. However, when the battery and criterion are administered to a new sample of 180 employees across different regional locations and the regression equation is applied, the observed correlation is only .42. The psychometrician explains this outcome as mathematically expected rather than indicative of any flaw in the battery itself.",
      "question": "The psychometrician's assertion that the observed decline is 'mathematically expected' most specifically refers to which phenomenon?",
      "options": {
        "A": "Restriction of range, because the disproportionate representation of high performers in the original sample narrowed score variability, and correcting for this in the new sample produced a smaller observed correlation.",
        "B": "Regression to the mean, because extreme values in the predictor composite in the original sample would be expected to yield less extreme scores in the new sample, pulling the correlation toward zero.",
        "C": "Shrinkage, because regression weights derived from one sample capitalize on chance variance unique to that sample, and when those weights are applied during cross-validation to a new sample, the multiple R predictably decreases.",
        "D": "Differential item functioning, because items on the cognitive ability test may have functioned differently for employees in the new regional locations, reducing the predictive relationship with performance ratings."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Restriction of range is a tempting distractor because the vignette deliberately mentions that the development sample overrepresented top performers, which could narrow predictor score variance and suppress correlations. However, restriction of range produces a lower coefficient in the restricted sample itself rather than explaining a decline when weights derived from one sample are generalized to another. The psychometrician's framing of the drop as 'mathematically expected' specifically refers to the properties of cross-validation, not variance restriction.",
        "B": "Regression to the mean describes a statistical phenomenon in which extreme scores on one measurement tend to be less extreme on a subsequent measurement due to measurement error. While it affects score distributions across measurement occasions, it does not specifically explain why a regression-derived validity coefficient would be expected to shrink when applied to a new sample, making this option incorrect.",
        "C": "Shrinkage is correct. Multiple regression equations are derived by optimally weighting predictors for a specific sample, inevitably capitalizing on random, sample-specific variance. When those weights are applied to a new, independent sample through cross-validation, the multiple R predictably declines. The psychometrician's characterization of this drop as 'mathematically expected' directly reflects the known statistical property of shrinkage.",
        "D": "Differential item functioning (DIF) occurs when items function differently across subgroups defined by characteristics such as geographic location or demographic background, which is superficially relevant given that the new sample came from different regional locations. However, DIF is an item-level analysis concept unrelated to the generalization of regression-derived prediction equations across samples, so it does not explain the observed validity coefficient decline."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-117-vignette-L4",
      "source_question_id": "117",
      "source_summary": "Shrinkage is associated with cross-validation and refers to the fact that a validity coefficient is likely to be smaller than the original coefficient when the predictor(s) and criterion are administered to another sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "capitalize"
      ],
      "vignette": "A research team at a health system uses data from 300 patients to build a composite score from six psychological questionnaires intended to forecast treatment dropout within six months. The resulting composite performs impressively in the derivation data, with a correlation of .67 with the dropout criterion. A senior methodologist cautions that the composite was optimized to capitalize on patterns in that specific dataset, patterns that will not fully reproduce in new patients. When 250 new patients are enrolled and the same composite is computed using identical scoring rules, the correlation with dropout falls to .44. A junior researcher suggests this drop occurred because the new patients were recruited from a different clinic with a somewhat more homogeneous population, and the team debates which explanation is correct.",
      "question": "The senior methodologist's concern most precisely describes which psychometric phenomenon, and why does it better account for the observed outcome than the junior researcher's alternative?",
      "options": {
        "A": "Restriction of range, because the more homogeneous new clinic population reduced variance in the composite scores, directly attenuating the observed correlation with dropout in the replication sample.",
        "B": "Predictive validity decay, because the longer the interval between scale development and replication, the greater the drift in the underlying constructs, reducing the composite's forecasting power.",
        "C": "Shrinkage, because the composite weights were derived by optimizing fit to chance-level covariance in the original sample, and this sample-specific optimization does not generalize fully to new patients regardless of population differences.",
        "D": "Attenuation due to measurement error, because unreliability in the six questionnaires reduced the true-score correlation between the composite and dropout, and this effect was more pronounced in the new, less variable sample."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Restriction of range is the explanation favored by the junior researcher and is a highly defensible distractor: if the new clinic's population is more homogeneous, score variance may be restricted, which would suppress correlations. However, restriction of range specifically requires that the variance of scores in the replication sample be demonstrably narrower, and it does not account for the optimization of composite weights to chance patterns in the original dataset — which is what the senior methodologist's concern describes. Shrinkage would occur even without any difference in population homogeneity.",
        "B": "Predictive validity decay (sometimes called criterion-related validity drift) refers to the reduction in a predictor's validity over time as the construct-criterion relationship changes. While plausible in longitudinal contexts, the vignette does not indicate a long time gap between development and replication, and the methodologist frames the concern as a property of how the composite was derived, not a temporal change in the constructs.",
        "C": "Shrinkage is correct. The senior methodologist specifically warns that the composite was optimized to capitalize on chance-level patterns in the derivation sample — this is the definitional mechanism of shrinkage. Composite or regression weights fit the idiosyncratic variance of one sample, and when scoring rules are applied identically to a new sample, the correlation predictably drops. This explanation holds regardless of whether the new sample is more or less homogeneous, which is why it better accounts for the finding than restriction of range.",
        "D": "Attenuation due to measurement error is a real validity concern: unreliable measures produce lower observed correlations with criteria. However, measurement error is a property of the instruments themselves and would affect both the original and replication samples comparably. The vignette frames the drop as linked to how the composite was built and generalized, not to the reliability of the questionnaires, so this explanation does not capture the specific mechanism described."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-117-vignette-L5",
      "source_question_id": "117",
      "source_summary": "Shrinkage is associated with cross-validation and refers to the fact that a validity coefficient is likely to be smaller than the original coefficient when the predictor(s) and criterion are administered to another sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of researchers uses a large group of volunteers to identify the best combination of scores from several questionnaires for predicting which individuals will complete a twelve-week wellness program. After extensive analysis, they settle on a formula that assigns different weights to each questionnaire and produces a single summary number; in this original group, the relationship between this number and actual program completion is quite strong. A colleague recommends applying the exact same formula — with the same weights — to a group of participants from a different city. When they do so, the relationship between the formula-derived number and program completion is noticeably weaker, even though both groups appear demographically similar and the same questionnaires were used. The first team is puzzled, but the colleague says this outcome was entirely predictable given how the formula was built.",
      "question": "The colleague's prediction that this outcome was 'entirely predictable given how the formula was built' most specifically refers to which psychometric phenomenon?",
      "options": {
        "A": "Restriction of range, because the second group from a different city likely had less variability in their questionnaire scores, which would reduce the strength of the observed relationship with program completion.",
        "B": "Shrinkage, because the formula weights were optimized to fit incidental patterns in the first group's data, and those sample-specific patterns do not fully replicate when the same formula is applied to an independent group.",
        "C": "Regression to the mean, because participants with extreme predicted-completion scores in the first group would be expected to have less extreme actual scores in a new group, pulling the observed relationship toward zero.",
        "D": "Criterion unreliability, because program completion is a binary outcome that may be measured inconsistently across different city locations, producing a weaker observed relationship in the replication group."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Restriction of range is a strong distractor because the vignette mentions that the second group comes from a different city, which could plausibly suggest a different distribution of questionnaire scores. However, the vignette explicitly states both groups appear demographically similar, which undermines the restriction-of-range explanation. More importantly, restriction of range refers to limited score variance in a sample, whereas the colleague's comment points to something inherent in how the formula itself was constructed — which is the signature of shrinkage.",
        "B": "Shrinkage is correct. When a weighting formula is developed by optimizing the relationship between a composite and a criterion in one dataset, the weights inevitably capture some chance-level covariance unique to that sample. When those same weights are applied without modification to a new, independent sample, the observed relationship predictably weakens because the chance patterns do not generalize. The colleague's statement that the result was 'entirely predictable given how the formula was built' is the key cue pointing to this mechanism rather than to a property of the second sample.",
        "C": "Regression to the mean describes the statistical tendency for scores that are extreme on one occasion to be less extreme on a second occasion, due to measurement error rather than true change. While this can reduce apparent relationships across measurement occasions, it does not specifically explain why a formula derived from one group would yield a weaker relationship in a new group. The phenomenon here is about formula generalization, not score extremity.",
        "D": "Criterion unreliability is a plausible concern because binary outcomes like program completion can be subject to inconsistent recording, and a different city location might introduce criterion measurement differences. However, the vignette does not indicate any inconsistency in how completion was recorded, and the colleague attributes the predictable weakness to the formula's construction rather than to criterion measurement problems, making shrinkage the more precise and appropriate explanation."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-027-vignette-L1",
      "source_question_id": "027",
      "source_summary": "Kuder-Richardson Formula 20 (KR-20) can be used to estimate a test's internal consistency reliability when test items are scored dichotomously.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "dichotomously scored",
        "internal consistency",
        "KR-20"
      ],
      "vignette": "A school psychologist develops a 40-item knowledge quiz in which every item is scored dichotomously (correct = 1, incorrect = 0). She wants to estimate the test's internal consistency using a single administration. A colleague recommends computing KR-20, noting that this formula is specifically designed for dichotomously scored items and provides an estimate of internal consistency reliability.",
      "question": "Which reliability estimation method is most appropriate in this situation, and what is its primary purpose?",
      "options": {
        "A": "Split-half reliability corrected with the Spearman-Brown formula, which estimates internal consistency by correlating scores from two halves of the test and then adjusting for test length.",
        "B": "Test-retest reliability, which measures the stability of scores across two separate administrations of the same test and reflects temporal consistency rather than item homogeneity.",
        "C": "Kuder-Richardson Formula 20 (KR-20), which estimates internal consistency reliability specifically for tests composed of dichotomously scored items using a single administration.",
        "D": "Coefficient alpha (Cronbach's alpha), which estimates internal consistency for items scored on a continuous or polytomous scale rather than strictly dichotomous right/wrong scoring."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Split-half reliability does measure internal consistency and requires only one administration, but it arbitrarily divides the test into two halves and may yield different estimates depending on how the split is made. KR-20 is preferred because it considers all possible item pairings simultaneously for dichotomous items.",
        "B": "Test-retest reliability assesses score stability over time by correlating scores from two administrations. The scenario specifies a single administration and interest in item homogeneity, which is internal consistency — not temporal stability.",
        "C": "Correct. KR-20 is specifically designed to estimate internal consistency reliability for tests composed entirely of dichotomously scored items (0 or 1) using data from a single administration, making it the optimal choice here.",
        "D": "Coefficient alpha is a generalization of KR-20 that applies to items scored on continuous or polytomous (multi-point) scales. When items are strictly dichotomous, KR-20 is the appropriate and more specific formula; alpha would be mathematically equivalent in this case but KR-20 is the named standard for dichotomous items."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-027-vignette-L2",
      "source_question_id": "027",
      "source_summary": "Kuder-Richardson Formula 20 (KR-20) can be used to estimate a test's internal consistency reliability when test items are scored dichotomously.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "dichotomous",
        "internal consistency"
      ],
      "vignette": "A researcher constructs a 30-item screening tool for identifying cognitive impairment in elderly patients. Each item is answered as either 'pass' or 'fail,' yielding a dichotomous score. Although the tool will eventually be validated against neuropsychological gold-standard measures, the researcher's immediate goal is to examine internal consistency from a single testing session. She notes that several items appear to vary in difficulty, which she finds noteworthy but not necessarily problematic at this stage.",
      "question": "Which statistical method should the researcher use to estimate the reliability of this screening tool?",
      "options": {
        "A": "Kuder-Richardson Formula 20 (KR-20), because it computes internal consistency reliability for dichotomously scored items in a single administration, taking item difficulty variability into account.",
        "B": "Cronbach's alpha, because it is the most commonly used measure of internal consistency and is appropriate regardless of whether items are scored dichotomously or on a multi-point scale.",
        "C": "Parallel forms reliability, because the researcher has two sets of items (pass/fail categories) that can be treated as equivalent forms and correlated to yield a reliability estimate.",
        "D": "Concurrent validity coefficient, because the researcher plans to compare the screening tool with an established neuropsychological measure, which is the most direct index of whether the tool is measuring what it is supposed to measure."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. KR-20 is the appropriate method because the items are scored dichotomously and the researcher wants internal consistency from a single administration. The formula explicitly incorporates item difficulty (p-values) across items, making it well-suited even when item difficulties vary.",
        "B": "Cronbach's alpha is mathematically equivalent to KR-20 when items are dichotomous, but it is technically designed for polytomous or continuous item scores. In measurement contexts, KR-20 is the standard named formula for dichotomous items, and selecting alpha over KR-20 reflects a lack of precision about the appropriate tool.",
        "C": "Parallel forms reliability requires two independently constructed but equivalent test forms administered either simultaneously or across occasions. 'Pass' and 'fail' are outcome categories for a single item, not separate test forms — this option misapplies the concept.",
        "D": "Concurrent validity assesses the relationship between the new measure and an established criterion measured at roughly the same time. While the researcher plans future validity work, the current goal is reliability estimation — specifically internal consistency — not validity."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-027-vignette-L3",
      "source_question_id": "027",
      "source_summary": "Kuder-Richardson Formula 20 (KR-20) can be used to estimate a test's internal consistency reliability when test items are scored dichotomously.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "item difficulty"
      ],
      "vignette": "A test developer creates a civil service examination consisting of 50 multiple-choice questions, each scored as simply correct or incorrect. After a pilot administration, she calculates item difficulty indices for each question and observes considerable variability across items — some are answered correctly by 90% of examinees, while others are answered correctly by only 20%. She wishes to produce a single summary coefficient that reflects how consistently the items on this test are measuring the same underlying construct, without administering the test a second time. A colleague suggests using a formula that directly incorporates each item's difficulty index into the reliability calculation.",
      "question": "Which reliability coefficient is most consistent with the colleague's recommendation and the characteristics of this test?",
      "options": {
        "A": "Coefficient alpha (Cronbach's alpha), because it is a general internal consistency formula that can be applied to any item format, including correct/incorrect scored items with varying difficulty levels.",
        "B": "Split-half reliability with Spearman-Brown correction, because dividing the 50-item test into two 25-item halves and correlating those halves yields an internal consistency estimate that accounts for the full test length.",
        "C": "Kuder-Richardson Formula 20 (KR-20), because it is the reliability formula specifically designed for dichotomously scored items that explicitly incorporates each item's difficulty index (proportion correct) into the computation.",
        "D": "Test-retest reliability, because the variability in item difficulty across administrations is best captured by examining how stable total scores are when the same examinees complete the test on two separate occasions."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Cronbach's alpha measures internal consistency and can technically be applied to dichotomous items (yielding results equivalent to KR-20), but it does not explicitly incorporate item difficulty indices into its formula. The colleague specifically referenced a formula using item difficulty, which precisely identifies KR-20, not alpha.",
        "B": "Split-half reliability does estimate internal consistency from a single administration and uses the Spearman-Brown correction to adjust for reduced test length, but it does not directly incorporate individual item difficulty values into the calculation. The colleague's recommendation to use a formula built around item difficulty points specifically to KR-20.",
        "C": "Correct. KR-20 is specifically developed for dichotomously scored items and its formula explicitly includes the proportion of examinees answering each item correctly (p) and incorrectly (q = 1 − p) — precisely the item difficulty index. It provides an internal consistency estimate from a single administration.",
        "D": "Test-retest reliability assesses temporal stability and requires two administrations. Variability in item difficulty across examinees is not the same as score instability across time, and the scenario explicitly calls for a single-administration approach."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-027-vignette-L4",
      "source_question_id": "027",
      "source_summary": "Kuder-Richardson Formula 20 (KR-20) can be used to estimate a test's internal consistency reliability when test items are scored dichotomously.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "proportion correct"
      ],
      "vignette": "A psychometrician is evaluating a newly developed occupational aptitude battery in which each of the 45 items yields a binary outcome. She notes that the proportion correct for individual items ranges from .15 to .85 across the standardization sample, reflecting heterogeneous item characteristics. She computes a reliability index using a formula that requires only the number of items, the variance of total test scores, and the product of each item's proportion correct multiplied by its proportion incorrect — values she derives entirely from the single pilot dataset. The coefficient she obtains is .82, which she reports as the primary evidence that the battery's items are measuring a coherent underlying dimension.",
      "question": "Which reliability method did the psychometrician most likely use?",
      "options": {
        "A": "Cronbach's coefficient alpha, because it requires only item variances and total score variance from a single administration and is the most widely reported index of internal consistency across all item formats.",
        "B": "Kuder-Richardson Formula 20 (KR-20), because it requires the number of items, total score variance, and each item's proportion-correct multiplied by proportion-incorrect — matching exactly the inputs described — and is designed for dichotomously scored items.",
        "C": "Kuder-Richardson Formula 21 (KR-21), because it also uses proportion-correct values across items and is computationally simpler, requiring only the mean and variance of total scores rather than individual item statistics.",
        "D": "Split-half reliability with Spearman-Brown correction, because dividing the items into two subsets, computing their correlation, and correcting for length also requires only a single dataset and yields an internal consistency coefficient."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The vignette precisely describes the KR-20 formula: it requires k (number of items), the variance of total scores, and the sum of p×q values (proportion correct × proportion incorrect) for each item — all derivable from a single administration of a binary-scored test. The reported coefficient reflects internal consistency.",
        "A": "Cronbach's alpha computes internal consistency using item variances and total score variance, not the product of proportion-correct and proportion-incorrect for each item. Although alpha is mathematically equivalent to KR-20 for dichotomous items, the specific inputs described (p×q per item) uniquely identify KR-20.",
        "C": "KR-21 is a simplified version of KR-20 that assumes all items have equal difficulty, substituting the overall mean for individual item p-values. It does not require computing each item's proportion correct individually. The vignette explicitly describes using each item's individual proportion correct and incorrect, ruling out KR-21.",
        "D": "Split-half reliability with Spearman-Brown correction does use a single dataset but involves splitting items into two halves and computing the correlation between half-scores, then applying the correction formula. It does not require or use the product of each item's proportion correct and proportion incorrect."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-027-vignette-L5",
      "source_question_id": "027",
      "source_summary": "Kuder-Richardson Formula 20 (KR-20) can be used to estimate a test's internal consistency reliability when test items are scored dichotomously.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A measurement specialist is asked to evaluate a new 60-item assessment used in a corporate hiring context. Each question has a single objectively verifiable correct answer, and applicants receive one point for a correct response and zero for an incorrect one. Using data gathered during a single administration to 300 job applicants, the specialist calculates a summary number by taking into account how many questions there are, how spread out the total scores are across all applicants, and — for every individual question — the fraction of people who got it right multiplied by the fraction who got it wrong. The resulting value of .79 is presented to management as evidence that the questions on the test are pulling together in the same direction. Management, impressed by the figure, asks whether a different calculation using only the average score and the total score spread would have been easier to perform and would have yielded the same value; the specialist says no.",
      "question": "Which psychometric procedure did the specialist use to generate the coefficient of .79?",
      "options": {
        "A": "Cronbach's coefficient alpha, because it is computed from a single administration using item-level variances and total score variance, and is widely regarded as the standard internal consistency estimate for assessments in applied settings.",
        "B": "Kuder-Richardson Formula 20 (KR-20), because it is derived from a single administration using the number of items, the total score variance, and the per-item product of proportion correct and proportion incorrect, and it specifically applies when every item is scored as right or wrong.",
        "C": "Kuder-Richardson Formula 21 (KR-21), because it uses only the number of items, the mean total score, and total score variance — making it computationally simpler — and the specialist's description matches this reduced set of inputs.",
        "D": "Split-half reliability with Spearman-Brown correction, because dividing the 60 items into two sets of 30, correlating total scores from each half, and then correcting for test length also uses a single dataset and produces an internal consistency estimate."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The specialist's procedure matches KR-20 precisely: a single administration, binary scoring (1 or 0), and a formula incorporating k (number of items), total score variance, and the sum of p×q for each individual item. Crucially, the specialist tells management that using only the mean and total spread would yield a different (typically lower) value — this rules out KR-21, which uses that simpler input set and is a lower-bound approximation of KR-20.",
        "A": "Cronbach's alpha uses item variances (not proportion-correct × proportion-incorrect explicitly) and total score variance. While it is mathematically equivalent to KR-20 for binary items, the description of computing each item's fraction-correct times fraction-incorrect specifically names KR-20's formula. Additionally, alpha is typically associated with polytomous scales, making KR-20 the more precise identification here.",
        "C": "KR-21 is a simplified approximation that requires only the mean total score and total score variance, assuming equal item difficulties. The specialist explicitly states that using only the mean and total spread would not yield the same value — this directly contradicts the use of KR-21, which is defined by that simpler computation and tends to underestimate KR-20 when item difficulties vary.",
        "D": "Split-half reliability divides items into two groups, correlates the half-test scores, and applies the Spearman-Brown correction. It does not involve computing a per-item product of correct and incorrect proportions for every question, and the Spearman-Brown formula's inputs differ substantially from those described by the specialist."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-082-vignette-L1",
      "source_question_id": "082",
      "source_summary": "When using a selection test to estimate future job performance, the test should have adequate predictive validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "predictive validity",
        "job performance",
        "selection test"
      ],
      "vignette": "A large manufacturing company wants to adopt a new cognitive ability assessment for hiring entry-level workers. After administering the selection test to applicants, the company waits twelve months and then correlates each hired employee's test score with their supervisor-rated job performance. The psychologist overseeing the study explains that this longitudinal correlation coefficient is the primary index used to evaluate whether the measure is appropriate for personnel decisions. She notes that higher values of this coefficient indicate that the selection test does a better job of forecasting future outcomes.",
      "question": "Which psychometric property is the psychologist evaluating when she correlates selection test scores with supervisor-rated job performance collected one year later?",
      "options": {
        "A": "Concurrent validity",
        "B": "Predictive validity",
        "C": "Content validity",
        "D": "Construct validity"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Concurrent validity is also a form of criterion-related validity, but it is assessed when the test scores and criterion measures are collected at approximately the same point in time. Here, there is a twelve-month gap between assessment and criterion collection, which distinguishes this design as predictive rather than concurrent.",
        "B": "Predictive validity is the correct answer. It is established by administering a test to individuals and then correlating those scores with a criterion measure (job performance) obtained at a meaningful future time point, which is precisely the design described in the vignette.",
        "C": "Content validity refers to the systematic evaluation of whether a test's items adequately represent the domain of interest. It does not involve correlating test scores with an external criterion, so it does not fit this longitudinal, criterion-focused design.",
        "D": "Construct validity refers to the degree to which a test measures the theoretical construct it purports to measure, typically evaluated through patterns of convergent and discriminant evidence. While construct validity is broader, the specific method described — correlating scores with a future criterion — is the defining procedure for predictive validity."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-082-vignette-L2",
      "source_question_id": "082",
      "source_summary": "When using a selection test to estimate future job performance, the test should have adequate predictive validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "criterion-related validity",
        "shrinkage"
      ],
      "vignette": "A regional hospital system develops a structured interview protocol to screen registered nurse applicants. The protocol includes ratings of communication skill, clinical reasoning, and professionalism. After hiring 120 nurses and collecting performance evaluations eighteen months later, the psychometrician cross-validates the interview scores in a second independent sample and notices that the validity coefficient drops notably compared to the original derivation sample. Several administrators argue the drop reflects poor interview design, but the psychometrician explains it is an expected statistical artifact. Notably, all nurses in both samples had already been pre-screened with a minimum GPA requirement before receiving the interview.",
      "question": "The drop in the validity coefficient from the derivation sample to the cross-validation sample is best explained by which psychometric phenomenon?",
      "options": {
        "A": "Restriction of range",
        "B": "Attenuation due to unreliability",
        "C": "Shrinkage",
        "D": "Regression to the mean"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Restriction of range occurs when the range of scores on a predictor or criterion is artificially narrowed — as might happen here because the GPA pre-screen limits who receives the interview — and this can suppress validity coefficients. However, restriction of range would affect both samples similarly and does not specifically explain why the coefficient is higher in the derivation sample than in the cross-validation sample. The vignette points to the cross-validation drop specifically, which is shrinkage.",
        "B": "Attenuation due to unreliability refers to the reduction in an observed validity coefficient caused by measurement error in the predictor or criterion; it is addressed by correcting for unreliability rather than by cross-validating. While unreliability could lower validity coefficients generally, it does not explain a drop specifically from derivation to cross-validation sample.",
        "C": "Shrinkage is the correct answer. When a regression equation or validity coefficient is derived from one sample, it capitalizes on chance associations specific to that sample. When applied to a new, independent sample, the coefficient predictably drops — a phenomenon called shrinkage. The psychometrician's description of this as an expected statistical artifact matches shrinkage precisely.",
        "D": "Regression to the mean describes the tendency for extreme scores on one measurement occasion to move closer to the group mean on a subsequent occasion. While it is a real statistical phenomenon, it pertains to repeated measurement of the same individuals over time, not to differences in validity coefficients across two distinct samples."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-082-vignette-L3",
      "source_question_id": "082",
      "source_summary": "When using a selection test to estimate future job performance, the test should have adequate predictive validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "restriction of range"
      ],
      "vignette": "A cybersecurity firm developed a rigorous technical aptitude exam years ago and found a strong relationship between exam scores and analyst performance across all applicants. More recently, the HR department reports that the same exam now shows a much weaker relationship with on-the-job performance. An external consultant notes that over the past few years, the firm changed its hiring practice so that only candidates who score above the 80th percentile on the aptitude exam are hired. The internal team wonders whether the exam has simply become outdated, but the consultant argues the hiring threshold itself is distorting the statistical outcome. The firm's test scores are otherwise reliable and the performance ratings are methodologically sound.",
      "question": "Which psychometric problem best explains why the relationship between the aptitude exam and job performance has weakened under the new hiring practice?",
      "options": {
        "A": "Criterion contamination",
        "B": "Restriction of range",
        "C": "Low test-retest reliability",
        "D": "Shrinkage from cross-validation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Criterion contamination occurs when knowledge of a person's predictor score influences ratings on the criterion measure, inflating the apparent relationship. Here, the vignette specifies that performance ratings are methodologically sound and does not suggest supervisors are aware of exam scores when making ratings. This does not fit criterion contamination.",
        "B": "Restriction of range is the correct answer. By hiring only candidates above the 80th percentile, the firm truncates the distribution of exam scores in the employed sample. With little variability in the predictor, the correlation with job performance is mathematically constrained to be smaller than it would be across the full range of scores — precisely what the consultant is describing.",
        "C": "Low test-retest reliability would manifest as inconsistency in individual scores across administrations, not as a systematic weakening of the exam-performance correlation tied to a hiring threshold. The vignette also states the test is reliable, ruling this out.",
        "D": "Shrinkage from cross-validation refers to the expected drop in a validity coefficient when a regression equation derived from one sample is applied to a new independent sample. The vignette does not describe cross-validation — there is one sample with a shifted hiring cut-score, not two independent derivation and validation samples."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-082-vignette-L4",
      "source_question_id": "082",
      "source_summary": "When using a selection test to estimate future job performance, the test should have adequate predictive validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "longitudinal"
      ],
      "vignette": "A graduate school of social work adopts a structured writing sample scored by faculty at the point of admission. Five years into using this rubric, administrators note that the rubric scores show excellent internal consistency and that scores remain stable when the same essays are re-scored six months later. Despite these encouraging properties, a newly hired assessment director points out that the rubric has never been formally linked to any measure of students' success after they complete the program. The director proposes a longitudinal data collection effort in which alumni who were admitted with the rubric are tracked and their career outcomes are gathered and correlated back to original rubric scores. A faculty committee argues that the rubric is valid because it clearly maps onto the competencies described in the social work curriculum.",
      "question": "The assessment director's proposed study is primarily designed to establish which form of evidence that is currently absent from the rubric's evaluation record?",
      "options": {
        "A": "Content validity",
        "B": "Concurrent validity",
        "C": "Construct validity",
        "D": "Predictive validity"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Content validity is precisely what the faculty committee is invoking when they argue the rubric maps onto social work curriculum competencies. The vignette presents this as the evidence already available, not the evidence the director seeks. Content validity does not involve correlating scores with a criterion measure collected at a future time point.",
        "B": "Concurrent validity also involves correlating test scores with a criterion, but the criterion is collected at approximately the same time as the test. The director's proposal explicitly involves tracking alumni after program completion — meaning the criterion (career outcomes) will be gathered at a substantially later time than the original rubric scores, which defines predictive rather than concurrent validity.",
        "C": "Construct validity addresses whether the test measures the theoretical construct it is intended to measure, typically through convergent and discriminant patterns or factor-analytic methods. While a longitudinal study could contribute construct-related evidence broadly, the specific design described — correlating admission scores with post-program outcomes — is precisely the methodology that defines predictive criterion-related validity.",
        "D": "Predictive validity is the correct answer. The director proposes administering (already-collected) rubric scores at the time of admission and then correlating them with criterion measures (career outcomes) gathered at a later time. This time-separated, score-to-criterion design is the defining feature of predictive validity, and the vignette confirms that this evidence is currently absent."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-082-vignette-L5",
      "source_question_id": "082",
      "source_summary": "When using a selection test to estimate future job performance, the test should have adequate predictive validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A consulting firm creates a short structured rating exercise for candidates applying to entry-level analyst positions. The ratings assigned during the hiring process are stored but play no role in the final hiring decision, which is made entirely on the basis of interviews and references. Two years later, the firm compiles supervisors' annual assessments of job performance for all analysts who were hired during this period and computes a statistical index linking these assessments back to the original ratings. The resulting index is disappointingly low, and a senior partner suggests the poor result is due to the fact that analysts were hired without regard for how they scored on the rating exercise — meaning the company employs analysts who scored both very low and very high. A statistician on the team disagrees, arguing that the low index is most likely an inflated problem because in a future application, where the firm would actually use the exercise to screen out low scorers, the index would almost certainly be even lower than what is currently observed.",
      "question": "Which psychometric concept most directly supports the statistician's argument that using the rating exercise for screening would yield an even lower statistical index than the one currently observed?",
      "options": {
        "A": "Shrinkage",
        "B": "Criterion contamination",
        "C": "Restriction of range",
        "D": "Attenuation due to criterion unreliability"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Shrinkage refers to the drop in a validity coefficient when an equation derived from one sample is applied to a new independent sample, because the original equation capitalized on chance. The statistician is not describing a cross-validation procedure — the concern is about what would happen to the index when only high-scorers are hired, which is a range issue, not a sample-generalization issue.",
        "B": "Criterion contamination occurs when knowledge of predictor scores influences criterion ratings, which would artificially inflate the relationship between them. The vignette specifies that the rating exercise played no role in hiring or in supervisors' annual evaluations, making contamination unlikely. Furthermore, contamination would increase the index, not decrease it as the statistician predicts.",
        "C": "Restriction of range is the correct answer. The statistician's argument is precisely a range-restriction argument: currently, the full spectrum of analyst rating scores is represented in the sample because hiring was blind to those scores. If the firm begins screening out low scorers, only the upper portion of the score distribution would be employed, truncating variability in the predictor. With reduced variance, the correlation between ratings and job performance would be mathematically attenuated — yielding an even lower index than currently observed. This is the canonical prediction made when anticipating the effects of range restriction.",
        "D": "Attenuation due to criterion unreliability refers to the suppression of a validity coefficient caused by random measurement error in the criterion measure. While inconsistent supervisor ratings could lower the index, the statistician's argument is specifically about what happens when screening is introduced — a change in who is hired, not a change in measurement quality. The vignette also gives no indication that supervisory assessments are unreliable."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-060-vignette-L1",
      "source_question_id": "060",
      "source_summary": "The multitrait-multimethod matrix is used to evaluate a test's construct validity by assessing its convergent and divergent validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "multitrait-multimethod matrix",
        "convergent validity",
        "divergent validity"
      ],
      "vignette": "A researcher develops a new self-report measure of emotional regulation and wants to evaluate its construct validity. She administers the new measure along with an observer-rated scale of emotional regulation and a behavioral task measuring impulsivity. She expects scores on the new measure to correlate highly with the observer-rated emotional regulation scale (convergent validity) and weakly with a measure of unrelated constructs such as arithmetic ability. She organizes all correlations among the measures into a multitrait-multimethod matrix to systematically evaluate the pattern of relationships. The researcher determines that the matrix will provide the most comprehensive evidence about the measure's construct validity.",
      "question": "Which psychometric approach is the researcher using to evaluate the construct validity of her new emotional regulation measure?",
      "options": {
        "A": "Criterion-related validity study comparing new scores to a gold-standard diagnosis",
        "B": "Content validity evaluation using expert panel ratings of item representativeness",
        "C": "Factor analysis to identify the latent structure underlying the new measure's items",
        "D": "Multitrait-multimethod (MTMM) matrix analysis assessing convergent and divergent validity"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Criterion-related validity examines whether a test predicts a specific criterion (e.g., a diagnosis or outcome), not whether similar constructs measured by different methods converge and dissimilar constructs diverge. It does not produce a matrix of trait-method correlations.",
        "B": "Content validity concerns whether test items adequately sample the domain of interest and is typically established through expert review, not through examining patterns of correlations across multiple traits and methods.",
        "C": "Factor analysis can contribute to construct validity evidence by revealing the internal latent structure of a test, but it does not systematically compare correlations across multiple traits measured by multiple methods as the MTMM matrix does.",
        "D": "This is correct. The MTMM matrix, introduced by Campbell and Fiske (1959), organizes correlations among multiple traits each measured by multiple methods to evaluate convergent validity (high correlations between same-trait, different-method pairs) and divergent validity (low correlations between different-trait pairs), providing comprehensive construct validity evidence."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-060-vignette-L2",
      "source_question_id": "060",
      "source_summary": "The multitrait-multimethod matrix is used to evaluate a test's construct validity by assessing its convergent and divergent validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "construct validity",
        "convergent"
      ],
      "vignette": "A clinical psychologist is developing a structured interview measure of anxiety and administers it alongside a well-validated self-report anxiety questionnaire and a self-report measure of general happiness to a sample of 200 adults ranging widely in age and education level. She notes that the new interview measure correlates strongly with the self-report anxiety questionnaire but only weakly with the happiness measure. She also ensures that the interview and questionnaire correlations exceed the correlation between anxiety and happiness regardless of the measurement method used. Based on this pattern, she concludes that she has good evidence for construct validity.",
      "question": "The pattern of correlations described — where same-construct measures across methods correlate strongly and different-construct measures correlate weakly — is most consistent with which form of validity evidence?",
      "options": {
        "A": "Convergent validity alone, established through correlations between parallel measurement methods",
        "B": "Multitrait-multimethod validation providing evidence of both convergent and divergent validity",
        "C": "Predictive validity, reflecting the interview measure's ability to forecast future anxiety outcomes",
        "D": "Internal consistency, demonstrated by high item-to-total correlations within the anxiety interview"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "While convergent validity is part of the picture, this answer is incomplete. The scenario also explicitly describes low correlations between different constructs (anxiety vs. happiness), which represents divergent (discriminant) validity. MTMM validation requires both components, not convergent validity alone.",
        "B": "This is correct. The scenario describes the hallmark MTMM pattern: high monotrait-heteromethod correlations (same construct, different methods = convergent) combined with low heterotrait correlations (different constructs = divergent/discriminant), with the additional criterion that same-trait correlations exceed different-trait correlations regardless of method.",
        "C": "Predictive validity refers to the degree to which scores forecast a future criterion outcome (e.g., later clinical diagnosis). The scenario describes concurrent cross-method comparisons among measures administered at the same time, not prediction of a future criterion.",
        "D": "Internal consistency (e.g., coefficient alpha or item-total correlations) concerns the degree to which items within a single test measure the same underlying construct. The scenario involves correlations between entirely different instruments, not the internal structure of one test."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-060-vignette-L3",
      "source_question_id": "060",
      "source_summary": "The multitrait-multimethod matrix is used to evaluate a test's construct validity by assessing its convergent and divergent validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "discriminant"
      ],
      "vignette": "A research team creates a new performance-based measure of executive functioning and wants to understand what the measure actually reflects psychologically. They administer the new measure to 150 participants along with a clinician-rated executive functioning scale, a self-report attention questionnaire, and a processing speed task. Interestingly, the new measure correlates at r = .62 with the clinician rating and only r = .18 with the self-reported attention score. The researchers note that this latter finding — demonstrating discriminant evidence — strengthens confidence in the measure's meaning. However, a skeptic argues the strong correlation with the clinician rating may simply reflect shared method bias rather than shared construct content.",
      "question": "The full pattern of results described — including the concern raised by the skeptic — is most directly addressed by which psychometric procedure?",
      "options": {
        "A": "Known-groups validity study comparing the measure's scores across diagnostic groups",
        "B": "Confirmatory factor analysis testing a hypothesized factor structure across subgroups",
        "C": "Multitrait-multimethod matrix analysis evaluating convergent and discriminant validity across methods",
        "D": "Criterion contamination analysis controlling for shared method variance between predictor and criterion"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Known-groups validity examines whether a test differentiates between groups expected to differ on the construct (e.g., clinical vs. nonclinical samples). While useful for construct validation, it does not address the pattern of cross-method, cross-trait correlations or the skeptic's concern about method bias.",
        "B": "Confirmatory factor analysis tests whether a specified latent structure fits the data and can support construct validity, but it does not directly evaluate whether same-trait correlations across methods are larger than different-trait correlations, nor does it systematically address method-effect variance as the MTMM matrix does.",
        "C": "This is correct. The MTMM matrix directly addresses both the convergent evidence (r = .62 with clinician rating) and the discriminant evidence (r = .18 with attention), and it specifically addresses the skeptic's concern: in the MTMM framework, same-construct heteromethod correlations must exceed same-method heterotrait correlations to rule out method bias as the primary explanation.",
        "D": "Criterion contamination refers to a threat to validity in which the criterion measure is influenced by knowledge of the predictor scores, which is a different concern from method bias. While both involve artifactual inflation of correlations, criterion contamination is not the framework used to systematically map convergent and discriminant patterns across multiple traits and methods."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-060-vignette-L4",
      "source_question_id": "060",
      "source_summary": "The multitrait-multimethod matrix is used to evaluate a test's construct validity by assessing its convergent and divergent validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "heteromethod"
      ],
      "vignette": "A psychometrician is reviewing a recently published validation study for a new interview-based measure of depression. The study reports that the interview correlates r = .71 with a self-report depression inventory and r = .69 with a clinician-completed depression checklist. The study also reports that the interview correlates r = .61 with a self-report anxiety inventory and r = .58 with a clinician-completed anxiety checklist. A reviewer notes that the heteromethod correlations involving the same construct are only modestly larger than those involving different constructs, and that within the same method, the depression–anxiety correlations are nearly as large as the depression–depression correlations. The psychometrician concludes this pattern raises serious concerns about the instrument's validity.",
      "question": "What specific validity concern is the psychometrician identifying based on this pattern of correlations?",
      "options": {
        "A": "The measure demonstrates adequate convergent validity but lacks evidence of criterion-related validity",
        "B": "The measure's internal consistency is inflated due to item overlap between depression and anxiety content",
        "C": "The MTMM matrix pattern fails to support discriminant validity because different-construct correlations rival same-construct correlations",
        "D": "The measure shows poor test-retest stability because method effects account for most of the observed variance"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The scenario does not address prediction of an external criterion (criterion-related validity); it describes a pattern of correlations among concurrent measures of the same and different constructs. Moreover, the concern is not about low convergent correlations but about the failure of those correlations to clearly exceed the different-construct correlations.",
        "B": "Internal consistency concerns the covariation among items within a single scale (e.g., coefficient alpha), not the pattern of between-scale correlations. Item overlap between constructs could contribute to high inter-scale correlations, but the psychometrician's concern as framed is about the MTMM correlation pattern, not item-level homogeneity within the scale.",
        "C": "This is correct. A well-functioning MTMM matrix requires that same-trait heteromethod correlations (convergent validity diagonal) exceed different-trait correlations (both heteromethod and monomethod). Here, depression–anxiety correlations (r = .61, .58) are nearly as large as depression–depression correlations (r = .71, .69), violating the discriminant validity criterion and suggesting the instrument may not measure depression distinctly from anxiety.",
        "D": "Test-retest stability concerns the reproducibility of scores over time across repeated administrations and is assessed by correlating scores from two time points. The scenario describes correlations among concurrent measures administered at a single time point across different methods, which is the MTMM framework — not a temporal stability analysis."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-060-vignette-L5",
      "source_question_id": "060",
      "source_summary": "The multitrait-multimethod matrix is used to evaluate a test's construct validity by assessing its convergent and divergent validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team creates three different ways to capture how well a person manages difficult emotions and three different ways to capture how socially skilled a person is. Each participant in a large study completes all six procedures. When the team lines up the numbers, they find that the three emotional management scores relate strongly to each other regardless of which procedure was used to obtain them. The social skill scores also relate strongly to each other across procedures. However, the team is troubled to notice that using the same procedure to measure emotional management and social skills produces correlations almost as high as measuring the same thing two different ways. A consultant tells the team this second finding is actually the more important threat to what they were trying to demonstrate about their new procedure.",
      "question": "What is the consultant most likely identifying as the primary threat to what the research team was attempting to demonstrate?",
      "options": {
        "A": "The three procedures for measuring emotional management lack sufficient reliability, inflating cross-construct correlations",
        "B": "The study's sample is too homogeneous in social skill level, artificially restricting the range of observed scores",
        "C": "The new procedure fails to demonstrate discriminant validity because same-method correlations between different constructs rival same-construct cross-method correlations",
        "D": "The correlation between emotional management and social skills reflects a shared higher-order factor, indicating the two constructs are not theoretically distinct"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Low reliability in the emotional management procedures would attenuate all correlations involving those procedures, reducing both convergent and cross-construct correlations — this would make it harder, not easier, to detect the problem the consultant is describing. The scenario describes correlations that are too high within a method, which is the opposite pattern from what low reliability would produce.",
        "B": "Restriction of range in social skill scores would reduce the magnitude of all correlations involving social skills, tending to lower rather than inflate the cross-construct, same-method correlations. While restriction of range is a meaningful psychometric threat, it does not explain why same-method, different-construct correlations are artificially elevated to rival same-construct, different-method correlations.",
        "C": "This is correct. In a multitrait-multimethod analysis, a key criterion for discriminant (divergent) validity is that same-construct, different-method correlations must exceed different-construct, same-method correlations. The consultant is identifying that the inflated same-method correlations between emotional management and social skills — likely due to method variance or shared response format — threaten discriminant validity and undermine the construct interpretation of the new procedure.",
        "D": "A shared higher-order factor could explain why emotional management and social skills correlate, but this would be a theoretical or conceptual concern about construct independence, not a methodological finding tied specifically to whether the same procedure was used. The consultant's concern is explicitly tied to the observation that same-method correlations rival cross-method same-construct correlations, which is a measurement design problem — the signature of method bias in the MTMM framework — not a factor-structure issue."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-071-vignette-L1",
      "source_question_id": "071",
      "source_summary": "In factor analysis, \"oblique\" means that the factors extracted are correlated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "factor analysis",
        "oblique rotation",
        "correlated factors"
      ],
      "vignette": "A psychometrician develops a new personality assessment and runs an exploratory factor analysis on the item responses. She chooses an oblique rotation method because she suspects the underlying constructs are not independent. After examining the output, she confirms that the correlated factors share meaningful variance. She notes this is consistent with her theoretical model that personality dimensions are interrelated.",
      "question": "Which of the following best describes what the use of oblique rotation in this factor analysis indicates?",
      "options": {
        "A": "The factors extracted are assumed to be uncorrelated with one another",
        "B": "The factors extracted are permitted to be correlated with one another",
        "C": "The items within each factor have equal factor loadings",
        "D": "The total variance explained is maximized by removing cross-loadings"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This describes orthogonal rotation (e.g., varimax), not oblique rotation. Orthogonal methods constrain factors to be uncorrelated, which is the conceptual opposite of what oblique rotation does.",
        "B": "Oblique rotation methods (e.g., promax, direct oblimin) allow extracted factors to correlate with one another. This is appropriate when the researcher theorizes or observes that the underlying constructs share variance, as in many psychological domains.",
        "C": "Equal factor loadings describe a special case sometimes assumed in parallel analysis or tau-equivalent models; this is unrelated to whether factors are permitted to correlate under oblique rotation.",
        "D": "Removing cross-loadings is a goal related to simple structure and is pursued in various rotation methods; it is not the defining characteristic of oblique rotation and does not specifically imply correlated factors."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-071-vignette-L2",
      "source_question_id": "071",
      "source_summary": "In factor analysis, \"oblique\" means that the factors extracted are correlated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "factor analysis",
        "rotation method"
      ],
      "vignette": "A researcher constructs a 40-item cognitive ability test intended to measure verbal reasoning, working memory, and processing speed. Although these constructs are theoretically distinct, he acknowledges that cognitive abilities tend to co-vary in the general population. He selects a rotation method for his factor analysis specifically because it allows the resulting factors to share variance rather than forcing them to be independent. The test is being normed on a large adult sample across multiple clinical sites.",
      "question": "Which rotation method did the researcher most likely choose, and what property does it assume about the extracted factors?",
      "options": {
        "A": "Varimax rotation, which constrains factors to be orthogonal (uncorrelated)",
        "B": "Oblique rotation, which permits factors to be correlated",
        "C": "Principal components analysis, which maximizes total variance accounted for by each component",
        "D": "Quartimax rotation, which simplifies the row structure of the loading matrix while keeping factors uncorrelated"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Varimax is an orthogonal rotation that constrains factors to remain uncorrelated. Because the researcher specifically wants to allow correlated factors to reflect the known co-variation among cognitive abilities, varimax would be an inappropriate choice here.",
        "B": "Oblique rotation (e.g., promax, direct oblimin) allows the extracted factors to correlate. When theory and empirical evidence suggest that constructs share variance — as is common for cognitive abilities — oblique rotation is the preferred choice over orthogonal alternatives.",
        "C": "Principal components analysis (PCA) is a data reduction technique that maximizes variance accounted for by successive components, but it is not a rotation method. Rotation is applied after extraction and does not describe PCA itself.",
        "D": "Quartimax is an orthogonal rotation method that simplifies the factor structure by minimizing the number of factors needed to explain each variable. Like varimax, it constrains factors to be uncorrelated and therefore does not fit the researcher's stated goal."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-071-vignette-L3",
      "source_question_id": "071",
      "source_summary": "In factor analysis, \"oblique\" means that the factors extracted are correlated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "promax"
      ],
      "vignette": "A graduate student is validating a new self-report measure of emotional dysregulation and runs a factor analysis using promax. Her advisor notes that the factor correlation matrix produced by this analysis shows moderate correlations among three extracted factors, ranging from .35 to .52. The student initially worries this means the measure lacks discriminant validity because the factors are not cleanly separated. Her advisor explains that the pattern of correlations is actually expected and informative given the rotation method she chose.",
      "question": "Why does the advisor indicate that the factor intercorrelations are expected given the student's analytic choice?",
      "options": {
        "A": "Because promax is an orthogonal rotation that produces uncorrelated factors by design, intercorrelations near zero would be expected",
        "B": "Because promax is an oblique rotation that explicitly allows factors to correlate, so obtaining a factor correlation matrix with non-zero values is a direct result of the method",
        "C": "Because factor correlations in the moderate range indicate poor model fit and suggest the number of factors should be reduced",
        "D": "Because promax maximizes the variance of factor loadings within each factor, which automatically produces correlated factors as an artifact of variance equalization"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect because promax is an oblique rotation method, not an orthogonal one. Orthogonal methods such as varimax constrain factors to be uncorrelated; promax explicitly relaxes this constraint, making the presence of a factor correlation matrix a defining feature of the output.",
        "B": "Promax is an oblique rotation in which the initial orthogonal solution is raised to a power to sharpen loadings, and factors are then freed to correlate. The resulting factor correlation matrix with moderate intercorrelations is precisely the expected output of this method and does not automatically indicate poor validity or fit.",
        "C": "Moderate factor intercorrelations do not by themselves indicate poor model fit or mandate a reduction in factors. In domains like emotional dysregulation where constructs genuinely overlap, moderate intercorrelations between obliquely rotated factors reflect the construct space rather than a methodological problem.",
        "D": "Promax does work by raising factor loadings to a power to approach simple structure, but this process does not 'automatically produce correlated factors as an artifact of variance equalization.' The correlations among factors in oblique solutions reflect actual shared variance in the data, not an analytic artifact."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-071-vignette-L4",
      "source_question_id": "071",
      "source_summary": "In factor analysis, \"oblique\" means that the factors extracted are correlated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "simple structure"
      ],
      "vignette": "A test developer publishes a new multidimensional clinical inventory and reports that she pursued simple structure during her factor analytic work. Critics note that in her final solution, a factor correlation matrix is reported and several factors show non-trivial inter-factor correlations. The developer responds that including the factor correlation matrix was not an oversight but rather an expected and theoretically justified feature of her solution. One reviewer argues the correlated factors indicate the constructs are not sufficiently distinct, while another reviewer supports the developer's approach, pointing out that the presence of the factor correlation matrix reveals the rotation family she used.",
      "question": "What does the presence of a factor correlation matrix in the developer's solution most directly reveal about her factor analytic approach, and why does it justify the inter-factor correlations?",
      "options": {
        "A": "It reveals she used a confirmatory factor analytic model, in which correlated residuals between factors are permitted when the model fit demands it",
        "B": "It reveals she used principal axis factoring, which produces communality estimates that necessitate reporting factor covariances",
        "C": "It reveals she used an oblique rotation, which allows factors to correlate; the factor correlation matrix is a standard output of oblique methods and does not inherently threaten construct distinctiveness",
        "D": "It reveals she used a hierarchical factor model, in which all lower-order factors necessarily correlate because they load on a higher-order general factor"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Confirmatory factor analysis can permit correlated residuals or correlated factors, but CFA does not routinely produce a standalone 'factor correlation matrix' as a defining output in the same way oblique exploratory rotation does. The scenario describes an exploratory factor analytic solution pursuing simple structure, which is characteristic of EFA with oblique rotation rather than CFA.",
        "B": "Principal axis factoring is an extraction method, not a rotation method, and it does not produce a factor correlation matrix as a result of communality estimation. The factor correlation matrix is a product of oblique rotation, which is independent of the extraction method used.",
        "C": "The factor correlation matrix is a defining and expected output of oblique rotation methods (e.g., direct oblimin, promax). When a researcher uses oblique rotation to pursue simple structure while allowing factors to correlate, reporting the factor correlation matrix is standard practice. Its presence confirms the oblique rotation family was used and reflects the theoretical position that constructs share variance.",
        "D": "A hierarchical factor model does involve correlations among lower-order factors attributable to a higher-order general factor, and this would also produce a factor correlation matrix. However, the scenario emphasizes the rotation family (simple structure pursuit during EFA), not the specification of a hierarchical model with a superordinate factor. A hierarchical solution requires an additional analytic step beyond standard oblique rotation and is not the most direct interpretation of the factor correlation matrix in this context."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-071-vignette-L5",
      "source_question_id": "071",
      "source_summary": "In factor analysis, \"oblique\" means that the factors extracted are correlated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A measurement researcher develops a battery designed to assess several psychological constructs that, based on prior theory, are unlikely to be completely independent of one another. When she analyzes the data, she selects a mathematical procedure that produces two distinct outputs: one table showing how strongly each item relates to each underlying dimension, and a separate table showing how those dimensions relate to each other. A colleague reviewing her work argues that the second table is unnecessary and suggests she should have used a procedure that would have made that second table impossible to generate. The researcher defends her choice, explaining that forcing the dimensions to be unrelated to each other would have distorted the solution and produced a less accurate picture of the construct space.",
      "question": "What property of the researcher's chosen analytic procedure is the colleague's critique and the researcher's defense most fundamentally about?",
      "options": {
        "A": "Whether the researcher used an extraction method that accounts for only shared variance among items rather than total variance",
        "B": "Whether the researcher allowed the underlying dimensions to correlate with one another rather than constraining them to be independent",
        "C": "Whether the researcher tested a pre-specified model with fixed relationships among dimensions rather than letting patterns emerge from the data",
        "D": "Whether the researcher partitioned total score variance into components attributable to the dimensions versus error, affecting reliability estimates"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The distinction between extracting shared versus total variance refers to whether a researcher used common factor analysis (e.g., principal axis factoring) versus principal components analysis. While this is a legitimate psychometric distinction, it does not explain why a second table showing relationships among dimensions would or would not be generated — that distinction is specifically about rotation type.",
        "B": "The second table — showing how the extracted dimensions relate to each other — is the factor correlation matrix, which is produced only when an oblique (rather than orthogonal) rotation is used. The colleague's criticism amounts to preferring orthogonal rotation, which constrains dimensions to be uncorrelated and therefore produces no factor correlation matrix. The researcher's defense is that forcing independence among theoretically related constructs distorts the factor solution, which is the core justification for oblique rotation.",
        "C": "The distinction between a pre-specified model and a data-driven approach refers to confirmatory versus exploratory factor analysis, not to the rotation method. Both exploratory and confirmatory approaches can involve correlated factors, and the scenario's emphasis on two distinct output tables (loadings and factor intercorrelations) points specifically to the oblique versus orthogonal rotation distinction rather than the EFA versus CFA distinction.",
        "D": "Partitioning score variance into dimension-related and error components relates to reliability estimation (e.g., coefficient alpha, generalizability theory) and is a separate psychometric concern from factor rotation. The presence or absence of a table describing relationships among dimensions does not bear on how total score variance is decomposed for reliability purposes."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-095-vignette-L1",
      "source_question_id": "095",
      "source_summary": "Applicants who score above the cutoff on a job selection test but receive unsatisfactory scores on a measure of job performance six months later are false positives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "predictive validity",
        "cutoff score",
        "job performance"
      ],
      "vignette": "A human resources department administers a cognitive ability test to all job applicants and uses a cutoff score to make hiring decisions. Six months after hiring, supervisors rate each employee's job performance using a standardized measure. Researchers reviewing the data find a group of employees who scored above the cutoff score on the selection test but received unsatisfactory ratings on the job performance measure. The researchers are cataloguing these cases to evaluate the test's predictive validity.",
      "question": "In the context of evaluating the selection test's predictive validity, what term best describes employees who scored above the cutoff but later received unsatisfactory job performance ratings?",
      "options": {
        "A": "False negatives",
        "B": "True negatives",
        "C": "False positives",
        "D": "True positives"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "False negatives are individuals who score below the cutoff (i.e., the test predicts failure) but would have actually performed satisfactorily on the job. These applicants are incorrectly rejected. This does not describe employees who scored above the cutoff and then performed poorly.",
        "B": "True negatives are individuals who scored below the cutoff and would indeed have performed unsatisfactorily if hired. The test correctly identified them as unsuitable. This category does not apply to employees who passed the cutoff.",
        "C": "This is correct. False positives are individuals the test predicts will succeed (score above the cutoff) but who subsequently fail to meet performance standards. They were incorrectly classified as likely successful employees, making their cases evidence of predictive errors in the selection system.",
        "D": "True positives are individuals who scored above the cutoff and also performed satisfactorily on the job, meaning the test correctly predicted their success. This describes accurate predictions, not the erroneous cases described in the vignette."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-095-vignette-L2",
      "source_question_id": "095",
      "source_summary": "Applicants who score above the cutoff on a job selection test but receive unsatisfactory scores on a measure of job performance six months later are false positives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "selection test",
        "criterion"
      ],
      "vignette": "A large retail company implements a new personality-based selection test intended to identify candidates likely to excel in customer service roles. All applicants who score at or above a predetermined threshold are hired, while those below are rejected. Despite having a high coefficient alpha indicating strong internal consistency, the test seems to produce a troubling pattern: a substantial number of hired employees — particularly those from urban branches — receive poor customer satisfaction scores from their supervisors six months into the job. The HR team is specifically concerned about the group that passed the selection test but later failed to meet the criterion for acceptable job performance.",
      "question": "What classification best describes the employees who passed the selection test but subsequently failed to meet the performance criterion?",
      "options": {
        "A": "False positives",
        "B": "False negatives",
        "C": "Low sensitivity cases",
        "D": "Shrinkage cases"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. False positives are individuals who score above the selection cutoff — meaning the test predicts they will perform well — but who subsequently fail to perform satisfactorily on the criterion measure. The urban branch detail is a neutral distractor; it does not change the classification.",
        "B": "False negatives are individuals who score below the cutoff and are rejected, but who would have performed satisfactorily had they been hired. These individuals are incorrectly excluded. This does not describe employees who were hired (passed the test) and then performed poorly.",
        "C": "Sensitivity refers to a test's ability to correctly identify true positives — those who will actually succeed. Low sensitivity would mean the test misses many people who would succeed. While related to test accuracy, 'low sensitivity cases' is not the established term for individuals who pass a selection test and later fail the criterion.",
        "D": "Shrinkage refers to the reduction in a regression equation's predictive validity when it is applied to a new sample after being derived from an initial one. It is a statistical phenomenon involving cross-validation, not a classification label for individual test-takers who passed the selection threshold and later underperformed."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-095-vignette-L3",
      "source_question_id": "095",
      "source_summary": "Applicants who score above the cutoff on a job selection test but receive unsatisfactory scores on a measure of job performance six months later are false positives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "cutoff"
      ],
      "vignette": "An industrial-organizational psychologist is validating a new cognitive screening battery designed to predict managerial success. The battery shows impressive correlations with supervisor ratings when examined using concurrent validity methods on current employees, but the psychologist is aware that this may not translate to accurate predictions for future applicants. After the battery is deployed for six months, an audit reveals that a notable proportion of newly hired managers — all of whom scored at or above the established cutoff — are performing below expectations according to their annual reviews. The psychologist suspects the battery may be generating a systematic type of classification error, distinct from the error made when capable applicants are incorrectly turned away.",
      "question": "What type of classification error is the psychologist most concerned about in the employees described in this audit?",
      "options": {
        "A": "False negatives",
        "B": "False positives",
        "C": "Restriction of range errors",
        "D": "Low positive predictive value"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. False positives are individuals who score above the cutoff — indicating the test predicts success — but who subsequently perform below expectations. The vignette specifies that the error is 'distinct from the error made when capable applicants are incorrectly turned away,' which describes false negatives, helping rule out option A and confirm this is the correct error type.",
        "A": "False negatives are the classification error in which applicants who scored below the cutoff would have performed satisfactorily if hired. The vignette explicitly distinguishes the error of concern from this type, noting capable applicants incorrectly rejected is the other error. Therefore, false negatives do not describe the group highlighted in the audit.",
        "C": "Restriction of range is a methodological artifact occurring when the range of scores on the predictor or criterion is truncated, often because only selected individuals are included in the validity study. While it is a relevant concern when only hired employees are studied, it describes a statistical bias affecting validity coefficients — not a classification for individual erroneous predictions.",
        "D": "Positive predictive value (PPV) is the proportion of individuals who score above the cutoff who also succeed on the criterion — it reflects the overall accuracy of positive predictions. While low PPV would be implied by a high rate of false positives, PPV is a test-level index of accuracy, not a label for individual erroneous classification outcomes. The vignette asks about the classification of specific individuals, not a test-level statistic."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-095-vignette-L4",
      "source_question_id": "095",
      "source_summary": "Applicants who score above the cutoff on a job selection test but receive unsatisfactory scores on a measure of job performance six months later are false positives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "specificity"
      ],
      "vignette": "A municipal fire department adopts a physical fitness and cognitive reasoning battery to screen firefighter candidates, with the goal of admitting only those likely to perform safely and effectively. The battery's specificity is considered adequate, meaning it correctly excludes most individuals who would have performed poorly. However, an internal review conducted 18 months after implementation reveals a troubling subset of admitted candidates: they cleared the admission threshold comfortably, yet their field performance evaluations — scored by independent incident commanders who were blind to test results — placed them in the 'unsatisfactory' category. The review board is debating whether this pattern is explained by a flaw in the test's construct representation or by an inherent limitation in the accuracy of predictions made at the group boundary.",
      "question": "How should the employees identified in the 18-month internal review most precisely be classified within the framework used to evaluate predictive accuracy?",
      "options": {
        "A": "True negatives whose field evaluations reflect rater bias",
        "B": "False negatives reflecting inadequate test sensitivity",
        "C": "False positives reflecting predictive error above the threshold",
        "D": "Low specificity cases resulting from an overly permissive cutoff"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The employees in question scored above the admission threshold (a 'positive' classification by the test) but subsequently received unsatisfactory field performance evaluations. This is precisely the definition of a false positive: the test predicted success but the criterion outcome was failure. The mention of specificity is a peripheral cue — high specificity means few false positives among true negatives, but does not preclude false positives among those above the cutoff, making this option the most precise classification.",
        "A": "True negatives are individuals who score below the cutoff and would also have performed poorly, meaning the test correctly rejected them. The individuals in the review were admitted (scored above the cutoff), so they cannot be true negatives. While rater bias is a legitimate validity concern, it does not reclassify someone who was admitted and performed poorly.",
        "B": "False negatives reflect individuals who were rejected (scored below the cutoff) but would have performed satisfactorily. Low sensitivity means the test misses true successes. The individuals described cleared the threshold comfortably and were admitted, so this classification reverses the direction of the error and does not apply.",
        "D": "Low specificity refers to a test that incorrectly classifies true failures as successes — i.e., it admits too many people who will fail, producing an elevated false positive rate. However, this is a test-level index describing a pattern across many cases, not the classification applied to individual cases. Moreover, the vignette states specificity is adequate; a few false positives can still occur even with adequate specificity, so this option conflates individual classification with test-level accuracy statistics."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-095-vignette-L5",
      "source_question_id": "095",
      "source_summary": "Applicants who score above the cutoff on a job selection test but receive unsatisfactory scores on a measure of job performance six months later are false positives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A consulting firm administers a structured battery of tasks to job applicants and uses the combined score to decide whom to hire. Of all the people hired, one subgroup stands out in a follow-up conducted more than half a year later: their task scores at the time of hiring were comfortably above what the organization considered an acceptable threshold, yet the independent ratings they received from supervisors who had no knowledge of the original task scores placed them squarely in the 'does not meet expectations' category. Interestingly, the firm had celebrated the battery's ability to correctly screen out applicants who later turned out to be poor fits when tested informally in a pilot group, and some reviewers suggested that the subgroup's poor showing might reflect the supervisors' tendency to rate recent hires more harshly. Others argued that the battery simply measured something slightly different from what the supervisors were asked to evaluate.",
      "question": "What psychometric phenomenon most precisely describes the specific subgroup of hired individuals identified in the follow-up?",
      "options": {
        "A": "False negatives produced by low test sensitivity",
        "B": "False positives reflecting predictive classification error",
        "C": "Criterion contamination from supervisor knowledge of test scores",
        "D": "Construct underrepresentation reducing concurrent validity"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The subgroup scored above the hiring threshold (a positive prediction of success) but received unsatisfactory supervisor ratings — the hallmark of a false positive in a predictive validity framework. The red herrings (supervisors rating harshly, battery measuring something different, the battery's strength at screening out true failures) all point toward alternative explanations, but the defining feature is unambiguous: the test predicted success for individuals who subsequently failed the criterion.",
        "A": "False negatives occur when the test predicts failure (score below the threshold) for individuals who would actually have succeeded. The subgroup in question was hired — they passed the threshold — so the direction of prediction is reversed. Low sensitivity refers to the test's failure to identify true successes, which describes a different group: those rejected who would have performed well, not those admitted who performed poorly.",
        "C": "Criterion contamination occurs when the criterion measure (here, supervisor ratings) is influenced by knowledge of the predictor scores, inflating or distorting the apparent relationship between test and performance. The vignette explicitly states that supervisors were blind to the original task scores, ruling out this explanation. While rater harshness is raised as a competing explanation, the absence of score knowledge eliminates criterion contamination as the mechanism.",
        "D": "Construct underrepresentation occurs when a test fails to measure the full breadth of the construct it is intended to assess, reducing validity. While one reviewer suggests the battery measured something different from what supervisors evaluated — a hint toward this concept — construct underrepresentation is a test-level validity threat affecting the overall predictor-criterion relationship, not a classification label for a specific subgroup of individuals. The question asks about the precise term for the identified subgroup, which remains false positives regardless of why the predictive error occurred."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-038-vignette-L1",
      "source_question_id": "038",
      "source_summary": "A problem with using percent agreement as a measure of inter-rater reliability is that it may overestimate reliability because it's affected by chance agreement.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "percent agreement",
        "chance agreement",
        "inter-rater reliability"
      ],
      "vignette": "A research team is developing a new behavioral observation checklist to assess aggression in children. Two trained raters independently code whether each of 50 behaviors is 'aggressive' or 'non-aggressive.' The team calculates percent agreement and finds that the raters agree 90% of the time, leading them to conclude that inter-rater reliability is excellent. A senior psychometrician reviews the study and cautions that the 90% figure may substantially overestimate true reliability. She explains that because raters are working with only two categories, a meaningful portion of their agreement is attributable to chance agreement rather than genuine concordance.",
      "question": "The senior psychometrician's concern most directly illustrates which fundamental limitation of percent agreement as a measure of inter-rater reliability?",
      "options": {
        "A": "Percent agreement is insensitive to the magnitude of disagreements, treating a one-category discrepancy the same as a large discrepancy.",
        "B": "Percent agreement fails to account for the proportion of agreement that would be expected by chance alone, causing reliability to be overestimated.",
        "C": "Percent agreement reflects only the consistency of a single rater across time rather than the consistency between two independent raters.",
        "D": "Percent agreement is an inappropriate metric when the number of behavioral categories is fewer than five, because it inflates the discrimination index."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This describes a limitation of unweighted agreement statistics, such as simple kappa, compared to weighted kappa — it concerns ordinal severity of disagreements, not the chance-agreement inflation problem that prompted the psychometrician's concern.",
        "B": "This is correct. With only two response categories, raters would agree roughly 50% of the time by chance alone. Percent agreement does not subtract this expected baseline, so the observed 90% agreement conflates true reliability with random coincidence, leading to overestimation.",
        "C": "This describes test-retest or intra-rater reliability, not inter-rater reliability. The scenario involves two independent raters coding the same behaviors simultaneously, so temporal consistency of a single rater is not the relevant concern.",
        "D": "The number of behavioral categories affects the base rate of chance agreement but does not define a minimum threshold of five categories for percent agreement to be appropriate. This option conflates percent agreement with the discrimination index, which is an item analysis statistic unrelated to the issue described."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-038-vignette-L2",
      "source_question_id": "038",
      "source_summary": "A problem with using percent agreement as a measure of inter-rater reliability is that it may overestimate reliability because it's affected by chance agreement.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "inter-rater reliability",
        "chance"
      ],
      "vignette": "A team of clinical researchers is training five graduate students to diagnose personality disorders using a structured interview protocol. After a brief training session, two of the students independently interview and diagnose 40 clients, each of whom either meets or does not meet criteria for borderline personality disorder. Because borderline personality disorder is relatively rare in the sample, most clients receive a 'no diagnosis' rating, and the two raters agree in 85% of cases. Satisfied with this result, the supervisor prepares to report high inter-rater reliability in the study's methods section. A more experienced colleague warns that the 85% agreement figure is likely misleading given the base rate of the disorder in this sample.",
      "question": "Which limitation of the chosen reliability metric most directly explains why the experienced colleague believes the 85% figure is misleading?",
      "options": {
        "A": "The metric ignores systematic rater bias, which is especially problematic when one rater consistently assigns more diagnoses than the other.",
        "B": "The metric confounds the stability of diagnoses over time with the degree of agreement between raters, making it inappropriate for this cross-sectional design.",
        "C": "The metric does not correct for the proportion of agreement that would be expected by chance, and a low base rate of diagnosis inflates that chance component considerably.",
        "D": "The metric is insensitive to the content validity of the structured interview, meaning high agreement could reflect a shared misunderstanding of the diagnostic criteria."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. When a disorder is rare, both raters will frequently assign 'no diagnosis' regardless of their true agreement, producing high overlap by chance. Percent agreement does not subtract this expected-by-chance baseline, so the 85% figure is inflated — a correction for chance (as in Cohen's kappa) would likely yield a much lower estimate.",
        "A": "Systematic rater bias is a real concern in inter-rater reliability studies and would affect accuracy, but percent agreement would actually remain high even when one rater is consistently more lenient, as long as they agree on which clients receive which rating. The colleague's specific concern is about the base-rate-driven chance agreement, not directional bias.",
        "B": "Confounding temporal stability with rater agreement is a limitation of test-retest designs, not of the simultaneous independent-rating design described. Both raters evaluated the same clients at the same time, so time is not a variable.",
        "D": "Content validity refers to whether an instrument adequately samples the domain it intends to measure; it is a property of the interview instrument, not of the agreement metric. Even a content-valid instrument could yield inflated percent agreement due to chance, but the colleague's concern is specifically about the metric's susceptibility to base-rate-driven chance agreement."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-038-vignette-L3",
      "source_question_id": "038",
      "source_summary": "A problem with using percent agreement as a measure of inter-rater reliability is that it may overestimate reliability because it's affected by chance agreement.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "kappa"
      ],
      "vignette": "A hospital psychology department evaluates a new structured risk-assessment tool for suicide risk. Two licensed psychologists independently rate 60 patients as either 'high risk' or 'low risk,' and the department head reports 92% agreement between raters. A consultant recommends supplementing the report with Cohen's kappa, which yields a value of only .48. The department head is puzzled, noting that the raters seem to agree most of the time and that a large majority of patients in this secure inpatient unit were classified as high risk by both raters. The consultant explains that the discrepancy between the two statistics reveals a specific and well-known flaw in the simpler index.",
      "question": "What does the substantial gap between the 92% agreement figure and the kappa of .48 most specifically reveal about the simpler reliability index in this context?",
      "options": {
        "A": "The 92% agreement figure is artificially elevated because raters were not blinded to each other's ratings, introducing correlated error that kappa corrects for.",
        "B": "The 92% agreement figure overestimates reliability because it does not account for the proportion of agreement attributable to chance, which is high when most patients share the same risk category.",
        "C": "The 92% agreement figure is misleading because it reflects internal consistency across items on the tool rather than true agreement between independent evaluators.",
        "D": "The 92% agreement figure is inflated because it treats all disagreements as equally important, whereas kappa weights disagreements based on their clinical severity."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Rater blinding is important for avoiding influence effects, but percent agreement does not mathematically correct for correlated error regardless of blinding status. The gap between percent agreement and kappa in this scenario is driven by the high base rate of 'high risk' ratings, not by rater communication. Kappa does not directly correct for correlated error either.",
        "B": "This is correct. Because most patients in this inpatient unit are classified 'high risk,' both raters will frequently assign that category by chance alone. Percent agreement includes this chance overlap, whereas Cohen's kappa subtracts the expected chance agreement before computing the ratio of observed-to-possible agreement above chance. The high base rate dramatically raises chance agreement, explaining why kappa (.48) is so much lower than percent agreement (92%).",
        "C": "Internal consistency reliability (e.g., coefficient alpha or split-half) concerns agreement across items within a single instrument for a single rater. The scenario describes two raters providing a single overall rating per patient, so internal consistency is not relevant here.",
        "D": "Weighted kappa adjusts for the severity of disagreements on ordinal scales, but the scenario uses a dichotomous (high/low) rating scale where weighting is not applicable. Standard (unweighted) kappa, which is what the consultant used, does not weight disagreements; the gap between percent agreement and kappa here is entirely due to the chance-correction, not severity weighting."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-038-vignette-L4",
      "source_question_id": "038",
      "source_summary": "A problem with using percent agreement as a measure of inter-rater reliability is that it may overestimate reliability because it's affected by chance agreement.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "base rate"
      ],
      "vignette": "A neuropsychology clinic adopts a new screening protocol in which two psychologists independently classify each patient as either 'cognitively impaired' or 'cognitively intact' based on a brief battery. In the clinic's population, nearly 80% of patients present with documented cognitive impairment. After reviewing 100 cases, the clinic director announces that the screening agreement between the two psychologists has been outstanding — a figure rarely achieved in clinical settings — and cites this as evidence that the protocol is reliably implemented. A consulting psychometrician examining the same data is skeptical and argues that the high agreement figure is not what it appears to be, even if neither psychologist is biased and both are highly trained. The clinic director counters that the consistency of the raters' classifications speaks for itself.",
      "question": "Which psychometric issue most precisely explains why the consulting psychometrician is skeptical of the clinic director's conclusion, despite the absence of rater bias or training deficits?",
      "options": {
        "A": "The agreement figure is inflated because the dichotomous nature of the classification scale artificially restricts the range of possible disagreement scores, attenuating the metric's sensitivity to genuine rater differences.",
        "B": "The agreement figure is elevated because a high prevalence of impairment in this population means raters would frequently agree on the majority classification by chance alone, and the metric used does not subtract this expected-by-chance agreement.",
        "C": "The agreement figure may be spuriously high because, without an established gold standard for cognitive impairment, both raters may be responding to the same irrelevant cues rather than to true cognitive status.",
        "D": "The agreement figure is an overestimate because raters in naturalistic settings exhibit criterion drift over time, gradually shifting their thresholds in a correlated manner that produces artificial consensus."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. With 80% of patients classified as cognitively impaired, the expected probability that two independent raters both assign 'impaired' by chance is approximately 0.80 × 0.80 = .64, and the expected agreement on 'intact' is 0.20 × 0.20 = .04, yielding total chance agreement near 68%. Percent agreement does not subtract this baseline, so the 'outstanding' figure largely reflects the base rate distribution rather than rater skill. A chance-corrected index (e.g., kappa) would be substantially lower.",
        "A": "Restriction of range is a genuine psychometric concern — for example, a narrow range of true scores reduces the observed correlation between measures. However, the dichotomous scale here does not restrict the range of disagreement scores in a way that inflates observed agreement; rather, the high base rate of one category is what creates the inflation. Range restriction typically attenuates reliability coefficients rather than inflating them.",
        "C": "The absence of a gold standard is a criterion validity concern, not a reliability concern. High agreement between raters can still be meaningful even without a gold standard; the psychometrician's skepticism is specifically about the metric's inability to distinguish genuine concordance from chance concordance — a property independent of gold-standard availability.",
        "D": "Criterion drift is a real longitudinal validity concern in which raters' classification thresholds shift over time, potentially producing correlated changes. However, the scenario describes a single review of 100 cases without reference to temporal change, and the psychometrician's argument does not invoke temporal drift. The core issue is the static overestimation produced by an unweighted agreement metric in a high-base-rate setting."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-038-vignette-L5",
      "source_question_id": "038",
      "source_summary": "A problem with using percent agreement as a measure of inter-rater reliability is that it may overestimate reliability because it's affected by chance agreement.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A behavioral health organization wants to assess how consistently two staff members classify client files. Each staff member works through the same set of 80 files independently, sorting each into one of two categories. When the results are compared, the pair is found to agree on nearly 9 out of every 10 files. Most of the files end up in the same category regardless of which staff member reviews them, largely reflecting patterns in the organization's clientele. A methodologist reviewing the study design points out that the impressive-sounding result could be almost entirely explained without assuming that the two staff members have any particular skill or shared understanding of what distinguishes the two categories, and that an organization relying on this number to make staffing decisions would be drawing the wrong conclusion.",
      "question": "What specific flaw in the way the organization measured consistency most directly supports the methodologist's concern?",
      "options": {
        "A": "The organization measured consistency across only two categories, which limits the degree of precision with which genuine agreement can be distinguished from superficial similarity in classification patterns.",
        "B": "The organization used a metric that counts all agreements equally regardless of their source, meaning that agreements driven purely by the tendency of both staff members to use the more common category are treated the same as agreements reflecting genuine evaluative concordance.",
        "C": "The organization did not account for the possibility that one staff member systematically applied more lenient standards than the other, which would produce high apparent overlap even when the two individuals were operating on different criteria.",
        "D": "The organization measured agreement on individual files rather than on aggregated summary ratings, which artificially elevates the consistency estimate by multiplying the number of opportunities for coincidental matches."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Because the organization's clientele predominantly falls into one category, both staff members will frequently assign that category independently of any genuine shared understanding — and these agreements are counted the same as agreements that reflect true discriminative skill. The metric (percent agreement) does not correct for this expected-by-chance overlap, so the 90% figure is artificially high. A correction for chance agreement — as in Cohen's kappa — would reveal a much more modest level of true concordance.",
        "A": "Having only two categories does amplify the chance-agreement problem, but this option frames the flaw as a precision problem with category number rather than as the failure to subtract expected chance agreement. The core issue is not that two categories are too few for precise measurement, but that the metric does not distinguish chance-driven from skill-driven agreement. Two highly skilled raters with two categories would show genuinely high kappa even though percent agreement would also be high.",
        "C": "Systematic leniency differences (rater bias) do affect agreement, but the methodologist's point is that the high rate could be explained without assuming either leniency differences or shared skill — i.e., even two people randomly assigning the predominant category would agree almost as often. The metric's flaw here is not sensitivity to directional bias but insensitivity to the base-rate contribution to chance agreement.",
        "D": "Measuring agreement at the level of individual files rather than summary ratings is standard practice in inter-rater reliability studies and is not a design flaw. In fact, aggregating to summary ratings would reduce statistical power and obscure item-level variation. This option incorrectly implies that using more observations inflates reliability, when in fact larger samples stabilize the estimate rather than bias it upward."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-104-vignette-L1",
      "source_question_id": "104",
      "source_summary": "Before adding a new selection test to the procedure that's currently being used to make hiring decisions, you would want to make sure the new selection test has adequate incremental validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "incremental validity",
        "selection test",
        "criterion-related validity"
      ],
      "vignette": "A human resources consulting firm currently uses a structured interview to make hiring decisions for sales positions. The firm is considering adding a new personality inventory to their selection battery. A psychometrician advises that before adopting the new measure, they must demonstrate that the personality inventory provides incremental validity over and above the structured interview in predicting job performance, a criterion-related validity concern. The psychometrician explains that simply showing the new test correlates with job performance is insufficient if the structured interview already captures that variance.",
      "question": "The psychometrician's recommendation centers on which psychometric concept?",
      "options": {
        "A": "Incremental validity",
        "B": "Concurrent validity",
        "C": "Content validity",
        "D": "Convergent validity"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Incremental validity refers to the degree to which a new predictor improves prediction of a criterion beyond what is already achieved by existing predictors. This is precisely the concern raised: does the personality inventory add unique predictive power over the structured interview alone?",
        "B": "Concurrent validity assesses whether scores on a test correlate with criterion scores measured at the same point in time, but it does not address whether the new test adds predictive utility beyond what an existing test already provides. It is a type of criterion-related validity, but it does not capture the 'over and above' requirement described here.",
        "C": "Content validity refers to the degree to which test items representatively sample the domain of interest (e.g., job tasks). It is not concerned with prediction of external criteria or with comparisons between predictors, so it does not fit this scenario.",
        "D": "Convergent validity, a component of construct validity, evaluates whether a test correlates strongly with other measures of the same construct. While a personality inventory might show convergent validity with another personality scale, this does not address whether it predicts job performance above and beyond an existing selection tool."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-104-vignette-L2",
      "source_question_id": "104",
      "source_summary": "Before adding a new selection test to the procedure that's currently being used to make hiring decisions, you would want to make sure the new selection test has adequate incremental validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "selection battery",
        "predictive validity"
      ],
      "vignette": "A large hospital system currently uses a structured behavioral interview to select nursing staff, and administrators report acceptable turnover rates. A vendor approaches the hospital with a new cognitive ability test, claiming it has strong predictive validity for healthcare job performance. Despite the vendor's data, the hospital's psychologist notes that the cognitive ability test and the structured interview likely measure overlapping constructs, and that the correlation between the new test and job performance may not reflect unique predictive power. The psychologist recommends a specific statistical evaluation before adding the test to the existing selection battery.",
      "question": "What psychometric property is the hospital psychologist most concerned with establishing before adopting the new cognitive ability test?",
      "options": {
        "A": "Predictive validity of the cognitive ability test alone",
        "B": "Incremental validity of the cognitive ability test over the structured interview",
        "C": "Test-retest reliability of the cognitive ability test across nursing applicants",
        "D": "Discriminant validity between the cognitive ability test and the structured interview"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Incremental validity specifically addresses whether a new predictor contributes unique variance in predicting a criterion beyond what existing predictors already account for. Because the psychologist suspects overlap between the two tools, she must determine whether the cognitive ability test improves prediction of job performance over and above what the structured interview already provides.",
        "A": "Predictive validity concerns the correlation between test scores and future criterion performance, which the vendor has already demonstrated. However, this does not address whether the new test adds anything to the prediction already achieved by the structured interview, which is the central question here.",
        "C": "Test-retest reliability assesses score stability across time and is a reliability rather than validity concern. While reliability is a prerequisite for validity, the psychologist's expressed concern about overlapping constructs and unique predictive power points specifically to a validity question, not a reliability one.",
        "D": "Discriminant validity evaluates whether a measure does NOT correlate with constructs it should theoretically differ from, and is part of construct validation. Although the psychologist notes potential construct overlap, the clinical concern here is about predicting job performance outcomes (criterion-related), not about construct differentiation per se."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-104-vignette-L3",
      "source_question_id": "104",
      "source_summary": "Before adding a new selection test to the procedure that's currently being used to make hiring decisions, you would want to make sure the new selection test has adequate incremental validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A police department already administers a psychological fitness-for-duty interview and a background integrity check to all applicants. After several hiring cycles, a consultant proposes adding a structured personality measure shown to correlate significantly (r = .38) with supervisor ratings of officer performance. The department's research director points out that both existing tools already tap into integrity and interpersonal functioning, noting that the R² for predicting supervisor ratings barely changes when the personality measure is added to a regression model that already includes the two existing predictors. The research director argues the new test should not be adopted despite its statistically significant correlation with the criterion.",
      "question": "The research director's argument is best described by which of the following psychometric concepts?",
      "options": {
        "A": "Restriction of range attenuating the personality measure's correlation with job performance",
        "B": "Shrinkage reducing the cross-validated correlation of the existing battery",
        "C": "Lack of incremental validity of the personality measure over existing predictors",
        "D": "Insufficient criterion-related validity of the personality measure itself"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Incremental validity is demonstrated when a new predictor accounts for meaningful additional variance in the criterion beyond what existing predictors already explain. The research director's observation that R² barely changes when the personality measure is added to the regression model containing existing predictors is the defining indicator that incremental validity is absent, justifying the recommendation against adoption.",
        "A": "Restriction of range occurs when the range of scores on a predictor or criterion is artificially narrowed in the sample, attenuating observed correlations. Nothing in the scenario suggests the sample of applicants has been selected or truncated in a way that would reduce the personality measure's correlation, making this an implausible interpretation of the described findings.",
        "B": "Shrinkage refers to the decrease in a validity coefficient when a regression equation derived in one sample is applied to a new sample, due to capitalization on chance. While shrinkage is a genuine concern in personnel selection research, the director's argument is about the current model's failure to improve with the new predictor — not about cross-sample generalization of an existing equation.",
        "D": "Insufficient criterion-related validity would mean the personality measure itself does not predict job performance — but the scenario explicitly states it correlates significantly (r = .38) with supervisor ratings, establishing its criterion-related validity. The problem is not that the test fails to predict the criterion, but that it predicts nothing beyond what the existing tools already predict."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-104-vignette-L4",
      "source_question_id": "104",
      "source_summary": "Before adding a new selection test to the procedure that's currently being used to make hiring decisions, you would want to make sure the new selection test has adequate incremental validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "R-squared"
      ],
      "vignette": "A federal transportation agency uses two established selection tools — a cognitive aptitude battery and a situational judgment test — to hire air traffic controllers. An independent research team conducts a study in which a new emotional regulation inventory is administered to 400 applicants, alongside the existing tools, with supervisor performance ratings collected one year later. The emotional regulation inventory correlates r = .41 with performance ratings, higher than either of the existing tools individually. However, when the team runs a hierarchical regression entering the existing two predictors in the first step and the new inventory in the second step, the R-squared change at the second step is .009 and is not statistically significant. The research team concludes that the agency should not add the new inventory to its selection battery.",
      "question": "Which psychometric issue most precisely justifies the research team's conclusion?",
      "options": {
        "A": "The emotional regulation inventory demonstrates criterion-related validity but lacks incremental validity over the existing battery",
        "B": "The emotional regulation inventory's validity coefficient will likely shrink substantially upon cross-validation, making adoption premature",
        "C": "The moderate correlation between the new inventory and performance ratings reflects concurrent rather than predictive validity, limiting generalizability",
        "D": "Multicollinearity among predictors artificially suppresses the unique variance attributed to the new inventory, indicating a measurement artifact rather than a true validity deficit"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "The emotional regulation inventory demonstrates criterion-related validity through its significant bivariate correlation with performance (r = .41). However, the hierarchical regression reveals that its unique contribution (ΔR² = .009, ns) beyond the two existing predictors is negligible, meaning it lacks incremental validity. This is precisely the evidence used to justify not expanding the selection battery, as the inventory offers no practical improvement in prediction over what already exists.",
        "B": "Shrinkage is a legitimate concern in validation research, referring to the drop in validity when an equation is applied to a new sample. However, the research team's conclusion is based on observed ΔR² in the current sample, not on projections about cross-sample performance. Invoking shrinkage would be relevant if they were concerned about generalizing the existing battery's equation, not about evaluating the new predictor's additive value.",
        "C": "Predictive validity is distinguished from concurrent validity by the temporal gap between predictor administration and criterion measurement — here, performance ratings were collected one year after testing, clearly meeting the standard for predictive validity. The distinction between concurrent and predictive validity does not explain the research team's conclusion, which rests on the size and significance of the validity increment rather than study timing.",
        "D": "Multicollinearity can suppress unique variance estimates in regression, and the emotional regulation inventory's overlap with the existing predictors may partly explain the small ΔR². However, multicollinearity is a statistical artifact concern that would call for additional analytic steps (e.g., examining tolerance or VIF), not a justification for the team's practical conclusion. The team's argument is properly framed around incremental validity, not measurement artifacts requiring correction."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-104-vignette-L5",
      "source_question_id": "104",
      "source_summary": "Before adding a new selection test to the procedure that's currently being used to make hiring decisions, you would want to make sure the new selection test has adequate incremental validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A manufacturing company already screens job applicants using two well-established tools, and over several years these tools have reliably identified workers who perform well on the production floor. A consultant introduces a third screening tool with impressive numbers: in a separate sample of workers from a different industry, this tool strongly predicted who would succeed on the job. Excited by these findings, the operations director wants to immediately incorporate the new tool into their hiring process. A cautious organizational psychologist on the team runs her own analysis using the company's current applicant pool and finds that when she accounts for what the two existing tools already tell her about an applicant, the third tool adds almost nothing to her ability to predict how workers will actually perform. The psychologist recommends against adopting the new tool.",
      "question": "The organizational psychologist's recommendation is best supported by which of the following psychometric concepts?",
      "options": {
        "A": "The new tool's validity evidence comes from a different population, raising concerns about generalizability of its correlation with job outcomes to the current setting",
        "B": "The new tool lacks incremental validity because it does not improve prediction of job performance beyond what the existing tools already provide",
        "C": "The new tool's strong performance in the external sample is likely to shrink when applied to the company's specific applicant pool due to statistical regression to the mean",
        "D": "The two existing tools may be capturing a narrower range of applicant ability, artificially reducing the apparent contribution of the new tool in the internal analysis"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Incremental validity refers to whether a new predictor adds meaningful predictive power for a criterion beyond what is already achieved by existing predictors. The psychologist's internal analysis — showing that accounting for existing tool scores leaves the new tool with virtually no additional predictive power — is the textbook demonstration of absent incremental validity, and it is precisely this finding that justifies not expanding the battery.",
        "A": "Generalizability across populations (sometimes addressed through validity generalization or transportability analyses) is a real and legitimate concern when applying findings from one sample to another setting. The different industry sample does raise this issue, and it is a plausible reason for caution. However, the psychologist's specific analytic finding — that the tool adds nothing when entered after the existing predictors — points to incremental validity as the more precise and operationally relevant explanation for her recommendation.",
        "C": "Shrinkage refers to the drop in a predictive relationship when a statistical model developed in one sample is applied to a new sample, due to overfitting or capitalization on chance. While this could explain why the external tool's impressive numbers might not replicate, the psychologist did not apply the external model to her sample — she conducted a fresh analysis. Her finding of near-zero unique contribution reflects a validity question, not a cross-sample generalization of a previously derived equation.",
        "D": "Restriction of range occurs when the applicant sample is truncated on a predictor or criterion (e.g., only hired workers are studied), which can reduce observed correlations. If the two existing tools have been used to screen out low scorers, the remaining applicant pool might show reduced variability, potentially underestimating the new tool's contribution. This is a plausible alternative explanation — but the scenario describes an analysis of current applicants (not only hired employees), and the psychologist's conclusion is drawn from the pattern of unique variance in that full applicant pool, which is more precisely explained by incremental validity."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-093-vignette-L1",
      "source_question_id": "093",
      "source_summary": "The standard error of estimate is used to construct a confidence interval around an examinee's predicted criterion score.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "standard error of estimate",
        "predicted criterion score",
        "confidence interval"
      ],
      "vignette": "A psychometrician is explaining how to interpret scores from a newly validated employment selection test. After computing the regression equation linking test scores to supervisor ratings of job performance, she notes that any single predicted criterion score carries uncertainty. She explains that by using the standard error of estimate, practitioners can construct a confidence interval around each examinee's predicted criterion score, giving a range within which the true job performance rating is likely to fall. The psychometrician emphasizes that this practice is essential for responsible interpretation of predictive validity data.",
      "question": "What psychometric procedure is the psychometrician describing when she places a range around each applicant's predicted job performance rating?",
      "options": {
        "A": "Constructing a confidence interval using the standard error of estimate",
        "B": "Constructing a confidence interval using the standard error of measurement",
        "C": "Computing a shrinkage-corrected validity coefficient",
        "D": "Determining the discrimination index for each test item"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. The standard error of estimate quantifies the typical deviation between predicted and actual criterion scores in a regression context, and it is used to build a confidence interval around any given predicted criterion score.",
        "B": "The standard error of measurement (SEM) is also used to construct confidence intervals, but it applies to an observed test score to estimate the range of a true score — not to a predicted criterion score in a regression equation. Confusing SEM with the standard error of estimate is a common error.",
        "C": "Shrinkage correction adjusts a validity coefficient downward to account for overfitting to a specific sample, improving generalizability. It does not involve placing a range around an individual's predicted criterion score.",
        "D": "The discrimination index reflects how well a test item differentiates high- from low-scoring examinees; it is an item-level statistic used in test construction, not a procedure for placing uncertainty bands around predicted scores."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-093-vignette-L2",
      "source_question_id": "093",
      "source_summary": "The standard error of estimate is used to construct a confidence interval around an examinee's predicted criterion score.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "predictive validity",
        "regression"
      ],
      "vignette": "A large urban school district has developed a reading readiness test administered to kindergartners. The test was validated against end-of-first-grade reading scores, and the district's psychologist reports strong predictive validity. During a parent meeting, a parent asks how confident the district can be that their child's predicted end-of-year reading score is accurate. The psychologist, noting that the child scored at the 70th percentile on the readiness test, explains that any score produced by the regression equation is accompanied by a margin of uncertainty. He describes a formal procedure for quantifying this uncertainty and expressing it as a range around the child's predicted score.",
      "question": "Which statistical concept is the psychologist using to express the uncertainty around the child's predicted first-grade reading score?",
      "options": {
        "A": "The standard error of measurement applied to the criterion score",
        "B": "A shrinkage-adjusted validity estimate",
        "C": "The coefficient of determination",
        "D": "The standard error of estimate used to construct a confidence interval around the predicted criterion score"
      },
      "correct_answer": "D",
      "option_explanations": {
        "D": "This is correct. In predictive validity studies, the standard error of estimate reflects the average discrepancy between predicted and actual criterion scores; it is used to form a confidence interval that communicates how far the child's true score might deviate from the regression-based prediction.",
        "A": "The standard error of measurement is used to estimate the range of a true score around an observed test score; it applies to test scores themselves, not to criterion scores predicted by a regression equation, so it does not fit this situation.",
        "B": "Shrinkage adjustment corrects the validity coefficient for capitalization on chance in the derivation sample and produces a more conservative estimate of the predictor–criterion relationship for new samples. It does not yield a range around a single individual's predicted score.",
        "C": "The coefficient of determination (r²) expresses the proportion of criterion variance explained by the predictor and describes overall model fit, but it does not produce an individual-level confidence interval around a specific predicted criterion value."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-093-vignette-L3",
      "source_question_id": "093",
      "source_summary": "The standard error of estimate is used to construct a confidence interval around an examinee's predicted criterion score.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "regression equation"
      ],
      "vignette": "A graduate student is presenting findings from a study validating a personality measure as a predictor of therapy outcome. Her advisor asks how she would communicate to a clinician the degree of precision associated with a specific client's forecasted improvement score. The student replies that she would take the output from the regression equation, and then apply a well-known formula that uses the predictor–criterion correlation and the variability of actual outcome scores to compute a value. She would then use that value to mark off an upper and lower bound around the forecasted score. The advisor notes, approvingly, that this method appropriately accounts for the fact that even a highly valid predictor does not yield perfect forecasts.",
      "question": "What is the student describing when she constructs upper and lower bounds around the predicted outcome score?",
      "options": {
        "A": "A confidence interval derived from the standard error of measurement",
        "B": "A credibility interval based on cross-validation shrinkage",
        "C": "A confidence interval derived from the standard error of estimate",
        "D": "A specificity-adjusted prediction band based on base rates"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The standard error of estimate is computed from the validity coefficient and the standard deviation of the criterion and reflects prediction error in a regression context. Placing a confidence interval around a predicted criterion score using this value is the standard procedure for communicating forecast precision.",
        "A": "The standard error of measurement is used to build confidence intervals around an observed test score to estimate an examinee's true score. It is computed from the reliability coefficient and score standard deviation — not from the predictor–criterion correlation — so it does not apply to predicted criterion scores.",
        "B": "A credibility interval in psychometrics typically refers to a Bayesian-derived range in meta-analytic or validity generalization contexts, reflecting the distribution of true validity coefficients. It addresses whether a validity coefficient generalizes across settings, not the precision of a single individual's predicted criterion score.",
        "D": "Specificity and base rates are concepts from diagnostic decision-making and signal detection theory; they help evaluate how well a test correctly identifies negatives or adjusts for condition prevalence. They do not produce regression-based prediction bands around forecasted criterion scores."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-093-vignette-L4",
      "source_question_id": "093",
      "source_summary": "The standard error of estimate is used to construct a confidence interval around an examinee's predicted criterion score.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "criterion variance"
      ],
      "vignette": "A consulting psychologist presents a technical report on a cognitive battery used by a law enforcement agency to screen officer candidates. He notes that the validity coefficient between the battery and academy performance is r = .52, and he provides each candidate's predicted academy performance value derived from the linear equation. A senior administrator asks whether these predicted values can be taken at face value when making selection decisions. The psychologist explains that the residual spread around predictions — which is partly a function of criterion variance not captured by the predictor — must be formally quantified and incorporated into each candidate's predicted value before comparisons between candidates can be made responsibly. He proceeds to compute a single index of this residual spread and use it to build a range for each candidate's predicted score.",
      "question": "What procedure is the psychologist using to support responsible comparison of candidates' predicted scores?",
      "options": {
        "A": "Correcting validity coefficients for restriction of range to improve criterion variance coverage",
        "B": "Applying the standard error of measurement to communicate uncertainty in each candidate's observed battery score",
        "C": "Adjusting the validity coefficient for shrinkage to account for sample-specific capitalization on chance",
        "D": "Using the standard error of estimate to construct a confidence interval around each candidate's predicted criterion score"
      },
      "correct_answer": "D",
      "option_explanations": {
        "D": "This is correct. The 'residual spread' around predictions that the psychologist references is precisely what the standard error of estimate captures — the typical deviation of actual criterion scores from predicted values. Using it to bound each candidate's predicted score is the appropriate method for conveying prediction uncertainty before making consequential comparisons.",
        "A": "Restriction of range correction adjusts a validity coefficient that has been artificially deflated because the sample was selected on either the predictor or criterion. While relevant when a law enforcement agency has already screened applicants, this procedure produces a corrected correlation coefficient, not a confidence interval around individual predicted scores.",
        "B": "The standard error of measurement quantifies uncertainty in an observed test score relative to the examinee's latent true score, based on reliability. Although it could inform how much a candidate's battery score might fluctuate, it does not address the uncertainty in a predicted criterion value derived from a regression equation.",
        "C": "Shrinkage correction reduces the validity coefficient to reflect expected drop in predictive accuracy when the regression equation is applied to a new sample. It improves the validity estimate's generalizability but does not produce an individual-level range around a specific predicted criterion score."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-093-vignette-L5",
      "source_question_id": "093",
      "source_summary": "The standard error of estimate is used to construct a confidence interval around an examinee's predicted criterion score.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher has developed a brief questionnaire that she believes can forecast how well people will do in a leadership development program six months later. After collecting data from 200 participants, she fits a line through the cloud of points relating questionnaire scores to program completion ratings. She is satisfied with how closely questionnaire scores and program ratings cluster around that line. A colleague then raises a concern: when the researcher gives a specific forecast number for a new applicant, that number is not exact, and decisions made solely on that number could be unfair. The researcher responds by computing a value derived from how tightly the actual program ratings cluster around her line — taking into account both how strong the association is and how spread out those ratings are — and then adds and subtracts a fixed multiple of that value from each applicant's individual forecast number to produce a range. She argues that any comparison between two applicants should consider whether their forecast ranges overlap before treating one as superior.",
      "question": "What procedure is the researcher using when she adds and subtracts a fixed multiple of her computed value from each applicant's individual forecast number?",
      "options": {
        "A": "Constructing a confidence interval using the standard error of measurement around each applicant's observed questionnaire score",
        "B": "Constructing a confidence interval using the standard error of estimate around each applicant's predicted criterion score",
        "C": "Applying a shrinkage formula to adjust the forecasting equation for overfitting to the derivation sample",
        "D": "Computing a specificity-based decision band to account for base rates of program success"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The value the researcher computes — derived from the tightness of actual criterion scores around the regression line, incorporating both the strength of the predictor–criterion association and the spread of criterion scores — is the standard error of estimate. Multiplying it by a fixed constant and placing it above and below each applicant's predicted score is precisely how a confidence interval around a predicted criterion score is constructed.",
        "A": "The standard error of measurement would be computed from the questionnaire's reliability coefficient and the spread of questionnaire scores, and it would be placed around each applicant's observed questionnaire score to estimate their true score range. The researcher's procedure, however, is applied to predicted program completion ratings — the criterion side — not to the questionnaire scores themselves, ruling this out.",
        "C": "Shrinkage formulas (e.g., Wherry's formula) are applied to the validity coefficient or regression equation to anticipate how much predictive accuracy will decrease when the equation is used with a new sample. This procedure modifies the equation or coefficient globally and does not produce applicant-level forecast ranges, making it incompatible with what the researcher describes.",
        "D": "Specificity-based decision bands and base rate adjustments come from signal detection and diagnostic decision-making frameworks. They address the probability that a test correctly identifies true negatives or the background frequency of an outcome, not the quantification of prediction error around a regression-derived individual forecast."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-04-vignette-L1",
      "source_question_id": "04",
      "source_summary": "A true/false test is likely to have the lowest reliability coefficient compared to other test formats, all other things being equal.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "reliability coefficient",
        "true/false",
        "item format"
      ],
      "vignette": "A psychometrician is comparing four versions of an anxiety screening instrument, each using a different item format but covering the same content. Version A uses a true/false format, Version B uses a 4-point Likert scale, Version C uses a 7-point Likert scale, and Version D uses a multiple-choice format with five options. She calculates the reliability coefficient for each version and notes a clear ordering in the results. All other test construction features — item content, length, and sample — are held constant across versions.",
      "question": "Based on principles of item format and reliability, which version is expected to yield the LOWEST reliability coefficient?",
      "options": {
        "A": "Version B, the 4-point Likert scale",
        "B": "Version A, the true/false format",
        "C": "Version D, the five-option multiple-choice format",
        "D": "Version C, the 7-point Likert scale"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. A 4-point Likert scale provides four response categories, generating more score variance than a binary format. Greater variance in item responses generally supports a higher reliability coefficient compared to true/false items.",
        "B": "Correct. True/false items are dichotomous, allowing only two possible responses. This severely restricts score variance at the item level, which in turn limits the reliability coefficient. All else being equal, binary-format tests consistently produce lower reliability estimates than tests with more response options.",
        "C": "Incorrect. A five-option multiple-choice format provides substantially more response gradation than a true/false item, producing greater score variance and thus a higher reliability coefficient. Multiple-choice formats are generally among the more reliable item types.",
        "D": "Incorrect. A 7-point Likert scale offers the greatest number of response options in this comparison, maximizing score variance at the item level and typically yielding the highest reliability coefficient of the four formats listed."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-04-vignette-L2",
      "source_question_id": "04",
      "source_summary": "A true/false test is likely to have the lowest reliability coefficient compared to other test formats, all other things being equal.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "internal consistency",
        "response options"
      ],
      "vignette": "A graduate student constructs a 40-item knowledge test on cognitive behavioral therapy techniques for a research methods course. Half the items are written as binary yes/no questions and half as five-option multiple-choice questions. After administering the test to 150 doctoral students — a group that is notably homogeneous in their CBT training background — she finds that the yes/no section yields a noticeably lower internal consistency estimate than the multiple-choice section. Her advisor mentions that the homogeneity of the sample may also be suppressing overall reliability, but that item format differences are still the primary explanation for the discrepancy between the two sections.",
      "question": "Which psychometric principle BEST explains why the yes/no section produced a lower internal consistency estimate than the multiple-choice section?",
      "options": {
        "A": "Restriction of range in the sample reduced observed score variance more severely for the multiple-choice section than for the yes/no section.",
        "B": "The yes/no format produced a lower difficulty index, causing floor effects that reduced inter-item correlations.",
        "C": "Fewer response options per item limits score variance, which reduces the magnitude of inter-item correlations and therefore lowers internal consistency.",
        "D": "The yes/no items had a lower discrimination index because they were drawn from a narrower content domain than the multiple-choice items."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. Restriction of range does suppress reliability, and the homogeneous sample is a real concern here; however, the advisor explicitly identifies item format as the primary explanation. Restriction of range would affect both sections similarly, not explain the discrepancy between them.",
        "B": "Incorrect. A low difficulty index (items that are too easy or too hard) can reduce inter-item correlations, but this is a separate issue from the number of response options. The vignette does not indicate that yes/no items were systematically easier or harder — it attributes the difference to format, not difficulty.",
        "C": "Correct. Binary items (yes/no or true/false) restrict the variance of each item's score distribution to a maximum at p = .50. With less variance, the potential magnitude of inter-item correlations is attenuated, which directly reduces internal consistency coefficients such as coefficient alpha.",
        "D": "Incorrect. The discrimination index reflects how well an item differentiates high from low scorers, and a low discrimination index does reduce internal consistency. However, the vignette specifies that both item sets cover the same content domain (CBT techniques), so content narrowness is not the distinguishing factor here."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-04-vignette-L3",
      "source_question_id": "04",
      "source_summary": "A true/false test is likely to have the lowest reliability coefficient compared to other test formats, all other things being equal.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "coefficient alpha"
      ],
      "vignette": "A state licensing board is reviewing three candidate examinations developed by different test vendors. Examination 1 consists of 100 items, each answered with a simple agree/disagree response. Examination 2 consists of 60 items rated on a six-step frequency scale. The board notes that both exams were administered to large, diverse candidate pools, and that Examination 1 has a higher item count, which the board initially believes should compensate for any format-related disadvantages. A consultant is asked to explain why Examination 1's coefficient alpha remains substantially lower than Examination 2's, even after correcting for test length using the Spearman-Brown formula.",
      "question": "What is the MOST likely explanation for why Examination 1's reliability remains lower than Examination 2's even after the Spearman-Brown correction is applied?",
      "options": {
        "A": "Examination 1's larger sample introduced more measurement error, reducing the coefficient alpha relative to Examination 2.",
        "B": "The Spearman-Brown correction assumes parallel forms, and the two exams are not parallel, which invalidates the comparison.",
        "C": "The agree/disagree format restricts item-level score variance, attenuating inter-item correlations in ways that adding more items of the same binary format cannot fully remedy.",
        "D": "Examination 1's longer length introduced construct-irrelevant variance through fatigue effects, which depressed its coefficient alpha below what the Spearman-Brown prediction would suggest."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. Larger, more diverse samples generally increase observed score variance, which tends to raise rather than lower reliability estimates. Sample size itself does not introduce measurement error; if anything, Examination 1's large diverse pool should have helped, not hurt, its alpha.",
        "B": "Incorrect. The Spearman-Brown formula does assume certain conditions (e.g., equal item quality), and its application across non-parallel tests requires caution. However, this is a general caveat that applies to any cross-test comparison and does not specifically explain why a binary-format exam with more items still falls short — the question asks for the most likely explanation, which is format-driven variance restriction.",
        "C": "Correct. The Spearman-Brown prophecy formula predicts reliability gains from adding more items of the same type. However, if each additional item is binary (agree/disagree), the per-item variance ceiling remains low. Because coefficient alpha is a function of inter-item covariances relative to total score variance, adding more low-variance items provides diminishing returns compared to a format that inherently generates greater item-level variance.",
        "D": "Incorrect. Test fatigue is a legitimate threat to reliability in long tests, but the Spearman-Brown correction accounts for added length by modeling the expected effect of additional items. Fatigue would need to be an unusually severe and specific confound to override a length correction, and the vignette provides no evidence of fatigue effects — this is a plausible but less parsimonious explanation than the format-driven variance restriction."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-04-vignette-L4",
      "source_question_id": "04",
      "source_summary": "A true/false test is likely to have the lowest reliability coefficient compared to other test formats, all other things being equal.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "Two research teams each develop a 50-item employment screening battery measuring conscientiousness. Team A uses items with only two mutually exclusive answer choices per item, while Team B uses items with six graded answer choices per item. Both batteries are administered to the same large applicant pool, and both are constructed with equivalent content coverage, similar item-total correlations, and comparable construct validity evidence. The test director reviews the psychometric reports and observes that Team B's battery consistently outperforms Team A's on one key metric, even though Team A's test appears superficially advantageous because binary items eliminate guessing artifacts and force a decisive judgment from respondents. The director concludes that the difference is attributable not to content or construct coverage but to a fundamental mathematical property of the scoring distributions.",
      "question": "Which psychometric property BEST accounts for the test director's observation that Team B's battery outperforms Team A's on the key metric?",
      "options": {
        "A": "Team B's six-choice items produce higher difficulty indices, bringing items closer to the optimal difficulty level for maximizing discrimination.",
        "B": "Team A's binary format reduces the variance of individual item scores, which attenuates inter-item covariances and thereby lowers the reliability estimate relative to Team B.",
        "C": "Team A's binary format increases the standard error of measurement by widening the confidence interval around each applicant's true score.",
        "D": "Team B's graded items reduce acquiescence response bias, which inflates apparent internal consistency in Team A's battery and makes the comparison misleading."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Difficulty index (proportion passing) reflects item easiness, not directly the number of response options. While six-choice items may distribute responses differently, optimizing difficulty is a separate consideration from format-driven variance restriction. This option conflates item difficulty with the format's effect on score variance.",
        "B": "Correct. Binary items have a maximum possible variance of 0.25 (p × q, where p = q = 0.50), whereas six-graded-choice items can achieve substantially higher variance. Since reliability coefficients such as coefficient alpha are mathematically dependent on inter-item covariances relative to total score variance, the restricted per-item variance in Team A's binary format depresses inter-item covariances and therefore lowers the overall reliability estimate. This is a direct mathematical consequence, not a content or construct issue.",
        "C": "Incorrect. The standard error of measurement (SEM) is related to reliability (SEM = SD√(1−r)), so lower reliability does increase SEM. However, SEM is a consequence of lower reliability, not an independent explanation for it — the question asks for what accounts for the difference in reliability, and SEM is downstream of that difference, not the causal mechanism.",
        "D": "Incorrect. Acquiescence bias (the tendency to agree with items regardless of content) can inflate internal consistency by artificially aligning item responses. However, this would be a threat for Team A only if items were keyed in a single direction, and the vignette specifies the items are 'mutually exclusive choices,' not agree/disagree statements. More importantly, the director explicitly attributes the difference to a mathematical scoring property, not response bias."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-04-vignette-L5",
      "source_question_id": "04",
      "source_summary": "A true/false test is likely to have the lowest reliability coefficient compared to other test formats, all other things being equal.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A test development committee is comparing two newly constructed instruments designed to measure the same psychological construct in a clinical population. Instrument X presents each question so that the person being assessed must choose one of two diametrically opposite statements, a design the committee chose specifically to eliminate any ambiguity in responses and to ensure that no intermediate or hedged answers are possible. Instrument Y, by contrast, allows respondents to place themselves along a continuum with several steps between the two poles. Both instruments were piloted on the same group of 300 adults, have the same number of questions, and were developed with equal care to ensure each question taps the intended construct. After scoring, one instrument consistently produces a higher value on the metric used to evaluate whether the questions within each instrument are working together coherently — despite the fact that Instrument X's design was originally praised for forcing clearer decisions from respondents.",
      "question": "Which explanation BEST accounts for why Instrument Y consistently outperforms Instrument X on the metric that evaluates within-instrument coherence?",
      "options": {
        "A": "Instrument X's forced-choice design introduces social desirability bias more severely than Instrument Y's continuum format, which suppresses the coherence metric by creating systematic response distortion.",
        "B": "Instrument Y's multi-step continuum allows each item's score to carry more distributional spread, which mathematically increases the strength of relationships among items and thereby raises the within-instrument coherence metric.",
        "C": "Instrument X's two-option design increases the likelihood of random guessing among ambivalent respondents, introducing unsystematic error that reduces the coherence metric more than Instrument Y's format does.",
        "D": "Instrument Y's graduated response scale is better suited for measuring psychological constructs on a continuum, making the questions more content-valid, which in turn raises the coherence metric by improving construct alignment."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Social desirability bias is a real threat to self-report measurement, and forced-choice formats are sometimes used precisely to reduce it (not increase it). Even if some bias existed, the vignette attributes the difference to the structural design of the response format, and social desirability would not systematically explain a consistent advantage for Instrument Y across a general clinical pilot sample.",
        "B": "Correct. When each item can only take one of two values, the maximum possible variance for that item is constrained (p × q ≤ 0.25). In contrast, a multi-step continuum allows item scores to span a wider range of values, producing greater variance. The metric evaluating within-instrument coherence — internal consistency — is mathematically dependent on the covariances among items relative to total score variance. Higher per-item variance allows for larger covariances, which raises the coherence metric regardless of whether the content or construct coverage has changed. This is a mathematical property of the scoring distribution, not a reflection of content quality.",
        "C": "Incorrect. Random guessing introduces unsystematic (random) error, which does lower reliability. However, the vignette specifies that the two-option design was chosen to eliminate ambiguity and force decisive responses — there is no indication that respondents are guessing randomly. The committee's intent and the described design argue against guessing as the primary mechanism, making this an appealing but ultimately unsupported explanation.",
        "D": "Incorrect. Content validity reflects the degree to which items adequately sample the domain of the construct. While a continuum format may intuitively seem more aligned with measuring psychological traits, the vignette explicitly states both instruments were developed with equal care to tap the intended construct. Higher content validity improves construct coverage but does not directly raise internal consistency unless it also changes the statistical relationships among item scores — the mechanism described in option B."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-07-vignette-L1",
      "source_question_id": "07",
      "source_summary": "In the context of factor analysis, \"orthogonal\" means uncorrelated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "factor analysis",
        "orthogonal rotation",
        "uncorrelated"
      ],
      "vignette": "A psychometrician develops a new personality inventory and submits the item pool to factor analysis. After extracting four factors, she applies an orthogonal rotation to the solution. She reports that the resulting factors are uncorrelated with one another, meaning that a person's score on one factor provides no information about their score on any other factor. She contrasts this with an alternative rotation method that allows the factors to correlate.",
      "question": "What does the term 'orthogonal' specifically mean in the context of this factor analysis?",
      "options": {
        "A": "The factors are statistically independent and share zero correlation with one another.",
        "B": "The factors are allowed to correlate with one another to reflect realistic psychological constructs.",
        "C": "Each item loads on one and only one factor, with all cross-loadings constrained to zero.",
        "D": "The number of factors retained equals the number of eigenvalues greater than 1.0."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. In factor analysis, 'orthogonal' literally means at right angles, which translates statistically to zero correlation between factors. An orthogonal rotation such as Varimax produces factors that are uncorrelated, meaning knowing a person's score on one factor tells you nothing about their score on another.",
        "B": "Incorrect. Allowing factors to correlate describes an oblique rotation (e.g., Promax, Oblimin), which is the alternative to orthogonal rotation. Oblique rotation is often preferred when the underlying constructs are theoretically expected to correlate, which is the opposite of the orthogonal assumption.",
        "C": "Incorrect. Constraining each item to load on exactly one factor with zero cross-loadings describes a simple structure ideal, not the definition of orthogonality. Simple structure is a goal that can be pursued through either orthogonal or oblique rotation and is a separate concept from the correlation between factors.",
        "D": "Incorrect. Retaining factors with eigenvalues greater than 1.0 is the Kaiser criterion, a rule for deciding how many factors to extract before rotation. It has no bearing on whether the retained factors are orthogonal (uncorrelated) or oblique (correlated)."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-07-vignette-L2",
      "source_question_id": "07",
      "source_summary": "In the context of factor analysis, \"orthogonal\" means uncorrelated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "factor analysis",
        "Varimax"
      ],
      "vignette": "A research team is constructing a multidimensional measure of occupational stress. They administer the 60-item questionnaire to 400 hospital workers, a sample that skews heavily toward nurses, and submit the data to exploratory factor analysis. The team chooses Varimax rotation to finalize their factor structure. They note with satisfaction that the resulting subscale scores can be entered simultaneously into regression models without concern about multicollinearity between predictors derived from the subscales.",
      "question": "Why can the researchers enter the subscale scores simultaneously into regression without multicollinearity concerns?",
      "options": {
        "A": "The large sample size of 400 participants provides enough statistical power to suppress multicollinearity.",
        "B": "Varimax is an orthogonal rotation, which constrains the resulting factors to be uncorrelated with one another.",
        "C": "The questionnaire's high internal consistency ensures that items cluster cleanly and do not overlap across subscales.",
        "D": "The homogeneous nurse-heavy sample restricts the range of scores, artificially reducing correlations among subscales."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Varimax is a type of orthogonal rotation. Orthogonal means uncorrelated, so factor scores derived from an orthogonal solution are, by definition, uncorrelated with each other. Uncorrelated predictors do not produce multicollinearity in regression, which is precisely why the researchers can safely enter all subscale scores simultaneously.",
        "A": "Incorrect. Sample size affects statistical power and the stability of factor solutions, but it does not reduce the conceptual or statistical correlation between subscale scores. Even with 4,000 participants, if the factors were allowed to correlate (oblique rotation), multicollinearity could still be a concern.",
        "C": "Incorrect. Internal consistency (e.g., coefficient alpha) reflects the degree to which items within a single subscale hang together, not the degree to which different subscales correlate with one another. High internal consistency is about reliability within a factor, not independence between factors.",
        "D": "Incorrect. Restriction of range in a homogeneous sample can attenuate correlations, but this is a sampling artifact that would distort results rather than a psychometric property of the rotation method. This explanation describes a methodological limitation, not a principled reason for subscale independence."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-07-vignette-L3",
      "source_question_id": "07",
      "source_summary": "In the context of factor analysis, \"orthogonal\" means uncorrelated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "rotation"
      ],
      "vignette": "A graduate student is analyzing data from a new measure of executive functioning. She extracts three factors and, after consulting with her advisor, applies a rotation method that preserves right-angle relationships among the factor axes in geometric space. The student is initially drawn toward an alternative rotation because some executive functioning subprocesses, like working memory and inhibitory control, are known to be empirically related in the neuropsychological literature. However, her advisor argues that for the purposes of building a scoring key with clean, independent subscales, the chosen rotation is more practical. When the scoring key is complete, the correlation between any pair of subscale total scores is approximately zero.",
      "question": "What is the key psychometric property that makes the correlation between subscale scores approximately zero?",
      "options": {
        "A": "The rotation method used constrains the factors to be uncorrelated, meaning each factor axis is perpendicular to the others.",
        "B": "The rotation method allows factor axes to shift toward clusters of items, producing oblique factors that nonetheless happen to be empirically uncorrelated in this sample.",
        "C": "The three-factor solution achieved simple structure, ensuring that each item loads on exactly one factor and does not contribute to other subscale scores.",
        "D": "The subscales have high discriminant validity, meaning each subscale correlates more strongly with external criteria uniquely related to that subscale than with the other subscales."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The clue is 'right-angle relationships among the factor axes in geometric space,' which is the geometric definition of orthogonality — factors at 90 degrees to one another are uncorrelated by definition. The vignette's red herring is the known empirical correlation among executive functions, which might tempt the student toward oblique rotation, but the advisor's choice of a rotation preserving right angles explains the zero correlations between subscales.",
        "B": "Incorrect. An oblique rotation allows factor axes to move away from 90-degree angles toward clusters of items, which by definition permits factors to correlate. If the rotation were oblique, factors could be empirically uncorrelated by chance, but that would not be a designed property; the vignette describes a principled geometric constraint, not a coincidental empirical outcome.",
        "C": "Incorrect. Simple structure refers to the pattern of item loadings — ideally each item loads high on one factor and near zero on others — and is a desirable property of any rotation solution. Simple structure can reduce cross-contamination between subscales, but it does not mathematically force the correlation between factor scores to be zero; only orthogonality does that.",
        "D": "Incorrect. Discriminant validity is a construct validity concept that evaluates whether a scale does not correlate with measures of theoretically distinct constructs. It describes the relationship between a subscale and external criteria, not the internal relationship among subscales derived from the same factor analysis. High discriminant validity does not explain why two subscales from the same instrument are uncorrelated."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-07-vignette-L4",
      "source_question_id": "07",
      "source_summary": "In the context of factor analysis, \"orthogonal\" means uncorrelated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "perpendicular"
      ],
      "vignette": "A measurement researcher presents findings from a new 45-item cognitive abilities battery. He reports that the axes representing the latent dimensions in his final solution were kept perpendicular to one another throughout the analytic process. Critics at a conference suggest that his choice of method was theoretically inappropriate because cognitive abilities are well-known to share a general intelligence factor, implying they should co-vary. The researcher counters that, regardless of theoretical debates, the practical consequence of his analytic choice is that any two composite scores derived from his battery convey entirely non-redundant information. He notes that this property is especially desirable for a battery intended to identify differentially weak cognitive domains for intervention planning.",
      "question": "What specific mathematical property of the researcher's solution explains why the composite scores convey entirely non-redundant information?",
      "options": {
        "A": "The composite scores were derived from factors that are uncorrelated, a consequence of maintaining perpendicular factor axes.",
        "B": "The battery achieved high convergent validity because items measuring the same ability cluster tightly together on each factor.",
        "C": "The researcher used confirmatory rather than exploratory methods, which fixes cross-loadings at zero and prevents shared variance across composites.",
        "D": "The battery was standardized on a representative normative sample, ensuring that score distributions do not artificially overlap across composites."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The subtle hint word is 'perpendicular.' In factor analysis, perpendicular axes are orthogonal axes, and orthogonal factors are by definition uncorrelated. Uncorrelated factors produce composite scores that share no variance — they are non-redundant. The vignette is designed to appear as though it is primarily about the theoretical debate over general intelligence (suggesting oblique rotation would be preferable), but the researcher's key point is the mathematical consequence of perpendicularity: zero correlation between composites.",
        "B": "Incorrect. Convergent validity refers to the degree to which a measure correlates with other measures of the same construct, and it is a construct validity concept assessed with external criteria, not within a factor solution. Tight item clustering on a single factor reflects simple structure and within-factor homogeneity, not the independence between separate composite scores.",
        "C": "Incorrect. Confirmatory factor analysis (CFA) can fix cross-loadings to zero, which reduces item-level contamination across factors. However, fixing cross-loadings does not by itself make factors uncorrelated — a CFA model can estimate correlated factors even with cross-loadings constrained to zero. The non-redundancy of composites depends on whether factor correlations are constrained to zero (orthogonality), not solely on the cross-loading structure.",
        "D": "Incorrect. Standardization on a representative normative sample affects the metric and reference population of scores but has no bearing on whether the underlying latent factors are correlated. A test normed on the most representative sample possible would still produce correlated composites if the rotation method permitted factor correlations."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-07-vignette-L5",
      "source_question_id": "07",
      "source_summary": "In the context of factor analysis, \"orthogonal\" means uncorrelated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A test developer builds a battery with four separate scores, each intended to measure a distinct psychological characteristic. After completing the statistical analysis used to derive the scores, she announces that she has chosen a method that, by design, guarantees the four scores will have zero relationships with each other in the population. A colleague argues that this choice was a mistake because the four characteristics being measured are, in everyday life, substantially interrelated. The developer acknowledges the real-world interrelation but explains that her priority was ensuring that each score adds unique information when all four are used together to predict a clinical outcome. A third researcher notes that a different, equally common version of the same analytic method would have allowed the scores to correlate, which would have been more consistent with the natural structure of the domain.",
      "question": "What feature of the developer's chosen analytic method is directly responsible for guaranteeing zero relationships among the four scores?",
      "options": {
        "A": "The method constrains the underlying dimensions to be uncorrelated with one another, a property defined by the geometric angle between their mathematical representations.",
        "B": "The method assigns each test item to exactly one score category with no overlap, preventing any single item from contributing to more than one score.",
        "C": "The method standardizes each score to have a mean of zero and a standard deviation of one before computing relationships, which arithmetically removes shared variance.",
        "D": "The method estimates each score's reliability separately and removes measurement error from each score before computing their relationships, which attenuates artifactual overlap."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The scenario describes an orthogonal rotation in factor analysis. The key details are: a method that guarantees zero relationships among derived scores by design, a contrasted alternative that permits correlations, and the developer's goal of unique predictive contribution for each score. All of these describe orthogonality — the constraint that factor axes are perpendicular (at right angles), which forces factor correlations to zero. The red herrings are the real-world interrelation of the constructs (suggesting oblique rotation) and the predictive goal (suggesting regression-based approaches).",
        "B": "Incorrect. Assigning each item to exactly one score with no overlap describes simple structure — the ideal loading pattern in factor analysis where items have high loadings on one factor and near-zero loadings on others. Simple structure reduces cross-contamination at the item level but does not itself constrain factor correlations to zero. A simple structure solution can be achieved with either orthogonal or oblique rotation.",
        "C": "Incorrect. Standardizing scores to mean zero and standard deviation one (z-scores) changes the metric but does not remove correlations between variables. Two z-scored variables can still correlate as strongly as their raw counterparts. Standardization is a scaling procedure, not a method for ensuring statistical independence between scores.",
        "D": "Incorrect. Removing measurement error from scores before computing their correlations describes disattenuation — a correction procedure that actually increases observed correlations toward their true-score values, the opposite of reducing them to zero. This technique is used to estimate what correlations would be if measures were perfectly reliable, not to guarantee independence among scores."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-03-vignette-L1",
      "source_question_id": "03",
      "source_summary": "Coefficient alpha, also known as Cronbach's alpha, is used to determine a test's internal consistency reliability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "coefficient alpha",
        "internal consistency",
        "Cronbach's alpha"
      ],
      "vignette": "A psychometrician develops a 40-item self-report scale measuring trait anxiety. After administering the scale to a large normative sample, she computes Cronbach's alpha to evaluate the internal consistency of the instrument. The resulting coefficient alpha of .91 indicates that the items are highly interrelated and appear to be measuring the same underlying construct. She concludes that the scale has strong internal consistency reliability.",
      "question": "Which type of reliability is the psychometrician evaluating when she computes Cronbach's alpha?",
      "options": {
        "A": "Test-retest reliability",
        "B": "Internal consistency reliability",
        "C": "Parallel forms reliability",
        "D": "Inter-rater reliability"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Test-retest reliability assesses score stability across two separate administrations of the same test over time. The psychometrician computed a single-administration statistic (coefficient alpha) rather than correlating scores from two testing occasions, so this does not apply.",
        "B": "Coefficient alpha (Cronbach's alpha) is the primary index of internal consistency reliability, reflecting the degree to which all items on a scale measure the same underlying construct. A value of .91 confirms strong inter-item coherence, making this the correct answer.",
        "C": "Parallel forms reliability is estimated by correlating scores from two different but equivalent forms of the same test administered to the same group. No alternate form was mentioned here; only a single form was administered.",
        "D": "Inter-rater reliability quantifies the degree of agreement between two or more raters or scorers. Because this is a self-report scale scored objectively, rater agreement is not relevant to the reliability index being computed."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-03-vignette-L2",
      "source_question_id": "03",
      "source_summary": "Coefficient alpha, also known as Cronbach's alpha, is used to determine a test's internal consistency reliability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "internal consistency",
        "item-total correlation"
      ],
      "vignette": "A researcher constructs a 25-item measure of resilience for use with college students who have experienced adverse childhood events. After collecting data from 300 participants, she examines item-total correlations and finds they range from .42 to .68, suggesting strong internal consistency across items. The researcher notes that her sample is predominantly female (78%), which she acknowledges as a demographic limitation. She computes a single reliability coefficient based on the average inter-item covariance to summarize the overall consistency of the scale.",
      "question": "Which reliability statistic is the researcher most likely computing to quantify the internal consistency of her resilience measure?",
      "options": {
        "A": "Spearman-Brown corrected split-half reliability coefficient",
        "B": "Pearson product-moment correlation between two administrations",
        "C": "Coefficient alpha",
        "D": "Intraclass correlation coefficient"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The Spearman-Brown formula is used to correct a split-half reliability coefficient for the effect of test length when a test is divided into two halves. Although split-half is also an internal consistency method, the vignette specifies that the reliability index is based on the average inter-item covariance across all items simultaneously, which is the defining computation of coefficient alpha rather than a split-half approach.",
        "B": "A Pearson correlation between two administrations would estimate test-retest reliability. The vignette describes a single administration, so a stability index across time is not being computed here.",
        "C": "Coefficient alpha summarizes internal consistency by averaging the inter-item covariances relative to total score variance, precisely matching the description of a reliability coefficient based on average inter-item covariance from a single administration. High item-total correlations further support interpreting this as a coherent, internally consistent scale.",
        "D": "The intraclass correlation coefficient (ICC) is used primarily to assess inter-rater reliability or the consistency of ratings across multiple raters or repeated measurements. Because the vignette involves a single self-report administration with no raters, the ICC is not the appropriate statistic."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-03-vignette-L3",
      "source_question_id": "03",
      "source_summary": "Coefficient alpha, also known as Cronbach's alpha, is used to determine a test's internal consistency reliability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "inter-item"
      ],
      "vignette": "A graduate student is developing a new depression screening questionnaire for primary care settings. After administering 30 items to 250 adult patients, he notices that removing one particular item substantially raises the average inter-item relationship among the remaining 29 items. His advisor points out that the test was administered on two separate occasions two weeks apart, and the scores correlated at .85, which the advisor praises as an indicator of temporal stability. The student, however, is more interested in whether items within a single administration cohesively tap the same dimension of depression.",
      "question": "The student's primary concern about the questionnaire corresponds to which psychometric property?",
      "options": {
        "A": "Test-retest reliability",
        "B": "Internal consistency reliability",
        "C": "Content validity",
        "D": "Parallel forms reliability"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Test-retest reliability, reflected in the .85 correlation across two administrations two weeks apart, is explicitly praised by the advisor. This is the red herring in the vignette. The student's concern, however, is not about temporal stability but about whether the items within a single administration cohere — the defining question of internal consistency reliability.",
        "B": "Internal consistency reliability captures the degree to which items within a single test administration measure the same underlying construct. The student's focus on how removing one item elevates average inter-item relationships — a core diagnostic step in computing and improving coefficient alpha — directly reflects a concern with internal consistency.",
        "C": "Content validity refers to how well the items representatively sample the domain of interest (e.g., depression). Although a depression questionnaire developer would eventually consider content validity, the student's specific concern is quantitative item cohesion within a single administration, not representativeness of the item domain.",
        "D": "Parallel forms reliability requires administering two independently constructed but equivalent test forms to the same group and correlating the scores. Only one form of the questionnaire is described in the vignette, so parallel forms reliability cannot be the focus of the student's concern."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-03-vignette-L4",
      "source_question_id": "03",
      "source_summary": "Coefficient alpha, also known as Cronbach's alpha, is used to determine a test's internal consistency reliability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "homogeneity"
      ],
      "vignette": "A test publisher releases a 50-item occupational stress inventory and reports a reliability coefficient of .93 based on a single administration to 500 employees. A measurement consultant reviewing the technical manual notes that the reported statistic assumes all items share equal true-score variance and that the items collectively reflect a homogeneity in the latent variable being measured. She further observes that the manual fails to report any data from a second administration or an alternate form, yet the publisher markets the instrument as 'highly reliable.' A critic argues that the .93 figure may be artificially inflated because the scale contains several highly redundant item pairs that are nearly identical in wording.",
      "question": "The reliability coefficient of .93 reported in the technical manual is best identified as which of the following?",
      "options": {
        "A": "Split-half reliability corrected with the Spearman-Brown formula",
        "B": "Test-retest reliability",
        "C": "Coefficient alpha",
        "D": "Kuder-Richardson Formula 20 (KR-20) reliability"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Split-half reliability, even when corrected with the Spearman-Brown formula, estimates internal consistency by dividing the test into two halves and correlating them. Unlike coefficient alpha, which uses all possible split combinations simultaneously, the Spearman-Brown approach depends on how the two halves are formed and does not directly incorporate the assumption of item homogeneity across all items. The vignette's description of a single summary statistic based on item homogeneity and no alternative form points more specifically to coefficient alpha.",
        "B": "Test-retest reliability requires correlating scores from two separate administrations of the same test. The vignette explicitly states no data from a second administration were reported, ruling this out. The .93 was derived from a single testing session.",
        "C": "Coefficient alpha is derived from a single administration and explicitly rests on the assumption that items reflect a homogeneous latent construct — matching the consultant's observation. The critic's point that redundant items inflate the coefficient is a well-documented limitation of alpha specifically: items with nearly identical wording create spuriously high inter-item correlations that drive alpha upward without representing genuinely broader construct coverage.",
        "D": "KR-20 is mathematically equivalent to coefficient alpha but applies only to dichotomously scored items (e.g., right/wrong). An occupational stress inventory typically uses polytomous Likert-type response formats, making KR-20 inapplicable. Coefficient alpha is the appropriate generalization for items with more than two response options."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-03-vignette-L5",
      "source_question_id": "03",
      "source_summary": "Coefficient alpha, also known as Cronbach's alpha, is used to determine a test's internal consistency reliability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team administers a 35-item questionnaire about workplace satisfaction to 400 employees at a single session and computes a summary statistic that comes out to .87. Notably, one item about commute length shows virtually no relationship to the other 34 items; when that item is removed, the summary statistic rises to .91. The team had originally intended to readminister the questionnaire six months later to assess score stability, but funding ran out before they could do so. A consultant reviewing the data suggests that the high value of .87 is partly a function of the large number of items on the questionnaire and cautions the team against concluding that the questionnaire would necessarily perform similarly if the sample had been restricted to senior-level managers only.",
      "question": "The summary statistic of .87 that the research team computed is best described as which type of reliability estimate?",
      "options": {
        "A": "Test-retest reliability",
        "B": "Parallel forms reliability",
        "C": "Coefficient alpha",
        "D": "Split-half reliability"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Test-retest reliability is computed by correlating scores from two administrations of the same test to the same group across a time interval. The vignette explicitly states that a second administration was never completed due to funding constraints, ruling out test-retest reliability. The statistic was derived entirely from the single session of data.",
        "B": "Parallel forms reliability requires administering two independently developed but content-equivalent forms of a test and correlating the resulting scores. Only one form of the questionnaire exists in the vignette, and there is no mention of an alternate form being constructed or administered.",
        "C": "Coefficient alpha is estimated from a single administration and reflects how consistently all items measure the same underlying dimension. The finding that removing one poorly related item raises the statistic, the consultant's note that a greater number of items tends to increase the index (a known artifact of alpha's sensitivity to test length), and the single-session design all converge on coefficient alpha as the correct identification. The mention of restriction of range for senior managers is a red herring referencing a validity/generalizability concern, not a different reliability method.",
        "D": "Split-half reliability involves dividing a test's items into two halves, correlating the half-scores, and typically applying a correction formula for test length. Although split-half is also an internal consistency approach derived from a single administration, it does not directly produce sensitivity to individual item removal in the way described, nor is it the statistic for which test length (number of items) is a primary, well-known inflating factor in the same way it is for alpha. The vignette's details collectively align more precisely with the properties and known limitations of coefficient alpha."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-06-vignette-L1",
      "source_question_id": "06",
      "source_summary": "A test has divergent validity when it has low correlations with tests that measure unrelated constructs.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "divergent validity",
        "unrelated constructs",
        "low correlations"
      ],
      "vignette": "A psychometrician developing a new measure of working memory computes correlations between the new test and several existing instruments. She finds low correlations between her working memory measure and a standardized test of extraversion. She notes that this pattern of low correlations with unrelated constructs supports an important form of construct validity. The psychometrician reports that divergent validity has been established because the measures tap conceptually distinct domains.",
      "question": "Which psychometric concept is most directly illustrated by the low correlations between the working memory measure and the extraversion test?",
      "options": {
        "A": "Divergent validity",
        "B": "Convergent validity",
        "C": "Concurrent validity",
        "D": "Content validity"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Divergent (discriminant) validity is demonstrated when a test shows low correlations with measures of theoretically unrelated constructs, confirming that the test is not inadvertently measuring something other than its intended construct.",
        "B": "Incorrect. Convergent validity is demonstrated by high correlations between a new test and established measures of the same or closely related constructs — the opposite pattern from what is described here.",
        "C": "Incorrect. Concurrent validity refers to how well a test correlates with a criterion measure administered at approximately the same time; it does not specifically address the relationship between theoretically unrelated constructs.",
        "D": "Incorrect. Content validity concerns whether test items adequately sample the full domain of the construct being measured, and is typically established through expert review rather than correlation analysis with other instruments."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-06-vignette-L2",
      "source_question_id": "06",
      "source_summary": "A test has divergent validity when it has low correlations with tests that measure unrelated constructs.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "construct validity",
        "unrelated"
      ],
      "vignette": "A research team develops a self-report measure of trait anxiety for use with college students. After collecting data from a large undergraduate sample — which the authors note skews younger and has higher educational attainment than the general population — they correlate scores on the new anxiety scale with scores on an established measure of spatial reasoning ability. The resulting correlation is r = .04, which the team interprets as evidence supporting the construct validity of their measure. They argue that anxiety and spatial reasoning are theoretically unrelated domains and that finding a near-zero relationship between them strengthens their confidence in the test.",
      "question": "The team's interpretation of the near-zero correlation between the anxiety scale and the spatial reasoning measure best illustrates which form of validity evidence?",
      "options": {
        "A": "Predictive validity",
        "B": "Convergent validity",
        "C": "Divergent validity",
        "D": "Face validity"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. Divergent (discriminant) validity is established when a test shows negligible correlations with measures of theoretically unrelated constructs. The near-zero r between anxiety and spatial reasoning — domains with no theoretical overlap — is a textbook demonstration of this form of construct validity evidence.",
        "A": "Incorrect. Predictive validity refers to how well test scores forecast a criterion outcome measured at a future point in time (e.g., GPA or job performance), which is not what is being evaluated here.",
        "B": "Incorrect. Convergent validity is evidenced by high correlations between the new measure and other measures of the same or theoretically similar constructs; the team found the opposite pattern.",
        "D": "Incorrect. Face validity reflects whether a test appears on its surface to measure what it claims to measure, and is evaluated through subjective inspection of item content rather than through correlation-based statistical analysis."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-06-vignette-L3",
      "source_question_id": "06",
      "source_summary": "A test has divergent validity when it has low correlations with tests that measure unrelated constructs.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "multitrait-multimethod"
      ],
      "vignette": "A test developer uses a multitrait-multimethod matrix to evaluate her new measure of emotional regulation. She finds that scores on her emotional regulation scale correlate strongly (r = .78) with scores on an established empathy scale and moderately (r = .55) with scores on a mindfulness inventory — both encouraging patterns. However, the developer is particularly pleased that her emotional regulation scale correlates only r = .09 with a well-validated measure of verbal intelligence, even though verbal ability is known to influence performance on many self-report questionnaires. She argues this low correlation with verbal intelligence provides the strongest form of evidence for the test's discriminant properties.",
      "question": "The developer's conclusion about the r = .09 correlation between emotional regulation and verbal intelligence most directly reflects which type of validity evidence?",
      "options": {
        "A": "Convergent validity",
        "B": "Divergent validity",
        "C": "Incremental validity",
        "D": "Criterion-related validity"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Divergent (discriminant) validity is demonstrated by low correlations between a test and measures of theoretically distinct constructs. The r = .09 with verbal intelligence — a domain considered unrelated to emotional regulation — confirms that the scale is measuring something conceptually separate, which is precisely the discriminant evidence the developer is citing.",
        "A": "Incorrect. Convergent validity is supported by the high correlations with empathy (r = .78) and mindfulness (r = .55), not the near-zero correlation with verbal intelligence. Convergent evidence involves related constructs, not unrelated ones.",
        "C": "Incorrect. Incremental validity refers to whether a test adds predictive power over and above other already-available measures for predicting a specific outcome; the scenario describes correlational patterns among constructs, not prediction of an external criterion.",
        "D": "Incorrect. Criterion-related validity (concurrent or predictive) examines how well test scores relate to a criterion outcome and is not primarily concerned with distinguishing theoretically unrelated constructs from one another."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-06-vignette-L4",
      "source_question_id": "06",
      "source_summary": "A test has divergent validity when it has low correlations with tests that measure unrelated constructs.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "multicollinearity"
      ],
      "vignette": "A neuropsychologist administers a newly developed measure of processing speed to a sample of adults recovering from mild traumatic brain injury. Concerned about multicollinearity in his validation battery, he computes correlations between the processing speed measure and a standardized personality inventory assessing conscientiousness and a social desirability scale. He reports correlations of r = .11 and r = .07, respectively. Notably, the measure also shows strong correlations with a psychomotor reaction-time task (r = .71) and a coding subtest from a major intelligence battery (r = .68), patterns the neuropsychologist treats as less novel because they were theoretically expected. The neuropsychologist concludes that the low correlations with the personality and social desirability measures are actually the most informative evidence in his validation report, since they confirm the test is not confounded by response style or personality traits.",
      "question": "The neuropsychologist's emphasis on the low correlations with the personality inventory and social desirability scale as the 'most informative' validity evidence best reflects which psychometric concept?",
      "options": {
        "A": "Convergent validity",
        "B": "Discriminant validity",
        "C": "Construct underrepresentation",
        "D": "Concurrent validity"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Discriminant (divergent) validity is evidenced by low correlations between a test and measures of theoretically unrelated constructs such as personality traits and social desirability. The neuropsychologist correctly identifies these near-zero correlations — rather than the expected high correlations with related processing measures — as the most compelling discriminant evidence, because they rule out construct contamination.",
        "A": "Incorrect. Convergent validity is represented by the high correlations the neuropsychologist treats as unsurprising (r = .71 with reaction time; r = .68 with the coding subtest); it is not the focus of his most informative evidence claim, which centers on the low correlations with unrelated measures.",
        "C": "Incorrect. Construct underrepresentation occurs when a test fails to adequately sample the full breadth of the target construct, leading to an overly narrow measure. There is no evidence in the scenario that the processing speed test omits important facets of the construct.",
        "D": "Incorrect. Concurrent validity involves correlating a new test with an established criterion measure administered at roughly the same time; while some of the correlations described are concurrent, the neuropsychologist's specific interpretive claim about the low correlations with personality and social desirability is a discriminant rather than criterion-related validity argument."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-06-vignette-L5",
      "source_question_id": "06",
      "source_summary": "A test has divergent validity when it has low correlations with tests that measure unrelated constructs.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A graduate student designs a battery of assessments for a study on attention. After collecting data, she notices that scores on her new attention-focused instrument move closely in step with scores from another well-known attention measure (r = .82) and a similar cognitive tracking task (r = .76) — results she anticipated and considers unremarkable. She is far more excited about a separate finding: her instrument's scores have almost no relationship (r = .06) with an independently administered questionnaire about how much people enjoy socializing with others. She tells her advisor that this second finding, not the first, constitutes the most convincing demonstration that her tool is actually capturing what it is supposed to capture and nothing else.",
      "question": "The graduate student's reasoning about the r = .06 correlation with the socializing questionnaire most directly exemplifies which psychometric principle?",
      "options": {
        "A": "Convergent validity",
        "B": "Incremental validity",
        "C": "Divergent validity",
        "D": "Predictive validity"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. Divergent (discriminant) validity is established when a measure shows very low correlations with instruments assessing unrelated constructs — here, preference for socializing, which is theoretically unrelated to attentional capacity. The student's logic — that this near-zero correlation is the best proof that her instrument captures only the intended construct — is a precise application of divergent validity reasoning.",
        "A": "Incorrect. Convergent validity is reflected in the high correlations with the established attention measure and the cognitive tracking task (r = .82 and r = .76), which the student explicitly considers unremarkable. Convergent validity involves theoretically similar constructs, not unrelated ones like sociability.",
        "B": "Incorrect. Incremental validity refers to the degree to which a measure predicts a specific outcome above and beyond what can already be predicted by existing measures. The student is not predicting any outcome; she is comparing correlational patterns across conceptually distinct domains.",
        "D": "Incorrect. Predictive validity concerns how well scores on a measure forecast a future criterion variable (e.g., job performance or academic achievement). The scenario involves concurrent correlations between instruments and makes no reference to predicting future behavior or outcomes."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-10-vignette-L1",
      "source_question_id": "10",
      "source_summary": "The correction for attenuation formula is used to estimate the effect of increasing a predictor's reliability on its criterion-related validity coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "correction for attenuation",
        "reliability",
        "validity coefficient"
      ],
      "vignette": "A psychometrician is reviewing a newly developed cognitive screening instrument that yields a modest validity coefficient of .42 when correlated with a supervisor-rated job performance criterion. She notes that the predictor's reliability estimate is only .60, which she suspects is artificially suppressing the observed relationship. She decides to apply the correction for attenuation formula to estimate what the validity coefficient would be if the predictor had perfect reliability. The resulting disattenuated value is substantially higher than the observed .42, confirming her hypothesis.",
      "question": "Which psychometric procedure did the psychometrician use to estimate the theoretical upper bound of the predictor's criterion-related validity?",
      "options": {
        "A": "Restriction of range correction, which adjusts a validity coefficient upward when the sample used for validation is more homogeneous than the intended population, thereby underestimating the true relationship.",
        "B": "Correction for attenuation, which removes the attenuating effect of measurement error in the predictor (and/or criterion) to estimate what the validity coefficient would be if reliability were perfect.",
        "C": "Cross-validation shrinkage estimation, which quantifies how much a regression-derived validity coefficient is expected to drop when applied to a new sample due to capitalization on chance.",
        "D": "Concurrent validity adjustment, which re-estimates a validity coefficient by replacing a predictive design with a simultaneous criterion measurement to remove temporal decay in the relationship."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Restriction of range correction is applied when the validation sample is truncated on the predictor or criterion, causing underestimation of the true validity. While it also produces an upward adjustment to the observed coefficient, the scenario specifies that the suppression stems from low reliability, not from a restricted sample — making this option incorrect.",
        "B": "Correct. The correction for attenuation uses the formula r_true = r_observed / √(r_xx * r_yy) to estimate the validity that would be observed if measurement error were absent. The psychometrician explicitly identified low predictor reliability as the problem and applied this formula to obtain the disattenuated coefficient.",
        "C": "Cross-validation shrinkage estimation addresses the overfitting of a multiple regression equation to a specific sample; it predicts a decrease, not an increase, in the validity coefficient. The scenario involves a simple bivariate correlation and improvement due to reliability, not regression-based prediction error.",
        "D": "Concurrent validity adjustment is not a standard psychometric correction procedure. Concurrent validity refers to the study design in which predictor and criterion are measured simultaneously, and switching from predictive to concurrent designs does not involve a mathematical correction to the coefficient itself."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-10-vignette-L2",
      "source_question_id": "10",
      "source_summary": "The correction for attenuation formula is used to estimate the effect of increasing a predictor's reliability on its criterion-related validity coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "measurement error",
        "disattenuated"
      ],
      "vignette": "A research team at a university hospital develops a self-report anxiety scale intended to predict treatment dropout rates in a community mental health sample. Despite strong face validity and high item-total correlations within the scale, the observed correlation between anxiety scores and treatment dropout is only .35. The lead researcher, who is working with a predominantly low-income, bilingual population, suspects that inconsistent administration conditions across sites introduced substantial measurement error into the predictor scores. She applies a statistical procedure to estimate what the correlation would look like if this error were entirely removed, and the resulting disattenuated estimate rises to .61.",
      "question": "Which statistical procedure did the researcher apply to produce the higher, theoretically corrected validity estimate?",
      "options": {
        "A": "Correction for attenuation, which estimates the validity coefficient that would be obtained if the predictor (and/or criterion) were measured without error, by dividing the observed correlation by the square root of the product of their reliability coefficients.",
        "B": "Restriction of range correction, which estimates the true validity in the full population by statistically compensating for the narrowed variance introduced when only clinic-enrolled patients — not the full community — were sampled.",
        "C": "Cross-validation, which applies the regression weights derived from one subsample to a holdout subsample to provide a less biased estimate of how well the anxiety scale generalizes to new patients.",
        "D": "Standard error of measurement adjustment, which scales each individual's observed score toward the population mean to reduce the influence of random error on any given test administration."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The correction for attenuation is specifically designed to remove the suppressive effect of measurement error on an observed validity coefficient. The researcher's concern about inconsistent administration introducing error into predictor scores directly maps onto this procedure, and the jump from .35 to .61 is consistent with a disattenuated estimate.",
        "B": "Restriction of range correction is plausible here because a clinic-enrolled sample could be more homogeneous than the community at large. However, the scenario explicitly attributes the low validity to measurement error from inconsistent administration — not to sample truncation — making attenuation correction the more precise answer.",
        "C": "Cross-validation is a resampling procedure used to evaluate the generalizability of a predictive model by testing it on held-out data. It does not produce an upwardly corrected coefficient, and it does not address measurement error in the predictor; it is used to detect shrinkage, not to correct for attenuation.",
        "D": "The standard error of measurement (SEM) describes the precision of an individual score estimate and is used in constructing confidence intervals around observed scores. It is not used to adjust a validity coefficient, and 'scaling scores toward the population mean' describes regression to the mean, not an SEM adjustment."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-10-vignette-L3",
      "source_question_id": "10",
      "source_summary": "The correction for attenuation formula is used to estimate the effect of increasing a predictor's reliability on its criterion-related validity coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "reliability"
      ],
      "vignette": "A graduate student is validating a brief neuropsychological screening battery for predicting functional independence in older adults with mild cognitive impairment. The observed correlation between battery scores and a clinician-rated functional independence measure is .38, which the dissertation committee deems disappointingly low. One committee member points out that the battery was administered over two brief, poorly standardized sessions, and that the functional independence ratings were made by different clinicians using subjective judgment — both conditions known to introduce noise into the data. The student runs a formula-based analysis to estimate the theoretical validity coefficient that would exist if both sources of unreliability were entirely eliminated, arriving at a much higher corrected estimate. A second committee member cautions that this corrected value exceeds 1.0 and therefore may not be practically interpretable.",
      "question": "What analytical procedure did the student use, and what does the resulting estimate represent?",
      "options": {
        "A": "Restriction of range correction applied to both the predictor and criterion simultaneously; the corrected value represents the validity that would exist in a more heterogeneous population where cognitive impairment severity spans a wider range.",
        "B": "Correction for attenuation applied to both the predictor and criterion; the corrected value represents the estimated validity coefficient that would be obtained if both instruments had perfect reliability, free from measurement error.",
        "C": "Multiple regression with cross-validation; the corrected value represents the shrinkage-adjusted R² that accounts for the small sample size and the number of predictors in the battery.",
        "D": "Incremental validity analysis; the corrected value represents the additional variance in functional independence explained by the neuropsychological battery over and above the clinician ratings alone."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The correction for attenuation formula — r_true = r_observed / √(r_xx × r_yy) — corrects for unreliability in both the predictor and the criterion simultaneously. When both reliabilities are low, the denominator becomes small, and the corrected estimate can mathematically exceed 1.0, which is precisely the artifact the committee member flagged. This perfectly describes the student's procedure and outcome.",
        "A": "Restriction of range correction addresses situations in which sample truncation on the predictor or criterion artificially lowers the observed correlation. The scenario's problem is explicitly attributed to poor standardization and subjective rating introducing noise — not to a restricted range of cognitive impairment severity — so this correction does not apply.",
        "C": "Multiple regression with cross-validation produces a shrinkage estimate that is lower than the original R², not higher. The scenario describes a single bivariate correlation that is being corrected upward, which is inconsistent with the cross-validation procedure, which corrects downward for overfitting.",
        "D": "Incremental validity analysis compares the predictive utility of a new predictor over and above an existing one using hierarchical regression. In this scenario there is no comparison of predictors; the student is correcting a single bivariate correlation for attenuation, which is a fundamentally different analytic goal."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-10-vignette-L4",
      "source_question_id": "10",
      "source_summary": "The correction for attenuation formula is used to estimate the effect of increasing a predictor's reliability on its criterion-related validity coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "true score"
      ],
      "vignette": "A testing company is under contract to demonstrate that its personnel selection battery predicts supervisor ratings of job performance in a large manufacturing firm. The observed correlation between battery scores and performance ratings is .29, which the company's psychologist presents as evidence of meaningful criterion-related validity. However, an independent reviewer notes that the performance ratings were collected using a hastily assembled single-item global rating scale with no anchor descriptions, and she estimates its true score variance is only a small fraction of its total observed variance. The reviewer argues that if one were to mathematically remove the noise introduced by this rating scale's poor quality, the underlying relationship between the construct measured by the battery and the construct of job performance is considerably stronger than .29 — potentially around .55. She further notes that this same logic could be applied to the battery itself if its reliability were also imperfect.",
      "question": "The independent reviewer's argument is best understood as an application of which psychometric concept?",
      "options": {
        "A": "Restriction of range, because performance ratings collected within a single firm are likely truncated relative to the broader labor market, causing the .29 correlation to underestimate the population-level validity of the battery.",
        "B": "Correction for attenuation, because the reviewer is estimating what the observed validity coefficient would become if the criterion's (and potentially the predictor's) measurement error were statistically removed, revealing the relationship between the underlying true scores.",
        "C": "Criterion contamination, because the global rating scale likely reflects factors unrelated to true job performance — such as supervisor liking — which introduces construct-irrelevant variance that suppresses the valid relationship.",
        "D": "Differential item functioning, because the single-item rating scale may systematically disadvantage certain demographic subgroups within the manufacturing workforce, producing a spuriously low correlation with battery scores."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The reviewer's reasoning maps precisely onto the correction for attenuation: she identifies that the criterion (performance ratings) has low reliability (small true score variance relative to total variance), and she argues that removing this error would reveal a stronger underlying relationship. Her mention of extending the same logic to the battery reflects the full bivariate form of the formula, r_true = r_observed / √(r_xx × r_yy). The phrase 'true score variance' is the peripheral hint embedded in the scenario.",
        "A": "Restriction of range is a plausible answer because single-firm samples often have reduced variance in job performance compared to the general workforce. However, the reviewer's argument is explicitly about the quality and noise level of the rating scale itself — the small fraction of true score variance — not about sample homogeneity, which distinguishes attenuation from range restriction.",
        "C": "Criterion contamination refers to systematic bias in a criterion measure — for example, when a supervisor's knowledge of test scores influences their performance ratings. This would inflate, not suppress, the criterion validity coefficient. The scenario describes random error from a poorly constructed scale, not systematic construct-irrelevant bias.",
        "D": "Differential item functioning (DIF) is a concept applied to item-level analysis of test fairness across subgroups; it refers to items that function differently for groups of equal ability. The scenario involves no item-level analysis, no subgroup comparison, and no fairness evaluation — it is entirely about the aggregate reliability of a criterion measure."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-10-vignette-L5",
      "source_question_id": "10",
      "source_summary": "The correction for attenuation formula is used to estimate the effect of increasing a predictor's reliability on its criterion-related validity coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research director at a staffing firm is frustrated because her team's new candidate assessment tool keeps showing weak relationships with how well new hires actually perform on the job, despite the tool being internally coherent and theoretically grounded. An outside consultant reviews the situation and focuses not on the assessment tool itself but on how job performance is being tracked: performance is captured by a single quarterly score that varies enormously from manager to manager based on personal judgment, mood, and shifting standards. The consultant runs a calculation using only the tool's known consistency index and the performance tracking system's known consistency index, and she produces a new, higher number that she says represents what the relationship between the two underlying constructs actually is — stripped of all noise from imperfect measurement on either side. The research director is cautioned that this new number is a theoretical ceiling, not an achieved result, and that it carries practical limitations if the consistency estimates themselves are inaccurate.",
      "question": "What is the consultant's calculation best described as, and what does the resulting number represent?",
      "options": {
        "A": "A correction for the narrowed range of candidates in the sample, producing an estimate of how strongly the tool would predict performance if it were administered to a more diverse applicant pool rather than only those hired.",
        "B": "A correction for attenuation, producing an estimate of the theoretical validity between the two underlying constructs if both the assessment tool and the performance tracking system were measured without any random error.",
        "C": "A shrinkage correction applied to the observed relationship, producing a more conservative estimate of how well the tool would perform on a new group of candidates not used during the original evaluation.",
        "D": "A specificity adjustment applied to the performance tracking system, producing an estimate of how accurately high performers are identified when the subjectivity of manager judgment is statistically controlled."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Despite the complete absence of technical jargon, the scenario describes the correction for attenuation in behavioral terms: the consultant uses the consistency indices (reliability estimates) of both the tool and the performance tracking system to compute a corrected coefficient that represents the relationship between the two underlying constructs free from measurement noise. The cautions about it being a theoretical ceiling and about the accuracy of the consistency estimates are standard caveats associated with this correction.",
        "A": "A range restriction correction addresses sample truncation — the phenomenon where only hired candidates (not all applicants) are assessed, potentially narrowing the variance of predictor scores. The scenario does not describe differential selection rates or a truncated sample; the problem is explicitly attributed to inconsistent manager judgment in measuring the outcome, pointing to unreliability rather than range restriction.",
        "C": "Shrinkage correction (cross-validation) produces an estimate lower than the observed relationship, not higher. The consultant produces a higher number than the observed relationship, which is the opposite of what shrinkage correction does. Additionally, shrinkage addresses overfitting in regression models, not the noise introduced by unreliable criterion measurement.",
        "D": "Specificity is a signal-detection concept referring to the proportion of true negatives correctly identified by a test — it is not a statistical correction applied to a validity coefficient. While the scenario does involve subjectivity in the performance measure, 'statistically controlling for subjectivity' via a specificity adjustment is not a recognized psychometric procedure and does not produce an upward correction to a bivariate relationship."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-09-vignette-L1",
      "source_question_id": "09",
      "source_summary": "A test's sensitivity is the proportion of people who have a disorder and are correctly identified by the test as having the disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "sensitivity",
        "true positives",
        "disorder"
      ],
      "vignette": "A psychologist is evaluating a newly developed screening instrument for major depressive disorder. She administers the test to 200 individuals who have already been diagnosed with the disorder through a structured clinical interview. Of these 200 individuals, the test correctly identifies 170 as having the disorder, yielding a rate of 170/200 = 0.85. The psychologist notes that the test's ability to detect true positives is strong, and she concludes this is a key psychometric strength of the instrument when screening populations where the disorder is prevalent.",
      "question": "Which psychometric property is the psychologist evaluating when she calculates the proportion of individuals who have the disorder and are correctly identified by the test?",
      "options": {
        "A": "Specificity",
        "B": "Positive predictive value",
        "C": "Sensitivity",
        "D": "Concurrent validity"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Specificity refers to the proportion of individuals who do NOT have the disorder and are correctly identified by the test as not having it (true negatives divided by all true negatives plus false positives). The vignette describes performance among those who DO have the disorder, making specificity incorrect.",
        "B": "Positive predictive value is the proportion of people who test positive who actually have the disorder, which depends on base rates in the population. The vignette calculates a proportion from those already confirmed to have the disorder, not from those who tested positive, so this does not fit.",
        "C": "Sensitivity is defined as the proportion of individuals who truly have the disorder and are correctly identified as positive by the test (true positives divided by true positives plus false negatives). The calculation of 170/200 from confirmed cases matches this definition exactly.",
        "D": "Concurrent validity refers to the degree to which a test's scores correlate with scores on an established criterion measure administered at the same time. While criterion-related, it involves a correlation coefficient across scores rather than a hit-rate calculation among confirmed cases."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-09-vignette-L2",
      "source_question_id": "09",
      "source_summary": "A test's sensitivity is the proportion of people who have a disorder and are correctly identified by the test as having the disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "screening",
        "false negatives"
      ],
      "vignette": "A research team develops a brief cognitive screening instrument for early-stage Alzheimer's disease and pilots it at a memory disorders clinic. The clinic's patient population is older adults who are predominantly low-income and have limited formal education, a factor the researchers note may affect raw score distributions. When the instrument is administered to 150 patients with confirmed Alzheimer's diagnoses, it fails to flag 45 of them as impaired. The researchers are concerned about the large number of false negatives and debate whether the instrument is suitable for clinical use in populations where missed cases carry significant consequences.",
      "question": "The researchers' primary concern about the instrument — specifically the proportion of confirmed cases that the test fails to detect — reflects a weakness in which psychometric property?",
      "options": {
        "A": "Negative predictive value",
        "B": "Sensitivity",
        "C": "Specificity",
        "D": "Predictive validity"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Negative predictive value is the proportion of people who test negative who truly do not have the disorder. While it involves negative results, it is calculated from those who tested negative and depends on prevalence, not from confirmed cases as described here.",
        "B": "Sensitivity is the proportion of people who truly have the disorder and are correctly detected by the test. Here, only 105 of 150 confirmed cases are identified (sensitivity = 0.70), and the 45 false negatives directly represent a failure of sensitivity. The concern about missed cases in high-stakes screening highlights why sensitivity is the critical property.",
        "C": "Specificity concerns the proportion of individuals without the disorder who are correctly identified as negative. The vignette exclusively describes performance among patients who DO have Alzheimer's, so specificity is not at issue in this scenario.",
        "D": "Predictive validity refers to how well test scores forecast a future criterion outcome, typically assessed via correlation. The researchers are not examining future outcomes but rather how accurately the test identifies current confirmed cases, which is criterion-related but specifically a signal-detection property."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-09-vignette-L3",
      "source_question_id": "09",
      "source_summary": "A test's sensitivity is the proportion of people who have a disorder and are correctly identified by the test as having the disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "criterion"
      ],
      "vignette": "A hospital system adopts a brief behavioral checklist to triage patients entering the emergency department for possible suicidality. Administration records from the past year show that among the 90 patients who were subsequently confirmed as at acute suicide risk through full psychiatric evaluation, the checklist had raised a clinical flag for 63 of them. The hospital administrator notes approvingly that the checklist has a very high rate of correctly identifying patients without risk — 95% of non-at-risk patients are correctly cleared — and suggests this makes it a strong instrument. However, a consulting psychologist argues that a different statistical property, not the one the administrator cited, is actually more concerning given the severity of missing a true case.",
      "question": "Which psychometric property is the consulting psychologist most likely emphasizing as the more critical concern for this instrument?",
      "options": {
        "A": "Specificity",
        "B": "Positive predictive value",
        "C": "Concurrent validity coefficient",
        "D": "Sensitivity"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Specificity is the very property the administrator is praising — 95% of non-at-risk patients are correctly cleared. The psychologist is directing attention away from specificity toward the property that captures missed true cases, so specificity is the administrator's metric, not the psychologist's concern.",
        "B": "Positive predictive value represents the proportion of those who test positive who actually have the condition, and it is influenced by base rates. While relevant to clinical utility, it does not directly capture the proportion of true cases missed, which is the psychologist's specific concern given the consequence of false negatives.",
        "C": "A concurrent validity coefficient measures the correlation between the checklist and the psychiatric evaluation as criterion measures given simultaneously. This is a broader validity index that does not directly address the rate at which true cases are missed, making it an indirect and imprecise characterization of the psychologist's concern.",
        "D": "Sensitivity captures the proportion of true positive cases that are correctly identified. Only 63 of 90 confirmed at-risk patients were flagged (sensitivity ≈ 0.70), meaning 30% of truly at-risk individuals were missed. In a high-stakes triage context, this false-negative rate is precisely the property a consulting psychologist would flag as the critical concern."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-09-vignette-L4",
      "source_question_id": "09",
      "source_summary": "A test's sensitivity is the proportion of people who have a disorder and are correctly identified by the test as having the disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "base rate"
      ],
      "vignette": "A managed care organization evaluates a 10-item anxiety disorder screener before deploying it across its primary care network. The validation study is conducted in a specialty anxiety clinic where the base rate of diagnosable anxiety disorders is 80%. In that sample, the instrument correctly identifies 88% of those with an anxiety disorder and correctly clears 72% of those without one. When the same instrument is deployed in primary care — where the base rate drops to 15% — practitioners notice that a much smaller proportion of patients who screen positive actually receive a confirmed diagnosis on follow-up evaluation. An organizational consultant argues that clinicians are misattributing this deterioration in apparent performance to a change in the instrument's ability to detect true cases, when in fact that underlying ability has not changed.",
      "question": "The consultant is clarifying that the metric clinicians are observing decline in primary care is NOT the same as the property that remained stable across settings. Which property remained stable?",
      "options": {
        "A": "Positive predictive value",
        "B": "Specificity",
        "C": "Sensitivity",
        "D": "Negative predictive value"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Positive predictive value (PPV) is explicitly what clinicians are observing decline — when base rates fall from 80% to 15%, PPV drops substantially even if the instrument has not changed. The consultant is drawing a contrast between PPV and the stable property, so PPV is the metric that changed, not the one that remained stable.",
        "B": "Specificity — the proportion of true negatives correctly identified — also does not change with shifts in base rate; however, the vignette specifies that clinicians are noticing fewer confirmed diagnoses among those who screen positive (a PPV phenomenon), and the consultant is pointing to the detection ability for true cases as stable. Specificity concerns non-cases, not the detection of true cases that the consultant emphasizes.",
        "C": "Sensitivity is an intrinsic property of the instrument reflecting the proportion of true cases correctly identified; it does not depend on the prevalence of the disorder in the population. The instrument's 88% hit rate among those who truly have anxiety remains constant regardless of whether the setting is a specialty clinic or primary care. The consultant is correctly noting that sensitivity is stable while PPV has fallen due to reduced base rates.",
        "D": "Negative predictive value (NPV) does change with base rates — as prevalence decreases, NPV increases. Since the consultant is identifying a property that did NOT change, and NPV does change with base rates, this option is incorrect. NPV also concerns those who test negative, not the instrument's ability to detect true cases."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-09-vignette-L5",
      "source_question_id": "09",
      "source_summary": "A test's sensitivity is the proportion of people who have a disorder and are correctly identified by the test as having the disorder.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A large urban school district introduces a brief teacher-completed rating form intended to identify children who may need referral for a particular learning difficulty. An independent team reviews data from 240 children who had already been confirmed through extensive individualized evaluation as having the learning difficulty. Of these, the rating form had generated a referral recommendation for 168. The review team notes that the district is pleased with the form because 91% of children without the learning difficulty are correctly spared unnecessary referral, and district officials credit this as evidence the form 'works well.' The independent team, however, points out that the property officials are celebrating is not the one most relevant to the stated purpose of finding children who need help, and that on the property most directly tied to that purpose, the form performs considerably worse.",
      "question": "Which property is the independent review team identifying as most relevant to the form's stated purpose, and on which the form performs considerably worse than officials assume?",
      "options": {
        "A": "The proportion of children who received a referral recommendation and were later confirmed to have the learning difficulty",
        "B": "The proportion of confirmed cases of the learning difficulty that the form correctly flagged for referral",
        "C": "The degree to which scores on the rating form correlate with scores on the individualized evaluation administered at the same time",
        "D": "The proportion of children without the learning difficulty who are correctly identified as not needing referral"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This describes positive predictive value — what proportion of those flagged actually have the difficulty. While it is affected by base rates and is clinically relevant, it is not what the review team is emphasizing. The team is focused on the form's ability to find children who need help, which requires examining performance among the confirmed cases, not among those who were flagged.",
        "B": "This describes sensitivity — the proportion of children who truly have the learning difficulty who are correctly identified by the form. Of the 240 confirmed cases, only 168 were flagged (sensitivity = 0.70 or 70%). The district's stated purpose is to find children who need help, which means missing 30% of confirmed cases is a direct failure of this property. The independent team is correctly pointing out that the property officials celebrated (correctly clearing non-cases) is different from — and worse than — the property needed to fulfill the form's purpose.",
        "C": "This describes concurrent validity, assessed via a correlation coefficient between the rating form and the individualized evaluation. While relevant to validation, the team's concern is not about the linear relationship between scores but about the hit rate among confirmed cases, which is a signal-detection property rather than a correlational one.",
        "D": "This is the very property — correctly clearing children who do not have the learning difficulty — that district officials are celebrating at 91%. The review team explicitly distinguishes this from the property most relevant to finding children who need help, so this option reflects the officials' focus, not the team's concern."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-01-vignette-L1",
      "source_question_id": "01",
      "source_summary": "For a test that consists of 50 true/false questions, the optimal average item difficulty level (p) is .75.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "difficulty index",
        "true/false",
        "optimal"
      ],
      "vignette": "A psychometrician is constructing a 50-item true/false knowledge test to maximize score variability among examinees. During item analysis, she reviews the difficulty index (p) for each item. She knows that for forced-choice formats with two response options, the optimal average item difficulty differs from that used with multiple-choice formats. Her supervisor asks her to identify the target average p value that will best maximize score variance and test reliability for this true/false instrument.",
      "question": "What is the optimal average item difficulty level (p) for a 50-item true/false test designed to maximize score variability?",
      "options": {
        "A": "p = .50, because this value maximizes variance for any item format regardless of the number of response options.",
        "B": "p = .75, because for a two-option forced-choice format, the optimal p is midway between chance (.50) and perfect performance (1.0).",
        "C": "p = .85, because true/false items should be kept easy to ensure high internal consistency across all examinees.",
        "D": "p = .60, because this value balances item difficulty with discrimination and is optimal for most educational tests."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. A p of .50 maximizes variance for items with no guessing floor, but true/false items have a chance level of .50, meaning random responding already yields .50 correct. Setting p = .50 for true/false items would actually reflect items near chance performance, not optimal discrimination.",
        "B": "Correct. For a two-alternative forced-choice format, the chance level is .50. The optimal p is the midpoint between chance and 1.0, which equals (.50 + 1.0) / 2 = .75. This maximizes score variance while accounting for guessing, thereby optimizing test reliability.",
        "C": "Incorrect. Setting average p at .85 produces a ceiling effect, reducing score variability among higher-ability examinees and lowering the test's ability to discriminate among individuals. High internal consistency depends on item variance, not item easiness.",
        "D": "Incorrect. A p of .60 is sometimes cited as a general target for multiple-choice items with four options, where chance is .25 and the optimal p approximates (.25 + 1.0) / 2 ≈ .625. This logic does not apply to two-option formats, where the optimal p is higher at .75."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-01-vignette-L2",
      "source_question_id": "01",
      "source_summary": "For a test that consists of 50 true/false questions, the optimal average item difficulty level (p) is .75.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "item difficulty",
        "two-option"
      ],
      "vignette": "A school psychologist is developing a 50-item classroom screening instrument for a rural district with limited testing resources. The items are all two-option (true/false) questions about reading comprehension strategies. Pilot data reveal that the average item difficulty across the 50 items is .62, and the test's split-half reliability estimate is lower than expected. The psychologist, who notes that the student sample has an unusually wide age range, wonders whether item difficulty targeting or sample characteristics are responsible for the reduced reliability.",
      "question": "Based on psychometric principles, what adjustment to average item difficulty would most directly improve the test's reliability for this two-option format?",
      "options": {
        "A": "Increase average item difficulty to p = .75, because for a two-option format this value maximizes item variance by targeting the midpoint between chance and ceiling performance.",
        "B": "Decrease average item difficulty to p = .50, because items at this level provide maximal variance for any type of test regardless of response format.",
        "C": "Increase average item difficulty to p = .85, because easier items produce higher item-total correlations in heterogeneous samples.",
        "D": "Retain p = .62, because the wide age range creates restriction of range that independently suppresses reliability regardless of item difficulty."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. For a two-option (true/false) format with a guessing floor of .50, the optimal average p is (.50 + 1.0) / 2 = .75. Shifting from the current p = .62 toward .75 would increase item variance and, consequently, improve split-half reliability.",
        "B": "Incorrect. A p of .50 maximizes variance only for items with a guessing floor of zero. For two-option items where random performance already yields .50 correct, items at p = .50 would reflect near-chance performance and provide minimal useful discrimination above guessing.",
        "C": "Incorrect. Easier items (higher p values, such as .85) reduce variance because most examinees answer them correctly, compressing score distributions and reducing item-total correlations. Easier items do not produce higher reliability in heterogeneous samples.",
        "D": "Incorrect. While a wide age range could introduce construct-irrelevant variance that affects reliability, this explanation does not address the core psychometric issue: the average item difficulty of .62 is suboptimal for a two-option format. Adjusting item difficulty is the most direct and theoretically grounded remedy."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-01-vignette-L3",
      "source_question_id": "01",
      "source_summary": "For a test that consists of 50 true/false questions, the optimal average item difficulty level (p) is .75.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A test developer reviews pilot data for a new 50-item credentialing exam. All items are presented in a binary correct/incorrect format with only two possible responses. The average proportion-correct across items is .63, and a reliability analysis yields a coefficient alpha of .71, which falls below the .80 threshold required for high-stakes decisions. The developer notes that the item-total correlations are moderately strong (average r = .35), and suggests that the relatively homogeneous candidate pool — all of whom have completed identical training — may be suppressing the reliability estimate. A colleague disagrees and argues that the primary problem is that score variance is not being maximized at the item level.",
      "question": "Which psychometric principle most directly supports the colleague's argument that item-level adjustments — rather than sample homogeneity — are the primary driver of the suboptimal reliability?",
      "options": {
        "A": "For a two-alternative item format, maximizing score variance requires targeting an average p of approximately .75, and the current average of .63 falls below this optimum, leaving item-level variance — and thus alpha — unnecessarily low.",
        "B": "Restriction of range from sample homogeneity compresses score variance at the test level independent of item difficulty, meaning that even optimally targeted items would yield low reliability in this population.",
        "C": "The moderate item-total correlations suggest that increasing the number of items, rather than adjusting difficulty, would more effectively raise coefficient alpha by extending test length per the Spearman-Brown formula.",
        "D": "Coefficient alpha is primarily a function of item intercorrelations rather than item difficulty, so the suboptimal alpha is better explained by insufficient item content overlap than by current difficulty levels."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. For a two-alternative forced-choice format with a chance level of .50, optimal p = (.50 + 1.0) / 2 = .75. The current average of .63 is below this optimum, meaning item variance is not being maximized. Because coefficient alpha is a function of item variance and covariance, suboptimal item difficulty directly suppresses alpha independent of sample characteristics.",
        "B": "Incorrect. Restriction of range from a homogeneous sample does reduce reliability, but the colleague's argument specifically attributes the problem to item-level variance, not sample-level variance. Furthermore, the vignette does not provide strong evidence that the sample is unusually homogeneous — it only notes identical training, which does not necessarily create severe restriction of range.",
        "C": "Incorrect. While the Spearman-Brown formula confirms that adding items increases reliability, this approach addresses test length rather than the item difficulty optimization the colleague is advocating. Extending test length with items at a suboptimal difficulty level would still yield less reliability than optimally targeted items, and it does not address the colleague's specific argument.",
        "D": "Incorrect. Coefficient alpha is mathematically related to both item intercorrelations and item variances. When items are uniformly at a suboptimal difficulty level, their individual variances — not just their intercorrelations — are reduced. The colleague's argument about item-level variance is specifically about difficulty targeting, which alpha does reflect, contrary to this option's claim."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-01-vignette-L4",
      "source_question_id": "01",
      "source_summary": "For a test that consists of 50 true/false questions, the optimal average item difficulty level (p) is .75.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "guessing"
      ],
      "vignette": "A psychometrician at a licensing board is troubleshooting a 50-item credentialing battery that has repeatedly failed to achieve acceptable reliability estimates across three pilot administrations. Reviewing the item statistics, she observes that the average proportion-correct is .64 and that item discrimination indices are generally adequate (average D = .30). She considers whether guessing behavior is distorting the results, because the binary response format makes it impossible to rule out random correct responses. A consultant suggests that the root cause is that the test items are systematically mistargeted for the format being used, and that correcting for this alone — without changing items or sample composition — would substantially improve reliability.",
      "question": "What specific mistargeting is the consultant most likely referring to, and what correction would address it?",
      "options": {
        "A": "The items are too difficult for the binary format; raising average p from .64 toward .75 would align item difficulty with the format's optimal level, maximizing item variance and improving reliability.",
        "B": "The items are mistargeted because the binary format inflates guessing, requiring the use of a correction-for-guessing formula to adjust raw scores before computing reliability.",
        "C": "The discrimination indices are acceptable but the items' content coverage is too narrow; broadening item content to span more facets of the construct would reduce redundancy and improve reliability.",
        "D": "The test is too short for its binary format; applying the Spearman-Brown prophecy formula to project the reliability of a longer version would reveal that doubling item count is the primary needed correction."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. For a two-alternative binary format, the guessing floor is .50, making the optimal average p = (.50 + 1.0) / 2 = .75. The current average of .64 falls below this optimum, meaning item variance is suboptimal for this format. The consultant's reference to 'format-based mistargeting' precisely describes this mismatch — the items are not mistargeted for the sample but for the two-option format's optimal difficulty level.",
        "B": "Incorrect. A correction-for-guessing formula adjusts scores by penalizing wrong answers to estimate true knowledge, but it does not address the core issue of suboptimal average item difficulty. Furthermore, correction-for-guessing is typically applied to multiple-choice formats and does not directly remedy the format-difficulty mismatch the consultant describes.",
        "C": "Incorrect. Content coverage relates to content validity rather than the format-based difficulty optimization the consultant references. Adequate discrimination indices (average D = .30) suggest that content and item quality are not the primary problems; the specific mismatch between current average p and the two-option format's optimal p is the more targeted explanation.",
        "D": "Incorrect. While the Spearman-Brown formula can project how reliability would improve with more items, the consultant specifically states that correcting the mistargeting alone — without adding items or changing the sample — would improve reliability. Doubling item count does not address the suboptimal difficulty targeting and would perpetuate the same mismatch at greater length."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-01-vignette-L5",
      "source_question_id": "01",
      "source_summary": "For a test that consists of 50 true/false questions, the optimal average item difficulty level (p) is .75.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A measurement specialist reviews an assessment developed for a professional certification program. The instrument contains exactly 50 questions, each of which has only two possible answers — one correct and one incorrect — with no partial credit. Across three independent pilot studies conducted with representative candidate groups, the average proportion of candidates answering each question correctly has stabilized at .65, and the overall score consistency index remains disappointingly below the acceptable threshold despite adequate inter-item relationships and an appropriate number of questions. The specialist argues that none of the sampled groups, item wordings, or administration conditions are responsible for the persistent problem; rather, the items are correctly constructed but numerically aimed at the wrong point on the performance continuum for this particular answer format. She asserts that shifting the average correct-response rate in one specific direction by approximately ten percentage points would bring the instrument into alignment with the format's theoretically optimal targeting and meaningfully raise the consistency index without replacing a single item or testing a different group.",
      "question": "What does the specialist most likely mean, and what shift is she recommending?",
      "options": {
        "A": "The specialist is recommending that average item difficulty be increased from .65 to approximately .75, because for a two-choice format the theoretically optimal average proportion-correct is midway between the chance level and perfect performance, and the current average falls below that optimum.",
        "B": "The specialist is recommending that average item difficulty be decreased from .65 to approximately .55, because items that are slightly harder than chance level produce maximum score spread in formats where random responding already yields many correct answers.",
        "C": "The specialist is recommending that average item difficulty be increased from .65 to approximately .85, because easier items reduce floor effects and produce higher inter-item agreement, which directly raises the consistency index.",
        "D": "The specialist is recommending that average item difficulty be increased from .65 to approximately .75, because .75 is universally optimal for all fixed-length tests regardless of response format, and the current average simply falls below this general psychometric benchmark."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. For a two-choice (binary) format, random responding produces a correct-response rate of .50 — this is the guessing floor. The theoretically optimal average p is the midpoint between this floor and perfect performance: (.50 + 1.0) / 2 = .75. The current average of .65 falls below this optimum, so shifting upward by approximately ten percentage points to .75 would maximize item-level variance and, consequently, the overall score consistency index.",
        "B": "Incorrect. Decreasing difficulty toward .55 moves items closer to the chance level of .50 for a two-choice format, not further from it. Items near chance performance provide minimal information about true differences among examinees and would reduce — not increase — score variance and the consistency index.",
        "C": "Incorrect. Raising average p to .85 produces near-ceiling performance, compressing score distributions and reducing the variance needed for high consistency. Easier items do not increase inter-item agreement in a way that benefits the consistency index; they reduce variance in individual item scores, which reduces, not raises, overall score variability.",
        "D": "Incorrect. The option correctly identifies .75 as the recommendation but provides a flawed rationale. The optimal p of .75 is not universally applicable to all test formats — it is specifically derived from the two-choice format's guessing floor of .50. For a four-option multiple-choice format, the optimal p would be approximately .625, derived from a chance level of .25. The correct answer must reference the format-specific derivation, not a universal benchmark."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-08-vignette-L1",
      "source_question_id": "08",
      "source_summary": "Before using a selection test to estimate how well job applicants will do on a measure of job performance one year after they're hired, you would want to make sure the selection test has adequate predictive validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "predictive validity",
        "job performance",
        "selection test"
      ],
      "vignette": "A human resources department wants to implement a new cognitive ability selection test to screen job applicants for an entry-level analyst position. The hiring manager wants to know whether scores on the selection test will accurately forecast employees' job performance ratings collected one year after hire. Before adopting the test company-wide, the psychologist consulting on the project explains that a key psychometric property of the test must be established. The psychologist notes that this property specifically concerns the relationship between test scores obtained at the time of application and a criterion measure assessed at a later point in time.",
      "question": "Which type of validity should the consulting psychologist establish before recommending use of this selection test?",
      "options": {
        "A": "Content validity",
        "B": "Concurrent validity",
        "C": "Predictive validity",
        "D": "Construct validity"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Content validity refers to whether a test adequately samples the domain of knowledge or skills it is intended to measure. It does not address the relationship between test scores and a future criterion measure such as job performance ratings collected one year later.",
        "B": "Concurrent validity is a form of criterion-related validity in which the test and the criterion are measured at approximately the same point in time. Because job performance ratings are collected one year after testing, the temporal separation rules out concurrent validity as the appropriate type here.",
        "C": "Predictive validity is the form of criterion-related validity in which test scores obtained at one point in time are correlated with a criterion measure obtained at a later point in time. Establishing a significant correlation between selection test scores and job performance ratings one year post-hire is exactly what predictive validity evidence provides.",
        "D": "Construct validity concerns whether a test measures the theoretical construct it purports to measure, typically evaluated through patterns of convergent and discriminant evidence. While important broadly, it does not specifically address the forecasting relationship between a selection test and a future job performance criterion."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-08-vignette-L2",
      "source_question_id": "08",
      "source_summary": "Before using a selection test to estimate how well job applicants will do on a measure of job performance one year after they're hired, you would want to make sure the selection test has adequate predictive validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "criterion",
        "forecasting"
      ],
      "vignette": "A large technology company is evaluating a new personality-based screening instrument for hiring software engineers. The company is interested in whether scores on the instrument, obtained during the application process, can be used for forecasting which candidates will receive high performance evaluations from their supervisors 12 months into employment. The instrument was developed by a team with strong expertise in personality theory and has excellent internal consistency. However, the consulting industrial-organizational psychologist cautions that good internal consistency alone does not guarantee the type of criterion-related evidence the company needs.",
      "question": "What specific type of validity evidence must be established before using this instrument to predict future supervisor performance evaluations?",
      "options": {
        "A": "Predictive validity",
        "B": "Concurrent validity",
        "C": "Incremental validity",
        "D": "Face validity"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Predictive validity requires demonstrating a correlation between test scores obtained at one time point and a criterion measure collected at a future time point. The 12-month gap between instrument administration during hiring and supervisor performance evaluations is precisely the design needed to establish predictive validity, which is what the company requires.",
        "B": "Concurrent validity is established when the predictor and criterion are measured at essentially the same time. Because the performance evaluations occur 12 months after the instrument is administered, concurrent validity does not capture the temporal forecasting relationship the company is seeking.",
        "C": "Incremental validity refers to the degree to which a test adds predictive power over and above what is already provided by other available predictors. While useful in test batteries, incremental validity is not the foundational type of criterion-related evidence being described here, where the question is simply whether the test predicts the criterion at all.",
        "D": "Face validity refers to how plausible or relevant a test appears to test-takers on the surface, not whether it actually predicts a criterion. High face validity can improve applicant motivation and acceptance, but it provides no empirical evidence that the instrument will forecast future job performance."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-08-vignette-L3",
      "source_question_id": "08",
      "source_summary": "Before using a selection test to estimate how well job applicants will do on a measure of job performance one year after they're hired, you would want to make sure the selection test has adequate predictive validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "criterion-related"
      ],
      "vignette": "A regional hospital system wants to use a structured situational judgment test to select nursing candidates. The test was originally validated on a sample of experienced nurses who completed both the test and a patient-care quality rating on the same day, yielding a strong criterion-related validity coefficient. A psychometrician reviewing the selection plan points out that the original validation study's design is fundamentally mismatched with the hospital's intended use, in which performance ratings will not be available until one year after hire. The hospital currently has access only to the original validation data, and the psychometrician recommends that a new study with an appropriate design be conducted before deployment.",
      "question": "Why does the psychometrician consider the original validation study insufficient for the hospital's intended use?",
      "options": {
        "A": "The original study established concurrent validity, but the intended use requires predictive validity evidence linking test scores at hire to performance ratings assessed one year later.",
        "B": "The original study failed to establish criterion-related validity of any kind, making the test unsuitable for personnel selection.",
        "C": "The original study only demonstrated content validity, whereas the hospital needs evidence that the test measures the nursing performance construct.",
        "D": "The original study's sample of experienced nurses may have inflated reliability estimates, reducing generalizability to applicants."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "When the test and criterion are administered on the same day, the study design establishes concurrent validity, not predictive validity. Because the hospital intends to use the test to forecast performance one year after hire, a predictive validity study — in which test scores at hiring are correlated with performance ratings collected later — is required. The original concurrent design cannot substitute for this temporal evidence.",
        "B": "The original study did establish criterion-related validity, specifically the concurrent type. The problem is not an absence of criterion-related validity evidence but rather that the wrong subtype was demonstrated for the hospital's prospective use case.",
        "C": "Content validity concerns whether test items adequately sample the relevant domain of skills or knowledge; it is not evaluated by correlating test scores with an external criterion. The original study's correlation with a performance rating on the same day is criterion-related evidence, not content validity, so this description is inaccurate.",
        "D": "While restriction of range and sample characteristics can affect validity coefficients, the psychometrician's concern is specifically about the temporal design mismatch — same-day administration versus one-year lag — not about reliability inflation. Reliability and validity are distinct psychometric properties, and reliability estimates from the experienced-nurse sample do not directly address the concurrent-versus-predictive validity distinction."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-08-vignette-L4",
      "source_question_id": "08",
      "source_summary": "Before using a selection test to estimate how well job applicants will do on a measure of job performance one year after they're hired, you would want to make sure the selection test has adequate predictive validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "temporal"
      ],
      "vignette": "A consulting firm has developed a cognitive ability battery and reports that it correlates strongly with supervisor ratings when both measures are collected from currently employed workers during the same evaluation cycle. Excited by these findings, a client organization proposes using the battery to screen job applicants, with the expectation that high scorers will be strong performers when evaluated 18 months after hire. A senior psychometrician reviews the validation report and warns that the empirical evidence provided, despite its high validity coefficient, may not support the proposed application. She notes that the temporal relationship between predictor and criterion in the original study differs in a critical way from the one the client intends to create.",
      "question": "What is the most precise reason the psychometrician concludes that the existing validity evidence is insufficient for the client's proposed use?",
      "options": {
        "A": "The validation study likely suffers from restriction of range because currently employed workers represent a pre-selected group, attenuating the obtained validity coefficient relative to an applicant pool.",
        "B": "The validation study established concurrent validity by correlating test and criterion scores measured at the same time, whereas selecting applicants to predict performance 18 months later requires predictive validity evidence.",
        "C": "The validation study used supervisor ratings as the criterion, but job performance at 18 months is better captured by objective productivity metrics, undermining criterion relevance.",
        "D": "The validation study demonstrated criterion-related validity in general, which is sufficient for personnel selection regardless of whether the design was concurrent or predictive."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The original study is a concurrent validity design because the test and supervisor ratings were collected from employed workers during the same evaluation cycle. Using such a battery to forecast performance 18 months after hire requires predictive validity evidence, in which test scores are obtained from applicants before hire and correlated with criterion measures collected substantially later. The concurrent design does not establish whether the test predicts future performance.",
        "A": "Restriction of range is a genuine threat in this scenario because incumbent workers represent a selected subgroup, which could attenuate the validity coefficient. However, while this is a real methodological concern, it is a secondary issue; the psychometrician's core objection is the temporal mismatch between the concurrent design used and the predictive application intended. Restriction of range does not address whether concurrent evidence can substitute for predictive evidence.",
        "C": "Criterion relevance — the degree to which the criterion measure actually captures job performance — is an important consideration in validation. However, the psychometrician's stated concern is specifically about the temporal relationship between predictor and criterion, not about whether supervisor ratings are the best criterion measure. There is no indication that supervisor ratings are inappropriate as a performance index.",
        "D": "This option is incorrect because concurrent and predictive validity designs answer fundamentally different questions. Concurrent validity shows that the test relates to the criterion at a single point in time, which may reflect shared method variance or static cross-sectional relationships. Predictive validity specifically addresses whether the test forecasts future criterion performance from a point in time before the criterion is measured — the exact inference needed for applicant selection with an 18-month outcome lag."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-08-vignette-L5",
      "source_question_id": "08",
      "source_summary": "Before using a selection test to estimate how well job applicants will do on a measure of job performance one year after they're hired, you would want to make sure the selection test has adequate predictive validity.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A manufacturing company administers a battery of problem-solving tasks to all of its current warehouse employees as part of an annual review and finds that employees who score higher on the tasks also receive higher ratings from their floor supervisors during that same review period. The company then decides to administer the same tasks to all new job applicants and use the scores to determine whom to hire, planning to check whether those hired performed well when they are reviewed by supervisors roughly a year later. Before this plan is implemented, an outside expert advises that the company's existing data — despite showing a strong statistical relationship between task scores and supervisor ratings — does not actually demonstrate whether the tasks can serve the purpose the company now has in mind. The expert strongly recommends collecting an entirely new set of data structured in a specific way before proceeding.",
      "question": "What type of validity evidence does the expert determine is missing, and why is the existing evidence inadequate to fill that gap?",
      "options": {
        "A": "Concurrent validity; because the annual review data were collected from applicants rather than employees, they cannot generalize to a hiring context.",
        "B": "Predictive validity; because the existing data were collected from current employees with both measures obtained at the same time, they do not demonstrate whether task scores obtained before employment can forecast supervisor ratings assessed approximately one year later.",
        "C": "Construct validity; because the relationship between problem-solving tasks and supervisor ratings was found in a single organization, there is insufficient evidence that the tasks measure a generalizable psychological attribute relevant to job performance.",
        "D": "Incremental validity; because the company has not shown that the problem-solving tasks predict performance above and beyond what simpler screening tools already in use would provide."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The existing data represent a concurrent validity design: problem-solving task scores and supervisor ratings were both collected from incumbent employees during the same annual review, meaning the temporal separation between predictor and criterion is essentially zero. The company's intended application requires a predictive validity design, in which task scores are collected from applicants before hire and correlated with supervisor ratings obtained approximately one year after hire. Only such a study can demonstrate that early task performance forecasts later job performance — the inference the company actually needs to justify its selection decision.",
        "A": "This option misidentifies the type of validity and misstates the reason for inadequacy. The existing data were collected from current employees, not applicants, which is actually an accurate description of the incumbent sample used in a concurrent study. However, the core problem is not who the participants were but rather that both measures were gathered simultaneously, which precludes drawing inferences about forecasting future performance. Concurrent validity, properly established, does involve simultaneously collected predictor and criterion data from employees.",
        "C": "Construct validity concerns whether the tasks actually measure the psychological attribute they are purported to measure (e.g., general problem-solving ability) and whether that attribute is theoretically linked to job performance. While construct evidence is always relevant, the expert's concern is more specific and practical: the existing data structure — simultaneous measurement from incumbents — does not support the company's plan to use scores from applicants to forecast performance a year later. The absence of predictive evidence is the proximate gap, not a deficiency in construct-level theorizing.",
        "D": "Incremental validity addresses whether one predictor adds unique predictive information beyond what another predictor already provides. This becomes relevant when multiple assessment tools are being compared in a battery. The company's situation is more fundamental: it has not yet established that the problem-solving tasks predict future job performance at all. Questions about incremental contribution over other tools are premature when the basic predictive relationship has not been demonstrated."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-02-vignette-L1",
      "source_question_id": "02",
      "source_summary": "An assumption of classical test theory is that measurement error is random.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "classical test theory",
        "measurement error",
        "random"
      ],
      "vignette": "A graduate student is studying the foundations of psychometrics for her comprehensive exams. Her professor explains that in classical test theory, every observed score is composed of a true score and an error component. The professor emphasizes that measurement error is assumed to be random, meaning it is unsystematic and unpredictable across examinees and occasions. Because of this assumption, errors are expected to average out to zero across many administrations. The student notes that this property distinguishes classical test theory from frameworks that allow for systematic error.",
      "question": "Which assumption of classical test theory is most directly illustrated by the professor's explanation?",
      "options": {
        "A": "The true score of an examinee remains perfectly stable across all testing occasions.",
        "B": "Measurement error is random and unsystematic, averaging to zero across repeated administrations.",
        "C": "The standard error of measurement is equal across all score levels on the test.",
        "D": "Parallel forms of a test must have identical means and variances to estimate true scores."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "While classical test theory does treat true scores as stable latent quantities, the assumption highlighted by the professor is specifically about the nature of error — that it is random, not about the stability of true scores per se. Stability of true scores is a related but distinct concept.",
        "B": "This is correct. A core assumption of classical test theory is that measurement error is random and unsystematic. Because errors are random, they are expected to cancel out (average to zero) when aggregated across many examinees or occasions, allowing true scores to be estimated from observed scores.",
        "C": "The assumption that the standard error of measurement is equal across all score levels is actually characteristic of classical test theory in its basic form, but it is a limitation rather than the central assumption being described. The professor's statement specifically targets the random nature of error, not its homogeneity across score levels.",
        "D": "Parallel forms reliability does require equivalent means and variances across forms, but this is a methodological requirement for a specific reliability estimation approach, not the foundational assumption about the random nature of error that the professor is describing."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-02-vignette-L2",
      "source_question_id": "02",
      "source_summary": "An assumption of classical test theory is that measurement error is random.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "true score",
        "systematic"
      ],
      "vignette": "A test development team is reviewing a newly constructed cognitive ability measure for use in personnel selection. A psychometrician on the team points out that one item consistently inflates scores for examinees who were tested in the morning compared to those tested in the afternoon, apparently because the item references a news event that morning test-takers were more likely to have seen. The team discusses whether this pattern violates a fundamental measurement assumption. The psychometrician is particularly concerned because this pattern means the error is not unsystematic — it is tied to a specific, identifiable source. The team notes that the presence of this kind of directional, non-random error undermines a key premise of their psychometric framework, even though the test as a whole shows adequate internal consistency.",
      "question": "Which measurement assumption is most directly threatened by the pattern the psychometrician identifies?",
      "options": {
        "A": "That observed scores are composed of a true score and a random error component",
        "B": "That the test's items measure a single, unidimensional construct",
        "C": "That item difficulty indices are consistent across all subgroups of examinees",
        "D": "That the test-retest reliability coefficient is stable over time"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. Classical test theory assumes that the error in any observed score is random and unsystematic. The morning-versus-afternoon advantage is a systematic, directional error tied to an identifiable cause, which directly violates the assumption that error is random. When error is systematic, it cannot be expected to average out, and the decomposition of observed scores into true scores plus random error breaks down.",
        "B": "Unidimensionality is an important assumption for certain reliability indices (e.g., coefficient alpha) and for item response theory models. However, the described problem is not about whether items measure multiple constructs — it is about one item producing directional, non-random score differences based on testing time, which is an error-randomness issue.",
        "C": "Consistency of item difficulty indices across subgroups relates to differential item functioning (DIF), a validity and fairness concern. While the morning/afternoon difference could be analyzed as DIF, the psychometrician's core concern is about the systematic, non-random nature of the error — not simply group-level difficulty differences.",
        "D": "Test-retest reliability assesses score stability across time and is sensitive to practice effects and true score changes, not to within-administration systematic errors tied to time-of-day testing. The described problem occurs within a single administration context and targets the random-error assumption, not temporal stability."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-02-vignette-L3",
      "source_question_id": "02",
      "source_summary": "An assumption of classical test theory is that measurement error is random.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "observed score"
      ],
      "vignette": "A researcher administers a depression screening questionnaire to a large community sample on two separate occasions, four weeks apart. She notices that participants who scored unusually high on the first occasion tended to score lower on the second, and those who scored unusually low initially tended to score higher — even in the absence of any treatment or life events. The researcher initially suspects test-retest reliability is poor, pointing to the variability in scores across occasions. However, her advisor argues that this pattern is actually expected under the theoretical framework used to construct the instrument, because the observed score on any occasion reflects both the person's stable attribute and a transient component that fluctuates unpredictably. The advisor cautions that the pattern does not necessarily indicate a problem with the instrument's reliability.",
      "question": "The phenomenon the advisor describes — whereby extreme observed scores shift toward the mean on re-testing — is best explained by which principle?",
      "options": {
        "A": "Restriction of range attenuating the observed correlation between the two administrations",
        "B": "Regression to the mean arising from the random error component in observed scores",
        "C": "Low test-retest reliability indicating true score instability in the sample",
        "D": "Criterion contamination inflating scores on the first administration for high scorers"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Regression to the mean is a direct consequence of the random error assumption in classical test theory. When observed scores include random error, extreme scores on one occasion are partly extreme because of chance error. On a subsequent occasion, random error is again unsystematic, so extreme scorers are unlikely to receive the same extreme error — causing their scores to move toward the mean. The advisor is correct that this is expected and does not indicate a reliability problem per se.",
        "A": "Restriction of range refers to a narrowed distribution of scores that artificially lowers correlation coefficients between variables. While it can affect reliability estimates, it does not explain why extreme scorers drift toward the mean on re-testing — it would instead affect the magnitude of the test-retest correlation coefficient without producing a directional shift for extreme scorers.",
        "C": "Low test-retest reliability would imply that true scores themselves are unstable or that systematic errors are present. The advisor explicitly argues the opposite — that the pattern is expected even for a reliable instrument, because it results from random error. Attributing the drift to true score instability misidentifies random error as a reliability failure.",
        "D": "Criterion contamination occurs when knowledge of criterion performance influences test scores, typically a validity threat rather than a reliability concept. It does not explain the systematic drift of extreme scores toward the mean across occasions and is not related to the random error assumption of classical test theory."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-02-vignette-L4",
      "source_question_id": "02",
      "source_summary": "An assumption of classical test theory is that measurement error is random.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "unsystematic"
      ],
      "vignette": "A licensing board is reviewing score reports from a professional examination administered in two formats: a paper-and-pencil version and a computer-based version. Candidates who took the computer-based version consistently scored about four points higher than paper-and-pencil candidates with equivalent credentials, a difference confirmed across multiple administrations. A board member suggests this discrepancy simply reflects the variability one would expect from examinee-to-examinee differences in testing conditions. A psychometrician disagrees, arguing that the four-point advantage is unsystematic only if it varies unpredictably across examinees — but when an entire group gains the same consistent advantage, the error has become directional and is correlated with a known variable. She recommends the board investigate whether the score difference is attributable to a flaw in how the two formats are equated.",
      "question": "The psychometrician's argument hinges on which foundational measurement principle being violated?",
      "options": {
        "A": "The assumption that parallel forms of a test produce equivalent true score estimates across administration modes",
        "B": "The principle that measurement error must be uncorrelated with true scores and with other error sources to preserve observed-score interpretability",
        "C": "The assumption that error in observed scores is random and therefore cannot be systematically associated with a known grouping variable",
        "D": "The requirement that discrimination indices be equal across paper-and-computer item formats to ensure construct validity"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The psychometrician's core argument is that when an entire identifiable group (computer-based test-takers) gains a consistent, predictable advantage, the error is no longer random — it is correlated with a known variable (administration mode). Classical test theory assumes measurement error is random and unsystematic, which means errors should not cluster in a predictable direction for identifiable subgroups. A systematic group-level bias violates this fundamental assumption.",
        "A": "Parallel forms reliability does address whether different versions of a test produce equivalent scores, and equating is directly relevant here. However, the psychometrician's specific argument is not about parallel forms equivalence per se — it is about the nature of the error that causes the discrepancy. Choosing this option misses the deeper principle about the random-error assumption that underlies why the format difference constitutes a measurement problem.",
        "B": "The assumption that error is uncorrelated with true scores is indeed a component of classical test theory (specifically, that error and true scores are independent). However, the psychometrician's argument focuses on the fact that error is correlated with a known grouping variable (format), not that it correlates with true scores. While related, this option does not precisely capture her reasoning, which centers on the systematic, non-random nature of the error across a defined group.",
        "D": "Discrimination indices assess how well individual items differentiate high from low scorers and are used in item analysis to evaluate item quality. Equal discrimination across formats relates to item-level measurement invariance, which is a psychometric concern but not the principle the psychometrician invokes. Her argument is about score-level systematic error, not item-level discrimination properties."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-02-vignette-L5",
      "source_question_id": "02",
      "source_summary": "An assumption of classical test theory is that measurement error is random.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team administers a 60-item written assessment to several hundred participants and then repeats the assessment six months later. They observe that participants who were unusually tired, distracted, or rushed on the first occasion scored lower than their peers of similar ability, but these same individuals scored closer to their peers on the second occasion. Conversely, a subset of participants scored higher than expected the first time — scoring at the very top of the distribution — and these individuals scored noticeably lower on the second occasion, even though no remediation or feedback had occurred. A senior investigator notes that both patterns are entirely consistent with the theoretical assumptions underlying how this type of assessment was built, and that the patterns would disappear if one could isolate and measure only what the assessment is actually intended to capture. A junior researcher insists this proves the assessment is unreliable and should be revised, but the senior investigator argues that, on the contrary, the patterns are an expected mathematical consequence of how scores on this kind of instrument are constructed.",
      "question": "The senior investigator's explanation is best grounded in which measurement principle?",
      "options": {
        "A": "Regression to the mean as a consequence of including a random, unsystematic error component in every observed score",
        "B": "Poor test-retest reliability caused by true score fluctuation over a six-month interval",
        "C": "Ceiling and floor effects limiting score variability at extreme ends of the distribution",
        "D": "Low internal consistency resulting from items that measure different underlying constructs"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. The senior investigator's argument precisely describes regression to the mean, which is a direct mathematical consequence of the classical test theory assumption that error is random. Participants who scored extremely low on the first occasion did so partly because random error pushed their scores down; on re-test, random error is again unsystematic, so their scores are less likely to be as extreme. The investigator's statement that the pattern 'would disappear if one could isolate only the intended construct' refers to the true score — confirming the explanation is about random error contaminating observed scores.",
        "B": "Low test-retest reliability due to true score fluctuation could also produce score changes over six months, making this a compelling distractor. However, the senior investigator explicitly states that the patterns are expected and consistent with the instrument's theoretical assumptions — not that they indicate a problem. Poor reliability would be a problem the investigator would recommend addressing, not defending. The bidirectional drift of extreme scorers toward the mean is the specific signature of random error and regression to the mean, not of instability in true scores.",
        "C": "Ceiling and floor effects occur when a test lacks sufficient items at the very high or very low range of ability, causing scores to cluster at the extremes and limiting variability. This could explain why extreme scorers move inward if the test cannot differentiate among them. However, the vignette specifies that the individuals scoring at the top later scored 'noticeably lower' — not that they remained clustered at the ceiling — and the investigator explains this as theoretically expected, not as a floor/ceiling measurement artifact that would warrant instrument revision.",
        "D": "Low internal consistency arising from multidimensionality would indicate that items are measuring different constructs, which affects the meaningfulness of a total score. This could produce variability in scores if different constructs fluctuate independently over time. However, internal consistency is assessed within a single administration and does not produce the specific bidirectional regression pattern described. The investigator's defense of the instrument and reference to what would happen if only the intended construct were measured points to error randomness, not multidimensionality."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-05-vignette-L1",
      "source_question_id": "05",
      "source_summary": "When you assess the validity of a test, you are assessing its accuracy.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "construct validity",
        "accuracy",
        "measures what it purports"
      ],
      "vignette": "A test developer creates a new instrument designed to assess working memory capacity. To evaluate whether the test accurately measures what it purports to measure, she gathers evidence from factor analyses, correlations with established working memory batteries, and expert reviews of item content. She concludes that the test demonstrates strong construct validity. The developer's primary concern throughout this process was the accuracy of the instrument's measurement.",
      "question": "Which psychometric property was the developer primarily evaluating when she examined whether the test accurately measures the intended psychological construct?",
      "options": {
        "A": "Test-retest reliability",
        "B": "Construct validity",
        "C": "Parallel forms reliability",
        "D": "Predictive validity"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Test-retest reliability refers to the consistency or stability of scores across two administrations separated by time. It concerns how reproducibly the test measures, not whether it accurately captures the intended construct.",
        "B": "Construct validity is the degree to which a test accurately measures the theoretical construct it is designed to assess. The developer's use of factor analysis, convergent correlations, and expert review are all classic construct validity procedures aimed at confirming accuracy of measurement.",
        "C": "Parallel forms reliability evaluates consistency between two equivalent versions of the same test. It addresses whether alternate forms yield similar scores, not whether the test accurately measures the underlying construct.",
        "D": "Predictive validity is a type of criterion validity that examines how well test scores forecast a future outcome. While related to accuracy, it specifically concerns prediction of an external criterion rather than the accuracy of the construct itself."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-05-vignette-L2",
      "source_question_id": "05",
      "source_summary": "When you assess the validity of a test, you are assessing its accuracy.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "content validity",
        "domain"
      ],
      "vignette": "A licensed psychologist is hired to evaluate a newly developed anxiety questionnaire intended for use with adult outpatients in a general psychiatric clinic. She notes that the questionnaire was developed by a team of experts who carefully mapped each item to a comprehensive blueprint of the anxiety construct, ensuring all major facets of anxiety were proportionally represented. The psychologist is a specialist in anxiety disorders and has published extensively; she was specifically asked to judge whether the items sufficiently cover the full domain of anxiety rather than only one narrow aspect. Despite the questionnaire's high coefficient alpha of .91, her primary task remains focused on a different psychometric question.",
      "question": "What type of validity is the psychologist primarily being asked to evaluate?",
      "options": {
        "A": "Concurrent validity",
        "B": "Content validity",
        "C": "Convergent validity",
        "D": "Internal consistency"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Concurrent validity examines the relationship between test scores and scores on an established criterion measured at the same point in time. The psychologist is not correlating the new questionnaire with an existing criterion measure, so this does not apply.",
        "B": "Content validity refers to the degree to which a test's items adequately and representatively sample the full domain of the construct being measured. The psychologist's task — judging whether all major facets of anxiety are proportionally covered — is precisely a content validity evaluation.",
        "C": "Convergent validity is a component of construct validity that involves demonstrating that the test correlates highly with other measures of the same construct. The psychologist is reviewing item coverage, not computing or examining correlations with related instruments.",
        "D": "Internal consistency (reflected in the coefficient alpha of .91) is a reliability estimate indicating how consistently items on a test measure the same underlying dimension. Although mentioned in the vignette as a red herring, reliability is distinct from validity and not the psychologist's primary concern here."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-05-vignette-L3",
      "source_question_id": "05",
      "source_summary": "When you assess the validity of a test, you are assessing its accuracy.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "factor analysis"
      ],
      "vignette": "A research team develops a 40-item measure intended to assess five theoretically distinct dimensions of executive functioning. After administering the measure to 600 participants, they run a factor analysis and find that the items cluster into five factors matching the intended structure, and that each factor correlates only modestly with the others. Notably, all five factors show moderate correlations with a well-validated executive functioning battery. However, the measure's scores show inconsistent correlations with neuropsychological ratings collected two weeks later, leading some team members to question the measure's usefulness.",
      "question": "The finding that items cluster into the five theoretically predicted factors and that each factor shows modest inter-factor correlations most directly supports which type of validity?",
      "options": {
        "A": "Criterion validity",
        "B": "Content validity",
        "C": "Test-retest reliability",
        "D": "Construct validity"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Criterion validity would be supported by demonstrating that test scores correlate with an established external criterion, such as a neuropsychologist's ratings. The factor structure itself does not address criterion relationships; the inconsistent criterion correlations mentioned serve as a red herring pointing toward this option.",
        "B": "Content validity concerns the representativeness and coverage of test items across the intended construct domain, typically evaluated through expert review or systematic blueprinting. Factor analysis of empirical data is not a content validity procedure.",
        "C": "Test-retest reliability concerns score stability over time and would be evaluated by correlating scores from two administrations. The inconsistent two-week correlations in the vignette could mislead students toward this option, but those are criterion correlations, not reliability coefficients.",
        "D": "Construct validity is demonstrated when empirical evidence (such as factor analysis) confirms that a test's structure matches theoretical predictions about the construct. Finding five theoretically expected factors with modest intercorrelations — indicating both distinctiveness and relatedness — is a hallmark of construct validity evidence."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-05-vignette-L4",
      "source_question_id": "05",
      "source_summary": "When you assess the validity of a test, you are assessing its accuracy.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "convergent"
      ],
      "vignette": "A graduate student develops a 25-item self-report scale purporting to measure trait resilience. She finds that the scale yields a split-half reliability coefficient of .88 after Spearman-Brown correction, and its scores correlate strongly (.74) with a well-established resilience inventory administered the same day. Scores also show weak correlations (.12–.18) with measures of impulsivity and sensation-seeking, which theory predicts should be largely unrelated to resilience. When the student presents her findings to her advisor, she emphasizes the convergent correlation and the pattern of weak correlations with theoretically unrelated constructs as her central evidence.",
      "question": "The student's combined emphasis on the strong correlation with a related measure and the weak correlations with theoretically unrelated measures most directly supports which form of validity?",
      "options": {
        "A": "Concurrent validity",
        "B": "Discriminant validity",
        "C": "Construct validity",
        "D": "Predictive validity"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Concurrent validity is established by showing that a test correlates with an established criterion measured at the same time. The strong same-day correlation with the established resilience inventory (.74) fits concurrent validity on the surface and makes this a highly plausible distractor; however, concurrent validity only captures the single criterion correlation and does not account for the pattern of weak correlations with unrelated measures.",
        "B": "Discriminant validity refers specifically to the demonstration that a test does NOT correlate highly with measures of theoretically distinct constructs, and it is a component of construct validity. The weak correlations with impulsivity and sensation-seeking provide discriminant validity evidence, but focusing only on this component misses that the student is presenting both convergent and discriminant evidence together, which collectively constitute construct validity.",
        "C": "Construct validity is the overarching framework encompassing both convergent and discriminant evidence. The student's combined presentation — a strong convergent correlation with a theoretically related instrument and systematically weak correlations with unrelated constructs — represents the multitrait-multimethod logic that is the hallmark of construct validity. This is the correct and most comprehensive answer.",
        "D": "Predictive validity involves correlating test scores with a criterion measured at a future point in time. All criterion measures in this vignette were collected concurrently, and no future outcome data are mentioned, making predictive validity inapplicable despite the general concern with accuracy."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-05-vignette-L5",
      "source_question_id": "05",
      "source_summary": "When you assess the validity of a test, you are assessing its accuracy.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of organizational psychologists develops a new 30-item questionnaire to select job candidates for a highly demanding customer-service role. The questionnaire is administered to 200 newly hired employees, and six months later, supervisors rate each employee's job performance. The correlation between questionnaire scores and supervisor ratings is .41. Encouraged, the researchers then give the same questionnaire to a second sample of 180 employees hired under a new, more selective recruiting policy, and find that the same correlation drops to .19. A statistical consultant explains that the lower correlation in the second sample does not mean the questionnaire became less accurate at measuring the underlying quality it was designed to measure — rather, the narrower range of scores in the highly selected group mathematically attenuated the observed relationship.",
      "question": "The consultant's explanation points to which psychometric phenomenon as the most likely reason for the drop in correlation from the first to the second sample?",
      "options": {
        "A": "Regression to the mean",
        "B": "Restriction of range",
        "C": "Shrinkage",
        "D": "Low base rate"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Regression to the mean describes the tendency for extreme scores on a first measurement to be closer to the group average on a subsequent measurement, not because of changes in the instrument's accuracy but due to random error. While extreme scores are involved in selection, regression to the mean addresses score changes across repeated administrations for the same individuals, not the attenuation of a validity coefficient across groups with different variability.",
        "B": "Restriction of range occurs when the variability of scores in a sample is artificially narrowed — here, by a more selective hiring policy — causing the observed correlation between predictor and criterion to be lower than it would be in a full-range sample. The consultant is explicitly describing this phenomenon: the questionnaire's ability to measure the underlying quality did not change, but the narrow score distribution in the selected group mathematically reduced the observed correlation.",
        "C": "Shrinkage refers to the drop in a multiple regression equation's predictive accuracy when it is applied to a new sample rather than the derivation sample, because the original equation was overfitted to sample-specific variance. This concept involves cross-validation of regression weights, not the impact of a more selective hiring policy on score variability and correlation magnitude.",
        "D": "Low base rate refers to the rarity of a criterion event in the population, which limits the practical utility of even highly accurate tests in identifying true positives. Although base rates affect predictive utility, a low base rate does not mathematically attenuate the correlation coefficient in the way the consultant describes; the mechanism cited — narrowed score range reducing observed correlations — is distinct from base rate effects."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-001-vignette-L1",
      "source_question_id": "001",
      "source_summary": "The manual for a test of fluid intelligence reports that, for the standardization sample, Cronbach's alpha was .93, suggesting the test has adequate internal consistency reliability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "Cronbach's alpha",
        "internal consistency",
        "inter-item correlations"
      ],
      "vignette": "A psychometrician is evaluating a newly developed 40-item test of fluid intelligence. She computes Cronbach's alpha using data from the standardization sample and obtains a value of .93. She notes that the inter-item correlations are uniformly high across the item pool, suggesting that all items are tapping a common underlying construct. The test manual reports this finding as evidence of strong internal consistency reliability.",
      "question": "Which type of reliability is most directly indexed by the statistic reported in the test manual?",
      "options": {
        "A": "Test-retest reliability",
        "B": "Internal consistency reliability",
        "C": "Parallel forms reliability",
        "D": "Interrater reliability"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Test-retest reliability estimates score stability over time by correlating scores from two separate administrations of the same test. Cronbach's alpha does not involve repeated administrations and therefore does not address temporal stability.",
        "B": "Cronbach's alpha is a direct measure of internal consistency reliability. It evaluates the degree to which all items on a scale measure the same construct by examining the average inter-item covariance relative to total score variance, which is exactly what the high value of .93 reflects here.",
        "C": "Parallel forms reliability is estimated by correlating scores from two independently constructed versions of the same test administered to the same group. This approach requires two separate test forms and is not what Cronbach's alpha computes.",
        "D": "Interrater reliability quantifies the degree of agreement between two or more independent raters or scorers and is relevant for subjectively scored items. Cronbach's alpha does not involve multiple raters and therefore cannot speak to scoring consistency across judges."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-001-vignette-L2",
      "source_question_id": "001",
      "source_summary": "The manual for a test of fluid intelligence reports that, for the standardization sample, Cronbach's alpha was .93, suggesting the test has adequate internal consistency reliability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "coefficient alpha",
        "homogeneity"
      ],
      "vignette": "A research team develops a 35-item measure of abstract reasoning intended for use with adults ages 18–65. The test is administered to a large, demographically diverse standardization sample. After administration, the developers compute coefficient alpha and obtain a value of .91, concluding that the scale demonstrates high homogeneity. Notably, the test was administered in a single session with no retest component, and scores did not vary meaningfully by participant age within the sample.",
      "question": "The statistic reported by the developers most directly supports which psychometric property of the test?",
      "options": {
        "A": "Temporal stability of scores",
        "B": "Agreement between independently derived total scores",
        "C": "Internal consistency of the item set",
        "D": "Accuracy with which the test predicts an external criterion"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Temporal stability refers to the consistency of scores across two or more time points, typically assessed via test-retest reliability. Because the test was administered only once, the reported coefficient alpha cannot address whether scores remain stable over time.",
        "B": "Agreement between independently derived total scores is the concern of parallel forms reliability, which compares two separately constructed test versions. The developers used a single form and a single administration, so this interpretation does not apply.",
        "C": "Coefficient alpha directly measures internal consistency — the degree to which all items in a scale are intercorrelated and tap the same underlying construct. A value of .91 from a single administration with no retest is the hallmark evidence of strong internal consistency, and the term 'homogeneity' is synonymous with this property.",
        "D": "Accuracy of prediction from a test to an external criterion is the domain of criterion-related validity (predictive or concurrent), not reliability. Coefficient alpha tells us nothing about how well test scores correspond to or forecast performance on any outside measure."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-001-vignette-L3",
      "source_question_id": "001",
      "source_summary": "The manual for a test of fluid intelligence reports that, for the standardization sample, Cronbach's alpha was .93, suggesting the test has adequate internal consistency reliability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "standardization sample"
      ],
      "vignette": "A test publisher releases a new measure of fluid intelligence and reports in the technical manual that data from the standardization sample yielded a reliability coefficient of .93. The manual notes that all items were administered in one sitting and that the coefficient was derived by examining the average relationships among all item pairs relative to total score variability. Critics point out that the test was not administered to the same group on two separate occasions, and some reviewers question whether a single high coefficient is sufficient to establish the test's overall psychometric quality.",
      "question": "The reliability coefficient of .93 reported in the manual most accurately reflects which aspect of the test's psychometric properties?",
      "options": {
        "A": "The test produces consistent scores when administered on two separate occasions",
        "B": "The items collectively measure a single, cohesive construct",
        "C": "The test accurately classifies individuals relative to a known external standard",
        "D": "Scores obtained from two independently constructed versions of the test are equivalent"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Consistency across two separate occasions describes test-retest reliability. The vignette explicitly states there was no second administration, and the critics' comment about this omission is a red herring intended to draw attention toward test-retest; it does not change what the single-administration coefficient actually measures.",
        "B": "The description of the coefficient — derived from average inter-item relationships relative to total score variance, computed from a single administration — is the defining procedure for Cronbach's alpha, which measures internal consistency, or the degree to which items measure a unified construct. The high value of .93 supports strong item cohesion.",
        "C": "Accurate classification relative to an external standard concerns criterion-related validity, specifically concurrent validity when comparisons are made at the same time point. Reliability coefficients do not address the relationship between test scores and outside criteria.",
        "D": "Equivalence of scores across two independently constructed test forms is the concern of parallel forms reliability. The vignette describes a single form administered once, making this interpretation inapplicable regardless of how high the coefficient is."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-001-vignette-L4",
      "source_question_id": "001",
      "source_summary": "The manual for a test of fluid intelligence reports that, for the standardization sample, Cronbach's alpha was .93, suggesting the test has adequate internal consistency reliability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "covariance"
      ],
      "vignette": "A neuropsychologist reviewing a test manual for a new fluid intelligence battery notices that the publisher computed a single index of reliability using data from one administration session. The index was derived from a formula that partitions the total score variance into item-level components, specifically by quantifying the average covariance among all item pairs as a proportion of the total score variance. The resulting value of .93 was described by the publisher as reflecting 'strong measurement quality.' A colleague argues that because the test was never given twice, the manual tells clinicians nothing meaningful about whether scores will hold up over repeated evaluations.",
      "question": "The colleague's critique, while clinically relevant, misidentifies the type of measurement quality the reported index actually captures. Which psychometric property does the .93 coefficient most precisely quantify?",
      "options": {
        "A": "Temporal stability of scores across independent administrations",
        "B": "The degree of agreement between two separately normed versions of the same construct",
        "C": "The extent to which items share variance attributable to a common underlying factor",
        "D": "The sensitivity of the instrument in distinguishing between high- and low-ability examinees"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Temporal stability is assessed via test-retest reliability and requires two administrations separated by a time interval. The colleague's critique raises this concern as a distractor, and while it is a legitimate clinical question, it does not describe what the reported index — derived from a single-session, item-covariance-based formula — actually measures.",
        "B": "Agreement between two separately normed versions describes parallel forms reliability, which requires the construction of two distinct but equivalent test forms. The manual reports a single index from a single form, and there is no mention of a second version, making this interpretation inapplicable.",
        "C": "The formula described — partitioning total score variance by examining the average covariance among all item pairs — is precisely Cronbach's alpha. It quantifies the proportion of total variance attributable to shared item variance (i.e., a common factor), which is the operational definition of internal consistency. The .93 value indicates that items are strongly intercorrelated and measure a unified construct.",
        "D": "Sensitivity in distinguishing between high- and low-ability examinees is captured by the discrimination index (or item-total correlation) at the item level, or by a test's predictive validity at the scale level. Neither of these is what the covariance-based reliability coefficient addresses."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-001-vignette-L5",
      "source_question_id": "001",
      "source_summary": "The manual for a test of fluid intelligence reports that, for the standardization sample, Cronbach's alpha was .93, suggesting the test has adequate internal consistency reliability.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A publisher of a new cognitive ability battery reports a single number derived from data collected when the test was given to thousands of people on one occasion only. The number was computed by examining how much the scores on every individual question tended to rise and fall together, and then comparing that collective tendency to the spread of total scores across all test-takers. The resulting figure, .93, was described in the technical documentation as reflecting how well-integrated the questions are as a set. A reviewer notes approvingly that none of the questions appear to be 'going their own way,' while a separate critic argues the publisher should also check whether scores would look the same if the test were given again months later.",
      "question": "The figure of .93 most directly quantifies which characteristic of this battery?",
      "options": {
        "A": "Whether scores remain stable if the same people take the test at a later point in time",
        "B": "Whether two independently assembled versions of the battery would yield equivalent scores",
        "C": "The degree to which all questions collectively reflect a single common source of individual differences",
        "D": "Whether the battery accurately captures differences in ability that can be observed by independent evaluators"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Score stability over a later administration describes test-retest reliability, and the critic's comment in the vignette is specifically placed to draw attention toward this interpretation. However, the statistic was computed from a single administration only, and its derivation — examining how individual items co-vary relative to total score spread — has no logical connection to temporal consistency.",
        "B": "Equivalent scores across two independently assembled versions describes parallel forms reliability, which requires two separately constructed test forms. The vignette describes only one test administered once, and no second version is mentioned; the mention of 'well-integrated questions' does not imply a comparison across forms.",
        "C": "The procedure described — assessing the tendency of all individual items to rise and fall together relative to total score variance from a single administration — is the computational logic of Cronbach's alpha, the primary measure of internal consistency. The reviewer's observation that no questions are 'going their own way' is a plain-language description of high item intercorrelation, and the .93 figure reflects that all items share substantial variance attributable to a common underlying dimension.",
        "D": "Agreement among independent evaluators describes interrater reliability, which concerns scoring consistency rather than item cohesion. Nothing in the vignette involves multiple raters, human judgment in scoring, or comparisons between observers, so this interpretation does not fit the described procedure."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-003-vignette-L1",
      "source_question_id": "003",
      "source_summary": "A middle school student receives a full-scale IQ score of 105 on an intelligence test that has a mean of 100, standard deviation of 15, and standard error of measurement of 3, and the 95% confidence interval for this student's score is 99 to 111.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "concurrent validity",
        "criterion",
        "correlation"
      ],
      "vignette": "A test developer creates a new brief cognitive screening instrument and wants to establish its criterion-related validity. She administers both the new screening tool and a well-validated IQ battery to the same group of middle school students at the same time point. She then computes the correlation between scores on the two measures to determine how well the new test performs relative to the established criterion. The resulting coefficient is used to support the validity of the new instrument for identifying students who may need further evaluation.",
      "question": "What type of validity evidence is the test developer collecting in this scenario?",
      "options": {
        "A": "Predictive validity",
        "B": "Concurrent validity",
        "C": "Content validity",
        "D": "Construct validity"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Predictive validity is a form of criterion-related validity, but it involves correlating test scores with a criterion measure obtained at a future point in time. In this scenario, both measures are administered simultaneously, which rules out predictive validity.",
        "B": "Concurrent validity is established by correlating scores on a new instrument with scores on an established criterion measure administered at the same time. This scenario fits precisely: the new screening tool and the validated IQ battery are given concurrently, and their correlation provides concurrent validity evidence.",
        "C": "Content validity refers to the degree to which test items adequately sample the domain of interest. It is typically established through expert judgment and item review, not through correlation with an external criterion measure.",
        "D": "Construct validity refers to evidence that a test measures the theoretical construct it purports to measure. While related, construct validity is a broader category involving convergent and discriminant evidence, factor structure, and theory-driven predictions — not simply correlating two tests given at the same time."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-003-vignette-L2",
      "source_question_id": "003",
      "source_summary": "A middle school student receives a full-scale IQ score of 105 on an intelligence test that has a mean of 100, standard deviation of 15, and standard error of measurement of 3, and the 95% confidence interval for this student's score is 99 to 111.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "predictive validity",
        "criterion"
      ],
      "vignette": "A school psychologist at a large urban district is evaluating a brief cognitive screening instrument that was developed to identify which fourth-grade students will struggle academically by sixth grade. The psychologist administers the screening tool to all fourth graders and then waits two years to collect their sixth-grade GPA and standardized achievement scores as the criterion. Although the screening tool shows high internal consistency, the psychologist is most interested in whether early scores forecasted later academic outcomes. Several teachers note that many struggling students had also experienced high rates of absenteeism during the two-year interval, which the psychologist notes as a potential confound.",
      "question": "What form of validity is the psychologist primarily attempting to establish with this research design?",
      "options": {
        "A": "Concurrent validity",
        "B": "Incremental validity",
        "C": "Construct validity",
        "D": "Predictive validity"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Concurrent validity involves correlating test scores with a criterion measured at the same time. Here, the criterion (sixth-grade GPA) is collected two years after the initial screening, making this a prospective design inconsistent with concurrent validity.",
        "B": "Incremental validity refers to the degree to which a test improves prediction of a criterion above and beyond what is already predicted by other available measures. While the psychologist could investigate this, the scenario describes a basic prediction-over-time design, not a comparison of predictors.",
        "C": "Construct validity is broad evidence that a test measures its intended theoretical construct, gathered through multiple converging lines of evidence. The design here focuses specifically on whether early scores forecast a future criterion, which is a more targeted question.",
        "D": "Predictive validity is a form of criterion-related validity in which test scores obtained at one time point are correlated with criterion data collected at a later time point. The two-year gap between screening and criterion collection is the defining feature of this design, and the absenteeism detail is a confound that does not change the type of validity being studied."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-003-vignette-L3",
      "source_question_id": "003",
      "source_summary": "A middle school student receives a full-scale IQ score of 105 on an intelligence test that has a mean of 100, standard deviation of 15, and standard error of measurement of 3, and the 95% confidence interval for this student's score is 99 to 111.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "shrinkage"
      ],
      "vignette": "A research team develops a regression-based selection battery for identifying gifted middle school students and validates it on a large initial sample, obtaining an impressive multiple correlation of .78 between the battery and teacher-rated academic performance. Enthusiastic about the result, they publish the formula and recommend its broad adoption. When independent researchers apply the identical weighting formula to a new, similarly sized sample from a different school district, the multiple correlation drops to .54. The research team had not conducted any cross-validation procedures before publishing their recommendations.",
      "question": "What psychometric phenomenon best explains the drop in predictive accuracy observed in the new sample?",
      "options": {
        "A": "Restriction of range",
        "B": "Shrinkage",
        "C": "Criterion contamination",
        "D": "Regression to the mean"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Restriction of range occurs when the variability of scores in the sample is narrower than in the population, which artificially attenuates validity coefficients. While it could affect the new sample, the scenario describes a drop specifically tied to applying weights derived from one sample to another — the hallmark of shrinkage, not range restriction.",
        "B": "Shrinkage refers to the reduction in a validity coefficient (particularly a multiple correlation) when regression weights derived from a derivation sample are applied to a new, independent sample. Because regression equations capitalize on chance variation in the original sample, their predictive accuracy always decreases somewhat — often substantially — in cross-validation. The drop from .78 to .54 and the absence of cross-validation procedures are precisely what shrinkage predicts.",
        "C": "Criterion contamination occurs when the criterion measure is influenced by knowledge of the predictor scores, inflating the apparent validity coefficient. Here, the issue is not contamination of the criterion but rather overfitting of the regression weights to the derivation sample.",
        "D": "Regression to the mean is a statistical phenomenon where extreme scores on one measurement tend to be less extreme on a subsequent measurement. It describes score behavior over repeated administrations rather than the degradation of a regression formula's predictive accuracy when applied to a new sample."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-003-vignette-L4",
      "source_question_id": "003",
      "source_summary": "A middle school student receives a full-scale IQ score of 105 on an intelligence test that has a mean of 100, standard deviation of 15, and standard error of measurement of 3, and the 95% confidence interval for this student's score is 99 to 111.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "variability"
      ],
      "vignette": "A hospital system implements a structured clinical interview to predict which patients admitted for mild traumatic brain injury will still show cognitive difficulties at a six-month follow-up appointment. The interview is validated in the emergency department population, yielding a validity coefficient of .61 with follow-up neuropsychological performance. When the hospital later uses the same interview exclusively with patients referred to a specialized neurorehabilitation clinic — a group selected precisely because they already demonstrate significant, documented cognitive impairment — the validity coefficient drops to .29. Administrators initially attribute the decline to poor test quality, but a senior psychometrician disagrees and points to a feature of the selected sample.",
      "question": "What factor most likely accounts for the lower validity coefficient observed in the neurorehabilitation sample?",
      "options": {
        "A": "Shrinkage from overfitting the regression equation to the original sample",
        "B": "Criterion contamination introduced by the referring clinicians' knowledge of interview scores",
        "C": "Restriction of range due to reduced score variability in the specialized, high-impairment sample",
        "D": "Regression to the mean inflating scores in the original emergency department sample"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Shrinkage applies when regression weights developed in one sample are applied to a new sample, inflating the original coefficient due to capitalization on chance. The scenario describes a single cross-applied interview score correlation, not a multiple regression formula, and the psychometrician specifically implicates a sample characteristic rather than a statistical artifact of weighting.",
        "B": "Criterion contamination would inflate, not deflate, the validity coefficient by allowing criterion raters to be influenced by predictor scores. The scenario shows a decrease in validity, and there is no indication that follow-up neuropsychologists had access to the interview results before assessing patients.",
        "C": "Restriction of range occurs when a sample is selected on the basis of the predictor or criterion, reducing the variability of scores and thereby attenuating the correlation between them. The neurorehabilitation clinic serves only patients with documented significant impairment — a group truncated at the high-impairment end — reducing score variability on both the interview and the criterion. This compressed variability mathematically reduces the obtainable correlation, explaining the drop from .61 to .29 without any change in the interview's true predictive relationship.",
        "D": "Regression to the mean describes the tendency for extreme scores to move toward the population mean on retesting and is relevant to repeated measurement of the same individual over time. It does not explain why a validity coefficient would be lower in a clinically selected sample compared to a broader one."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-003-vignette-L5",
      "source_question_id": "003",
      "source_summary": "A middle school student receives a full-scale IQ score of 105 on an intelligence test that has a mean of 100, standard deviation of 15, and standard error of measurement of 3, and the 95% confidence interval for this student's score is 99 to 111.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A mid-sized technology company uses a brief problem-solving exercise developed by their HR team to decide which applicants to invite for final-round interviews. The exercise was originally tested on a pool of 300 general applicants and showed a strong relationship with supervisor performance ratings collected eighteen months later. Confident in the tool, the company now applies it only to candidates who have already passed a rigorous initial resume screen — meaning only applicants with highly similar educational backgrounds and work histories advance to take the exercise. When an external consultant analyzes the data from this second stage, she finds that scores on the exercise are barely related to subsequent supervisor ratings. The HR team suspects the tool has simply 'worn out,' but the consultant explains that the tool itself has not changed — rather, something about the way the company is now using it has undermined its apparent usefulness.",
      "question": "What psychometric phenomenon does the consultant's explanation most likely reflect?",
      "options": {
        "A": "Shrinkage resulting from applying regression weights derived in one sample to a new, independent sample",
        "B": "Criterion contamination caused by supervisors being aware of applicants' exercise scores before providing ratings",
        "C": "Restriction of range resulting from reduced score variability among the prescreened applicant pool",
        "D": "Regression to the mean causing high scorers in the original sample to perform more poorly at follow-up"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Shrinkage applies specifically when a multiple regression equation's weights, optimized in a derivation sample, are used to predict outcomes in a new sample, causing the multiple correlation to decline due to capitalization on chance. The scenario involves a simple bivariate relationship between exercise scores and supervisor ratings, not a regression formula with weighted predictors, so shrinkage is not the operative explanation.",
        "B": "Criterion contamination would cause supervisors' ratings to be biased by their knowledge of exercise performance, artificially inflating the relationship between the two measures. Here, the observed relationship is lower than expected, and there is no indication supervisors had access to exercise scores — making contamination both directionally and mechanistically wrong.",
        "C": "Restriction of range occurs when a sample is selected in a way that truncates the variability of the predictor or criterion variable. Because only pre-screened applicants — those with similar, strong backgrounds — now take the exercise, their scores cluster together in a narrow band, reducing the correlation between exercise performance and later supervisor ratings. The original strong relationship was observed in a heterogeneous pool; narrowing the pool to similar high-performers compresses variability and attenuates the validity coefficient, exactly as the consultant describes.",
        "D": "Regression to the mean describes how statistically extreme scores on one occasion tend to be less extreme on a subsequent occasion simply due to measurement error, affecting repeated measurements of the same individuals. The scenario compares two different groups across two different deployment contexts, not repeated assessment of the same people, so regression to the mean does not explain the observed pattern."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-009-vignette-L1",
      "source_question_id": "009",
      "source_summary": "To evaluate the predictive validity of a new aptitude test for college admissions, a test developer administers the test to a sample of high school juniors and seniors admitted to college without use of their aptitude test scores and then correlates the students' aptitude test scores with their GPAs at the end of their second year of college.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "predictive validity",
        "criterion",
        "future performance"
      ],
      "vignette": "A test developer wants to determine whether a new cognitive aptitude test can forecast college success. To assess predictive validity, she administers the test to a cohort of high school juniors and seniors who are admitted to college without any consideration of their aptitude scores. Two years later, she correlates each student's aptitude test score with their college GPA as the criterion measure of future performance. The resulting correlation coefficient is used to evaluate whether the test has practical forecasting utility.",
      "question": "Which type of validity evidence is the test developer collecting in this study?",
      "options": {
        "A": "Predictive validity",
        "B": "Concurrent validity",
        "C": "Content validity",
        "D": "Construct validity"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Predictive validity is established by administering a test at one point in time and then correlating scores with a criterion measure obtained at a later point in time. The time gap between testing and GPA collection, combined with the goal of forecasting future academic success, is the defining feature of predictive validity.",
        "B": "Incorrect. Concurrent validity also involves correlating test scores with a criterion measure, but both the test and the criterion are measured at the same point in time. In this scenario, the GPA criterion is collected two years after testing, which rules out concurrent validity.",
        "C": "Incorrect. Content validity refers to the degree to which test items representatively sample the domain being measured (e.g., whether the aptitude test covers the full range of relevant cognitive abilities). It does not involve correlating test scores with an external outcome measure.",
        "D": "Incorrect. Construct validity is the broadest form of validity and concerns whether a test measures the theoretical construct it purports to measure. While predictive validity can contribute evidence for construct validity, the specific procedure described — correlating scores with a future criterion — defines the narrower category of predictive validity."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-009-vignette-L2",
      "source_question_id": "009",
      "source_summary": "To evaluate the predictive validity of a new aptitude test for college admissions, a test developer administers the test to a sample of high school juniors and seniors admitted to college without use of their aptitude test scores and then correlates the students' aptitude test scores with their GPAs at the end of their second year of college.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "aptitude test",
        "correlation"
      ],
      "vignette": "A university research team develops a new aptitude test intended for use in college admissions decisions. Because the admissions office is concerned about fairness, the test is deliberately excluded from the admissions process for one entering class, and all students are admitted using traditional criteria. The research team notes that the sample includes a wide range of academic abilities, from students who barely met admissions standards to highly competitive scholars. At the end of the students' sophomore year, the team computes the correlation between each student's aptitude test score and their cumulative GPA. The team plans to use this correlation to support a validity argument for the test.",
      "question": "What type of validity evidence is the research team gathering?",
      "options": {
        "A": "Concurrent validity",
        "B": "Predictive validity",
        "C": "Discriminant validity",
        "D": "Incremental validity"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Predictive validity is demonstrated when a test administered at one time is correlated with a criterion measured at a meaningful later point. Here, aptitude scores obtained before college are correlated with sophomore GPA, clearly establishing a temporal sequence that defines predictive validity evidence.",
        "A": "Incorrect. Concurrent validity requires that the test and the criterion be administered at roughly the same time. Because GPA is measured two years after the aptitude test, a concurrent design is not being used; the temporal gap is the critical distinguishing detail.",
        "C": "Incorrect. Discriminant validity is a component of construct validity and involves showing that a test does NOT correlate highly with measures of theoretically unrelated constructs. The procedure described does not compare the aptitude test to a dissimilar measure; it relates the test to a relevant outcome criterion.",
        "D": "Incorrect. Incremental validity asks whether a test adds predictive power beyond what is already provided by existing measures (e.g., does the aptitude test improve GPA prediction over and above high school GPA alone?). The study does not compare the new test against any existing predictor, so incremental validity is not what is being assessed."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-009-vignette-L3",
      "source_question_id": "009",
      "source_summary": "To evaluate the predictive validity of a new aptitude test for college admissions, a test developer administers the test to a sample of high school juniors and seniors admitted to college without use of their aptitude test scores and then correlates the students' aptitude test scores with their GPAs at the end of their second year of college.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "criterion"
      ],
      "vignette": "A psychometrician is tasked with validating a newly developed reasoning assessment for a regional university consortium. To ensure fairness, consortium administrators decide that applicants' scores on the new test will play no role in determining who gets admitted, so the sample of enrolled students represents the consortium's typical admitted population rather than a select high-scoring subset. The psychometrician notes in her report that the reasoning assessment scores were gathered at the time of enrollment, and the criterion data — cumulative academic performance records — were collected at the end of the students' second year. Importantly, she notes that the high school students who were not admitted to the consortium are absent from her dataset entirely, which she flags as a potential threat to the correlation's magnitude. She concludes that the study design is appropriate for the stated validation purpose.",
      "question": "What form of validity evidence does this study design most directly provide, and what threat does the psychometrician's flag represent?",
      "options": {
        "A": "Concurrent validity; the threat is measurement error inflating the correlation",
        "B": "Predictive validity; the threat is restriction of range attenuating the correlation",
        "C": "Construct validity; the threat is criterion contamination reducing generalizability",
        "D": "Predictive validity; the threat is shrinkage when the formula is cross-validated"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The study exemplifies predictive validity: the test is administered before the criterion (GPA) is available, and the goal is to forecast future academic performance. The psychometrician's flag identifies restriction of range — because non-admitted applicants are excluded, the score range is narrowed, which mathematically attenuates (reduces) the observed validity coefficient relative to its true population value.",
        "A": "Incorrect. Concurrent validity would require the test and GPA criterion to be collected simultaneously, which is not the case here. Additionally, the described threat — the exclusion of non-admitted students — is a range restriction problem, not measurement error; measurement error would add random variance and reduce reliability, but range restriction operates by truncating the distribution.",
        "C": "Incorrect. Construct validity involves the full network of evidence about what a test measures, not a single predictive study. More critically, criterion contamination occurs when the criterion measure is influenced by knowledge of the test scores (e.g., professors grade more favorably when they know a student scored high), which is unrelated to the concern about missing non-admitted students.",
        "D": "Incorrect. Shrinkage is a real threat to predictive validity studies, but it refers to the degree to which a regression-based validity coefficient decreases when the prediction equation is applied to a new sample. The psychometrician is flagging a threat to the current study's coefficient, not to cross-validation of a regression formula, making shrinkage the wrong match for this specific concern."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-009-vignette-L4",
      "source_question_id": "009",
      "source_summary": "To evaluate the predictive validity of a new aptitude test for college admissions, a test developer administers the test to a sample of high school juniors and seniors admitted to college without use of their aptitude test scores and then correlates the students' aptitude test scores with their GPAs at the end of their second year of college.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "longitudinal"
      ],
      "vignette": "A consortium of high schools contracts with a measurement firm to evaluate a new screening battery. The firm administers the battery to all juniors and seniors at participating schools during a standard testing window, but school counselors are instructed not to use battery scores when recommending students for early college admission programs — a precaution taken to avoid score-based selection bias. The firm notes that the battery demonstrates high internal consistency across its subscales and that its items were developed by subject-matter experts to represent the full domain of academic reasoning. Eighteen months after the initial testing, the firm retrieves cumulative academic standing records from the colleges those students went on to attend and computes a correlation between the original battery scores and academic standing. The firm's final report emphasizes the longitudinal design as the study's central methodological feature and presents the resulting coefficient as its primary validity index.",
      "question": "Which validity concept does the firm's primary validity coefficient most directly represent, and why is the precaution about counselors' recommendations methodologically important?",
      "options": {
        "A": "Content validity; the precaution ensures items remain representative of the academic domain without bias from selection processes",
        "B": "Concurrent validity; the precaution prevents range restriction by keeping all students in the sample regardless of their battery scores",
        "C": "Predictive validity; the precaution prevents the battery scores from influencing who enters college, thereby avoiding range restriction in the criterion sample",
        "D": "Construct validity; the precaution eliminates criterion contamination by ensuring advisors do not let test scores affect their judgment of student performance"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The 18-month gap between battery administration and retrieval of college academic standing records defines a predictive validity design. The methodological precaution — forbidding counselors from using battery scores in college recommendations — is specifically designed to prevent score-based selection into the college sample; if high-scorers were preferentially admitted to college, the resulting sample would have restricted range on the predictor, attenuating the validity coefficient. This mirrors the classic anchor point design.",
        "A": "Incorrect. Content validity concerns whether test items adequately sample the relevant knowledge or skill domain, and it is established through expert judgment at the test development stage, not through a longitudinal correlation study. High internal consistency and expert item development are mentioned in the vignette, making this a deliberate red herring; those features are relevant to content and reliability but do not describe the primary validity coefficient being reported.",
        "B": "Incorrect. Concurrent validity requires that both the predictor and criterion be collected at the same time; the 18-month gap between battery administration and academic standing retrieval is inconsistent with a concurrent design. While the precaution does relate to range restriction, attributing it to concurrent validity misidentifies the design type, making this option partially correct in reasoning but wrong in its validity label.",
        "D": "Incorrect. Criterion contamination occurs when the criterion measure is influenced by knowledge of the test scores — for example, if professors who knew students' battery scores allowed that knowledge to affect grade assignments. The counselors' restriction prevents score-based selection into the college sample, not contamination of the GPA criterion, so construct validity and criterion contamination are the wrong framework for this precaution."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-009-vignette-L5",
      "source_question_id": "009",
      "source_summary": "To evaluate the predictive validity of a new aptitude test for college admissions, a test developer administers the test to a sample of high school juniors and seniors admitted to college without use of their aptitude test scores and then correlates the students' aptitude test scores with their GPAs at the end of their second year of college.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A measurement firm administers a new paper-and-pencil reasoning battery to eleventh and twelfth graders at thirty high schools across a state. Administrators at each school are specifically instructed that the battery results must not influence any decisions made about the students during that academic year, including scholarship recommendations and advanced course placements, so that the composition of the group remains as broad and unselected as possible. The battery shows strong internal agreement among its items and was built using expert panels who reviewed every item for domain coverage. Nearly two years after the battery was given, the firm contacts the colleges those students enrolled in and obtains records reflecting each student's standing at the end of their second academic year. The firm then computes a single index relating the battery scores to those standing records and presents it as evidence that the battery has practical utility for forecasting outcomes.",
      "question": "What psychometric property does the firm's index most directly demonstrate, and what is the key methodological purpose of instructing school administrators not to use the battery results?",
      "options": {
        "A": "The index demonstrates the battery's internal consistency, and the restriction on use prevents students from preparing differently for future assessments based on their scores",
        "B": "The index demonstrates the battery's ability to forecast a future outcome from an earlier measurement, and the restriction on use prevents the group of students who later enroll in college from being limited to only those who scored well on the battery",
        "C": "The index demonstrates that the battery measures the same construct as an existing well-validated instrument, and the restriction on use prevents advisors from changing student behavior in ways that would artificially inflate college performance",
        "D": "The index demonstrates that battery scores align with a measure collected at the same time as the battery itself, and the restriction on use ensures that students in different schools are treated consistently during the testing window"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The firm's index is a predictive validity coefficient: the battery was administered well before the outcome (college standing records), and the resulting correlation captures the battery's ability to forecast that future outcome. The instruction not to use results for decisions — scholarship recommendations, course placements — prevents score-based filtering of who ultimately enrolls in college. If only high-scorers were recommended for programs that increased college enrollment, the sample would be truncated on the predictor, reducing the range of scores and attenuating the correlation. Keeping decisions score-independent preserves a broad, unselected sample, protecting the validity coefficient from restriction of range.",
        "A": "Incorrect. Internal consistency describes the degree to which a test's items correlate with one another and is estimated at the time of administration, not through a two-year follow-up study. The vignette's mention of 'strong internal agreement' is a deliberate red herring designed to pull attention toward reliability. The restriction on use is unrelated to preventing differential test preparation; it is about keeping the downstream college sample free from selection based on battery scores.",
        "C": "Incorrect. Demonstrating that a battery measures the same construct as an existing instrument describes convergent validity, a component of construct validity, which would require correlating the new battery with a recognized measure of the same attribute. No such comparison is described. The concern about advisors artificially inflating college performance through changed behavior resembles a criterion contamination argument but does not match the actual precaution described, which targets score-based selection into college rather than advisor influence on GPA.",
        "D": "Incorrect. Collecting both the test and the outcome at the same time would describe a concurrent validity design, but the vignette clearly states that outcome records were obtained nearly two years after the battery was administered. Consistent treatment across schools is a testing standardization concern, not the methodological rationale for prohibiting score use in decisions; that prohibition specifically addresses who ends up in the college-enrolled sample."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-002-vignette-L1",
      "source_question_id": "002",
      "source_summary": "In the context of factor analysis, \"orthogonal\" means the factors extracted are uncorrelated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "factor analysis",
        "orthogonal rotation",
        "uncorrelated"
      ],
      "vignette": "A psychometrician is developing a new cognitive abilities test and conducts an exploratory factor analysis on the item responses from a large normative sample. She applies an orthogonal rotation to the extracted factors and examines the resulting factor loadings matrix. The researcher notes that the rotation she chose produces factors that are statistically uncorrelated with one another. She uses this structure to argue that the test measures several distinct, independent dimensions of cognitive ability.",
      "question": "Which of the following best describes what is meant by the orthogonal rotation used in this factor analysis?",
      "options": {
        "A": "The rotation allows factors to correlate with each other, producing a more realistic psychological model.",
        "B": "The rotation constrains the extracted factors to remain uncorrelated, preserving their independence.",
        "C": "The rotation maximizes the variance explained by a single dominant factor across all items.",
        "D": "The rotation adjusts factor loadings to ensure each item loads on exactly one factor with a value of 1.0."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This describes an oblique rotation (e.g., Promax or Oblimin), which explicitly permits factors to correlate. Orthogonal rotations, by definition, constrain factors to remain at 90-degree angles to each other, keeping their correlations at zero.",
        "B": "Correct. Orthogonal rotations such as Varimax maintain the mathematical independence of factors, meaning the correlation between any two factors remains zero. This is the defining characteristic that distinguishes orthogonal from oblique rotations.",
        "C": "This describes what occurs in an unrotated principal components analysis that retains only one factor, or what a principal axis approach prior to rotation would show. Orthogonal rotation redistributes variance across multiple factors and does not collapse solution onto one factor.",
        "D": "This describes an idealized 'simple structure' goal but misrepresents the mechanics of orthogonal rotation. Factor loadings are continuous values ranging from -1 to +1; rotation reshapes the loading pattern but does not force values to exactly 1.0."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-002-vignette-L2",
      "source_question_id": "002",
      "source_summary": "In the context of factor analysis, \"orthogonal\" means the factors extracted are uncorrelated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "factor analysis",
        "rotation method"
      ],
      "vignette": "A researcher developing a personality inventory for use in a clinical population performs a factor analysis on responses collected from 600 adults with diagnosed anxiety disorders. He selects a rotation method that explicitly maintains the factors as statistically independent from one another. Although many personality theorists in his field argue that personality dimensions are naturally interrelated, the researcher chooses this method to simplify interpretation and facilitate the calculation of composite scores for each dimension.",
      "question": "The rotation method chosen by this researcher is best characterized by which of the following properties?",
      "options": {
        "A": "It permits the factors to share variance, reflecting the assumption that underlying personality traits covary.",
        "B": "It removes items with low item-total correlations before extracting the factor structure.",
        "C": "It constrains the factors to be uncorrelated with each other, treating each dimension as mathematically independent.",
        "D": "It identifies the number of factors to retain by applying a parallel analysis criterion to the eigenvalue distribution."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This describes an oblique rotation method. While oblique rotation may better reflect the actual correlational structure of personality traits, the scenario explicitly states the researcher is maintaining statistical independence among factors — ruling out oblique approaches.",
        "B": "Item-total correlation analysis is a step in item analysis used to evaluate item quality and is performed prior to or separately from factor rotation decisions. It does not define the rotation method and is unrelated to whether factors are correlated.",
        "C": "Correct. Orthogonal rotation methods (e.g., Varimax) impose the constraint that extracted factors must remain uncorrelated. This simplifies interpretation and composite scoring, which matches the researcher's stated rationale even if it may not fully capture natural trait covariance.",
        "D": "Parallel analysis is a method for deciding how many factors to retain by comparing observed eigenvalues to those from random data. It is a factor retention criterion, not a rotation method, and does not determine whether factors will be correlated or uncorrelated."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-002-vignette-L3",
      "source_question_id": "002",
      "source_summary": "In the context of factor analysis, \"orthogonal\" means the factors extracted are uncorrelated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "Varimax"
      ],
      "vignette": "A test developer publishes a new multidimensional measure of executive functioning intended for neuropsychological assessment. In the technical manual, she reports using Varimax rotation and notes that the resulting solution produced six clearly interpretable clusters of items, each reflecting a distinct executive process. Critics of the instrument argue that, given the well-established neurological interconnections among executive processes, the analytic strategy may have imposed an artificial independence on dimensions that naturally covary in the brain. The developer counters that her approach was appropriate for the goal of producing psychometrically clean, separately scorable subscales.",
      "question": "What specific assumption underlying the Varimax rotation is the basis of the critics' concern?",
      "options": {
        "A": "Varimax rotation assumes that the number of factors to be retained is fixed at the number of eigenvalues greater than 1.0.",
        "B": "Varimax rotation assumes that item residuals are normally distributed after factor extraction.",
        "C": "Varimax rotation assumes that the extracted factors are uncorrelated, which may misrepresent constructs known to be related.",
        "D": "Varimax rotation assumes that each item should load on only one factor, preventing cross-loadings across subscales."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The Kaiser criterion (eigenvalues > 1.0) is a factor retention rule and is not an assumption built into Varimax rotation itself. Critics concerned about artificial independence between constructs would not reference eigenvalue rules as the source of distortion.",
        "B": "Normality of residuals is an assumption relevant to certain maximum likelihood estimation procedures and model fit testing, not a defining assumption of Varimax rotation. This option invokes a plausible-sounding psychometric concern that does not match the critics' argument.",
        "C": "Correct. Varimax is an orthogonal rotation method, meaning it constrains all extracted factors to have zero correlation with each other. Critics correctly identify that when underlying constructs are genuinely related — as executive processes neurologically are — this orthogonality assumption can distort the factor structure.",
        "D": "Simple structure, where each item loads primarily on one factor, is a goal that Varimax attempts to approximate, but this is a descriptive aspiration rather than a strict mathematical assumption. More importantly, the critics' concern is about inter-factor correlation, not about cross-loadings of individual items."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-002-vignette-L4",
      "source_question_id": "002",
      "source_summary": "In the context of factor analysis, \"orthogonal\" means the factors extracted are uncorrelated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "independence"
      ],
      "vignette": "A measurement researcher is reviewing two competing factor solutions for a well-established achievement test. Solution A was derived using a method that preserves the geometric independence of the factor axes, while Solution B was derived using a method that allows the axes to rotate freely toward the data, resulting in a factor correlation matrix showing moderate intercorrelations around r = .40. The researcher notes that Solution B explains slightly more variance in the observed items and produces factor loadings that more closely mirror the correlational structure of real-world academic skills. However, a colleague argues that Solution A is preferable because its structure simplifies the interpretation of composite scores and avoids multicollinearity problems when the factor scores are used as predictors in subsequent regression analyses.",
      "question": "The colleague's preference for Solution A is grounded in which core psychometric property of that solution?",
      "options": {
        "A": "Solution A uses a rotation method that permits factor scores to be summed directly because items are keyed in the same direction.",
        "B": "Solution A uses a rotation method that produces factors which are uncorrelated, avoiding redundancy in composite scores and regression predictors.",
        "C": "Solution A uses a rotation method that maximizes the total variance explained in the item pool by concentrating loadings on fewer factors.",
        "D": "Solution A uses a rotation method that controls for capitalization on chance by cross-validating the factor structure across subsamples."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Item keying direction (i.e., whether items are scored positively or negatively) is an element of test construction and scale scoring but is unrelated to the type of factor rotation used. Geometric independence of factor axes has no bearing on item keying.",
        "B": "Correct. Solution A describes orthogonal rotation, in which the factor axes are maintained at 90 degrees, ensuring zero correlation among the extracted factors. The colleague's argument rests precisely on this property: uncorrelated factors yield non-redundant composite scores and eliminate multicollinearity when factor scores serve as predictors — a direct consequence of orthogonality.",
        "C": "Concentrating variance on fewer factors describes the behavior of an unrotated or minimally rotated principal components solution, not a feature specific to orthogonal rotation. Orthogonal rotation redistributes variance across factors to improve simple structure rather than consolidating it.",
        "D": "Cross-validation across subsamples to control for capitalization on chance is a procedure associated with regression shrinkage estimation or confirmatory replication studies. It is not a property of any factor rotation method, orthogonal or oblique."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-002-vignette-L5",
      "source_question_id": "002",
      "source_summary": "In the context of factor analysis, \"orthogonal\" means the factors extracted are uncorrelated.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team builds a new questionnaire intended to separately measure three distinct psychological strengths. After collecting data from a large community sample, they run an analysis that groups the questionnaire items into three clusters based on shared patterns of response. They then apply a mathematical adjustment to the three clusters so that, by design, no two clusters share any statistical relationship with each other — even though a secondary analysis of participants' scores on the three clusters reveals they correlate with each other at around r = .45. A reviewer praises the clean separation achieved by the mathematical adjustment but cautions that the imposed structure may have masked genuine relationships among the strengths that could be theoretically and practically important.",
      "question": "The mathematical adjustment described in the vignette is best identified as which of the following?",
      "options": {
        "A": "Oblique rotation, which allows the extracted dimensions to correlate and better reflect the natural covariance among constructs.",
        "B": "Orthogonal rotation, which constrains the extracted dimensions to be uncorrelated regardless of how the raw scores on those dimensions actually relate.",
        "C": "Item parceling, which aggregates individual items into composite units to reduce measurement error and improve model fit.",
        "D": "Confirmatory factor analysis, which tests whether a prespecified factor structure fits the observed data within acceptable tolerance limits."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Oblique rotation is a plausible-sounding answer because the vignette emphasizes that the raw cluster scores do in fact correlate at r = .45, which is exactly the kind of result oblique rotation would accommodate. However, the vignette explicitly states the adjustment was designed so no two clusters share any statistical relationship — the defining characteristic of orthogonal, not oblique, rotation. The observed correlations among raw scores are a separate finding that contrasts with the imposed mathematical structure.",
        "B": "Correct. Orthogonal rotation is the mathematical adjustment that forces extracted dimensions (factors) to be uncorrelated by construction, setting their inter-factor correlations to zero. The vignette's key detail is the deliberate design to eliminate any statistical relationship among clusters — this is the defining property of orthogonal rotation — while the observed r = .45 among raw scores highlights why the reviewer cautions that this may misrepresent reality.",
        "C": "Item parceling involves combining individual items into aggregate units (parcels) before modeling, often to reduce parameter estimation burden in structural equation modeling. It is a data preparation technique, not a rotation procedure, and it does not impose zero correlations among dimensions.",
        "D": "Confirmatory factor analysis is a structural modeling technique used to test whether a theoretically specified factor structure is consistent with observed data; it evaluates fit rather than imposing a particular correlation constraint between factors. The vignette describes an adjustment applied after clustering items, not a fit-testing procedure applied to a prespecified model."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-010-vignette-L1",
      "source_question_id": "010",
      "source_summary": "The item difficulty index ranges from 0 to +1.0, with 0 indicating a very difficult item.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "difficulty index",
        "item analysis",
        "p-value"
      ],
      "vignette": "A test developer conducts an item analysis on a newly constructed licensure examination. After scoring a pilot administration, she calculates the p-value for each item as part of a standard item analysis procedure. For one particular item, the difficulty index is computed to be 0.04. The test developer flags this item for revision before the next administration.",
      "question": "Based on the difficulty index value of 0.04, what does the test developer most likely conclude about this item?",
      "options": {
        "A": "The item is very easy because nearly all examinees answered it correctly.",
        "B": "The item is very difficult because very few examinees answered it correctly.",
        "C": "The item has poor discriminating power because it does not differentiate between high and low scorers.",
        "D": "The item has a near-perfect point-biserial correlation with the total test score."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect. A difficulty index of 0.04 means only 4% of examinees answered the item correctly, not nearly all. An easy item would have a difficulty index close to 1.0 (e.g., 0.90 or higher), indicating that most examinees answered correctly.",
        "B": "This is correct. The difficulty index (p-value) represents the proportion of examinees who answered the item correctly. A value of 0.04 means only 4% answered correctly, placing this item at the most difficult end of the 0-to-1.0 scale, close to 0.",
        "C": "This is incorrect. Discriminating power is captured by the discrimination index or the point-biserial correlation, not the difficulty index. While an extremely difficult item often has low discrimination (because almost no one gets it right), the difficulty index itself does not directly measure differentiation between high and low scorers.",
        "D": "This is incorrect. The point-biserial correlation is a separate statistic reflecting how well an item correlates with the total test score. A difficulty index of 0.04 provides no direct information about this correlation; in fact, items answered correctly by very few examinees tend to have restricted variance, which typically limits their discriminating power."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-010-vignette-L2",
      "source_question_id": "010",
      "source_summary": "The item difficulty index ranges from 0 to +1.0, with 0 indicating a very difficult item.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "difficulty index",
        "item analysis"
      ],
      "vignette": "A graduate student is assisting with the development of a classroom achievement test for a large introductory psychology course. After the first administration to 200 students, she conducts an item analysis and notices that one multiple-choice item yielded a difficulty index of 0.92. She also notes that this particular item covers a topic students found confusing during lectures, which surprised her given the high index value.",
      "question": "What does a difficulty index of 0.92 indicate about this item?",
      "options": {
        "A": "The item is very difficult, as indicated by the score near the upper end of the scale.",
        "B": "The item has high internal consistency with the overall test, suggesting strong construct representation.",
        "C": "The item is very easy, meaning the vast majority of students answered it correctly.",
        "D": "The item has a high discrimination index, effectively separating high-scoring from low-scoring students."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect. A common misconception is that a value close to 1.0 indicates greater difficulty because 'higher seems harder.' In fact, the difficulty index is a proportion of correct responses, so a value of 0.92 means 92% answered correctly — making it a very easy item, not a very difficult one.",
        "B": "This is incorrect. Internal consistency is a property of the overall test measured by statistics such as coefficient alpha or split-half reliability, not by the difficulty index of a single item. The difficulty index only conveys the proportion of correct responses for that item.",
        "C": "This is correct. The difficulty index ranges from 0 to 1.0, where values close to 1.0 indicate that a large proportion of examinees answered correctly — meaning the item is easy. A value of 0.92 means 92% of students got the item right, classifying it as an easy item regardless of the topic's perceived complexity in lectures.",
        "D": "This is incorrect. The discrimination index is a separate statistic that measures how well an item distinguishes between high and low scorers on the overall test. An item with a difficulty index of 0.92 is likely to have a low discrimination index because nearly everyone answers it correctly, leaving little variance to differentiate examinees."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-010-vignette-L3",
      "source_question_id": "010",
      "source_summary": "The item difficulty index ranges from 0 to +1.0, with 0 indicating a very difficult item.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "proportion correct"
      ],
      "vignette": "A psychometrician is reviewing a 50-item cognitive abilities test administered to 500 job applicants. During the review, she notices that item 37 has a proportion correct of 0.08 and flags it for potential removal. Several colleagues argue that the item should be retained because it shows a strong point-biserial correlation of 0.45 with the total score and appears to measure an important construct. The psychometrician acknowledges the item's discriminating power but maintains her recommendation to reconsider the item on a separate, specific psychometric ground.",
      "question": "On what specific psychometric ground is the psychometrician most likely recommending reconsideration of item 37?",
      "options": {
        "A": "The item has low concurrent validity because it does not correlate well with external performance criteria.",
        "B": "The item is excessively difficult, as its proportion correct of 0.08 indicates that very few examinees answered it correctly.",
        "C": "The item contributes to low test-retest reliability because its content is too variable across administrations.",
        "D": "The item reduces internal consistency because its point-biserial correlation is inflated by restriction of range."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect. Concurrent validity refers to the degree to which a test score correlates with an external criterion measured at the same time. The psychometrician's concern here is not about external criterion relationships but about the proportion of examinees who correctly answered the item, which is a matter of item difficulty.",
        "B": "This is correct. The proportion correct of 0.08 is essentially the difficulty index, and a value this close to 0 indicates that only 8% of applicants answered the item correctly — meaning the item is extremely difficult. Regardless of its point-biserial correlation, an item that virtually no one answers correctly contributes little useful measurement information across the range of examinees and is typically flagged on the basis of excessive item difficulty.",
        "C": "This is incorrect. Test-retest reliability concerns the stability of scores across two administrations over time and is not a property inferred from a single-administration item statistic like proportion correct. The psychometrician's recommendation is based on the single-administration item data, not cross-temporal consistency.",
        "D": "This is incorrect. Restriction of range reduces observed correlations rather than inflating them. While extremely difficult items can restrict variance and suppress point-biserial values, the item here actually shows a strong point-biserial correlation of 0.45, so this explanation is inconsistent with the data. The primary concern is the item's extreme difficulty, not an artifact affecting its correlation."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-010-vignette-L4",
      "source_question_id": "010",
      "source_summary": "The item difficulty index ranges from 0 to +1.0, with 0 indicating a very difficult item.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A measurement specialist is reviewing a set of items for a high-stakes professional certification exam. She observes that item 14 was answered correctly by only 3 out of 400 examinees. Notably, all three examinees who answered correctly were among the highest scorers on the total exam. A junior colleague argues that the item should be retained because its variance profile suggests it is doing exactly what a good test item should do — sorting examinees. The measurement specialist disagrees and explains that, despite this apparent pattern, a fundamental property of the item itself renders it psychometrically problematic independent of its ability to sort examinees.",
      "question": "What fundamental item property is the measurement specialist most concerned about?",
      "options": {
        "A": "The item has near-zero discrimination because it fails to differentiate the majority of examinees from one another.",
        "B": "The item has a floor effect that artificially compresses scores at the lower end of the distribution.",
        "C": "The item has an extremely low difficulty index, meaning virtually no examinees answered it correctly.",
        "D": "The item contributes to low content validity because it overrepresents rare or advanced knowledge domains."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is a highly plausible distractor. Near-zero discrimination would mean the item does not differentiate high from low scorers. However, the vignette actually suggests some discrimination — all three correct respondents were high scorers. The measurement specialist's concern is not the discrimination pattern per se but the item's intrinsic difficulty level, which is captured by the difficulty index rather than the discrimination index.",
        "B": "This is incorrect. A floor effect refers to a test or scale failing to measure adequately at the low end of the construct, producing artificially compressed low scores. This is typically a property of the overall test score distribution rather than a single item's proportion-correct statistic. The concern here is specifically about how many examinees answered the item correctly, which is the difficulty index.",
        "C": "This is correct. With only 3 of 400 examinees answering correctly, the difficulty index (proportion correct) is approximately 0.0075 — extremely close to 0. The difficulty index ranging from 0 to 1.0 with values near 0 indicating extreme difficulty is the fundamental psychometric concern. Regardless of who got it right, an item answered correctly by fewer than 1% of examinees provides almost no useful score variance across the vast majority of the test-taking population.",
        "D": "This is incorrect. Content validity is a judgment about whether items adequately sample the construct domain and is established through expert review, not through item-response statistics alone. While the content of the item may be advanced, the measurement specialist's stated concern is about a fundamental psychometric property — identifiable from the item's response data — rather than a validity judgment about content representativeness."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-010-vignette-L5",
      "source_question_id": "010",
      "source_summary": "The item difficulty index ranges from 0 to +1.0, with 0 indicating a very difficult item.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A researcher has developed a new 60-item test to assess a particular cognitive skill. After administering the test to 300 participants, she tabulates the results for each question. One question stands out: every single participant got it wrong. A colleague reviewing the data notes that this particular question was the only one drawn from the most advanced chapter of the training manual, and he argues this demonstrates that the test is successfully capturing important distinctions between novices and experts. The researcher acknowledges the content source but counters that, given the outcome data alone, the question cannot be doing what her colleague hopes — regardless of what it is supposed to measure.",
      "question": "What specific measurement property of this question most directly supports the researcher's conclusion that it cannot function as her colleague describes?",
      "options": {
        "A": "The question has a discrimination index of zero because no variance exists between those who answered correctly and those who did not.",
        "B": "The question has a difficulty index of zero because no participants answered it correctly, making it impossible for the item to differentiate among examinees.",
        "C": "The question has a ceiling effect because all participants performed at the lowest possible level, compressing the score distribution.",
        "D": "The question reduces the internal consistency of the test because its item-total correlation cannot be computed meaningfully."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is an extremely strong distractor and explains the functional failure described. When no one answers correctly, the discrimination index is indeed zero — the item cannot separate high from low scorers because there is no correct-response group to compare. However, the discrimination index is a derivative statistic that depends on the underlying correct-response rate. The more fundamental property that explains why zero discrimination exists is the difficulty index itself equaling zero, making B the more precise and primary explanation.",
        "B": "This is correct. The difficulty index is the proportion of examinees who answered the item correctly. When every participant answers incorrectly, the difficulty index equals 0 — the extreme low end of the 0-to-1.0 scale, indicating maximum item difficulty. Because no one answered correctly, there is no variance on this item and it literally cannot differentiate any examinees, making the colleague's claim about expert-novice distinction impossible to support from this item alone.",
        "C": "This is incorrect but plausible. A ceiling effect occurs when scores cluster at the top of a scale, preventing measurement of higher performance. A floor effect — where scores cluster at the bottom — is the analogous concern here. However, 'ceiling effect' does not accurately describe a situation where all participants fail; the correct analog term is floor effect. More importantly, neither ceiling nor floor effect is the specific item-level measurement property at issue — the difficulty index is the precise psychometric term that captures the proportion-correct statistic being described.",
        "D": "This is incorrect. While it is true that computing an item-total correlation when everyone scores zero on an item is mathematically problematic (due to zero variance on the item), low or uncomputable internal consistency is a consequence of the item's psychometric failure rather than its cause. The root psychometric property that produces this problem — and that most directly supports the researcher's conclusion — is the difficulty index of zero, not the downstream effect on internal consistency."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-007-vignette-L1",
      "source_question_id": "007",
      "source_summary": "Sensitivity is the proportion of people with a disorder who are identified by a test as having the disorder, calculated by dividing the true positives identified by the test by the true positives plus the false negatives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "sensitivity",
        "true positives",
        "false negatives"
      ],
      "vignette": "A researcher develops a screening instrument for major depressive disorder and administers it to 200 confirmed cases of depression and 200 confirmed non-cases. The instrument correctly identifies 160 of the 200 confirmed depression cases as positive and misses the remaining 40, labeling them as negative. The researcher wants to evaluate how well the instrument detects true positives relative to the total number of actual cases, including both true positives and false negatives. She computes a ratio using these values to characterize this aspect of the instrument's criterion-related validity.",
      "question": "Which psychometric property is the researcher computing?",
      "options": {
        "A": "Specificity",
        "B": "Positive predictive value",
        "C": "Sensitivity",
        "D": "Negative predictive value"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Specificity is the proportion of true non-cases correctly identified as negative by the test (true negatives divided by true negatives plus false positives). This researcher is focused on correctly identified cases among confirmed cases, not confirmed non-cases, so specificity does not apply here.",
        "B": "Positive predictive value is the proportion of positive test results that are true positives, and depends heavily on base rate prevalence. The researcher is calculating a ratio among confirmed cases — not among all who test positive — so this is not positive predictive value.",
        "C": "Sensitivity is correctly computed as true positives divided by (true positives plus false negatives), which equals the proportion of actual cases correctly identified by the test. Here 160 true positives divided by 200 total actual cases yields a sensitivity of .80, which is exactly what the researcher is computing.",
        "D": "Negative predictive value is the proportion of negative test results that are true negatives, and is also influenced by base rate prevalence. This concept pertains to the accuracy of negative test results, not the detection rate among confirmed cases."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-007-vignette-L2",
      "source_question_id": "007",
      "source_summary": "Sensitivity is the proportion of people with a disorder who are identified by a test as having the disorder, calculated by dividing the true positives identified by the test by the true positives plus the false negatives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "false negatives",
        "criterion-related validity"
      ],
      "vignette": "A hospital system adopts a brief cognitive screening tool to flag older adults for possible dementia in a geriatric primary care setting. The development team administers the tool to 150 patients who have been independently diagnosed with dementia by a neuropsychologist and finds that the tool correctly flags 120 of them as impaired. Despite the tool's brevity and the fact that many patients were anxious during administration, the team notes concern about the 30 patients who were missed and received a normal classification. They want to quantify the rate at which the tool fails to detect confirmed cases relative to all confirmed cases.",
      "question": "Which index best captures the psychometric concern the team is quantifying?",
      "options": {
        "A": "Positive predictive value",
        "B": "Specificity",
        "C": "Sensitivity",
        "D": "Concurrent validity coefficient"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Positive predictive value reflects the probability that someone who tests positive actually has the condition, calculated among those who test positive rather than among confirmed cases. The team's concern focuses specifically on confirmed cases, not on the composition of positive test results, so this index does not match.",
        "B": "Specificity measures the proportion of true non-cases correctly classified as negative, which relates to avoiding false positives. The team is not evaluating how well the tool excludes non-cases; they are evaluating detection among confirmed cases, making specificity inapplicable.",
        "C": "Sensitivity captures precisely the proportion of confirmed cases correctly identified, computed as true positives divided by true positives plus false negatives. The 30 missed patients are false negatives, and the team's expressed concern about missing confirmed cases directly corresponds to imperfect sensitivity (120/150 = .80).",
        "D": "A concurrent validity coefficient refers to the correlation between the test and a criterion measure obtained at approximately the same time, expressed as a correlation coefficient rather than a proportion of detected cases. While this is a criterion-related validity concept, it does not specifically quantify the detection rate among confirmed cases."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-007-vignette-L3",
      "source_question_id": "007",
      "source_summary": "Sensitivity is the proportion of people with a disorder who are identified by a test as having the disorder, calculated by dividing the true positives identified by the test by the true positives plus the false negatives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "screening"
      ],
      "vignette": "A public health researcher develops a brief questionnaire to identify adults at risk for alcohol use disorder in primary care settings. When validated against structured diagnostic interviews in a sample of 300 adults with confirmed alcohol use disorder, the questionnaire correctly classifies 240 as at-risk. The researcher notes approvingly that the questionnaire also correctly identifies 85% of the 200 confirmed non-cases as not at risk, and she presents both figures to the clinic. A colleague argues that the more clinically important figure — the one relevant to not missing people who truly have the disorder — is the first statistic, not the second.",
      "question": "What is the colleague referring to when emphasizing the importance of the first statistic?",
      "options": {
        "A": "Specificity",
        "B": "Negative predictive value",
        "C": "Positive predictive value",
        "D": "Sensitivity"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "Specificity is the second figure the researcher presents — the proportion of confirmed non-cases correctly identified as negative (85% of 200). The colleague explicitly distinguishes the first statistic from the second, so specificity describes the second figure, not the one the colleague is highlighting.",
        "B": "Negative predictive value is the proportion of those who test negative who are true non-cases, a base-rate-dependent index that pertains to negative test results rather than confirmed cases. The colleague's concern about missing people who truly have the disorder points to detection among confirmed cases, not the accuracy of negative results.",
        "C": "Positive predictive value is the probability that a positive result indicates a true case, computed among those testing positive and influenced heavily by prevalence. The colleague's emphasis is on catching all confirmed cases — that is, among confirmed cases, not among test-positive individuals — so positive predictive value is not the index in question.",
        "D": "Sensitivity is the proportion of confirmed cases correctly identified as positive, calculated as true positives divided by true positives plus false negatives. The first statistic (240 out of 300 confirmed cases correctly flagged) equals .80 sensitivity, and the colleague's concern about 'not missing people who truly have the disorder' is the defining clinical motivation for prioritizing sensitivity."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-007-vignette-L4",
      "source_question_id": "007",
      "source_summary": "Sensitivity is the proportion of people with a disorder who are identified by a test as having the disorder, calculated by dividing the true positives identified by the test by the true positives plus the false negatives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "detection rate"
      ],
      "vignette": "A neuropsychologist is consulting on the psychometric evaluation of a new continuous performance test intended to assist in diagnosing ADHD in adolescents. The test developer reports that the instrument produces a high proportion of correct endorsements when applied to a confirmed clinical group, and that this detection rate was largely unaffected even when the base rate of ADHD in the validation sample dropped from 40% to 15%. A colleague reviewing the report suggests that this stability across base rates is a key conceptual advantage that distinguishes the statistic being reported from another commonly cited index the test developer also computed, which did fluctuate substantially as prevalence changed. The neuropsychologist must decide which statistic the developer is primarily highlighting in the initial claim.",
      "question": "Which psychometric statistic is the test developer primarily reporting when describing the proportion of correct endorsements within the confirmed clinical group?",
      "options": {
        "A": "Positive predictive value",
        "B": "Sensitivity",
        "C": "Kappa coefficient",
        "D": "Specificity"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Positive predictive value is the index that fluctuates substantially with changes in base rate prevalence — exactly what the colleague notes about the second statistic. On first read, the language 'correct endorsements' might suggest positive predictive value, but PPV is computed among those who test positive, not among confirmed clinical cases, and it is well established that PPV changes markedly as prevalence varies, making it the unstable index referenced by the colleague.",
        "B": "Sensitivity is the proportion of confirmed cases (the clinical group) correctly identified as positive, and it is computed entirely within the confirmed-case group, making it mathematically independent of the overall base rate or prevalence in the sample. This independence from base rate is the distinguishing conceptual advantage the developer is implicitly claiming, and the description of 'correct endorsements within the confirmed clinical group' precisely defines sensitivity.",
        "C": "A kappa coefficient measures inter-rater or test-criterion agreement corrected for chance, and while it does respond to changes in prevalence to some degree, it is not typically described as a 'detection rate' and is not computed as a proportion within the confirmed clinical group. Nothing in the scenario describes chance-corrected agreement, making kappa an implausible answer.",
        "D": "Specificity is the proportion of confirmed non-cases correctly identified as negative, not the proportion of confirmed cases correctly identified as positive. Although specificity shares with sensitivity the property of being base-rate independent, the developer's statistic pertains to the clinical group (confirmed cases), which by definition corresponds to sensitivity, not specificity."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-007-vignette-L5",
      "source_question_id": "007",
      "source_summary": "Sensitivity is the proportion of people with a disorder who are identified by a test as having the disorder, calculated by dividing the true positives identified by the test by the true positives plus the false negatives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of researchers examines two summary figures produced when a new brief checklist is evaluated against a thorough independent assessment process used as the reference standard. The first figure stays nearly constant whether the team draws their sample from a community clinic where the condition is common or from a general population setting where it is rare. The second figure changes considerably across these two settings, rising sharply when the condition becomes more common. The team argues that for settings where clinicians cannot afford to overlook anyone who truly has the condition, the first figure is the more appropriate basis for judgment, even though the second figure appears numerically higher in high-prevalence settings and might seem more impressive at first glance. Both figures are derived from the same two-by-two contingency table, but they are computed using different rows of that table.",
      "question": "Which property of the checklist is the team identifying as the primary basis for evaluating the instrument in settings where missing true cases is the critical concern?",
      "options": {
        "A": "The proportion of those who receive a positive result from the checklist who are confirmed to have the condition",
        "B": "The proportion of those who are confirmed to have the condition who receive a positive result from the checklist",
        "C": "The proportion of those who are confirmed not to have the condition who receive a negative result from the checklist",
        "D": "The overall proportion of results from the checklist that agree with the reference standard"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This describes positive predictive value — the proportion of test-positive results that are true positives — which is the figure computed from the column of those who test positive. The vignette explicitly describes the second figure as the one that fluctuates with prevalence and appears numerically higher in high-prevalence settings, which are defining properties of positive predictive value. It is not the stable figure the team is recommending, despite being a plausible candidate because it also relates to correct identification.",
        "B": "This is sensitivity — computed from the row of confirmed cases, as the proportion of true cases correctly identified. Sensitivity is mathematically independent of prevalence because it is derived entirely from the confirmed-case row of the two-by-two table, which is why it remains constant across the community and general population settings. The team's stated concern about 'not overlooking anyone who truly has the condition' is precisely the clinical motivation for prioritizing sensitivity, and the clue that the first figure uses a different row than the second figure distinguishes sensitivity (confirmed-case row) from positive predictive value (test-positive column).",
        "C": "This describes specificity — the proportion of confirmed non-cases correctly identified as negative, computed from the confirmed-non-case row. Like sensitivity, specificity is also stable across prevalence changes, which makes it a strong distractor. However, the team's concern is about missing true cases, which corresponds to the confirmed-case row, not the confirmed-non-case row. Specificity would be prioritized when avoiding false alarms is the critical concern, not when avoiding missed cases.",
        "D": "This describes overall accuracy or the proportion of agreement between the checklist and the reference standard, a global index that combines correct positives and correct negatives. Overall accuracy is heavily influenced by base rate prevalence because the dominant category (usually non-cases in low-prevalence settings) can inflate the figure, so this index would not remain constant across the two settings described, and it does not specifically address the concern about missing true cases."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-008-vignette-L1",
      "source_question_id": "008",
      "source_summary": "Raising the cutoff score of a new selection test will decrease the number of false positives and increase the number of true negatives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "cutoff score",
        "false positives",
        "true negatives"
      ],
      "vignette": "A human resources psychologist has developed a cognitive ability test to screen job applicants for a demanding technical role. After reviewing the initial validation data, she notices the current cutoff score is producing too many false positives — applicants who pass the test but later fail on the job. She decides to raise the cutoff score to reduce these erroneous acceptances. The psychologist explains to her team that this adjustment will have predictable effects on the classification accuracy of the test.",
      "question": "Which of the following best describes the expected outcome of raising the cutoff score on this selection test?",
      "options": {
        "A": "The number of true positives will increase and the number of false negatives will decrease.",
        "B": "The number of false positives will decrease and the number of true negatives will increase.",
        "C": "The test's predictive validity coefficient will improve, reducing shrinkage when cross-validated.",
        "D": "The sensitivity of the test will increase, correctly identifying more true cases of job success."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect because raising the cutoff score does the opposite — it reduces the pool of people who pass, which decreases true positives (correctly accepted qualified candidates) and increases false negatives (qualified candidates who are incorrectly rejected).",
        "B": "This is correct. Raising the cutoff score means fewer applicants meet the threshold, so fewer unqualified candidates are incorrectly accepted (fewer false positives) and more unqualified candidates are correctly rejected (more true negatives). This is a foundational consequence of adjusting selection cutoffs.",
        "C": "Predictive validity and shrinkage refer to the stability of a validity coefficient when cross-validated in a new sample. Changing the cutoff score does not alter the underlying correlation between test scores and criterion performance, so the validity coefficient itself is unaffected.",
        "D": "Sensitivity refers to the proportion of true positives correctly identified (i.e., the hit rate). Raising the cutoff score actually decreases sensitivity, because more truly qualified candidates will now score below the higher threshold and be incorrectly rejected."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-008-vignette-L2",
      "source_question_id": "008",
      "source_summary": "Raising the cutoff score of a new selection test will decrease the number of false positives and increase the number of true negatives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "cutoff score",
        "selection ratio"
      ],
      "vignette": "A large metropolitan police department uses a structured personality inventory as part of its hiring process. The department's psychologist notes that the current selection ratio is quite liberal, and many applicants who are accepted later show poor job performance or disciplinary problems. The psychologist recommends raising the cutoff score on the inventory to address this concern, acknowledging that the department will need to recruit from a larger applicant pool to fill its vacancies. Several of the psychologist's colleagues worry that this change will reduce the number of good candidates who are accepted.",
      "question": "If the cutoff score on the personality inventory is raised, which of the following changes to the test's classification outcomes is most likely?",
      "options": {
        "A": "Specificity will decrease because more unqualified applicants will be incorrectly accepted.",
        "B": "The base rate of job success in the hired group will decrease because fewer true positives enter the organization.",
        "C": "False positives will decrease and true negatives will increase, at the cost of more false negatives.",
        "D": "Concurrent validity will improve because the restricted range of scores will strengthen the criterion correlation."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect. Specificity is the proportion of truly unqualified applicants correctly rejected. Raising the cutoff score actually increases specificity, not decreases it, because more unqualified applicants will fall below the higher threshold and be correctly turned away.",
        "B": "This is incorrect. Raising the cutoff score typically increases the base rate of success among those hired, not decreases it, because the hired group is a more highly screened subset. The concern raised by colleagues about losing good candidates refers to increased false negatives, not a reduction in the base rate of success.",
        "C": "This is correct. A higher cutoff means fewer people pass the test overall. Among those who now fail, some will be truly unqualified (true negatives — a gain) while others will be genuinely qualified but incorrectly rejected (false negatives — a cost). The net effect is a decrease in false positives and an increase in true negatives.",
        "D": "Concurrent validity is an empirical relationship between test scores and a criterion measured at the same time. Changing the cutoff score does not alter the validity coefficient; that statistic reflects the test's predictive power across all score levels. Restriction of range would actually reduce, not increase, the observed correlation."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-008-vignette-L3",
      "source_question_id": "008",
      "source_summary": "Raising the cutoff score of a new selection test will decrease the number of false positives and increase the number of true negatives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "specificity"
      ],
      "vignette": "A clinical neuropsychologist is evaluating the performance of a new cognitive screening battery designed to identify early-stage dementia in community-dwelling older adults. Initial field testing reveals that many individuals without dementia are being flagged as at-risk, creating unnecessary patient distress and overloading specialist referral services. A colleague suggests the problem lies in how the threshold for a positive screening result is set and proposes adjusting it upward. The neuropsychologist notes that this change would improve specificity but cautions that the adjustment is not without trade-offs, particularly for patients with genuine early pathology who score near the boundary.",
      "question": "What is the most accurate description of what will happen to the screening battery's classification outcomes if the threshold for a positive result is raised?",
      "options": {
        "A": "Sensitivity will increase and the rate of missed true cases will decrease, improving the battery's clinical utility.",
        "B": "The positive predictive value will decrease because a higher threshold will reduce the proportion of correct identifications among all positive results.",
        "C": "False positives will decrease and true negatives will increase, but false negatives will also increase, reducing the battery's sensitivity.",
        "D": "The test's criterion-related validity coefficient will increase because restricting the score range tightens the relationship between test scores and diagnosis."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect and actually describes the opposite effect. Raising the threshold makes it harder to receive a positive result, so individuals with early dementia who score near the new boundary will now be missed, increasing false negatives and decreasing sensitivity — not increasing it.",
        "B": "This is partially plausible but incorrect. Positive predictive value (PPV) is the probability that a positive result is a true positive. Raising the threshold typically increases PPV because the positives that remain are more likely to be genuine cases; fewer false positives pollute the positive pool. The option's reasoning is reversed.",
        "C": "This is correct. Raising the cutoff increases the bar for a positive identification, which means fewer non-cases are flagged (fewer false positives, more true negatives — improved specificity). However, some genuine early-dementia cases who previously scored above the old threshold will now fall below the new one, increasing false negatives and decreasing sensitivity. The neuropsychologist's caution about patients near the boundary directly references this trade-off.",
        "D": "This is incorrect. The criterion-related validity coefficient is a statistical property of the test that reflects the overall relationship between test scores and the criterion across the full score distribution. Changing where the cutoff is set does not change the underlying correlation. Restriction of range, if anything, would attenuate the coefficient rather than strengthen it."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-008-vignette-L4",
      "source_question_id": "008",
      "source_summary": "Raising the cutoff score of a new selection test will decrease the number of false positives and increase the number of true negatives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "base rate"
      ],
      "vignette": "A state licensing board has developed a written examination for prospective clinical psychologists. After several years of administration, the board receives complaints that too many licensees are later found to have deficiencies in professional competence, suggesting the test is admitting candidates who should not pass. A psychometric consultant reviews the data and recommends a policy change that will reduce erroneous licensure decisions of that particular type. The board is initially resistant, concerned that the change will disproportionately harm candidates from groups with lower average scores on the examination and reduce the annual number of new licensees significantly. The consultant acknowledges these costs but argues that, given the relatively low base rate of truly competent candidates in the applicant pool, the proposed change will substantially improve the overall accuracy of the licensing decision for unqualified applicants.",
      "question": "The psychometric consultant's recommendation most directly addresses which classification outcome, and what is the mechanism of improvement?",
      "options": {
        "A": "The recommendation reduces false negatives by lowering the threshold required for licensure, thereby capturing more truly competent candidates who previously fell just below the cutoff.",
        "B": "The recommendation improves concurrent validity by aligning the cutoff with empirically established criterion performance levels, strengthening the test–criterion correlation.",
        "C": "The recommendation reduces false positives and increases true negatives by raising the cutoff score, trading increased false negatives for greater accuracy in rejecting unqualified applicants — an exchange made more defensible by the low base rate of truly competent candidates.",
        "D": "The recommendation increases sensitivity by recalibrating the examination's item difficulty index, ensuring the test more reliably detects competent candidates across the full ability range."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect and describes the opposite direction of change. Lowering the threshold would increase the number of people licensed, worsening the problem of erroneous licensure of incompetent candidates. The complaint driving the consultation is too many false positives, not too many false negatives, making a threshold reduction counterproductive.",
        "B": "This is incorrect. Concurrent validity is an empirical property of the test reflecting the correlation between test scores and a simultaneously measured criterion. Adjusting a cutoff score is a policy decision about where to draw the pass/fail line; it does not alter the underlying test–criterion correlation or constitute a validity-improving intervention.",
        "C": "This is correct. The complaints describe excessive false positives (incompetent candidates licensed). Raising the cutoff reduces false positives and increases true negatives (unqualified applicants correctly denied licensure). The consultant's reference to a low base rate of truly competent applicants is key: when the base rate is low, even a test with good validity produces many false positives at a liberal cutoff, so raising the threshold is especially impactful for improving accuracy in that cell. The acknowledged cost — more false negatives (competent candidates denied licensure) — aligns with the board's concern about reduced new licensees.",
        "D": "This is incorrect. Sensitivity refers to the proportion of truly competent candidates correctly licensed (true positive rate). The problem described is not a failure to detect competent candidates but rather the admission of incompetent ones. The item difficulty index is a property of individual test items (proportion answering correctly) and pertains to test construction, not cutoff policy. Adjusting item difficulty would require revising the test itself, not simply changing a passing standard."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-008-vignette-L5",
      "source_question_id": "008",
      "source_summary": "Raising the cutoff score of a new selection test will decrease the number of false positives and increase the number of true negatives.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "An organization that certifies financial advisors has been receiving complaints from client advocacy groups that too many certified advisors are later found to have caused financial harm to clients, suggesting the certification process is not filtering out unsuitable candidates effectively. In response, the certification body quietly changes the minimum passing requirement on its written examination, making it harder to receive a passing designation. Within two years, the number of new certifications issued each year drops noticeably, and several industry groups protest that many well-qualified candidates are now being turned away. An independent analyst reviewing the data confirms that the proportion of newly certified advisors who later cause harm has declined, but also notes that some candidates who likely would have performed well are no longer receiving certification. A researcher studying the program observes that the improvements in the 'correctly rejected' category are most pronounced because only a small fraction of all applicants in the pool are truly qualified to begin with.",
      "question": "Which of the following psychometric phenomena most precisely accounts for all the described outcomes — the decline in harmful certifications, the increase in well-qualified candidates turned away, the drop in annual certifications, and the researcher's observation about the applicant pool?",
      "options": {
        "A": "Decreased sensitivity and increased specificity resulting from raising the cutoff score, with the magnitude of improvement in the true-negative rate amplified by the low base rate of genuinely qualified applicants.",
        "B": "Increased predictive validity of the examination resulting from restriction of range in the certified group, which tightens the correlation between examination performance and later advisor conduct.",
        "C": "Reduced positive predictive value caused by a low base rate of qualified applicants, which means even a well-designed examination will produce mostly false positives regardless of where the passing threshold is set.",
        "D": "Regression to the mean in post-certification performance, whereby candidates who barely passed under the new higher threshold regress toward average performance levels, reducing observed harm rates over time."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. All four outcomes are explained by a single policy change — raising the cutoff score. Fewer harmful certifications = fewer false positives. More well-qualified candidates turned away = more false negatives (decreased sensitivity). Fewer total certifications = predictable consequence of a more stringent threshold. The researcher's observation about the small fraction of truly qualified applicants = a low base rate, which amplifies the gain in true negatives (specificity) when the cutoff rises, because most applicants near the boundary are unqualified and are now correctly rejected.",
        "B": "This is a plausible but incorrect distractor. Restriction of range does occur in the certified group after raising the cutoff, but it would attenuate rather than strengthen the test–criterion correlation. More importantly, restriction of range is a statistical artifact of who is observed, not an explanation for the full pattern of outcomes described — particularly the increase in turned-away qualified candidates and the base-rate dynamic.",
        "C": "This option correctly invokes the base rate and its relationship to positive predictive value, which is partially relevant to the researcher's observation. However, it misrepresents the mechanism: the organization's change did reduce harmful certifications (false positives), which means the intervention was effective. The claim that a well-designed test produces mostly false positives regardless of threshold ignores that raising the threshold is precisely the tool used to reduce false positives. This option cannot account for the observed decline in harm.",
        "D": "Regression to the mean is a real statistical phenomenon in which extreme scores on a first measurement tend to be less extreme on a second measurement. It could superficially explain lower harm rates over time. However, it does not explain the increase in turned-away qualified candidates, the drop in total certifications, or the researcher's observation about the applicant pool composition — all of which are consequences of a cutoff change, not of statistical regression. It also requires the assumption that barely-passing candidates were an extreme group, which is not stated."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-006-vignette-L1",
      "source_question_id": "006",
      "source_summary": "The point at which an item characteristic curve intercepts the Y (vertical) axis provides information about the probability of answering the item correctly by guessing.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "item characteristic curve",
        "Y-axis intercept",
        "guessing"
      ],
      "vignette": "A psychometrician is using item response theory to analyze a 50-item multiple-choice cognitive ability test. She plots the item characteristic curve for each item and notes where each curve intercepts the Y-axis. She explains to her graduate assistant that this Y-axis intercept is particularly informative about test-taker behavior when ability is at its theoretical minimum. The graduate assistant is trying to understand what specific parameter of examinee performance this intercept value reflects.",
      "question": "What does the point at which an item characteristic curve intercepts the Y-axis most directly indicate?",
      "options": {
        "A": "The probability of answering the item correctly by chance alone",
        "B": "The degree to which the item discriminates between high- and low-ability examinees",
        "C": "The difficulty level of the item as indexed by the proportion of examinees passing it",
        "D": "The standard error of measurement associated with the item at the mean ability level"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. In item response theory, the Y-axis intercept of the item characteristic curve represents the probability of a correct response when ability (theta) is at negative infinity — effectively, the probability of answering correctly by guessing. For a 4-option multiple-choice item, this value approximates 0.25.",
        "B": "Incorrect. The discrimination parameter (a-parameter in IRT) is reflected by the steepness or slope of the item characteristic curve at its inflection point, not by where the curve crosses the Y-axis. Higher discrimination produces a steeper S-shaped curve.",
        "C": "Incorrect. The difficulty parameter (b-parameter in IRT) corresponds to the point on the ability scale (X-axis) at which the probability of a correct response is 0.50, not the Y-axis intercept. The classical difficulty index (p-value) is also conceptually distinct from the Y-axis intercept.",
        "D": "Incorrect. The standard error of measurement is a reliability concept reflecting the precision of scores across the ability distribution; it is not read directly from the Y-axis intercept of an item characteristic curve, which concerns guessing probability."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-006-vignette-L2",
      "source_question_id": "006",
      "source_summary": "The point at which an item characteristic curve intercepts the Y (vertical) axis provides information about the probability of answering the item correctly by guessing.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "item characteristic curve",
        "pseudo-chance"
      ],
      "vignette": "A test development team is refining a licensure examination that uses four-answer multiple-choice items. During item analysis, the team notes that several items have Y-axis intercepts on their item characteristic curves that are substantially higher than 0.25, which concerns the senior psychometrician. She notes that examinees with very low ability scores are performing better on these items than the test's design intended, and she attributes this to a specific parameter often called the pseudo-chance level parameter. The team is also reviewing item-total correlations and coefficient alpha, but the senior psychometrician keeps returning to the Y-axis intercept issue as a distinct problem.",
      "question": "What psychometric phenomenon does the elevated Y-axis intercept of the item characteristic curve most directly reflect in this scenario?",
      "options": {
        "A": "Reduced internal consistency due to items that fail to correlate with the total score",
        "B": "The probability that examinees with minimal ability will answer the item correctly through random guessing",
        "C": "A floor effect indicating that the test is too difficult for low-ability examinees to demonstrate any true ability",
        "D": "Low item discrimination, as indicated by a shallow slope in the middle range of the ability continuum"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The Y-axis intercept of the item characteristic curve — the c-parameter or pseudo-chance level in 3-parameter IRT models — directly captures the probability of a correct response for examinees at the lowest end of the ability distribution, which is essentially the guessing probability. An intercept above 0.25 for a 4-option item suggests more successful guessing than expected by chance.",
        "A": "Incorrect. Internal consistency (e.g., coefficient alpha) reflects the average inter-item correlation across all items and is not directly readable from the Y-axis intercept of a single item's characteristic curve. An item with a high pseudo-chance parameter might still correlate reasonably well with the total score among higher-ability examinees.",
        "C": "Incorrect. A floor effect describes a situation where scores cluster at the bottom of a scale due to excessive difficulty, not a specific Y-axis intercept value. Elevated Y-axis intercepts actually reflect the opposite tendency — low-ability examinees succeed more than expected, which is inconsistent with a floor effect.",
        "D": "Incorrect. Low item discrimination is reflected by a shallow slope of the item characteristic curve (low a-parameter), not the Y-axis intercept. Discrimination concerns how well the item differentiates between ability levels throughout the midrange of the scale, not performance at the lowest ability extreme."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-006-vignette-L3",
      "source_question_id": "006",
      "source_summary": "The point at which an item characteristic curve intercepts the Y (vertical) axis provides information about the probability of answering the item correctly by guessing.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "item response theory"
      ],
      "vignette": "A researcher is evaluating a newly developed reasoning test for use with adolescents and applies item response theory modeling to the data. She is particularly puzzled by one item that shows a steep S-shaped curve with a high slope value but also has an unusually high starting point at the left end of the graph. Although the item clearly separates high- from low-ability students along most of the continuum, she notices that even students scoring at the very bottom of the distribution have a surprisingly high probability of getting the item right. A colleague suggests the problem may relate to the item's wording inadvertently providing clues that allow examinees to narrow down the correct answer without actually knowing the material.",
      "question": "The high starting point at the left end of the item characteristic curve most directly reflects which of the following?",
      "options": {
        "A": "The item's difficulty level, because items that most examinees answer correctly will show elevated curves throughout their range",
        "B": "The item's discrimination power, because a steep slope necessarily produces a higher starting point on the vertical axis",
        "C": "The probability of a correct response attributable to guessing among examinees with the lowest levels of the measured ability",
        "D": "A restriction of range artifact caused by the homogeneous ability level of the adolescent sample used in item calibration"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. In item response theory, the Y-axis intercept — corresponding to the c-parameter or lower asymptote — captures the probability of a correct response for examinees at the lowest extreme of the ability distribution, which is conceptually equivalent to guessing. The colleague's observation about clues enabling answer elimination is consistent with an inflated guessing parameter that raises the lower asymptote above the chance level expected for a purely random response.",
        "A": "Incorrect. Item difficulty in IRT is indexed by the b-parameter, which marks the ability level at which the probability of a correct response is midway between the lower asymptote and 1.0. An easy item shifts the curve leftward along the X-axis; it does not necessarily raise the Y-axis intercept. These are distinct parameters.",
        "B": "Incorrect. The slope (a-parameter) and the lower asymptote (c-parameter) are independent parameters in the 3-parameter logistic IRT model. A steep slope reflects high discrimination but does not mathematically determine or raise the Y-axis starting point. The vignette distinguishes the two by noting both a high slope and an unusual starting point.",
        "D": "Incorrect. Restriction of range refers to reduced variability in the sample, which can attenuate correlation-based statistics such as discrimination indices or validity coefficients. It does not specifically produce an elevated Y-axis intercept on an item characteristic curve and does not explain why low-ability examinees succeed at unexpectedly high rates on a particular item."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-006-vignette-L4",
      "source_question_id": "006",
      "source_summary": "The point at which an item characteristic curve intercepts the Y (vertical) axis provides information about the probability of answering the item correctly by guessing.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "lower asymptote"
      ],
      "vignette": "A psychometrician presents a graph of a single test item's performance to a group of trainees. The curve rises steeply and reaches a high plateau, suggesting it works well at separating examinees across most of the ability range. However, she draws attention to the lower asymptote of the curve, pointing out that it sits noticeably above zero and is nearly identical across several items on the test regardless of how difficult those items are. She comments that this consistency in the lower asymptote across items of varying difficulty is exactly what one would expect given the test's response format, and she challenges the trainees to explain what this convergent lower asymptote value specifically tells them about how examinees at the far left of the ability distribution are likely performing.",
      "question": "What does the consistent elevation of the lower asymptote across items of varying difficulty most directly communicate about examinee performance?",
      "options": {
        "A": "That the items share a similar degree of discrimination, as parallel lower asymptotes are produced when a-parameters are constrained to be equal across items",
        "B": "That examinees with negligible ability have a consistent baseline probability of answering correctly, reflecting the influence of random guessing given the test's response format",
        "C": "That the test has high parallel-forms reliability because items of differing difficulty produce equivalent lower performance thresholds",
        "D": "That the difficulty parameters of the items are artificially inflated by the homogeneity of item content, causing the curves to share a common starting elevation"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The lower asymptote of an item characteristic curve — the c-parameter — represents the probability of a correct response when ability is at its minimum, which operationally corresponds to random guessing. Its consistent value across items of varying difficulty is expected because guessing probability is determined by the number of response options (e.g., 1/4 for a 4-choice item), not by item difficulty. The psychometrician's comment about the response format driving this consistency underscores that the format — not content — controls the baseline guessing probability.",
        "A": "Incorrect. The a-parameter (discrimination) determines the steepness of the item characteristic curve's slope, not its Y-axis starting point. Items can have identical lower asymptotes while differing substantially in discrimination. Constraining a-parameters to equality would produce parallel mid-curve slopes, not a common lower asymptote.",
        "C": "Incorrect. Parallel-forms reliability concerns the correlation between scores obtained from two equivalent test versions administered to the same group and is a test-level statistic, not an item-level graphical feature. Similar lower asymptotes across items of varying difficulty reflect the guessing parameter, not any reliability coefficient between alternate forms.",
        "D": "Incorrect. Homogeneity of item content would more directly affect content validity or internal consistency; it does not mechanically elevate or equalize the lower asymptotes of item characteristic curves. The b-parameter (difficulty) shifts the curve along the X-axis but does not set the Y-axis starting point, which is determined by the c-parameter independent of difficulty."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-006-vignette-L5",
      "source_question_id": "006",
      "source_summary": "The point at which an item characteristic curve intercepts the Y (vertical) axis provides information about the probability of answering the item correctly by guessing.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A measurement specialist is explaining to a graduate seminar why a particular value drawn from a graphical representation of an item's performance is especially important when evaluating tests that require examinees to choose among a fixed set of possible answers. She notes that this value remains relatively stable across items on the same test regardless of how hard or easy those items are, and it is mathematically predictable from the number of options presented for each question. She emphasizes that this value specifically reflects what happens at the extreme low end of the measured continuum — essentially capturing what a person who truly knows nothing about the tested material would score on any given item simply by responding at random. A student in the seminar argues that this feature seems less important than the steepness of the curve or where along the horizontal axis the curve rises most sharply, but the specialist counters that ignoring this feature can lead to serious misinterpretation of how well even low-performing examinees are doing.",
      "question": "The value the specialist describes — stable across items, determined by response format, and reflecting performance at the extreme low end of the measured continuum — most directly represents which psychometric concept?",
      "options": {
        "A": "The difficulty index of the item, since this value reflects the proportion of the total group answering correctly and anchors the curve's vertical position",
        "B": "The standard error of measurement at the lower tail of the score distribution, which quantifies imprecision for examinees far below the mean",
        "C": "The probability of a correct response attributable to random guessing, represented by the Y-axis intercept of the item's performance curve",
        "D": "The discrimination index of the item, since the consistency of this value across items indicates that all items differentiate equally well between the lowest and highest scorers"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The specialist is describing the Y-axis intercept of the item characteristic curve — the lower asymptote or c-parameter in IRT — which represents the probability of a correct response for examinees at the lowest possible ability level, i.e., the guessing probability. Its mathematical predictability from the number of response options (e.g., 0.25 for 4-choice items) and its stability across items of varying difficulty are hallmark features of this parameter, distinguishing it from the slope and location parameters the student was focused on.",
        "A": "Incorrect. The difficulty index in classical test theory is the proportion of all examinees who answer an item correctly and summarizes performance across the entire ability distribution. It does not specifically reflect what happens at the lowest extreme of the continuum, nor is it mathematically determined by the number of response options. The specialist's emphasis on the value being stable regardless of item difficulty directly contradicts what a difficulty index would show.",
        "B": "Incorrect. The standard error of measurement quantifies the precision of a score at a given point on the ability scale and is a reliability-related concept applied at the test or score level. While it can be estimated separately for different score ranges, it is not a feature read from the Y-axis of an item performance curve and is not mathematically determined by the number of response options available for each item.",
        "D": "Incorrect. The discrimination index reflects how well an item differentiates between high- and low-ability examinees and corresponds to the slope of the item characteristic curve. Consistency across items would indicate equal discriminating power, which is a different property entirely from guessing probability. The specialist explicitly contrasts the value she is describing with the steepness of the curve (i.e., discrimination), making this option incompatible with the scenario."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-012-vignette-L1",
      "source_question_id": "012",
      "source_summary": "In a factor matrix, a communality indicates the proportion of variability in a single test that is accounted for by all of the identified factors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "communality",
        "factor matrix",
        "factor analysis"
      ],
      "vignette": "A psychometrician conducts a factor analysis on a newly developed personality inventory and examines the resulting factor matrix. For one of the test items, she reports a communality of .72. She explains to her research team that this value summarizes how much of that item's total variability is explained by the factors retained in the solution. The team asks her to clarify what exactly this index represents in the context of the overall factor analysis.",
      "question": "What does the communality of .72 for this item indicate?",
      "options": {
        "A": "The proportion of variance in the item accounted for by all identified factors combined",
        "B": "The correlation between the item and one specific factor in the factor matrix",
        "C": "The proportion of total test variance explained by the largest factor alone",
        "D": "The degree to which the item's scores remain stable across two separate administrations"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. A communality represents the proportion of an individual item's (or variable's) variance that is accounted for by all of the retained factors together in the factor solution, in this case 72% of the item's variability is explained by the combined factors.",
        "B": "This is incorrect. The correlation between a single item and a single factor is called a factor loading, not a communality. Communality aggregates the explanatory power across all factors, not just one.",
        "C": "This is incorrect. The proportion of total test variance explained by one factor is reflected in the eigenvalue or the percentage of variance associated with that factor, not in the communality, which is item-specific and accounts for all factors.",
        "D": "This is incorrect. Stability of scores across administrations refers to test-retest reliability, a reliability coefficient rather than a factor-analytic index. Communality has no temporal component."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-012-vignette-L2",
      "source_question_id": "012",
      "source_summary": "In a factor matrix, a communality indicates the proportion of variability in a single test that is accounted for by all of the identified factors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "factor analysis",
        "communality"
      ],
      "vignette": "A researcher develops a 40-item cognitive abilities battery and runs a factor analysis, retaining four factors. Upon inspecting the output, he notices that Item 17, which was written to measure working memory, has a communality of .38, considerably lower than most other items. The researcher is working with a sample of college undergraduates, which he later acknowledges may have reduced score variability compared to the general population. He wonders whether the low value for Item 17 means the item is poorly captured by the factor solution.",
      "question": "What does the communality of .38 for Item 17 specifically indicate?",
      "options": {
        "A": "Item 17's scores are highly unstable and would change substantially on re-administration",
        "B": "Item 17 shares little overlap with the other items when estimating internal consistency",
        "C": "Only 38% of Item 17's variance is explained by all four retained factors combined",
        "D": "Item 17 loads primarily on one factor and has minimal cross-loadings on the others"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. A communality of .38 means that the four retained factors collectively account for only 38% of the variance in Item 17, leaving 62% as unique or error variance not captured by the factor solution, suggesting the item is poorly represented by those factors.",
        "A": "This is incorrect. Score instability over time is indexed by test-retest reliability, not communality. Communality is a factor-analytic statistic that says nothing about temporal consistency of scores.",
        "B": "This is incorrect. Overlap with other items as it pertains to internal consistency is captured by item-total correlations or coefficient alpha, not communality. While both relate to shared variance, communality is specific to the factor solution rather than inter-item covariance for reliability purposes.",
        "D": "This is incorrect. Loading primarily on one factor with minimal cross-loadings describes the pattern of factor loadings for the item, not the communality. An item can have a high communality while loading exclusively on one factor if that factor accounts for most of its variance."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-012-vignette-L3",
      "source_question_id": "012",
      "source_summary": "In a factor matrix, a communality indicates the proportion of variability in a single test that is accounted for by all of the identified factors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "factor loading"
      ],
      "vignette": "A graduate student is reviewing the psychometric report for a new emotional regulation scale and reads: 'Item 9 exhibited a factor loading of .81 on Factor 2 and loadings below .15 on Factors 1 and 3. Despite this strong association with a single factor, the index reflecting how much of Item 9's total variance is captured by the solution as a whole was computed to be .68.' The student initially assumes this value is simply the squared factor loading from Factor 2, but her advisor corrects her. The advisor notes that the computed index accounts for contributions from all factors simultaneously, not just the dominant one.",
      "question": "What psychometric index is the advisor describing, and what does a value of .68 indicate in this context?",
      "options": {
        "A": "The item's factor loading, indicating it correlates .68 with the primary factor after rotation",
        "B": "The communality, indicating that 68% of Item 9's variance is accounted for by all three factors combined",
        "C": "The item-total correlation, indicating Item 9 shares 68% of its variance with the total score on the scale",
        "D": "The coefficient of determination for Item 9, indicating the squared multiple correlation predicting the item from the total test score"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The communality is the proportion of an item's variance explained by all retained factors, obtained by summing the squared factor loadings across all factors for that item. A value of .68 means 68% of Item 9's variance is jointly captured by Factors 1, 2, and 3 — not just the dominant Factor 2.",
        "A": "This is incorrect. The factor loading for Item 9 on Factor 2 was already stated to be .81, and factor loadings reflect the correlation between the item and one specific factor. The .68 value was explicitly distinguished from any single factor loading by the advisor.",
        "C": "This is incorrect. The item-total correlation is a reliability-related statistic computed during item analysis, reflecting how much an individual item covaries with the sum of all items. It is not derived from a factor solution and does not aggregate across factors.",
        "D": "This is incorrect. While a communality can be mathematically equivalent to a squared multiple correlation in some contexts, the 'coefficient of determination predicting the item from total test score' describes a regression-based index used in item analysis, not the factor-analytic communality, which is derived from factor loadings rather than a regression on the total score."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-012-vignette-L4",
      "source_question_id": "012",
      "source_summary": "In a factor matrix, a communality indicates the proportion of variability in a single test that is accounted for by all of the identified factors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "eigenvalue"
      ],
      "vignette": "During a dissertation defense, a doctoral candidate presents results from her principal components analysis of a vocational interest inventory. She reports that three components were retained based on the eigenvalue-greater-than-one criterion. Her committee chair points out that for Item 22, the h² statistic is .29, even though Item 22's highest bivariate association with any single extracted component is .49. The chair argues this discrepancy reveals something important: the value of .29 is not simply derived from the item's strongest association, and the remaining components contribute only trivially. One committee member suggests this low value means the item has poor reliability, while another argues it reflects the item's unique variance.",
      "question": "Which interpretation of the h² value of .29 for Item 22 is most accurate?",
      "options": {
        "A": "Item 22 has poor test-retest reliability, as indicated by the low h² value relative to its factor loading",
        "B": "Item 22's variance is mostly unique, and h² estimates the proportion attributable to systematic error shared across items",
        "C": "h² represents the communality, meaning only 29% of Item 22's variance is accounted for by all three retained components together",
        "D": "h² is the item's discriminability index, indicating that the item poorly differentiates between high- and low-scoring respondents"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. h² is the standard notation for communality in factor and components analysis. A communality of .29 means the three retained components together account for only 29% of Item 22's total variance, with the remainder being unique or error variance. The discrepancy between .29 and the squared loading of .49² = .24 plus small contributions from the other two components sums to .29, confirming it reflects all components jointly.",
        "A": "This is incorrect. Test-retest reliability reflects temporal stability of scores and is estimated via correlation between two administrations, not from any factor-analytic statistic. h² has no temporal component and cannot index reliability in this sense.",
        "B": "This is incorrect. Unique variance is actually the complement of communality (i.e., 1 − h²), representing the portion of variance not explained by the retained components. This option inverts the correct interpretation: a low h² means most variance is unique, but h² itself indexes the shared, not the unique, portion.",
        "D": "This is incorrect. A discriminability index (or discrimination index) is an item analysis statistic derived from classical test theory, reflecting how well an item distinguishes between high- and low-scorers on the total test. It is not derived from a factor solution and bears no direct relationship to h²."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-012-vignette-L5",
      "source_question_id": "012",
      "source_summary": "In a factor matrix, a communality indicates the proportion of variability in a single test that is accounted for by all of the identified factors.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A measurement researcher examines one particular question on a 30-question psychological assessment that was developed to capture several distinct but related aspects of human behavior. She uses a statistical procedure that groups questions together based on how similarly participants respond to them, ultimately identifying four such groupings. For the question in question, she calculates a specific index by squaring each of that question's four numerical associations — one for each grouping — and summing those four squared values. The resulting number, .31, is notably lower than the same index for most other questions on the assessment, even though the question's strongest individual association with any single grouping is moderate at .47. Her colleague suggests the low value simply means participants gave inconsistent answers to this question over time, but she disagrees.",
      "question": "What does the index of .31 most precisely represent, and what does it indicate about this question?",
      "options": {
        "A": "The question's internal consistency contribution, indicating it shares only 31% of variance with the overall set of questions when alpha is estimated",
        "B": "The communality, indicating that the four groupings together account for only 31% of the variability in responses to this question",
        "C": "The question's test-retest reliability coefficient, indicating temporal instability unrelated to how groupings were formed",
        "D": "The proportion of variance in the overall assessment explained by this question across all four groupings"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The procedure described is factor analysis (grouping items by response patterns into factors), and the index computed by squaring each factor loading and summing across all factors is precisely the communality (h²). A value of .31 means the four identified factors together explain only 31% of the variance in this item's responses, with the remaining 69% being unique or error variance not captured by the factor solution.",
        "A": "This is incorrect. Internal consistency (e.g., coefficient alpha) is estimated from inter-item covariances across the full item set and is not derived by squaring and summing a question's associations with identified groupings. While both involve shared variance, the described calculation procedure specifically defines communality, not an alpha-based internal consistency estimate.",
        "C": "This is incorrect. Test-retest reliability is a temporal consistency estimate derived from correlating scores across two points in time, not from any calculation involving groupings of items. The researcher's explicit disagreement with this interpretation within the vignette further rules it out, and the described mathematical procedure has no temporal component.",
        "D": "This is incorrect. The proportion of overall assessment variance explained by a single question is not a standard psychometric index derived in this way. The communality is item-specific — it tells us how much of one question's variance the groupings explain — not how much of the test's total variance that question accounts for."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-011-vignette-L1",
      "source_question_id": "011",
      "source_summary": "The split-half reliability coefficient for the second sample of students with grade point averages ranging from 2.0 to 4.0 will most likely be larger than .75, since the magnitude of a reliability coefficient is affected by the range of scores, and the second sample has a larger range of scores than the first sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "split-half reliability",
        "range of scores",
        "restriction of range"
      ],
      "vignette": "A psychometrician administers a study skills inventory to two samples of college students. The first sample consists only of honor-roll students with GPAs ranging from 3.5 to 4.0, while the second sample includes all enrolled students with GPAs ranging from 2.0 to 4.0. When she computes the split-half reliability coefficient for each sample, she finds that the coefficient for the first sample is .55 and predicts that the second sample will yield a substantially higher value. She attributes this difference to the effect of restriction of range on the magnitude of reliability coefficients.",
      "question": "Which psychometric principle best explains why the split-half reliability coefficient is expected to be higher for the second, broader sample?",
      "options": {
        "A": "Internal consistency reliability is inherently higher when fewer items are included in each half of the test, inflating coefficients in heterogeneous samples.",
        "B": "Restriction of range in the first sample artificially lowers the reliability coefficient, while the broader range of scores in the second sample allows the true reliability to be more accurately estimated.",
        "C": "The Spearman-Brown prophecy formula automatically corrects for sample heterogeneity, producing higher coefficients in samples with wider score distributions.",
        "D": "Criterion-related validity, not split-half reliability, is the statistic most sensitive to differences in score range across samples."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is incorrect. The number of items per half does not change between samples; item count is held constant in this scenario. Internal consistency is not inflated simply by sample heterogeneity through item reduction.",
        "B": "This is correct. Restriction of range in the honor-roll sample compresses score variability, which attenuates the correlation between test halves and thus lowers the split-half reliability coefficient. The broader GPA range in the second sample preserves natural score variability, allowing individual differences to manifest and the reliability coefficient to reflect the instrument's true consistency.",
        "C": "This is incorrect. The Spearman-Brown formula corrects for the fact that a split-half coefficient is based on only half the test length — it does not adjust for sample heterogeneity or differences in score range.",
        "D": "This is incorrect. Criterion-related validity describes the relationship between test scores and an external criterion; it is a distinct concept from reliability. While criterion validity can also be affected by range restriction, the scenario specifically concerns split-half reliability coefficients, not validity coefficients."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-011-vignette-L2",
      "source_question_id": "011",
      "source_summary": "The split-half reliability coefficient for the second sample of students with grade point averages ranging from 2.0 to 4.0 will most likely be larger than .75, since the magnitude of a reliability coefficient is affected by the range of scores, and the second sample has a larger range of scores than the first sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "reliability coefficient",
        "range of scores"
      ],
      "vignette": "A graduate student is conducting her dissertation research on a newly developed cognitive flexibility scale. She collects data from two university sites: one site recruited only students enrolled in advanced honors seminars (GPA 3.6–4.0), while the other site sampled from the general student population (GPA 2.0–4.0). Both samples are similar in age and gender composition. After computing internal consistency estimates using split halves for each site, she is puzzled to find that the general-population site produced a reliability coefficient nearly 0.25 points higher than the honors site, despite the scale performing similarly in terms of item means.",
      "question": "What is the most likely psychometric explanation for the discrepancy in reliability coefficients between the two sites?",
      "options": {
        "A": "The honors site sample, being more academically motivated, responded with greater consistency, which paradoxically reduces variance and lowers the reliability coefficient.",
        "B": "The honors site's narrower range of scores reduces the variability needed to detect consistent individual differences, thereby attenuating the reliability coefficient relative to the broader general-population sample.",
        "C": "The general-population site likely had more measurement error due to greater cognitive variability, which inflated the coefficient by increasing true-score variance more than error variance.",
        "D": "The difference in reliability coefficients reflects differences in test-retest stability between the two sites rather than any effect of score variability on internal consistency estimates."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. When a sample's scores cluster in a narrow band — as with the honors students whose GPAs range only from 3.6 to 4.0 — the variance in observed scores is diminished. Because reliability coefficients are fundamentally ratios of true-score variance to total variance, reducing score variability lowers the coefficient. The general-population sample's broader score range preserves the variance necessary to estimate reliability accurately.",
        "A": "This is incorrect. Greater response consistency among highly motivated students might seem like it would raise reliability, but the critical issue is variance reduction, not motivational consistency. When all respondents score similarly because they are drawn from a restricted ability range, the reliability coefficient is suppressed regardless of their motivation.",
        "C": "This is incorrect. Greater cognitive variability in the general-population sample does reflect larger true-score variance, which does raise the reliability coefficient — but attributing this to 'measurement error' being inflated is conceptually wrong. It is the true-score variance, not error variance, that increases, and this distinction is essential to understanding the reliability ratio.",
        "D": "This is incorrect. Test-retest reliability concerns stability of scores across two administrations separated in time. The scenario describes a single-administration split-half procedure, making test-retest stability an irrelevant concept here. The observed difference is an artifact of score range, not temporal stability."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-011-vignette-L3",
      "source_question_id": "011",
      "source_summary": "The split-half reliability coefficient for the second sample of students with grade point averages ranging from 2.0 to 4.0 will most likely be larger than .75, since the magnitude of a reliability coefficient is affected by the range of scores, and the second sample has a larger range of scores than the first sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "split-half"
      ],
      "vignette": "A researcher develops a workplace problem-solving assessment and pilots it in two organizations. Organization A is a highly selective consulting firm that only employs individuals with advanced degrees and exceptionally high aptitude scores; Organization B is a large manufacturing company with a diverse workforce spanning a wide range of educational backgrounds and skill levels. The split-half coefficient obtained from Organization A is .48, which the researcher notes is disappointingly low given that the items appeared to have good face validity and the scale had high internal consistency in earlier pilot work with mixed samples. Interestingly, item-level statistics from Organization A reveal that most items have very similar means and standard deviations, with very few extreme scorers.",
      "question": "The researcher's finding that the split-half coefficient is unexpectedly low in Organization A is most precisely explained by which of the following?",
      "options": {
        "A": "The items in Organization A function as a speeded test for highly skilled respondents, compressing scores toward a ceiling and reducing the split-half estimate.",
        "B": "The homogeneity of the Organization A sample truncates score variability, attenuating the correlation between test halves and thereby lowering the split-half reliability coefficient.",
        "C": "The items display poor content validity for the advanced skill level of Organization A's employees, causing random responding that lowers internal consistency.",
        "D": "The split-half method is inherently unreliable when applied to small, specialized occupational samples because the item halves are not truly parallel in such groups."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "This is a plausible red herring. Ceiling effects do compress scores and can reduce reliability, and they often occur in high-ability samples. However, the vignette specifies that item means and standard deviations are similar across items — not that scores cluster at the maximum. The more precise and general explanation is restriction of range due to sample homogeneity, which applies even in the absence of a strict ceiling effect.",
        "B": "This is correct. The detail that most items have very similar means and standard deviations with few extreme scorers directly indicates that score variability is restricted in Organization A. Reliability coefficients depend on variance in true scores relative to total score variance; when the sample is homogeneous, this ratio shrinks and the coefficient is attenuated, independent of the instrument's actual quality. The earlier high internal consistency in mixed samples supports this interpretation.",
        "C": "This is incorrect. Poor content validity would mean the items do not adequately represent the intended construct domain; however, the scenario specifically notes that face validity appeared good and internal consistency was high in earlier mixed-sample pilots. Content validity problems would be expected to persist across samples, not emerge only in a restricted one.",
        "D": "This is incorrect. While parallel-form assumptions are relevant to split-half methods, the split-half technique does not require large or diverse samples to be technically valid. The issue here is not a methodological flaw in the split-half approach itself but rather a sample characteristic — homogeneity — that suppresses the resulting coefficient."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-011-vignette-L4",
      "source_question_id": "011",
      "source_summary": "The split-half reliability coefficient for the second sample of students with grade point averages ranging from 2.0 to 4.0 will most likely be larger than .75, since the magnitude of a reliability coefficient is affected by the range of scores, and the second sample has a larger range of scores than the first sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A test developer publishes a manual reporting that her emotional regulation inventory has strong psychometric properties based on a normative sample drawn from outpatient mental health clinics across multiple diagnostic categories. A subsequent researcher administers the same inventory exclusively to individuals diagnosed with subclinical anxiety — a group that, by diagnostic definition, scores in a narrow band on emotional dysregulation. The researcher obtains a split-half estimate of .42 and concludes that the inventory is poorly suited for research use. She attributes the low coefficient to a flaw in the test's item construction. A colleague reviewing the data notes that the variance in observed scores for the subclinical anxiety group is dramatically smaller than the variance reported in the test manual's normative sample.",
      "question": "The colleague's observation most directly implicates which explanation for the discrepancy in split-half estimates between the two samples?",
      "options": {
        "A": "The subclinical anxiety sample represents a clinical population for whom the inventory's content validity is insufficient, producing unsystematic responding that depresses internal consistency.",
        "B": "The use of split-half reliability rather than coefficient alpha introduces measurement artifact when samples are clinically homogeneous, because the two halves become non-equivalent.",
        "C": "The reduced observed-score variance in the subclinical anxiety sample attenuates the correlation between test halves, producing a lower split-half coefficient even if the instrument's true reliability is unchanged.",
        "D": "Regression to the mean in the subclinical anxiety group causes scores to cluster around the scale midpoint, which inflates error variance relative to true-score variance and lowers reliability estimates."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The colleague's key observation is that observed-score variance is dramatically smaller in the subclinical anxiety sample than in the normative sample. Because the split-half reliability coefficient is a function of the ratio of true-score variance to total observed variance, a reduction in total variance — even without any change in the instrument's measurement precision — will lower the coefficient. The test itself has not changed; the sample's homogeneity has artificially attenuated the estimate.",
        "A": "This is a highly defensible distractor. Content validity concerns whether items adequately represent the construct, and a narrow clinical population might respond unsystematically if items do not tap their specific experience. However, the colleague's note focuses specifically on reduced variance — not on item irrelevance or unsystematic responding — making this explanation inconsistent with the observed data pattern.",
        "B": "This is incorrect but subtly plausible. Split-half reliability does assume that the two halves function as parallel measures, and clinical homogeneity might seem to disrupt this assumption. However, non-equivalence of halves is a concern about item sampling, not about sample score variability. Coefficient alpha would face the same attenuation problem in a restricted-range sample; switching methods would not resolve the fundamental issue.",
        "D": "This is incorrect. Regression to the mean is a phenomenon that occurs across repeated measurements — when extreme scorers at one time point score closer to the mean at a second time point. It is not a mechanism that operates within a single administration to redistribute scores toward the midpoint or to systematically alter the error-to-true-score variance ratio in the manner described."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-011-vignette-L5",
      "source_question_id": "011",
      "source_summary": "The split-half reliability coefficient for the second sample of students with grade point averages ranging from 2.0 to 4.0 will most likely be larger than .75, since the magnitude of a reliability coefficient is affected by the range of scores, and the second sample has a larger range of scores than the first sample.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team develops a questionnaire intended to measure how adaptable people are when faced with unexpected problems. They first test it on a group of employees at a single company known for hiring only through a rigorous selective process that filters for a very narrow band of problem-solving ability; everyone in this group tends to answer most questions in a very similar way, with scores bunched tightly together. The coefficient computed by splitting the questionnaire into two equal halves is .41. The team then tests the same questionnaire, with no changes to any items, on a large, open-enrollment community college population where students range from those who struggled academically to those who excelled; scores in this group spread widely across all possible values. The computed coefficient from the second group is .79. A consultant reviewing both datasets remarks that the questionnaire itself performed identically in both settings and that the difference in coefficients is entirely a function of who was tested, not how well the questionnaire was built.",
      "question": "The consultant's conclusion is best supported by which psychometric explanation?",
      "options": {
        "A": "The questionnaire items functioned as a power test for the selective-company group but as a speeded test for the community college group, producing different score distributions and thus different consistency estimates.",
        "B": "The selective-company group's greater familiarity with workplace problem-solving scenarios introduced systematic bias that suppressed agreement between the two halves of the questionnaire.",
        "C": "The community college group's wider spread of individual differences in the underlying trait preserved the statistical conditions necessary for the between-halves correlation to reflect the questionnaire's actual consistency, whereas the selective-company group's compressed scores obscured this consistency.",
        "D": "The larger absolute number of participants in the community college sample stabilized the coefficient estimate, while the smaller and more homogeneous selective-company sample produced an unstable and therefore lower coefficient due to sampling error."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. The consultant's key claim is that the questionnaire performed identically in both settings — meaning item quality, wording, and measurement precision were unchanged. The only difference was who was tested. In the selective-company group, scores clustered tightly, reducing the variance in observed scores. Because the coefficient that results from comparing two halves of a test is fundamentally a correlation, and correlations are attenuated when score variability is compressed, the .41 coefficient underestimates the instrument's true consistency. The community college group's wide score spread allowed individual differences to emerge and the correlation between halves to approach the questionnaire's actual consistency level.",
        "A": "This is a plausible distractor because power versus speeded administration is a real distinction that affects score distributions. However, nothing in the scenario describes time limits or the role of speed; the score differences are attributed to the ability range of participants, not to how quickly they completed the questionnaire. Applying a power/speed distinction here imports an unsupported mechanism.",
        "B": "This is a subtly attractive distractor because it appeals to the intuition that domain expertise might introduce bias in a workplace questionnaire. However, familiarity with the content domain would more likely produce systematic patterns that increase, not decrease, agreement between test halves. More importantly, the scenario explicitly states scores were simply bunched together — not that they were biased in direction — which points to variance restriction rather than systematic bias.",
        "D": "This is the most dangerous distractor at this level. Sample size does affect the stability of correlation estimates, and larger samples do yield more stable coefficients. However, the scenario does not specify that sample sizes differ, and even if they did, sampling error alone would produce fluctuation around a true value — not a consistent 0.38-point depression. The mechanism the consultant describes is about who was tested (their ability range), not how many were tested, making sample size an insufficient explanation for such a large and directional discrepancy."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-043-vignette-L1",
      "source_question_id": "043",
      "source_summary": "The incremental validity of a new selection test (predictor) is calculated by subtracting the base rate from the positive hit rate.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "incremental validity",
        "base rate",
        "positive hit rate"
      ],
      "vignette": "A personnel psychologist develops a new cognitive ability test intended to improve hiring decisions at a large manufacturing firm. Before adopting the test, she examines its incremental validity by comparing how many applicants the test correctly identifies as successful employees (the positive hit rate) against the proportion of all applicants who would succeed without using any test (the base rate). She subtracts the base rate from the positive hit rate and obtains a value of .18. The psychologist concludes that the new test modestly improves selection accuracy beyond what would be achieved by chance or by the existing procedure alone.",
      "question": "The value of .18 that the psychologist calculated is best described as which of the following?",
      "options": {
        "A": "The incremental validity of the new selection test",
        "B": "The predictive validity coefficient of the new test",
        "C": "The selection ratio for the hiring process",
        "D": "The sensitivity of the test in identifying successful employees"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. Incremental validity is defined as the improvement in predictive accuracy a new predictor provides over and above what already exists, and in criterion-related validity contexts it is computed as the positive hit rate minus the base rate. A value of .18 directly reflects this incremental gain.",
        "B": "Incorrect. A predictive validity coefficient refers to the correlation (e.g., Pearson r) between test scores and a criterion measured at a later time. It is not calculated by subtracting a base rate from a hit rate, and it is expressed as a correlation coefficient rather than a simple difference.",
        "C": "Incorrect. The selection ratio is the proportion of applicants who are hired out of all who apply. It influences validity but is not computed by comparing hit rates to base rates, and it does not quantify how much a test improves decision-making.",
        "D": "Incorrect. Sensitivity is a signal-detection concept referring to the proportion of true positives correctly identified by a test (true positives / [true positives + false negatives]). While related to hit rates, sensitivity is not calculated by subtracting a base rate and does not capture the incremental improvement a test provides over an existing base-rate decision."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-043-vignette-L2",
      "source_question_id": "043",
      "source_summary": "The incremental validity of a new selection test (predictor) is calculated by subtracting the base rate from the positive hit rate.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "predictor",
        "base rate"
      ],
      "vignette": "A mid-sized technology company asks an industrial-organizational psychologist to evaluate a new structured interview protocol designed to predict first-year job performance among software engineers. The company currently hires without formal screening, and 40% of all engineers hired under the old system are rated as high performers at year-end. When the structured interview is used as a predictor and candidates scoring above the cutoff are hired, 65% of those individuals are rated as high performers. The psychologist notes that the company's workforce is predominantly male and wonders whether gender may confound the analysis, but statistical checks indicate no differential prediction. She computes a single index to summarize how much the new interview improves upon making decisions without it.",
      "question": "What psychometric index did the psychologist compute, and how did she arrive at it?",
      "options": {
        "A": "Positive predictive value, calculated as true positives divided by all positive test results",
        "B": "Incremental validity, calculated by subtracting the base rate (.40) from the positive hit rate (.65)",
        "C": "Concurrent validity coefficient, calculated by correlating interview scores with current performance ratings",
        "D": "The discrimination index, calculated by comparing high-scorer and low-scorer pass rates on the criterion"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Incremental validity in selection contexts is computed as the positive hit rate minus the base rate. Here, .65 − .40 = .25, reflecting the degree to which the interview improves accurate placement decisions beyond what the unaided base rate would yield.",
        "A": "Incorrect. Positive predictive value (PPV) is indeed the proportion of test-positive individuals who truly meet the criterion, which equals the positive hit rate (.65 here). However, PPV is a signal-detection metric and is not computed by subtracting the base rate; it does not directly quantify the improvement the test provides over no testing.",
        "C": "Incorrect. Concurrent validity is assessed by administering the predictor and criterion measure at roughly the same time and computing their correlation. The scenario describes a predictive (prospective) design, and the statistic of interest is a difference score, not a correlation coefficient.",
        "D": "Incorrect. The discrimination index is an item-analysis statistic used in test construction to compare how high-scoring and low-scoring examinees respond to individual test items. It is not used to evaluate how much a selection procedure improves hiring outcomes relative to baseline rates."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-043-vignette-L3",
      "source_question_id": "043",
      "source_summary": "The incremental validity of a new selection test (predictor) is calculated by subtracting the base rate from the positive hit rate.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "selection"
      ],
      "vignette": "A healthcare organization introduces a personality-based screening battery to reduce nurse turnover. The HR team reports that, historically, 35% of all hired nurses remain with the organization past two years — a figure the consulting psychologist immediately flags as important context. After implementing the battery, 58% of nurses who scored above the recommended cutoff were still employed at the two-year mark. The psychologist is encouraged but reminds leadership that the battery's usefulness must be understood against the background of what the organization could accomplish without any formal screening whatsoever. She therefore computes an index comparing the two figures to express how much the battery actually adds.",
      "question": "Which concept most precisely describes the index the psychologist computed, and what value does it yield?",
      "options": {
        "A": "Positive predictive value of .58, reflecting the proportion of screened-in nurses who stayed past two years",
        "B": "Predictive validity expressed as a correlation between battery scores and tenure outcome",
        "C": "Incremental validity of .23, derived by subtracting the 35% base rate from the 58% positive hit rate",
        "D": "Specificity of .23, reflecting the proportion of true non-stayers correctly identified by the battery"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. Incremental validity quantifies the additional predictive accuracy a new test provides over and above baseline (no-test) decision-making. It is calculated as the positive hit rate minus the base rate: .58 − .35 = .23. The psychologist's emphasis on what the organization could achieve 'without any formal screening' is the defining clue pointing to this concept.",
        "A": "Incorrect. Positive predictive value (PPV) is indeed .58 in this scenario, as it equals the proportion of test-positive individuals who meet the criterion. However, PPV does not capture incremental gain over baseline decision-making because it ignores what the base rate alone would have produced; it is therefore not the index the psychologist computed.",
        "B": "Incorrect. A predictive validity coefficient is a correlation (r) between scores on the screening battery and the criterion (two-year tenure). Correlational coefficients are dimensionless values between −1 and +1 and are not computed by comparing percentages; the described calculation involves a simple difference, not a correlation.",
        "D": "Incorrect. Specificity refers to the proportion of true negatives correctly identified (true negatives / [true negatives + false positives]) — in other words, the battery's accuracy in flagging those who would leave. The value .23 matches numerically, but the scenario describes hit rates among retained employees, not a ratio of correct non-selections, making specificity the wrong construct here."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-043-vignette-L4",
      "source_question_id": "043",
      "source_summary": "The incremental validity of a new selection test (predictor) is calculated by subtracting the base rate from the positive hit rate.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "hit rate"
      ],
      "vignette": "A police department psychologist evaluates a newly developed situational judgment test (SJT) intended to reduce the proportion of new recruits who are later disciplined for misconduct. Department records show that, without any formal psychological screening, roughly one in four recruits historically avoids any disciplinary action in their first three years — a proportion the psychologist records carefully. After piloting the SJT for one academy cohort, she finds that 43% of recruits who passed the SJT cutoff completed three years without discipline. The SJT developers argue that these numbers demonstrate strong criterion-related validity, pointing to the fact that the test correctly categorizes nearly half of the selected recruits. The psychologist disagrees, countering that the critical figure is not the absolute hit rate but rather the difference between what the test achieves and what the department already achieves without it.",
      "question": "The statistic at the center of the psychologist's argument — the one she believes the developers are neglecting — is best characterized as which of the following?",
      "options": {
        "A": "Positive predictive value, because the 43% figure represents true positives among all positively screened recruits",
        "B": "Incremental validity, because the meaningful index is the positive hit rate minus the pre-existing base rate",
        "C": "Concurrent validity, because both the SJT scores and discipline records were analyzed within the same cohort",
        "D": "The false negative rate, because the psychologist is focusing on recruits the SJT failed to correctly screen out"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The psychologist's argument is that the absolute hit rate (.43) is misleading without comparing it to what the department already achieves by chance (the base rate of .25). Incremental validity is precisely this difference (.43 − .25 = .18), and her insistence on evaluating the test against the no-test baseline is the defining feature of this concept. The scenario is designed to make positive predictive value seem correct on first read, but PPV does not involve subtracting the base rate.",
        "A": "Incorrect. Positive predictive value equals the proportion of test-positive individuals who truly meet the criterion, which is .43 in this case. The developers are implicitly relying on PPV when they tout the 43% figure. However, the psychologist is not disputing what PPV equals — she is arguing that PPV alone is insufficient because it does not reveal whether the test improves on what the department already achieves, which is the essence of incremental validity.",
        "C": "Incorrect. Concurrent validity refers to a design in which the predictor and criterion are collected at approximately the same point in time, typically expressed as a correlation coefficient. The scenario describes a prospective predictive design (recruits screened and then followed for three years), and the statistic being debated is a difference between rates, not a correlation.",
        "D": "Incorrect. The false negative rate is the proportion of individuals who fail the criterion but were passed by the test (or, equivalently, 1 − sensitivity). While false negatives are a concern in personnel selection, the psychologist's specific critique focuses on the gap between the test's hit rate and the base rate, not on individuals who were cleared but later disciplined. False negative rate is a signal-detection index that does not capture incremental improvement over baseline decisions."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-043-vignette-L5",
      "source_question_id": "043",
      "source_summary": "The incremental validity of a new selection test (predictor) is calculated by subtracting the base rate from the positive hit rate.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A large retailer begins using a brief online questionnaire to decide which job applicants to advance to the interview stage. After one year, the HR director reviews outcomes and notes that, among all applicants the questionnaire flagged as promising, about 52% turned out to be strong employees by year-end supervisor ratings. A consultant is brought in and immediately asks a question that surprises the HR director: 'What percentage of the people you hired in the two years before the questionnaire existed turned out to be strong employees?' The answer is 39%. The consultant then argues that the single most important number — the one that tells the company whether the questionnaire is actually doing anything useful — is not the 52% but rather the gap between those two figures. The HR director had been planning to report the 52% to senior leadership as proof of success, but the consultant says doing so would overstate the questionnaire's contribution.",
      "question": "The index the consultant argues is most meaningful is best described by which of the following?",
      "options": {
        "A": "The positive predictive value of the questionnaire, which captures the accuracy of its favorable classifications",
        "B": "The incremental validity of the questionnaire, reflecting how much it improves on the existing success rate",
        "C": "The selection ratio, which quantifies what proportion of applicants the questionnaire recommends for hire",
        "D": "The sensitivity of the questionnaire, reflecting how well it captures all truly strong employees in the applicant pool"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The consultant is describing incremental validity: the degree to which a new selection tool improves predictive accuracy beyond the unaided baseline. The 39% represents the base rate (success without the questionnaire), and the 52% represents the positive hit rate (success among questionnaire-approved hires). Their difference (.52 − .39 = .13) is the incremental validity — the only figure that reveals whether the questionnaire adds anything beyond what hiring decisions would produce without it.",
        "A": "Incorrect. Positive predictive value is exactly the 52% figure the HR director planned to report — the proportion of 'positively screened' applicants who turned out to be strong. The consultant is specifically arguing that PPV alone overstates the questionnaire's value because it does not account for the pre-existing success rate. PPV and incremental validity are frequently confused, but PPV does not involve subtracting the base rate.",
        "C": "Incorrect. The selection ratio is the proportion of applicants who receive a passing recommendation from the screening tool. It directly affects the practical usefulness of any test and interacts with base rate and validity in Taylor-Russell tables, but it is not computed from success rates among hired employees, and it does not capture the improvement the questionnaire provides over baseline performance.",
        "D": "Incorrect. Sensitivity is a signal-detection concept equal to the proportion of all truly strong employees in the applicant pool who are correctly flagged by the questionnaire (true positives divided by all true positives plus false negatives). Calculating sensitivity would require knowing how many strong-performing candidates were rejected by the questionnaire — information not provided in this scenario. The consultant's argument is about improvement over baseline, which is a different calculation entirely."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-1-vignette-L1",
      "source_question_id": "1",
      "source_summary": "The reliability index is calculated by taking the square root of the reliability coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "reliability coefficient",
        "reliability index",
        "square root"
      ],
      "vignette": "A psychometrician is reviewing a newly developed cognitive assessment and reports that the reliability coefficient for the test is .81. A colleague asks how to compute the reliability index from this value. The psychometrician explains that the reliability index quantifies the correlation between observed scores and the true scores underlying them. She notes that the computation involves a straightforward transformation of the reliability coefficient.",
      "question": "Which operation correctly yields the reliability index from the reliability coefficient?",
      "options": {
        "A": "Square the reliability coefficient",
        "B": "Take the square root of the reliability coefficient",
        "C": "Subtract the reliability coefficient from 1.00",
        "D": "Divide the reliability coefficient by the number of items"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Squaring the reliability coefficient would produce the coefficient of determination (r²), which represents the proportion of variance in one variable explained by another. This is the reverse of the operation needed here and yields a smaller, not larger, value.",
        "B": "The reliability index is defined as the square root of the reliability coefficient, representing the correlation between observed scores and true scores. Taking the square root of .81 yields .90, which is the reliability index for this test.",
        "C": "Subtracting the reliability coefficient from 1.00 yields the proportion of error variance in the test (1 − rxx), not the reliability index. This value is used in error variance calculations but does not represent the correlation between observed and true scores.",
        "D": "Dividing the reliability coefficient by the number of items has no standard psychometric meaning and does not yield the reliability index. Item count is relevant to formulas like the Spearman-Brown prophecy formula, but that is a different application entirely."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-1-vignette-L2",
      "source_question_id": "1",
      "source_summary": "The reliability index is calculated by taking the square root of the reliability coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "reliability coefficient",
        "true scores"
      ],
      "vignette": "A researcher develops a 40-item personality inventory for use with adult outpatient populations and obtains a reliability coefficient of .64 using coefficient alpha. She wants to communicate to her research team the degree to which observed scores on the inventory reflect true scores, using a single index that expresses this as a correlation. One team member, a licensed clinician, is skeptical that the instrument is adequate for clinical decision-making given the moderate alpha value.",
      "question": "If the researcher wants to compute the index representing the correlation between observed and true scores, what value should she report?",
      "options": {
        "A": "0.41, obtained by squaring the reliability coefficient",
        "B": "0.36, obtained by subtracting the reliability coefficient from 1.00",
        "C": "0.80, obtained by taking the square root of the reliability coefficient",
        "D": "0.64, the reliability coefficient itself, which already expresses the correlation with true scores"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Squaring the reliability coefficient (.64² = .41) yields the coefficient of determination, which would describe the proportion of shared variance between two variables. This is not the reliability index; it is the inverse operation of what is needed.",
        "B": "Subtracting the reliability coefficient from 1.00 (1 − .64 = .36) gives the proportion of observed score variance attributable to error, sometimes called the error variance proportion. This is useful for understanding measurement error but does not represent the correlation between observed and true scores.",
        "C": "The reliability index is the square root of the reliability coefficient: √.64 = .80. This value expresses the correlation between observed scores and true scores, answering the researcher's question directly.",
        "D": "The reliability coefficient (.64) represents the proportion of observed score variance that is true score variance, not the correlation between observed and true scores. Those two quantities are related but distinct; the correlation (reliability index) is the square root of the coefficient, not the coefficient itself."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-1-vignette-L3",
      "source_question_id": "1",
      "source_summary": "The reliability index is calculated by taking the square root of the reliability coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "observed scores"
      ],
      "vignette": "During a psychometrics seminar, a graduate student presents findings from a newly constructed vocational interest survey. She reports that the instrument has strong internal consistency and that the proportion of observed score variance accounted for by true score variance is .49. Another student argues that this figure is too low and that the instrument is essentially measuring noise. The professor asks the class to identify the value that would express, as a correlation, the relationship between what the test measures and the hypothetical perfect version of that measurement. One student confidently provides an answer, but the professor notes the class frequently confuses this value with a related but conceptually distinct statistic.",
      "question": "Based on the information provided, what is the value the professor is asking the class to compute, and what does it represent?",
      "options": {
        "A": "0.49, representing the reliability coefficient — the proportion of observed score variance that is true score variance",
        "B": "0.51, representing the error variance proportion — the proportion of observed score variance attributable to unsystematic measurement error",
        "C": "0.70, representing the reliability index — the correlation between observed scores and true scores",
        "D": "0.24, representing the coefficient of determination — the proportion of variance in true scores explained by observed scores"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The value of .49 is the reliability coefficient (rxx), representing the proportion of observed score variance due to true score variance. While this is a real and important statistic, the professor is asking for the correlation between observed and true scores, which is the reliability index — a distinct value derived from this coefficient.",
        "B": "The error variance proportion (1 − rxx = 1 − .49 = .51) tells us what fraction of observed score variance is due to measurement error. This is a useful diagnostic value, but it does not express the correlation between observed scores and true scores that the professor requested.",
        "C": "The reliability index is the square root of the reliability coefficient: √.49 = .70. It represents the correlation between observed scores and true scores, expressing how closely the instrument's scores mirror the hypothetical true scores — exactly what the professor is asking for.",
        "D": "Squaring the reliability coefficient (.49² = .24) would generate a coefficient of determination, indicating the proportion of variance in one variable explained by another. This operation moves in the wrong direction; the reliability index requires taking the square root, not the square, of the reliability coefficient."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-1-vignette-L4",
      "source_question_id": "1",
      "source_summary": "The reliability index is calculated by taking the square root of the reliability coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "correlation"
      ],
      "vignette": "A test developer has constructed a neuropsychological screening battery and calculates that systematic variance in the instrument accounts for 36% of the total variance in obtained scores. She wants to derive a single numerical expression of how closely the battery's scores track the latent quantity the test is designed to capture — in other words, the degree to which the test scores and the underlying construct share a linear relationship. A consultant reviewing the report notes that most practitioners mistake a related squared quantity for this value and use it incorrectly when estimating how much information the test conveys about a person's standing on the underlying trait.",
      "question": "Which of the following correctly identifies the value the test developer is seeking and how it should be derived?",
      "options": {
        "A": "0.36, the reliability coefficient, obtained directly from the proportion of true score variance in observed scores",
        "B": "0.60, the reliability index, obtained by taking the square root of the reliability coefficient",
        "C": "0.64, the error variance proportion, obtained by subtracting the reliability coefficient from 1.00",
        "D": "0.13, a coefficient of determination, obtained by squaring the reliability coefficient"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The value .36 is the reliability coefficient, expressing the proportion of observed score variance that is true score variance. However, it does not directly express the linear (correlational) relationship between observed scores and true scores — that is the reliability index. The consultant's warning specifically targets the tendency to confuse this squared value with the correlation itself.",
        "B": "If 36% of observed score variance is systematic (true score variance), then the reliability coefficient rxx = .36, and the reliability index = √.36 = .60. This value expresses the correlation between observed scores and true scores — the linear relationship between the test's scores and the underlying latent construct the test developer seeks.",
        "C": "Subtracting the reliability coefficient from 1.00 (1 − .36 = .64) yields the proportion of observed score variance attributable to error. This value characterizes measurement imprecision rather than the degree of alignment between observed and true scores, and therefore does not answer the test developer's question.",
        "D": "Squaring the reliability coefficient (.36² = .13) produces a coefficient of determination, which would be used to describe shared variance between two separate variables in regression. This further reduces the reliability information rather than expressing the correlation between observed and true scores."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-1-vignette-L5",
      "source_question_id": "1",
      "source_summary": "The reliability index is calculated by taking the square root of the reliability coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A measurement specialist is reviewing documentation for a new employee selection tool. She notes that when all sources of unwanted variation in scores are pooled together, they account for exactly 19% of the spread in scores across test-takers. A colleague argues that this figure alone is sufficient to tell practitioners how accurately the tool tracks what it is supposed to track, and proposes reporting it directly to hiring managers as the key quality indicator. The specialist disagrees, explaining that the figure the colleague wants to report is a transformed version of the true quality indicator, and that the actual index expressing how closely scores mirror what is being measured requires a different arithmetic step applied to the complement of the 19% figure. She adds that confusing these two numbers leads practitioners to underestimate the tool's quality.",
      "question": "What arithmetic step must be applied to the complement of the 19% figure to obtain the index the specialist considers the true quality indicator?",
      "options": {
        "A": "Subtract the complement from 1.00 to obtain the proportion of variance accounted for by error",
        "B": "Multiply the complement by itself to obtain the proportion of variance in scores explained by the underlying attribute",
        "C": "Take the square root of the complement to obtain the correlation between obtained scores and the underlying attribute being measured",
        "D": "Divide the complement by the number of test components to obtain an average inter-component agreement index"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Subtracting the complement (.81) from 1.00 would return the original error proportion (.19), not a new or more informative statistic. This operation moves backward rather than yielding the quality indicator the specialist has in mind.",
        "B": "Multiplying the complement by itself (.81 × .81 = .6561) would square the reliability coefficient, producing an even smaller value that further understates the instrument's quality. Squaring the reliability coefficient yields the coefficient of determination, not the reliability index, and the specialist explicitly states the colleague's error involves using a transformed — not a further-transformed — version of the correct figure.",
        "C": "If 19% of score variance is error variance, then 81% is true score variance, making the reliability coefficient .81. The reliability index — which expresses the correlation between obtained scores and the latent attribute being measured — is the square root of this: √.81 = .90. The specialist's point is that practitioners who report .81 instead of .90 are using the squared version of the actual correlation, thereby underestimating how closely the tool tracks the underlying attribute.",
        "D": "Dividing the reliability coefficient by the number of test components has no standard psychometric definition and does not yield the reliability index. Average inter-component agreement is more closely related to split-half or inter-rater reliability estimation procedures, not to the transformation between the reliability coefficient and the reliability index."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-146-vignette-L1",
      "source_question_id": "146",
      "source_summary": "When the prevalence of a disorder increases, the positive predictive value increases and the negative predictive value decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "prevalence",
        "positive predictive value",
        "negative predictive value"
      ],
      "vignette": "A researcher is using a standardized screening instrument to detect a rare anxiety disorder in two separate clinics. In Clinic A, the disorder has a low prevalence of 5%, while in Clinic B the prevalence is 60%. The test's sensitivity and specificity remain constant across both settings. The researcher notices that positive predictive value is substantially higher in Clinic B, while negative predictive value is higher in Clinic A. She asks her colleague to explain how changes in prevalence affect these criterion-related validity indices.",
      "question": "Which of the following best describes the relationship between disorder prevalence and predictive values?",
      "options": {
        "A": "As prevalence increases, positive predictive value increases and negative predictive value decreases.",
        "B": "As prevalence increases, both positive predictive value and negative predictive value increase proportionally.",
        "C": "As prevalence increases, sensitivity increases and specificity decreases, which drives changes in predictive values.",
        "D": "As prevalence increases, negative predictive value increases and positive predictive value decreases."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. When prevalence rises, true positives become more common relative to false positives, raising positive predictive value. Simultaneously, true negatives become rarer relative to false negatives, lowering negative predictive value. This is the foundational relationship between base rate and predictive values in criterion-related validity.",
        "B": "This is incorrect. Predictive values do not move in the same direction together when prevalence shifts. Positive predictive value and negative predictive value move in opposite directions as prevalence changes, because increasing the proportion of true cases raises the likelihood that a positive result is accurate while reducing the likelihood that a negative result is accurate.",
        "C": "This is incorrect. Sensitivity and specificity are properties of the test itself and are not affected by prevalence — they remain constant even when the base rate of the disorder changes. It is the predictive values, not sensitivity and specificity, that vary as a function of prevalence.",
        "D": "This is incorrect. This reverses the actual relationship. A higher prevalence means more true cases exist in the population, which increases the probability that a positive test result reflects a true case (raising positive predictive value) and decreases the probability that a negative result rules out the disorder (lowering negative predictive value)."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-146-vignette-L2",
      "source_question_id": "146",
      "source_summary": "When the prevalence of a disorder increases, the positive predictive value increases and the negative predictive value decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "base rate",
        "predictive value"
      ],
      "vignette": "A neuropsychologist is validating a brief cognitive screening tool for early dementia detection. She administers the test to patients at a general community memory clinic, where the base rate of dementia is approximately 20%, and to residents of a specialized memory care unit where the base rate is approximately 75%. The neuropsychologist is also aware that the patient population at the community clinic tends to be younger and more educated, which she notes could affect test performance. Despite these demographic differences, the test's sensitivity and specificity remain stable across settings, but the proportion of positive test results that correctly identify true dementia cases differs dramatically between the two sites.",
      "question": "Which psychometric phenomenon most directly explains why the proportion of correct positive identifications is so much higher in the memory care unit than in the community clinic?",
      "options": {
        "A": "Restriction of range, which reduces the test's ability to discriminate between cases and non-cases in the specialized setting.",
        "B": "Concurrent validity, which is higher in the specialized setting because the criterion measure is administered at the same time as the screening tool.",
        "C": "Positive predictive value increasing as a function of higher base rate in the specialized setting.",
        "D": "Sensitivity increasing in the specialized setting due to the higher concentration of true cases, improving detection accuracy."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. Positive predictive value — the probability that a positive test result reflects a true case — rises as the base rate (prevalence) of the disorder increases. In the memory care unit, the higher proportion of true dementia cases means that a positive result is far more likely to be a genuine case, explaining the dramatic difference in correct positive identifications across settings.",
        "A": "This is incorrect. Restriction of range refers to a situation where variance in a variable is artificially limited, which can attenuate validity coefficients. While the specialized setting might have a narrower range of cognitive scores, restriction of range would be expected to reduce discriminative ability, not increase the proportion of correct positive identifications.",
        "B": "This is incorrect. Concurrent validity refers to how well a test correlates with a criterion measure administered at approximately the same time and is a property of the instrument itself, not a function of the base rate of the disorder in the testing population. It does not directly explain why a higher proportion of positive results are correct in one setting.",
        "D": "This is incorrect. Sensitivity is defined as the proportion of true cases that the test correctly identifies, and it is a property of the test that remains constant regardless of prevalence. The vignette explicitly states that sensitivity remains stable across settings, so a change in sensitivity cannot explain the observed difference in correct positive identifications."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-146-vignette-L3",
      "source_question_id": "146",
      "source_summary": "When the prevalence of a disorder increases, the positive predictive value increases and the negative predictive value decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "prevalence"
      ],
      "vignette": "A health psychologist deploys a validated depression screening questionnaire in two different primary care settings. In the first setting, a general family practice with a largely healthy patient population, only 1 in 10 patients meets criteria for depression. In the second setting, an oncology clinic where the prevalence of depression is approximately 50%, the same questionnaire is used with the same cutoff score. A clinic administrator reviews the data and notes that when the questionnaire flags a patient as depressed in the oncology clinic, that flag is correct far more often than in the family practice. However, she also observes that patients who screen negative in the oncology clinic are more likely to actually have depression than patients who screen negative in the family practice.",
      "question": "What psychometric principle best explains both observations simultaneously — the more accurate positive flags in the oncology clinic and the less reliable negative screens in that same setting?",
      "options": {
        "A": "The criterion-related validity of the questionnaire is higher in the oncology setting because the external criterion (clinical diagnosis) is more reliably measured in a specialized clinical environment.",
        "B": "The questionnaire's sensitivity is higher in the oncology clinic because the higher concentration of cases gives the test more opportunity to detect true positives.",
        "C": "Changes in the base rate of depression across settings alter positive predictive value and negative predictive value in opposite directions, independent of the test's sensitivity and specificity.",
        "D": "The cutoff score selected for the questionnaire has differential validity across settings, inflating false positives in the family practice and false negatives in the oncology clinic."
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "This is correct. When prevalence increases, positive predictive value (the accuracy of a positive result) increases because more true cases exist relative to false positives. Simultaneously, negative predictive value (the accuracy of a negative result) decreases because more true cases are available to generate false negatives. This simultaneous, opposing shift in both predictive values — with sensitivity and specificity held constant — is the defining feature of the base rate effect on predictive values.",
        "A": "This is incorrect. Criterion-related validity is a fixed psychometric property of the test itself and does not change simply because the clinical setting is more specialized. While the criterion measure might be more expertly applied in a specialty clinic, this does not directly account for the simultaneous pattern of more accurate positives and less accurate negatives described in the vignette.",
        "B": "This is incorrect. Sensitivity is a stable property of the test defined as true positives divided by all actual cases. It does not rise simply because more cases are present — sensitivity is calculated within the population of true cases only, making it independent of prevalence. Confusing sensitivity with positive predictive value is a common error that this distractor is designed to probe.",
        "D": "This is incorrect. Differential validity refers to the phenomenon where a test predicts outcomes differently for distinct subgroups (e.g., by race or gender) and relates to systematic bias in the test's construct. The scenario does not describe a subgroup bias problem; rather, it describes the mathematically expected effect of different base rates on predictive values when the test cutoff and psychometric properties remain unchanged."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-146-vignette-L4",
      "source_question_id": "146",
      "source_summary": "When the prevalence of a disorder increases, the positive predictive value increases and the negative predictive value decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "false negatives"
      ],
      "vignette": "A psychometrician is reviewing the performance of a newly validated PTSD screening instrument across three VA outpatient clinics. The test was developed and normed at a general outpatient clinic where roughly 15% of patients carry a PTSD diagnosis. When the instrument is deployed at a specialized combat veteran trauma unit where the disorder is present in nearly 70% of patients, clinicians report that they feel confident acting on positive screens but are increasingly frustrated that negative screens still reveal undetected cases during follow-up clinical interviews. Notably, the instrument's technical manual confirms that the test's ability to correctly identify those without PTSD has not changed across deployments. The psychometrician suspects that the instrument's usefulness differs markedly from its development context, despite stable test characteristics.",
      "question": "Which explanation most precisely accounts for the clinicians' observation that negative screens are misleading in the high-prevalence trauma unit, even though the test's ability to correctly identify non-cases has not changed?",
      "options": {
        "A": "The instrument has lower construct validity in the trauma unit population because PTSD manifests differently in combat veterans than in the civilian population on which the test was normed.",
        "B": "The high prevalence in the trauma unit increases the proportion of false negatives among all negative screens, reducing negative predictive value, even though specificity remains unchanged.",
        "C": "The instrument demonstrates predictive validity shrinkage when moved from the development sample to the specialized setting, because the trauma unit sample differs systematically from the normative group.",
        "D": "The cutoff score optimized for the 15% base rate overestimates the criterion threshold needed in a 70% base rate environment, artificially lowering the instrument's sensitivity in the new setting."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Specificity — the test's ability to correctly identify non-cases — is confirmed as unchanged, which rules out any change in the test itself. However, when prevalence is high, there are far more true cases in the screened population, and even a small fixed false-negative rate generates a large number of missed cases relative to the pool of negative screens. This drives down negative predictive value without any change in sensitivity or specificity. The clinicians are experiencing the practical consequence of unchanged test characteristics applied to a very different base rate.",
        "A": "This is incorrect. Construct validity concerns whether the test measures the intended psychological construct accurately. While it is plausible that PTSD presentation differs in combat veterans, the vignette explicitly states that test characteristics remain stable, which contradicts a construct validity failure. Construct validity problems would manifest as changes in sensitivity, specificity, or factor structure — not a base-rate-driven predictive value shift.",
        "C": "This is incorrect. Shrinkage refers to the reduction in a test's predictive validity coefficient when it is applied to a new sample different from the development sample, typically due to overfitting during test construction. Shrinkage affects the magnitude of validity coefficients and is particularly relevant for regression-based prediction, not for the binary predictive value indices described in the scenario.",
        "D": "This is incorrect. Sensitivity, like specificity, is a fixed property of the test at a given cutoff score and does not change as a function of prevalence. The vignette states that test characteristics are stable, and sensitivity is determined by the cutoff threshold relative to the score distributions of true cases — not by the proportion of cases in the population. Changing the base rate does not alter sensitivity."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-146-vignette-L5",
      "source_question_id": "146",
      "source_summary": "When the prevalence of a disorder increases, the positive predictive value increases and the negative predictive value decreases.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A large urban hospital system introduces a brief paper-and-pencil questionnaire to flag patients who may need immediate mental health intervention. The questionnaire was originally developed and tested in a community wellness center where most visitors were generally healthy. When the same questionnaire, with no modifications to its scoring or cutoffs, is rolled out in the hospital's psychiatric emergency department, staff notice two puzzling patterns: first, when the questionnaire says a patient needs intervention, it is almost always right; second, when the questionnaire says a patient does not need intervention, clinicians frequently disagree after conducting their own assessments and find that the patient actually does need help. A research consultant is brought in and confirms that the questionnaire's ability to correctly clear patients who genuinely do not need intervention is identical across both settings.",
      "question": "Which of the following best explains the two simultaneous patterns staff have observed — the highly accurate positive flags and the unreliable negative clearances — in the emergency psychiatric setting?",
      "options": {
        "A": "The questionnaire was developed in a low-acuity setting, so its items do not adequately capture the full range and severity of symptoms seen in emergency psychiatric patients, reducing its overall diagnostic accuracy.",
        "B": "Because a much larger proportion of patients in the emergency psychiatric setting actually need intervention, accurate positive identifications become more common while accurate negative clearances become less reliable, even when the instrument's detection accuracy for non-cases remains unchanged.",
        "C": "The staff in the emergency psychiatric department are applying more rigorous clinical standards during their follow-up assessments, making the questionnaire's negative results appear less accurate by comparison rather than reflecting a true change in test performance.",
        "D": "Moving the questionnaire from a low-acuity setting to a high-acuity setting introduces systematic response bias, as patients in psychiatric emergency departments are more likely to underreport symptoms on self-report measures, creating an artificial increase in negative results."
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. The key detail is that the questionnaire's ability to correctly clear non-cases is confirmed to be identical across settings — meaning specificity is unchanged. The two observed patterns (more reliable positives, less reliable negatives) are the precise expected outcome when a fixed-accuracy instrument is deployed in a setting where the proportion of people who actually need intervention is much higher. More true cases in the population means more true positives relative to false positives (raising positive predictive value) and more false negatives relative to true negatives (lowering negative predictive value) — all with no change in the test's intrinsic accuracy properties.",
        "A": "This is incorrect. This explanation invokes a content-related or construct-related validity problem — that the items do not adequately represent the construct in the new population. While plausible on its face, this would predict that the questionnaire's ability to clear non-cases would also change, which the vignette explicitly rules out. A content validity failure would affect accuracy broadly, not produce the specific asymmetric pattern described.",
        "C": "This is incorrect. This explanation attributes the pattern to differences in clinical judgment standards rather than test performance, which is a superficially reasonable alternative. However, it does not account for the simultaneous finding that positive flags have become more accurate in the same setting — if stricter clinical standards explained the discrepancy, one would expect the questionnaire's positive results to also appear less accurate by comparison, not more. The asymmetry is inconsistent with this explanation.",
        "D": "This is incorrect. Response bias from symptom underreporting in emergency psychiatric settings would plausibly increase false negatives, which partially fits the pattern of unreliable negative clearances. However, underreporting would not simultaneously make positive flags more accurate — it would likely reduce positive flags and therefore lower positive predictive value. The combination of both patterns described in the vignette is not explained by response bias, which makes this answer insufficient despite its surface plausibility."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-073-vignette-L1",
      "source_question_id": "073",
      "source_summary": "Cohen's kappa coefficient is the appropriate technique for determining the inter-rater reliability of the ratings made by the manager and assistant manager when they categorized employees as being ready or not ready for promotion.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "inter-rater reliability",
        "categorical",
        "chance agreement"
      ],
      "vignette": "A human resources researcher is evaluating the consistency between two raters who independently classified each of 80 employees into one of two categories: 'ready for promotion' or 'not ready for promotion.' The researcher needs a statistical index of inter-rater reliability that corrects for the level of chance agreement that would be expected given the base rates of each category. Because the data are categorical rather than continuous, a correlation coefficient would not be appropriate. The researcher consults a psychometrics textbook to identify the correct technique.",
      "question": "Which statistical technique is most appropriate for quantifying the agreement between the two raters in this scenario?",
      "options": {
        "A": "Pearson product-moment correlation",
        "B": "Cohen's kappa coefficient",
        "C": "Cronbach's coefficient alpha",
        "D": "Intraclass correlation coefficient"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The Pearson product-moment correlation is designed for two continuous variables and measures the linear relationship between them. It does not correct for chance agreement and is inappropriate for nominal categorical data such as a binary 'ready/not ready' classification.",
        "B": "Cohen's kappa coefficient is the correct choice. It is specifically designed to assess inter-rater reliability for categorical (nominal) data and adjusts observed agreement downward by the proportion of agreement expected by chance alone, making it the gold-standard index in this context.",
        "C": "Cronbach's coefficient alpha is a measure of internal consistency reliability, reflecting how well a set of items on a single instrument intercorrelate. It assesses consistency across items within one test, not agreement between two independent raters classifying observations.",
        "D": "The intraclass correlation coefficient (ICC) is used for inter-rater reliability when ratings are made on a continuous or ordinal scale, allowing for the decomposition of variance between raters and targets. Because the current data are nominal (two discrete categories), the ICC is not the appropriate choice."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-073-vignette-L2",
      "source_question_id": "073",
      "source_summary": "Cohen's kappa coefficient is the appropriate technique for determining the inter-rater reliability of the ratings made by the manager and assistant manager when they categorized employees as being ready or not ready for promotion.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "inter-rater reliability",
        "nominal"
      ],
      "vignette": "A large retail company recently overhauled its promotion process after concerns about inconsistency. Two senior managers independently reviewed performance files for 120 employees and placed each employee into one of two groups: those deemed promotion-ready and those not. The company employs many part-time workers, which created an uneven base rate, with roughly 75% of employees classified as not ready by both raters. A measurement consultant is asked to select a reliability index that accounts for this skewed base rate when estimating the true level of agreement beyond chance. The consultant notes that because the classification is nominal, techniques designed for continuous scores are not suitable.",
      "question": "Which reliability technique should the consultant recommend?",
      "options": {
        "A": "Spearman-Brown split-half reliability",
        "B": "Test-retest reliability coefficient",
        "C": "Percent agreement index",
        "D": "Cohen's kappa coefficient"
      },
      "correct_answer": "D",
      "option_explanations": {
        "A": "The Spearman-Brown formula is used to estimate how reliability changes when test length is altered, typically applied to split-half reliability calculations for a single instrument. It has no application to quantifying the agreement between two independent raters classifying employees.",
        "B": "Test-retest reliability measures the stability of scores from the same instrument administered on two separate occasions. It addresses temporal consistency of a test, not the consistency between two different raters making independent classifications at the same time.",
        "C": "A simple percent agreement index counts the proportion of cases on which two raters agree but does not correct for the proportion of agreement expected by chance. With a skewed base rate (75% not ready), two raters guessing randomly would still agree frequently, so percent agreement would artificially inflate the apparent reliability. This is precisely why Cohen's kappa is preferred.",
        "D": "Cohen's kappa coefficient is correct. It is designed for nominal categorical data and explicitly corrects for chance agreement based on the marginal frequencies of each category. Its correction is especially valuable when base rates are unequal, as in this scenario."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-073-vignette-L3",
      "source_question_id": "073",
      "source_summary": "Cohen's kappa coefficient is the appropriate technique for determining the inter-rater reliability of the ratings made by the manager and assistant manager when they categorized employees as being ready or not ready for promotion.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "categorical"
      ],
      "vignette": "A workforce development team at a regional bank wants to standardize its annual talent review. The store manager and the assistant manager each independently evaluate every employee and record a single judgment: the employee either meets the threshold for advancement or does not. After completing this process for 95 employees, the team notices that the two evaluators agreed on the vast majority of cases, but a statistician cautions that simple counts of agreement can be deceptive when one outcome is far more common than the other. The statistician adds that the chosen index must be sensitive to the categorical structure of the data and must not assume an underlying continuous distribution. The team asks the statistician to name the most defensible technique for quantifying reliability here.",
      "question": "Which technique is the statistician most likely recommending?",
      "options": {
        "A": "Phi coefficient",
        "B": "Cohen's kappa coefficient",
        "C": "Kendall's tau",
        "D": "Intraclass correlation coefficient"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "The phi coefficient measures the association between two dichotomous variables and is mathematically equivalent to a Pearson correlation for 2×2 tables. While it is designed for categorical data, it is an index of association rather than an index of agreement corrected for chance, making it less appropriate than Cohen's kappa when the goal is reliability estimation between raters.",
        "B": "Cohen's kappa is correct. It directly addresses both concerns raised by the statistician: it is designed for categorical (nominal) data without assuming an underlying continuous scale, and it explicitly corrects for the inflated agreement that occurs when one category dominates, producing a conservative and interpretable reliability estimate.",
        "C": "Kendall's tau is a nonparametric measure of rank-order association between two ordinal variables. It addresses ordinal data relationships between ordered ranks, not the agreement between two raters on a binary categorical classification, and does not correct for chance agreement.",
        "D": "The intraclass correlation coefficient partitions total variance into between-targets and between-raters components and is well-suited for continuous or interval-level ratings. Because the bank's outcome is strictly binary and categorical, the ICC's assumption of a measurable interval scale is violated, making Cohen's kappa the better choice."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-073-vignette-L4",
      "source_question_id": "073",
      "source_summary": "Cohen's kappa coefficient is the appropriate technique for determining the inter-rater reliability of the ratings made by the manager and assistant manager when they categorized employees as being ready or not ready for promotion.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "base rate"
      ],
      "vignette": "Two division heads at a financial services firm independently reviewed personnel dossiers for 110 employees and each assigned one of two labels indicating whether the employee was or was not a candidate for an accelerated leadership track. On first review, the psychometrician hired to evaluate the process noted that the two reviewers appeared to agree roughly 88% of the time, which seemed impressive. However, the psychometrician pointed out that because the base rate of employees flagged as leadership candidates was very low—only about 12%—two reviewers assigning labels at random would still agree a large proportion of the time simply by defaulting to the more common label. The psychometrician argued that the apparent 88% agreement figure was therefore a misleading overestimate of true rater consistency and that a different index was needed.",
      "question": "Which index is the psychometrician most likely advocating for in this scenario?",
      "options": {
        "A": "Intraclass correlation coefficient (two-way mixed model)",
        "B": "Scott's pi coefficient",
        "C": "Cohen's kappa coefficient",
        "D": "Tetrachoric correlation coefficient"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "The two-way mixed model ICC is a well-regarded index for inter-rater reliability when ratings are made on a continuous or multi-point ordinal scale and each target is rated by the same specific raters. Because the current data are binary and categorical, the ICC's continuous-scale assumptions are not met, making it less appropriate than Cohen's kappa.",
        "B": "Scott's pi is also a chance-corrected agreement coefficient for categorical data and is very similar to Cohen's kappa. However, Scott's pi uses the average of the two raters' marginal proportions to estimate expected chance agreement, rather than each rater's individual marginals as kappa does. While pi is a legitimate alternative, Cohen's kappa is the more widely recognized and cited standard in psychometric practice for this application, making it the best answer here.",
        "C": "Cohen's kappa is the correct answer. It directly solves the problem identified by the psychometrician: by subtracting the proportion of agreement expected by chance (derived from each rater's individual marginal frequencies) from the observed agreement, and normalizing by the maximum possible improvement over chance, kappa yields a conservative estimate of true inter-rater reliability uncontaminated by base rate asymmetry.",
        "D": "The tetrachoric correlation coefficient is designed to estimate the Pearson correlation between two underlying continuous normal distributions that have each been artificially dichotomized. It is an index of latent association, not of observed categorical agreement between raters, and makes distributional assumptions that are not warranted when the goal is simply to measure rater consistency on a nominal binary outcome."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-073-vignette-L5",
      "source_question_id": "073",
      "source_summary": "Cohen's kappa coefficient is the appropriate technique for determining the inter-rater reliability of the ratings made by the manager and assistant manager when they categorized employees as being ready or not ready for promotion.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A large organization recently asked two senior staff members to independently sort every employee in a particular division into one of two bins based on a set of written criteria, with no numeric scores involved. When results were compiled, most employees ended up in the same bin by both sorters, but a measurement consultant reviewing the data warned leadership not to celebrate yet. She explained that because one bin was much larger than the other, two people sorting randomly would still place most employees in the same bin most of the time just by following the path of least resistance. She noted that the figure everyone had been looking at told almost nothing about whether the two sorters were actually applying the criteria consistently, and that a different number—one specifically designed to remove the contribution of this mechanical overlap—was necessary before any conclusions could be drawn about consistency between the two sorters.",
      "question": "Which statistical approach is the consultant most likely recommending to properly evaluate consistency between the two sorters?",
      "options": {
        "A": "Computing a correlation between the two sorters' outcomes after assigning numeric codes to each bin",
        "B": "Computing a chance-corrected agreement coefficient based on the two sorters' individual sorting frequencies",
        "C": "Calculating the proportion of variance in sorting decisions shared across all employees",
        "D": "Averaging the two sorters' individual agreement rates with a criterion standard"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Assigning numeric codes (e.g., 0 and 1) to the two bins and computing a correlation treats the categorical outcome as if it were continuous and produces a measure of linear association. A Pearson correlation on dummy-coded binary outcomes does not correct for the inflated agreement caused by unequal bin sizes, so it would replicate the very problem the consultant is flagging rather than solving it.",
        "B": "This is the correct answer, describing Cohen's kappa coefficient. Kappa uses each sorter's own marginal frequencies to estimate how often two people would agree by chance given their individual tendencies, then subtracts that chance baseline from observed agreement and scales the result. This is precisely the adjustment the consultant describes as necessary to remove the 'mechanical overlap' caused by one bin being much larger than the other.",
        "C": "Calculating shared variance across all employees' sorting decisions describes a variance-partitioning approach such as an intraclass correlation or a similar decomposition. This family of techniques is most appropriate when ratings are made on a continuous or multi-point scale; applying it to two discrete categories produced by a binary sorting process does not yield a meaningful or directly interpretable reliability estimate for nominal data.",
        "D": "Averaging each sorter's agreement with a criterion standard would be a validity assessment—measuring how well each sorter matches an established correct answer—rather than a reliability assessment examining the consistency between the two sorters with each other. The consultant's concern is about inter-sorter consistency, not accuracy relative to an external standard."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-218-vignette-L1",
      "source_question_id": "218",
      "source_summary": "When a predictor has a criterion-related validity coefficient of .80, this means that 64% of variability in scores on the criterion is explained by variability in scores on the predictor.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "criterion-related validity",
        "validity coefficient",
        "variance explained"
      ],
      "vignette": "A psychometrician develops a new cognitive assessment and reports a criterion-related validity coefficient of .80 between the predictor scores and a measure of academic achievement used as the criterion. A colleague asks what percentage of variance in the criterion is explained by the predictor. The psychometrician notes that squaring the validity coefficient yields the coefficient of determination, which quantifies the variance explained. This value is then used to interpret the practical utility of the assessment.",
      "question": "Based on a criterion-related validity coefficient of .80, what percentage of variance in the criterion is explained by the predictor?",
      "options": {
        "A": "80%, because the validity coefficient directly represents the proportion of variance explained",
        "B": "64%, because squaring the validity coefficient (.80² = .64) yields the proportion of variance in the criterion explained by the predictor",
        "C": "40%, because the variance explained equals half the validity coefficient when the coefficient exceeds .50",
        "D": "89%, because the validity coefficient must be adjusted upward using the Spearman-Brown formula to yield true variance explained"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. The validity coefficient of .80 represents the correlation between predictor and criterion, not the proportion of variance explained. Variance explained is obtained by squaring the coefficient, yielding .64 or 64%, not 80%.",
        "B": "Correct. The coefficient of determination, obtained by squaring the validity coefficient (.80² = .64), represents the proportion of variance in the criterion that is accounted for by the predictor. This is a fundamental psychometric interpretation of any correlation-based validity index.",
        "C": "Incorrect. There is no psychometric rule that variance explained equals half of a validity coefficient above .50. This is a fabricated formula with no basis in measurement theory.",
        "D": "Incorrect. The Spearman-Brown formula is used to estimate the reliability of a lengthened or split-half test, not to adjust validity coefficients for variance explained. Applying it here conflates reliability estimation with validity interpretation."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-218-vignette-L2",
      "source_question_id": "218",
      "source_summary": "When a predictor has a criterion-related validity coefficient of .80, this means that 64% of variability in scores on the criterion is explained by variability in scores on the predictor.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "predictive validity",
        "criterion"
      ],
      "vignette": "A large urban school district pilots a new kindergarten readiness screener and administers it to 300 children in the fall. The following spring, end-of-year reading achievement scores serve as the criterion. The predictive validity coefficient between the screener and the reading achievement measure is reported as .80. District administrators, who are unfamiliar with psychometric statistics, want to know how much of the variability in children's reading achievement can be attributed to their screener performance. One administrator incorrectly assumes the answer is simply 80%.",
      "question": "How should the psychometrician correctly explain the proportion of variance in reading achievement explained by the screener?",
      "options": {
        "A": "The screener explains 80% of the variance in reading achievement, because the predictive validity coefficient directly reflects the proportion of shared variance between the two measures",
        "B": "The screener explains approximately 89% of the variance in reading achievement, because the validity coefficient must be corrected for attenuation before squaring",
        "C": "The screener explains 64% of the variance in reading achievement, because the coefficient of determination is obtained by squaring the validity coefficient (.80² = .64)",
        "D": "The screener explains 40% of the variance in reading achievement, because only the unique variance not shared with other predictors is interpretable in a single-predictor model"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. The administrator's assumption that 80% of variance is explained directly confuses the validity coefficient (a correlation) with the coefficient of determination. The correlation of .80 must be squared to yield the proportion of variance explained.",
        "B": "Incorrect. Correction for attenuation adjusts a validity coefficient upward to estimate what it would be if both measures were perfectly reliable, but this correction is not applied before interpreting variance explained in practice. It does not produce a figure of 89% in this context.",
        "C": "Correct. Squaring the predictive validity coefficient (.80² = .64) yields the coefficient of determination, meaning 64% of the variance in reading achievement scores is explained by the screener. This is the standard psychometric interpretation of a squared correlation.",
        "D": "Incorrect. The concept of unique variance applies in multiple regression contexts when partitioning variance among several predictors, not in interpreting a single bivariate validity coefficient. With one predictor, the full coefficient of determination (.64) applies."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-218-vignette-L3",
      "source_question_id": "218",
      "source_summary": "When a predictor has a criterion-related validity coefficient of .80, this means that 64% of variability in scores on the criterion is explained by variability in scores on the predictor.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "coefficient of determination"
      ],
      "vignette": "A personnel psychologist reports that a structured interview protocol correlates .80 with supervisor ratings of job performance collected six months after hiring. The organization is impressed, noting that the interview also has high internal consistency and was developed using a large, diverse normative sample. However, a senior consultant cautions that knowing the correlation alone does not tell the organization how much of the differences in employees' actual job performance the interview can account for. The consultant computes an additional index that more directly quantifies this explanatory relationship.",
      "question": "What value does the consultant's additional index yield, and what does it represent?",
      "options": {
        "A": ".80, representing the degree to which the interview scores and performance ratings share a linear relationship, which is itself the most direct measure of explanatory power",
        "B": ".64, representing the proportion of variance in job performance ratings that is explained by variability in interview scores, obtained via the coefficient of determination",
        "C": ".89, representing the corrected validity estimate after adjusting for the unreliability of supervisor ratings as the criterion measure",
        "D": ".36, representing the proportion of unexplained variance in job performance, which is the primary index consultants use to evaluate practical utility of selection tools"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. While .80 does describe the linear relationship between interview scores and performance ratings, the correlation coefficient itself does not directly quantify the proportion of variance explained. The consultant is computing a separate, derived index — the coefficient of determination — which requires squaring the correlation.",
        "B": "Correct. Squaring the validity coefficient (.80² = .64) yields the coefficient of determination, which tells the organization that 64% of the variability in job performance ratings is statistically accounted for by variability in interview scores. This is the index the consultant computes to answer the organization's practical question.",
        "C": "Incorrect. Correcting for attenuation due to criterion unreliability would yield a higher estimate of the 'true' validity, but it does not produce the value .89 in a straightforward way, and this correction is not described as what the consultant computes. The scenario specifies an index that quantifies the explanatory relationship, which is the coefficient of determination.",
        "D": "Incorrect. The value .36 does represent the proportion of unexplained variance (1 − .64), which is related to the concept but is not the index the consultant computes. Consultants evaluating predictive utility focus on the proportion of variance explained (.64), not the unexplained residual, as the primary interpretive value."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-218-vignette-L4",
      "source_question_id": "218",
      "source_summary": "When a predictor has a criterion-related validity coefficient of .80, this means that 64% of variability in scores on the criterion is explained by variability in scores on the predictor.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "shared variance"
      ],
      "vignette": "A test developer publishes a new measure of executive functioning and reports that it has strong psychometric properties, including high internal consistency and a large normative database. In a validation study, the executive functioning measure is administered to 250 adults, and scores are correlated with neuropsychological ratings of daily adaptive functioning obtained three months later. The correlation between the two measures is reported as .80. A reviewer of the study notes that this figure, while impressive, overstates the practical interpretive meaning unless one additionally considers how much shared variance the two measures actually have. The reviewer specifically critiques the developer for not reporting this supplementary index alongside the correlation.",
      "question": "What is the reviewer most likely pointing out needs to be reported, and what is the value of that index?",
      "options": {
        "A": "The standard error of estimate, which equals the criterion's standard deviation multiplied by the square root of 1 minus the squared correlation, providing an estimate of prediction error around .60 SD units",
        "B": "The corrected validity coefficient, which adjusts the observed .80 upward for restriction of range and measurement error, yielding an estimate closer to .90",
        "C": "The coefficient of determination, which equals .64 and directly quantifies the proportion of variance in adaptive functioning ratings explained by variability in executive functioning scores",
        "D": "The incremental validity index, which compares the .80 correlation to the base rate of adaptive functioning in the normative sample to determine whether the test adds practical value beyond chance prediction"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. The standard error of estimate is a useful index of prediction accuracy and is related to the coefficient of determination, but it quantifies prediction error in score units rather than directly expressing the proportion of explained variance. The reviewer's critique — that the reported correlation overstates interpretive meaning without additional context — is best addressed by reporting the proportion of shared variance, not the error of estimate.",
        "B": "Incorrect. Correcting for restriction of range or attenuation would increase the validity coefficient, moving it above .80, but the reviewer's concern is that the .80 is being interpreted as if it directly represents explained variance. Correcting upward addresses a different problem (underestimation of true validity) and does not resolve the concern about variance-explained interpretation.",
        "C": "Correct. The reviewer is pointing out that the correlation of .80 does not directly communicate how much of the criterion variance is accounted for by the predictor. The coefficient of determination (.80² = .64) provides that information, indicating that 64% of the variance in adaptive functioning ratings is explained by executive functioning scores. This is the supplementary index the reviewer expects to see reported.",
        "D": "Incorrect. Incremental validity refers to the improvement in prediction offered by a test over and above what is already predicted by other available information (e.g., base rates or existing measures). While it is a legitimate validity concept, it involves comparing multiple predictors and does not resolve the reviewer's concern about the variance-explained interpretation of a single bivariate correlation."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-218-vignette-L5",
      "source_question_id": "218",
      "source_summary": "When a predictor has a criterion-related validity coefficient of .80, this means that 64% of variability in scores on the criterion is explained by variability in scores on the predictor.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A research team develops a new screening tool intended to forecast how well individuals will function in a demanding professional training program. After administering the screening tool to 400 applicants and tracking their performance throughout the program, they compute a single number summarizing how strongly the two sets of scores move together, obtaining a value of .80. The team presents this figure to program directors, describing it as evidence that the tool is highly effective. A skeptical statistician on the review panel argues that the .80 figure, taken alone, makes the tool sound more powerful than it actually is, because most people reading the report will interpret the number as if it tells them directly how much of the differences in training performance the screening tool can predict. She recommends reporting a transformed version of the same number, which she calculates to be .64, as a more honest representation of the tool's explanatory reach.",
      "question": "What statistical concept does the skeptical statistician's value of .64 represent, and how is it derived?",
      "options": {
        "A": "The shrinkage-adjusted validity estimate, derived by applying a cross-validation correction formula to the original correlation to account for capitalization on chance in the original sample, yielding a more conservative estimate of .64",
        "B": "The proportion of variance in training performance explained by the screening tool, derived by squaring the original correlation coefficient of .80, which yields the coefficient of determination (.80² = .64)",
        "C": "The specificity of the screening tool, derived by computing the ratio of true negatives to total negatives in the applicant pool, which happens to equal .64 in this dataset",
        "D": "The corrected correlation after adjusting for restriction of range among applicants, derived by applying Thorndike's Case II correction to the observed .80, reducing it to .64 due to the selective nature of the applicant sample"
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Shrinkage refers to the decrease in a validity coefficient when it is cross-validated on a new sample, and correction formulas (e.g., the Wherry formula) do reduce the observed correlation. However, shrinkage corrections do not reliably produce .64 from .80 in a predictable mathematical way, and the scenario describes a transformation that the statistician herself calculates rather than an empirical cross-validation. The process described — transforming .80 to .64 by applying a fixed mathematical operation — is characteristic of squaring, not shrinkage correction.",
        "B": "Correct. The statistician is describing the coefficient of determination, obtained by squaring the correlation (.80² = .64). Her critique is that readers will interpret .80 as if 80% of variance in training performance is explained by the screening tool, when in fact only 64% is. The .64 figure is a more accurate representation of the proportion of shared variance between the predictor and criterion.",
        "C": "Incorrect. Specificity is a signal detection statistic that represents the proportion of true negatives correctly identified by a test, used when outcomes are dichotomous. It is not derived from squaring a continuous correlation, and it has no mathematical relationship to the .80 correlation reported in the scenario. While .64 could coincidentally equal a specificity value, the derivation described by the statistician — a direct transformation of the .80 — does not match how specificity is computed.",
        "D": "Incorrect. Thorndike's Case II correction adjusts an observed correlation upward (not downward) to estimate what the true-score correlation would be if the range of scores were not restricted. Restriction of range typically deflates validity coefficients, so the correction would increase .80, not reduce it to .64. Furthermore, the statistician explicitly describes .64 as a more conservative, honest representation of the tool's power — the opposite of what an upward range restriction correction would produce."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-168-vignette-L1",
      "source_question_id": "168",
      "source_summary": "An unrestricted range of scores and homogeneous content of test items is likely to produce the largest reliability coefficient for a newly developed achievement test.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "reliability coefficient",
        "restriction of range",
        "homogeneous items"
      ],
      "vignette": "A psychometrician is developing a new achievement test in mathematics for students in grades 3 through 8. She carefully writes items that all measure the same narrow construct — arithmetic fluency — and administers the test to a broad, nationally representative sample spanning the full range of ability levels. When she computes the reliability coefficient, it is exceptionally high. Her colleague asks which two conditions most strongly contributed to maximizing the reliability estimate.",
      "question": "Which combination of factors BEST explains why this test yielded such a high reliability coefficient?",
      "options": {
        "A": "Homogeneous item content and an unrestricted range of scores",
        "B": "Restriction of range in scores and heterogeneous item content",
        "C": "A large number of items and a high difficulty index for each item",
        "D": "High test-retest stability and low standard error of measurement"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. When all items tap the same narrow construct (homogeneous content), inter-item correlations are high, boosting internal consistency. An unrestricted score range maximizes variance, which directly inflates the reliability coefficient because reliability is a ratio of true-score variance to total variance.",
        "B": "Incorrect. Restriction of range reduces the variance of observed scores, which deflates — not inflates — reliability estimates. Heterogeneous item content lowers inter-item correlations, further reducing internal consistency rather than maximizing it.",
        "C": "Incorrect. While a larger number of items generally increases reliability via the Spearman-Brown formula, a uniformly high difficulty index means most examinees answer items correctly, compressing variance and thereby reducing the reliability coefficient. Item count alone does not explain the high reliability here.",
        "D": "Incorrect. Test-retest stability reflects temporal consistency, a separate reliability dimension from internal consistency. A low standard error of measurement is a consequence of high reliability, not a cause of it; this option confuses the outcome with the contributing conditions."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-168-vignette-L2",
      "source_question_id": "168",
      "source_summary": "An unrestricted range of scores and homogeneous content of test items is likely to produce the largest reliability coefficient for a newly developed achievement test.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "item-total correlation",
        "score variance"
      ],
      "vignette": "A research team develops a 40-item reading comprehension test intended for use across elementary and high school students. The team is pleased to find that every item shows a strong item-total correlation, indicating each item is measuring the same underlying skill. The test is piloted on a diverse sample that includes both struggling and advanced readers, producing a wide spread of total scores. The lead researcher, who is also concurrently validating the test against teacher grades, notes that the internal consistency estimate came out much higher than she had anticipated for a newly developed measure.",
      "question": "Which two psychometric conditions most directly account for the unexpectedly high internal consistency estimate on this new reading test?",
      "options": {
        "A": "Strong concurrent validity with teacher grades and a large pilot sample",
        "B": "High item-total correlations reflecting homogeneous content, and wide score variance from an unrestricted sample",
        "C": "High item-total correlations and a high average difficulty index across items",
        "D": "Wide score variance and strong concurrent validity with teacher grades"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. High item-total correlations indicate that all items measure the same construct (homogeneous content), which maximizes internal consistency. The wide spread of total scores — produced by sampling the full ability range — maximizes observed score variance, further inflating the reliability coefficient.",
        "A": "Incorrect. Concurrent validity (correlation with teacher grades) is a form of criterion-related validity, not a determinant of internal consistency. A large sample improves the stability of the estimate but does not itself cause a higher reliability coefficient.",
        "C": "Incorrect. A high average difficulty index means most items are answered correctly, which restricts variance around the mean and reduces the spread of scores. This compression of variance would lower, not raise, the reliability estimate despite strong item-total correlations.",
        "D": "Incorrect. While wide score variance contributes to a higher reliability coefficient, concurrent validity with teacher grades reflects criterion-related validity — a separate psychometric property. Validity evidence does not directly increase internal consistency estimates."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-168-vignette-L3",
      "source_question_id": "168",
      "source_summary": "An unrestricted range of scores and homogeneous content of test items is likely to produce the largest reliability coefficient for a newly developed achievement test.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "coefficient alpha"
      ],
      "vignette": "A state education department commissions a new science achievement test to be administered statewide to all middle school students. The test developer writes 50 items, each targeting different aspects of the science curriculum — earth science, biology, chemistry, and physics — believing that broad content coverage will strengthen the test. When the test is normed exclusively on students from a single above-average suburban district, the resulting coefficient alpha is disappointingly low, even though the items themselves appear well-written and free from bias. The department is puzzled because a similar test administered at a university lab school last year, using only biology items and a cross-district sample, yielded a much higher internal consistency estimate.",
      "question": "Which two conditions BEST explain why the university lab school's test yielded a higher coefficient alpha than the statewide test, despite both tests being achievement measures?",
      "options": {
        "A": "The lab school test had more items and a longer testing time, increasing its reliability through the Spearman-Brown effect",
        "B": "The lab school test was given to a more motivated sample and had clearer item instructions, reducing construct-irrelevant variance",
        "C": "The lab school test used homogeneous content covering only biology, and was normed on an unrestricted cross-district sample with greater score variance",
        "D": "The statewide test suffered from poor test-retest reliability because it was administered only once, whereas the lab school test used parallel forms"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The lab school test measured only one content area (biology), producing high inter-item correlations consistent with homogeneous item content. Norming on a cross-district sample generated an unrestricted range of ability, maximizing score variance and thus the coefficient alpha. The statewide test's multidimensional content and restricted above-average sample both suppressed reliability.",
        "A": "Incorrect. The vignette does not indicate any difference in item count or testing time between the two tests. While the Spearman-Brown prophecy formula does predict that adding items raises reliability, this mechanism is not operative here; the critical differences are content homogeneity and score range.",
        "B": "Incorrect. Motivation and instruction clarity relate to construct-irrelevant variance, which could affect validity and true-score estimates. However, the vignette explicitly states items appeared well-written and free from bias, so these factors do not distinguish the two tests' reliability outcomes.",
        "D": "Incorrect. Test-retest reliability concerns consistency across time and is a separate reliability paradigm from internal consistency. Coefficient alpha is computed from a single administration; it does not require multiple administrations or parallel forms. Describing the statewide test as lacking test-retest data misidentifies the source of the problem."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-168-vignette-L4",
      "source_question_id": "168",
      "source_summary": "An unrestricted range of scores and homogeneous content of test items is likely to produce the largest reliability coefficient for a newly developed achievement test.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "variance"
      ],
      "vignette": "A private testing company develops a licensure examination for entry-level nurses and initially pilots it only within a single elite nursing program whose graduates consistently cluster near the top of the scoring distribution. The resulting internal consistency estimate is unexpectedly low, prompting the psychometrician to argue that the items themselves are at fault — perhaps they are too similar to one another and measure redundant knowledge. A senior consultant disagrees, suggesting the item pool is actually well-constructed and that the low estimate reflects a different, sampling-related problem. When the test is subsequently administered to a geographically diverse cohort that includes graduates from programs varying widely in rigor, the internal consistency estimate rises substantially without any changes to the items.",
      "question": "The senior consultant's position is BEST supported by which psychometric explanation?",
      "options": {
        "A": "The initial pilot sample produced high inter-item correlations that masked true item redundancy, whereas the diverse sample corrected this by introducing construct-irrelevant variance",
        "B": "The limited variance in the elite sample artificially suppressed the reliability estimate, and expanding to an unrestricted range of ability restored the true-score variance ratio needed for a high coefficient",
        "C": "The initial sample's ceiling effects indicated poor content validity, and the diverse sample improved content representativeness by including examinees with different educational backgrounds",
        "D": "Shrinkage in the reliability estimate occurred because the elite sample was too small, and the larger diverse sample provided a more stable estimate through increased statistical power"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. Reliability coefficients are mathematically dependent on the variance of observed scores in the norming sample. The elite, clustered sample restricted score variance, compressing the ratio of true-score variance to total variance and producing an artificially low coefficient. Expanding to an unrestricted, diverse sample restored adequate score variance, allowing the coefficient to reflect the items' actual measurement quality — exactly the consultant's point.",
        "A": "Incorrect. This option inverts the logic: high inter-item correlations would indicate homogeneity (which raises reliability), not mask redundancy. Construct-irrelevant variance introduced by a heterogeneous sample would normally add noise, not improve internal consistency. The consultant's argument concerns sample-level variance, not item-level correlations.",
        "C": "Incorrect. Content validity concerns whether the test adequately samples the target domain and is assessed through expert judgment and test blueprints — not through the distribution of examinee scores. Ceiling effects in an elite sample are a sampling artifact affecting score variance, not evidence of poor content representativeness.",
        "D": "Incorrect. Shrinkage is a specific phenomenon in regression and cross-validation contexts, referring to the drop in a validity or prediction coefficient when applied to a new sample. It does not describe changes in reliability estimates due to sample composition. While larger samples yield more stable estimates, stability differs from the magnitude of the coefficient itself."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-168-vignette-L5",
      "source_question_id": "168",
      "source_summary": "An unrestricted range of scores and homogeneous content of test items is likely to produce the largest reliability coefficient for a newly developed achievement test.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of educators creates a written examination intended to distinguish the most skilled professionals in a field from those who are less skilled. To save resources, they first try out the examination only among staff at a single prestigious institution, where most people perform near the top of the possible score range. They find, to their frustration, that the examination seems to produce scores that are inconsistent — people who should be performing similarly seem to get very different scores on different parts of the test, and the overall measure of how well all parts of the exam agree with each other turns out to be quite low. Confident in the quality of their questions, they wonder if something about the group they tested — not the questions — is responsible. When they later give the same examination, unchanged, to professionals drawn from institutions of many different quality levels, the agreement among parts of the examination jumps dramatically.",
      "question": "Which combination of conditions BEST accounts for the dramatic improvement in the consistency of scores across exam sections when the second, more diverse group was tested?",
      "options": {
        "A": "The second group was larger, providing more statistical power to detect true relationships among exam sections, and the first group's small size inflated measurement error",
        "B": "The first group's near-uniform high performance compressed the spread of scores, masking the true consistency among exam sections; the broader mix of ability levels in the second group restored the score spread needed to reveal that consistency",
        "C": "The questions in the examination were too narrow in scope for the expert first group, producing floor effects that obscured item relationships, whereas the second group's varying backgrounds exposed the full content range",
        "D": "The second group's wider range of educational backgrounds introduced more true differences in ability that the exam was better designed to measure, improving the match between the exam's intended purpose and the sample's characteristics"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. When the first group clustered near the top of the score range (a ceiling effect caused by restriction of range), the spread of total scores was severely compressed. Reliability — specifically internal consistency, reflecting how well all parts of the exam agree — is mathematically dependent on this spread; low score variance deflates the reliability coefficient regardless of item quality. The second group, drawn from institutions of varying quality, produced a wide, unrestricted spread of scores, allowing the true agreement among exam sections (reflecting homogeneous item content) to emerge. This precisely matches the anchor concept: unrestricted range plus homogeneous content yields the highest reliability coefficient.",
        "A": "Incorrect. Sample size affects the stability and precision of reliability estimates — a larger sample yields a more trustworthy coefficient — but it does not fundamentally change the magnitude of that coefficient in the way observed here. The dramatic jump in consistency is explained by the change in score distribution, not by statistical power gained from a larger group.",
        "C": "Incorrect. The vignette describes near-ceiling performance (most people scoring near the top), not floor effects (most people failing). Floor effects would be associated with a group that lacks the skills to answer items correctly. Additionally, the questions were described as unchanged and of presumed quality; content scope was not identified as a problem, making this a plausible-sounding but inaccurate explanation.",
        "D": "Incorrect. While it is true that a more diverse group introduces a broader range of true ability differences, this option subtly misframes the causal mechanism. The improvement is not about better 'match' between exam purpose and sample characteristics in a validity sense — it is specifically about the mathematical role that score variance plays in the computation of internal consistency. This distractor blurs the distinction between construct validity (does the test measure what it intends?) and the psychometric conditions that produce a high reliability estimate."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-195-vignette-L1",
      "source_question_id": "195",
      "source_summary": "Classical test theory (CTT) is test based while item response theory (IRT) is item based, where CTT focuses on total test scores and does not provide a basis for predicting how an examinee or group will respond to a particular test item, while IRT focuses on responses to individual test items and provides the information needed to determine the probability that a particular examinee or group will correctly answer any specific item.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "item response theory",
        "probability",
        "individual items"
      ],
      "vignette": "A psychometrician is developing a new cognitive ability test and wants a framework that models responses to individual items rather than total test scores. Specifically, she wants to estimate the probability that an examinee with a given ability level will correctly answer each specific item. She notes that this approach characterizes each item by parameters such as difficulty and discrimination at the item level, and that the resulting model is not test-dependent but rather item-dependent. A colleague suggests this goal aligns with a particular psychometric framework rather than classical test theory.",
      "question": "Which psychometric framework best describes the approach the psychometrician is using?",
      "options": {
        "A": "Classical test theory (CTT), because it uses total test scores to estimate reliability and standard error of measurement for a given test form.",
        "B": "Item response theory (IRT), because it models the probability that an examinee with a specific ability level will correctly answer each individual item using item-level parameters.",
        "C": "Generalizability theory (G-theory), because it partitions variance in test scores across multiple facets such as items, raters, and occasions.",
        "D": "Factor analysis, because it identifies the latent traits underlying examinee responses across a set of test items."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. CTT focuses on total test scores and provides reliability estimates such as coefficient alpha or split-half reliability for the whole test, but it does not model the probability that a specific examinee will answer a specific item correctly — which is the defining feature of IRT.",
        "B": "Correct. IRT is an item-based framework that estimates the probability of a correct response to each individual item as a function of the examinee's ability level and item parameters (e.g., difficulty, discrimination, guessing). This is precisely the capability the psychometrician requires.",
        "C": "Incorrect. Generalizability theory extends CTT by using ANOVA-based methods to decompose score variance across multiple measurement facets (items, raters, occasions), but it does not model the probability of a correct response to a specific item as a function of ability.",
        "D": "Incorrect. Factor analysis is a statistical technique used to identify latent constructs underlying a set of observed variables; while it is used in test development to evaluate dimensionality, it does not model the probability of correct item responses as a function of individual examinee ability."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-195-vignette-L2",
      "source_question_id": "195",
      "source_summary": "Classical test theory (CTT) is test based while item response theory (IRT) is item based, where CTT focuses on total test scores and does not provide a basis for predicting how an examinee or group will respond to a particular test item, while IRT focuses on responses to individual test items and provides the information needed to determine the probability that a particular examinee or group will correctly answer any specific item.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "item-level",
        "total test score"
      ],
      "vignette": "A research team developing a licensure examination is debating two approaches to test construction. The team's senior psychometrician points out that one approach anchors all statistics to the total test score and cannot tell them how a specific subgroup of low-scoring examinees will perform on item 14 in isolation. She argues for an alternative framework that provides item-level information independent of the particular sample used to calibrate the test. The team notes that the licensure board has historically used the simpler approach, partly because it requires less computational power and smaller sample sizes.",
      "question": "Which of the following best captures the distinction the senior psychometrician is drawing between the two approaches?",
      "options": {
        "A": "Classical test theory is test-based and cannot predict performance on individual items, whereas item response theory is item-based and allows estimation of the probability of a correct response to a specific item independent of the sample.",
        "B": "Split-half reliability is sample-dependent and cannot generalize across test forms, whereas coefficient alpha provides a sample-independent estimate of internal consistency.",
        "C": "Content validity is test-based and does not predict criterion performance, whereas criterion validity is item-based and allows prediction of individual item responses.",
        "D": "Item response theory is test-based and requires large normative samples, whereas classical test theory is item-based and allows for smaller calibration samples."
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The senior psychometrician is describing the fundamental distinction between CTT (test-based, anchored to total scores, unable to predict performance on specific items for specific subgroups) and IRT (item-based, providing probability estimates for individual items that are theoretically sample-independent after calibration).",
        "B": "Incorrect. Both split-half reliability and coefficient alpha are CTT-based internal consistency estimates that are sample-dependent; neither approach is item-based or sample-independent. This option confuses two CTT reliability methods with the CTT vs. IRT distinction.",
        "C": "Incorrect. Content validity and criterion validity are both forms of validity evidence, not psychometric frameworks for item vs. test-level analysis. Neither is 'item-based' in the sense described by the senior psychometrician, and criterion validity predicts criterion performance at the score level, not individual item responses.",
        "D": "Incorrect. This reverses the actual characteristics: IRT is the item-based framework (not test-based), and CTT is the test-based framework that typically requires smaller samples. IRT does require larger samples for stable parameter estimation, but that is a practical limitation, not its defining conceptual feature."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-195-vignette-L3",
      "source_question_id": "195",
      "source_summary": "Classical test theory (CTT) is test based while item response theory (IRT) is item based, where CTT focuses on total test scores and does not provide a basis for predicting how an examinee or group will respond to a particular test item, while IRT focuses on responses to individual test items and provides the information needed to determine the probability that a particular examinee or group will correctly answer any specific item.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "theta"
      ],
      "vignette": "A graduate student is reviewing the psychometric properties of a large-scale educational assessment used across multiple school districts. She notices that the test manual reports a difficulty index and a discrimination index for each item, but the manual also states that 'the usefulness of these statistics is dependent on the characteristics of the examinee sample from which they were derived.' Frustrated, she proposes switching to a framework in which item statistics remain stable across different samples and in which a student's standing on the latent trait, expressed as theta, can be linked to the likelihood of answering any single question correctly. Her advisor cautions that the proposed framework requires substantially larger sample sizes and more complex estimation procedures.",
      "question": "The graduate student's proposal reflects a shift from which framework to which framework?",
      "options": {
        "A": "From generalizability theory to classical test theory, because generalizability theory produces sample-dependent item statistics while CTT offers sample-independent score estimates.",
        "B": "From classical test theory to item response theory, because CTT produces sample-dependent item statistics tied to total scores, while IRT provides sample-independent item parameters and links latent trait estimates to the probability of correct item responses.",
        "C": "From item response theory to classical test theory, because IRT item parameters are sample-dependent and theta is only useful when test scores are aggregated across many items.",
        "D": "From norm-referenced to criterion-referenced interpretation, because difficulty and discrimination indices are norm-referenced statistics that must be replaced with mastery cutoff scores to allow sample-independent interpretation."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Generalizability theory is an extension of CTT that partitions error variance across multiple measurement facets; it does not produce item statistics that are more sample-dependent than CTT, and it does not model individual item response probabilities linked to a latent trait like theta.",
        "B": "Correct. The manual's warning that difficulty and discrimination indices depend on the sample is a well-known limitation of CTT. The graduate student's proposal — using theta to estimate the probability of a correct response to a specific item with sample-independent parameters — is the defining feature of IRT, representing a shift from the CTT framework.",
        "C": "Incorrect. This reverses the actual distinction. IRT parameters, once calibrated on an adequate sample, are theoretically sample-independent (invariant), whereas CTT statistics (including difficulty and discrimination indices) are sample-dependent. Theta is IRT's estimate of latent ability, not a score aggregated across items in the CTT sense.",
        "D": "Incorrect. The distinction between norm-referenced and criterion-referenced interpretation concerns how scores are reported and interpreted (relative to a group vs. relative to a standard), not how item statistics are derived or how the probability of a correct item response is modeled. This option conflates score interpretation with psychometric framework selection."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-195-vignette-L4",
      "source_question_id": "195",
      "source_summary": "Classical test theory (CTT) is test based while item response theory (IRT) is item based, where CTT focuses on total test scores and does not provide a basis for predicting how an examinee or group will respond to a particular test item, while IRT focuses on responses to individual test items and provides the information needed to determine the probability that a particular examinee or group will correctly answer any specific item.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "invariance"
      ],
      "vignette": "A testing organization administers a professional certification examination to two cohorts: one drawn from a large urban region and one from a small rural area. After scoring the exam using traditional methods, the psychometrician observes that the difficulty and discrimination statistics for item 22 differ substantially across the two cohorts, making it impossible to compare item performance between groups without knowing which sample generated the statistics. A consultant proposes an alternative scoring model under which item characteristics remain stable regardless of which cohort is used for calibration, thereby supporting cross-group comparisons at the item level. The testing organization's current approach summarizes examinee performance with a single composite that aggregates performance across all items, providing no basis for predicting how any particular examinee will respond to item 22 specifically.",
      "question": "The psychometric limitation the consultant is addressing, and the framework being proposed, are best described by which of the following?",
      "options": {
        "A": "The limitation is low test-retest reliability due to cohort differences, and the proposed solution is to adopt parallel forms that equate total scores across administrations.",
        "B": "The limitation is sample-dependent item statistics and the inability to predict individual item performance, characteristic of classical test theory; the proposed framework is item response theory, which provides invariant item parameters and models the probability of a correct item response.",
        "C": "The limitation is differential item functioning undetected by the current scoring model, and the proposed solution is to conduct a confirmatory factor analysis to identify items that load differently across groups.",
        "D": "The limitation is restriction of range within each cohort distorting item-total correlations, and the proposed solution is to use a correction formula to adjust the discrimination index for range restriction."
      },
      "correct_answer": "B",
      "option_explanations": {
        "A": "Incorrect. Test-retest reliability concerns the stability of scores across time and is not the issue here; the problem is that item statistics vary across concurrent cohorts, not across time points. Parallel forms equate total scores but do not produce invariant item-level parameters or resolve the inability to predict individual item responses.",
        "B": "Correct. The psychometrician's observation that difficulty and discrimination indices differ across cohorts reflects the sample-dependent nature of CTT item statistics. The consultant's proposal — item parameters that remain stable regardless of the calibration sample, enabling cross-group item-level prediction — is the defining advantage of IRT (parameter invariance). The current approach's reliance on a composite total score with no basis for item-level prediction further identifies it as CTT.",
        "C": "Incorrect. Differential item functioning (DIF) analysis does examine whether items behave differently across groups and is often conducted within an IRT framework, but DIF is a specific statistical phenomenon (item bias), not the broader psychometric limitation described. The consultant is not simply proposing DIF analysis; rather, the proposal addresses the fundamental framework shift from CTT to IRT, of which DIF detection is only one application.",
        "D": "Incorrect. Restriction of range is a validity-related statistical concern that affects correlation coefficients, including discrimination indices. While range restriction could attenuate discrimination indices, it does not explain why item statistics would differ across a large urban cohort and a small rural cohort in a way that precludes cross-group comparison, nor does a range correction formula provide the invariant item parameters or item-level response probability modeling described by the consultant."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-195-vignette-L5",
      "source_question_id": "195",
      "source_summary": "Classical test theory (CTT) is test based while item response theory (IRT) is item based, where CTT focuses on total test scores and does not provide a basis for predicting how an examinee or group will respond to a particular test item, while IRT focuses on responses to individual test items and provides the information needed to determine the probability that a particular examinee or group will correctly answer any specific item.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Item Analysis and Test Reliability",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A measurement specialist is reviewing two approaches used by different research groups to evaluate the same set of questions given to job applicants. The first research group summarizes how well each question 'worked' by looking at the overall score that all applicants received and then computing how much each question's pattern of correct and incorrect answers tracked with that overall score — but the specialist notes that these summaries would be quite different if the questions were given to a very different group of applicants. The second research group, by contrast, builds a model for each question individually, estimating what the actual chances are that a particular applicant with a known standing on the underlying skill would get that question right — and this estimation remains consistent regardless of whether the applicants were mostly high- or low-skilled. The specialist recommends the second approach specifically because a key goal of the project is to predict, for any future applicant whose skill level has been estimated, exactly which questions they are likely to answer correctly before they even take the test.",
      "question": "The distinction the measurement specialist is drawing between the two approaches is best captured by which of the following?",
      "options": {
        "A": "The first approach reflects content validity, which evaluates whether test questions adequately sample the job domain, while the second reflects predictive validity, which estimates the probability that a specific applicant will succeed on each question given their prior performance.",
        "B": "The first approach reflects internal consistency reliability estimation, in which each item's relationship to the total score changes with the sample, while the second reflects test-retest reliability, in which stable individual-level predictions are made across administrations.",
        "C": "The first approach reflects classical test theory, which anchors item statistics to total scores in a sample-dependent way and cannot predict specific item responses, while the second reflects item response theory, which models the probability of a correct response to each individual item in a way that remains consistent across samples of varying ability.",
        "D": "The first approach reflects a norm-referenced scoring strategy in which item performance is evaluated relative to the group, while the second reflects a criterion-referenced strategy in which a fixed cutoff score determines mastery of each individual item."
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "Incorrect. Content validity and predictive validity are both forms of validity evidence that concern what a test measures and whether it predicts outcomes, respectively. Neither framework involves computing the probability of a correct response to a specific item as a function of a known individual skill level, nor does either address sample-dependence of item statistics in the way described.",
        "B": "Incorrect. Internal consistency reliability (e.g., coefficient alpha, item-total correlations) is indeed a CTT-based approach that is sample-dependent, which matches the first group's limitation. However, test-retest reliability concerns stability of scores across time rather than the prediction of specific item-level responses for individuals at known ability levels, so it does not capture what the second group's model accomplishes.",
        "C": "Correct. The first group's reliance on overall scores and item-total correlations that change with the applicant sample is the hallmark of CTT (test-based, sample-dependent). The second group's item-by-item modeling of the probability that a specific applicant at a known ability level will answer correctly — a probability that holds regardless of the ability distribution in the sample — precisely describes IRT. The specialist's goal of pre-testing predictions for future applicants is only achievable under IRT's framework.",
        "D": "Incorrect. Norm-referenced and criterion-referenced scoring are interpretive strategies concerning how scores are reported relative to a group or a fixed standard. While criterion-referenced mastery decisions are made at the item level in some applications, the framework described — estimating the probability of a correct response for a specific individual based on a quantified ability level and a model that is stable across samples — is not what criterion-referencing means, and norm-referencing does not inherently produce sample-dependent item parameters in the sense described."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-098-vignette-L1",
      "source_question_id": "098",
      "source_summary": "Rotation of the initial factor matrix simplifies the factor structure, thereby creating a matrix that is easier to interpret, where each test included in the factor analysis will have a high correlation (factor loading) with one of the factors and low correlations with the remaining factors, and the interpretation of and name given to each factor involves considering the tests that correlate highly with each factor.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "factor analysis",
        "factor loadings",
        "rotation"
      ],
      "vignette": "A psychometrician develops a new cognitive battery and subjects it to factor analysis. After the initial extraction, the factor matrix is difficult to interpret because many tests correlate moderately with several factors simultaneously. The researcher then applies rotation to the initial factor matrix, which results in each test having a high factor loading on one factor and near-zero loadings on the others. The researcher names each factor by examining which tests cluster together with high loadings on it.",
      "question": "Which purpose does rotation of the initial factor matrix primarily serve in this scenario?",
      "options": {
        "A": "To simplify the factor structure so that each variable loads highly on one factor and minimally on others, making interpretation clearer",
        "B": "To increase the total variance accounted for by the factor solution, thereby improving the overall fit of the model",
        "C": "To eliminate factors that fail to reach statistical significance, reducing the number of dimensions in the final solution",
        "D": "To convert oblique factors into orthogonal factors, ensuring that extracted dimensions are uncorrelated with one another"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. Rotation repositions the factor axes so that each variable aligns strongly with one factor and weakly with others, producing a simpler, more interpretable structure without changing the total variance explained.",
        "B": "This is incorrect. Rotation does not change the total variance accounted for by the factor solution; it merely redistributes the variance across factors to achieve a cleaner pattern of loadings.",
        "C": "This is incorrect. Decisions about the number of factors to retain (e.g., using eigenvalue criteria or scree plots) are made before rotation; rotation reorganizes the existing factor structure rather than eliminating factors.",
        "D": "This is incorrect. While orthogonal rotation produces uncorrelated factors, rotation methods can be either orthogonal or oblique; the defining purpose of rotation is structural simplification, not necessarily ensuring orthogonality."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-098-vignette-L2",
      "source_question_id": "098",
      "source_summary": "Rotation of the initial factor matrix simplifies the factor structure, thereby creating a matrix that is easier to interpret, where each test included in the factor analysis will have a high correlation (factor loading) with one of the factors and low correlations with the remaining factors, and the interpretation of and name given to each factor involves considering the tests that correlate highly with each factor.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "factor matrix",
        "factor loadings"
      ],
      "vignette": "A researcher is validating a new personality inventory intended for use with adult psychiatric outpatients. After initial extraction, the factor matrix shows that most items have moderate correlations with at least three of the five extracted factors, making it nearly impossible to decide which items belong to which construct. Despite the fact that the sample includes individuals with comorbid diagnoses — a detail the researcher worries might be distorting the results — the psychometrician recommends a purely mathematical realignment of the factor axes. Following this procedure, items cluster cleanly, with each item correlating strongly with only one factor, and the researcher labels each factor based on the content of its highest-loading items.",
      "question": "What procedure did the psychometrician most likely recommend to clarify the factor structure?",
      "options": {
        "A": "Increasing the sample size to reduce sampling error and stabilize the factor loadings across the item pool",
        "B": "Applying a confirmatory factor analysis model to test whether the a priori five-factor structure fits the data",
        "C": "Rotating the factor matrix to achieve a simpler structure in which each item loads highly on one factor and minimally on the others",
        "D": "Removing items with low item-total correlations to improve internal consistency before re-extracting the factors"
      },
      "correct_answer": "C",
      "option_explanations": {
        "A": "This is incorrect. While larger samples stabilize factor solutions, increasing sample size does not mathematically reorganize the factor axes to reduce cross-loadings; that is specifically what rotation accomplishes.",
        "B": "This is incorrect. Confirmatory factor analysis tests a pre-specified theoretical structure against the data rather than reorganizing an ambiguous initial solution to reveal the pattern of loadings empirically.",
        "C": "This is correct. The psychometrician recommended rotating the factor matrix, a procedure that repositions factor axes to simplify the loading pattern so each item aligns strongly with one factor and weakly with others, enabling meaningful labeling of factors.",
        "D": "This is incorrect. Removing low item-total correlation items is an item-analysis step aimed at improving internal consistency reliability, not a technique for clarifying a cross-loaded factor solution through structural realignment."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-098-vignette-L3",
      "source_question_id": "098",
      "source_summary": "Rotation of the initial factor matrix simplifies the factor structure, thereby creating a matrix that is easier to interpret, where each test included in the factor analysis will have a high correlation (factor loading) with one of the factors and low correlations with the remaining factors, and the interpretation of and name given to each factor involves considering the tests that correlate highly with each factor.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "construct validity"
      ],
      "vignette": "A test developer creates a 40-item intelligence battery and submits the data to exploratory analysis. The initial solution yields four dimensions, but the resulting table of correlations between items and dimensions is so complex that assigning items to any single dimension seems arbitrary. A colleague notes that several items appear to tap both verbal reasoning and processing speed simultaneously, which the developer suspects may threaten the battery's construct validity. To resolve this ambiguity, the developer applies a mathematical procedure that repositions the dimensional axes without altering the total amount of shared variance, after which each item aligns predominantly with a single dimension. The developer then inspects the highest-correlating items on each dimension and assigns conceptual labels accordingly.",
      "question": "Which psychometric procedure did the developer use to clarify the dimensional structure of the battery?",
      "options": {
        "A": "Factor rotation, which repositions the axes of the factor solution to simplify the pattern of item-factor correlations without changing total explained variance",
        "B": "Factor extraction using principal axis factoring, which identifies latent dimensions by iteratively estimating communalities until the solution stabilizes",
        "C": "Item discrimination analysis, which evaluates each item's ability to differentiate between high- and low-scoring examinees on the overall battery",
        "D": "Parallel analysis, which compares eigenvalues from the actual data to those from random data to determine the optimal number of factors to retain"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. Factor rotation repositions the factor axes to produce a simpler loading structure in which each item aligns strongly with one factor and minimally with others, enabling the developer to interpret and name each factor by examining its highest-loading items — all without changing the total variance explained.",
        "B": "This is incorrect. Principal axis factoring is an extraction method used to identify the initial factor solution; it determines how many factors underlie the data, not how to simplify the loadings of an already-extracted solution.",
        "C": "This is incorrect. Item discrimination analysis (discrimination index) is a classical test theory procedure that examines how well an item differentiates high and low scorers; it does not address the pattern of item-factor correlations across multiple latent dimensions.",
        "D": "This is incorrect. Parallel analysis is a factor retention criterion used to decide how many factors to keep; it compares observed eigenvalues to those expected by chance but does not reorganize or simplify the structure of an existing factor solution."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-098-vignette-L4",
      "source_question_id": "098",
      "source_summary": "Rotation of the initial factor matrix simplifies the factor structure, thereby creating a matrix that is easier to interpret, where each test included in the factor analysis will have a high correlation (factor loading) with one of the factors and low correlations with the remaining factors, and the interpretation of and name given to each factor involves considering the tests that correlate highly with each factor.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "orthogonal"
      ],
      "vignette": "A measurement researcher publishes a study comparing two versions of a cognitive abilities battery. In the first version, the initial extraction solution is reported directly; most subtests show substantial correlations with the first extracted dimension and progressively smaller but still notable correlations with subsequent dimensions, making subtest classification ambiguous. In the second version, the researcher applies an orthogonal transformation — specifically Varimax — that preserves the total communality of the solution while redistributing variance so that the dominant first dimension loses its privileged status. Critics argue the transformation artificially inflates the apparent independence of the constructs, yet the researcher defends the procedure by pointing out that the goal is interpretability: each subtest now correlates strongly with exactly one dimension and negligibly with the others, enabling transparent construct labeling.",
      "question": "What does the researcher's application of Varimax most directly illustrate?",
      "options": {
        "A": "Confirmatory factor analysis, in which a theoretically derived structure is imposed on the data and evaluated against observed covariance patterns",
        "B": "Factor rotation, which reorients the axes of the factor solution to simplify loadings and facilitate interpretation without altering total explained variance",
        "C": "Principal components extraction, in which linear combinations of observed variables are formed to maximize successive portions of total variance",
        "D": "Oblique rotation, in which factor axes are allowed to correlate so that the resulting factor structure reflects naturally interrelated constructs"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "This is correct. Varimax is an orthogonal rotation method that repositions factor axes to maximize the variance of squared loadings within each factor, producing a simple structure in which each variable loads highly on one factor and near zero on others — precisely what the researcher describes. Total communality is preserved.",
        "A": "This is incorrect. Confirmatory factor analysis tests a pre-specified model against data and yields fit indices; it does not involve repositioning axes of an exploratory solution to redistribute loadings, which is what the researcher did.",
        "C": "This is incorrect. Principal components extraction is the initial procedure that creates the unrotated solution; the researcher's step of applying Varimax comes after extraction and specifically addresses the interpretability of that solution rather than the extraction itself.",
        "D": "This is incorrect. Oblique rotation allows factors to correlate, which is the opposite of what Varimax does; the vignette explicitly states the transformation is orthogonal, meaning the resulting factors remain uncorrelated. Oblique methods (e.g., Oblimin, Promax) are a distinct class of rotation."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-098-vignette-L5",
      "source_question_id": "098",
      "source_summary": "Rotation of the initial factor matrix simplifies the factor structure, thereby creating a matrix that is easier to interpret, where each test included in the factor analysis will have a high correlation (factor loading) with one of the factors and low correlations with the remaining factors, and the interpretation of and name given to each factor involves considering the tests that correlate highly with each factor.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Content and Construct Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A measurement team has assembled 30 tasks thought to assess different mental abilities and administered them to a large sample. After the initial mathematical procedure that identifies underlying dimensions, they produce a table showing how strongly each task relates to each dimension, but the table is frustrating: almost every task has moderate relationships with multiple dimensions simultaneously, and no clear groupings emerge. Crucially, the total amount of shared variation captured by the dimensions remains exactly the same before and after the next step. In that next step, the team geometrically repositions the reference axes in the dimensional space, after which the table dramatically changes: each task now has one strong relationship and several near-zero relationships with the dimensions. The team then reads down each dimension, notes which tasks are most strongly associated with it, and assigns a conceptual name based on what those tasks have in common.",
      "question": "Which psychometric procedure is most precisely illustrated by the step that repositions the reference axes and produces the simplified table of relationships?",
      "options": {
        "A": "Factor rotation, which reorients factor axes to produce a simpler loading pattern in which each variable aligns predominantly with one factor, preserving total explained variance while enabling construct-based naming of factors",
        "B": "Factor extraction, which identifies the mathematical dimensions underlying a correlation matrix by decomposing shared variance into successive uncorrelated components",
        "C": "Item response theory calibration, which estimates the relationship between a latent trait and the probability of a specific response on each task, yielding item parameters that define each dimension",
        "D": "Exploratory structural equation modeling, which simultaneously estimates cross-loadings and allows every item to load on every factor while also specifying structural paths among latent variables"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "This is correct. The step described — geometrically repositioning the dimensional axes without changing total explained variance, after which each task has one high and several near-zero relationships, and factors are named by inspecting high-loading tasks — is the definition of factor rotation. The scenario carefully avoids using the term 'rotation' or 'factor loading' but describes exactly this process.",
        "B": "This is incorrect. Factor extraction is the initial step that creates the unrotated dimensional solution, which is precisely the step that produced the frustrating, ambiguous table. The key step in question comes after extraction and specifically transforms that initial solution; extraction does not itself reposition axes.",
        "C": "This is incorrect. Item response theory calibration estimates item parameters (difficulty, discrimination, guessing) on a latent continuum; it does not involve repositioning reference axes in a multidimensional space or produce a pattern of high-vs.-low relationships across multiple dimensions in the same way described.",
        "D": "This is incorrect. Exploratory structural equation modeling is designed to allow all items to have non-zero loadings on every factor simultaneously, which is the opposite pattern from what the procedure produced; the described outcome is clean, simplified loading structure with near-zero cross-loadings, not the all-factors-loaded model ESEM supports."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-183-vignette-L1",
      "source_question_id": "183",
      "source_summary": "The correction for attenuation formula is used to estimate the effects of increasing the reliability of a predictor and/or criterion on the criterion-related validity coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 1,
      "difficulty_label": "Easy",
      "hint_words": [
        "correction for attenuation",
        "reliability",
        "validity coefficient"
      ],
      "vignette": "A psychometrician is evaluating a newly developed cognitive screening test. The observed validity coefficient between the screening test and the criterion measure of neuropsychological functioning is only .35, but both instruments have known reliability limitations. The psychometrician applies the correction for attenuation formula to estimate what the validity coefficient would be if both the predictor and criterion had perfect reliability. The resulting disattenuated coefficient is substantially higher at .62, suggesting the true relationship between the constructs is stronger than the raw data indicate.",
      "question": "Which psychometric procedure is the psychometrician using to estimate the relationship between constructs when measurement error is removed?",
      "options": {
        "A": "Correction for attenuation",
        "B": "Restriction of range correction",
        "C": "Cross-validation shrinkage estimation",
        "D": "Standard error of measurement adjustment"
      },
      "correct_answer": "A",
      "option_explanations": {
        "A": "Correct. The correction for attenuation formula uses the reliabilities of both predictor and criterion to estimate what the validity coefficient would be if measurement error were eliminated, directly producing a disattenuated correlation as described.",
        "B": "Incorrect. Restriction of range correction adjusts validity coefficients when the sample is more homogeneous than the population of interest, not when measurement error from low reliability is depressing the observed correlation.",
        "C": "Incorrect. Cross-validation shrinkage estimation refers to the expected reduction in a regression-based validity coefficient when it is applied to a new sample, addressing sample-specificity rather than measurement error attenuation.",
        "D": "Incorrect. The standard error of measurement quantifies the expected variation in an individual's obtained scores due to unreliability but is not used as a formula to estimate disattenuated validity coefficients."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-183-vignette-L2",
      "source_question_id": "183",
      "source_summary": "The correction for attenuation formula is used to estimate the effects of increasing the reliability of a predictor and/or criterion on the criterion-related validity coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 2,
      "difficulty_label": "Medium",
      "hint_words": [
        "disattenuated",
        "reliability"
      ],
      "vignette": "A research team at a large urban hospital has developed a brief structured interview to predict treatment outcome in patients with comorbid anxiety and depression. When they correlate interview scores with six-month clinician-rated outcomes, the validity coefficient is a modest .28. A senior researcher notes that both the interview and the clinician ratings have only moderate reliability, and she calculates the disattenuated correlation to be .51. She explains to the junior staff that the observed validity coefficient underestimates the true relationship because measurement error in both measures attenuates the observed association.",
      "question": "What statistical procedure did the senior researcher use to produce the estimate of .51?",
      "options": {
        "A": "Restriction of range correction applied to the predictor scores",
        "B": "Correction for attenuation using the reliabilities of both the predictor and criterion",
        "C": "Multiple regression with reliability-weighted predictors",
        "D": "Incremental validity analysis controlling for shared method variance"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The correction for attenuation formula estimates the true score correlation by dividing the observed validity coefficient by the square root of the product of both instruments' reliability coefficients, yielding a higher disattenuated estimate as described.",
        "A": "Incorrect. The restriction of range correction is applied when the sample's variance on the predictor is artificially curtailed relative to the target population; the scenario describes a reliability problem, not a sampling restriction problem.",
        "C": "Incorrect. Multiple regression with reliability-weighted predictors is not a standard psychometric procedure for estimating disattenuated validity; it addresses prediction rather than correcting for attenuation from measurement error.",
        "D": "Incorrect. Incremental validity analysis examines whether a new predictor adds predictive power beyond existing measures, and controlling for shared method variance is a construct validity concern — neither addresses the attenuation of a validity coefficient by unreliability."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-183-vignette-L3",
      "source_question_id": "183",
      "source_summary": "The correction for attenuation formula is used to estimate the effects of increasing the reliability of a predictor and/or criterion on the criterion-related validity coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 3,
      "difficulty_label": "Hard",
      "hint_words": [
        "attenuation"
      ],
      "vignette": "A graduate student is puzzled because a new executive functioning battery shows high internal consistency (coefficient alpha = .91) but a surprisingly low correlation of .29 with supervisors' performance ratings on the job. A faculty advisor points out that supervisor ratings collected in a single session are notoriously inconsistent across raters, likely reflecting substantial measurement error in the criterion. The advisor uses a formula that accounts for the unreliability of both the test and the criterion to show that if both instruments measured without error, the estimated relationship between executive functioning and job performance would rise to .58. The student initially wonders whether the problem is simply a restricted range of ability scores in the professional sample.",
      "question": "Which procedure best explains the increase in the estimated relationship from .29 to .58?",
      "options": {
        "A": "Restriction of range correction, which inflates the coefficient by accounting for reduced variance in the selected professional sample",
        "B": "Correction for attenuation, which estimates the validity coefficient that would be obtained if measurement error were eliminated from both measures",
        "C": "Concurrent validity re-estimation after removing outliers that suppress the observed correlation",
        "D": "Structural equation modeling used to partial out error variance and estimate latent variable relationships"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The correction for attenuation formula uses the reliability of both the predictor and the criterion to estimate the disattenuated validity coefficient; the scenario specifically names unreliability of the criterion (supervisor ratings) and high internal consistency of the test, fitting this procedure precisely.",
        "A": "Incorrect. While the student's concern about restriction of range is a plausible red herring in a professional sample, the faculty advisor's procedure uses reliabilities, not variance estimates from a larger population, distinguishing it from a range restriction correction.",
        "C": "Incorrect. Removing outliers to improve an observed correlation is a data-cleaning strategy, not a formal psychometric formula, and the scenario describes a principled formula-based estimation procedure applied to known reliabilities.",
        "D": "Incorrect. Structural equation modeling can estimate latent variable correlations free of measurement error, which is conceptually related, but the scenario describes a straightforward formula using known reliability coefficients rather than a full SEM analysis."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-183-vignette-L4",
      "source_question_id": "183",
      "source_summary": "The correction for attenuation formula is used to estimate the effects of increasing the reliability of a predictor and/or criterion on the criterion-related validity coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 4,
      "difficulty_label": "Extremely Hard",
      "hint_words": [
        "true score"
      ],
      "vignette": "A psychometrician is reviewing the validity study for a pre-employment selection test used in law enforcement hiring. The observed correlation between test scores and academy performance ratings is .33. Because the agency only hires applicants who score above the 70th percentile, the psychometrician suspects that the restricted sample is responsible for the low coefficient and initially begins preparing a range restriction analysis. However, upon examining the data more closely, she notes that the sample is actually quite heterogeneous in test scores, while the academy performance ratings were each collected on a single occasion by a single rater. She then uses the test's known reliability and the estimated reliability of the single-rater performance measure to compute what the correlation would be between the true score on the test and the true score on the criterion.",
      "question": "Which procedure does the psychometrician ultimately apply, and why is it more appropriate than the alternative she initially considered?",
      "options": {
        "A": "Restriction of range correction, because even heterogeneous samples may have truncated upper tails that suppress the validity coefficient",
        "B": "Correction for attenuation, because the observed validity coefficient is suppressed by measurement error in the criterion rather than by reduced score variance in the sample",
        "C": "Differential item functioning analysis, because single-rater performance evaluations may introduce systematic bias that lowers criterion-related validity",
        "D": "Confirmatory factor analysis, because the low observed correlation reflects poor construct overlap rather than unreliability in either measure"
      },
      "correct_answer": "B",
      "option_explanations": {
        "B": "Correct. The psychometrician determines the sample is heterogeneous (ruling out restriction of range) but that the criterion has poor reliability due to single-occasion, single-rater measurement. The correction for attenuation uses the reliability coefficients of both measures to estimate the true score correlation, directly addressing the identified source of validity coefficient suppression.",
        "A": "Incorrect. Although restriction of range is a plausible concern in selection contexts and the initial red herring in the vignette, the psychometrician explicitly observes that the sample is heterogeneous in test scores, making this correction inappropriate. The correction for restriction of range requires evidence of artificially curtailed variance, not just a selection setting.",
        "C": "Incorrect. Differential item functioning analysis examines whether items function differently across demographic subgroups and is a fairness/bias procedure, not a method for estimating disattenuated validity or correcting for unreliability in criterion ratings.",
        "D": "Incorrect. Confirmatory factor analysis can assess construct overlap but is not the formula-based procedure described; furthermore, the scenario attributes the low validity to criterion unreliability rather than to a lack of shared construct variance, which CFA would address."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "JQ-TES-183-vignette-L5",
      "source_question_id": "183",
      "source_summary": "The correction for attenuation formula is used to estimate the effects of increasing the reliability of a predictor and/or criterion on the criterion-related validity coefficient.",
      "domain_code": "PMET",
      "domain_name": "Psychometrics & Research Methods",
      "subdomain": "Test Validity - Criterion-Related Validity",
      "question_type": "vignette",
      "difficulty_level": 5,
      "difficulty_label": "Almost Impossible",
      "hint_words": [],
      "vignette": "A team of researchers develops a short questionnaire intended to identify individuals likely to struggle with a complex job skill. When they compare questionnaire scores to scores on a hands-on performance task collected by trainees who each independently evaluate a different subset of participants, the relationship between the two measures is disappointingly weak at .26. The research team notes that the questionnaire itself produces very consistent scores when people complete it twice, and several members argue that the selection process used to recruit participants kept the group fairly similar to the broader applicant population. A statistician on the team, however, points out that asking a different untrained evaluator to rate each participant only once introduces a great deal of inconsistency into the performance scores, and she proposes a mathematical procedure to estimate what the relationship between the underlying tendencies being measured would look like if this inconsistency were eliminated. Applying this procedure yields an estimated relationship of .54.",
      "question": "Which psychometric procedure did the statistician apply?",
      "options": {
        "A": "Restriction of range correction, adjusting for the homogeneity introduced by the selection process used to recruit participants",
        "B": "Cross-validation, which re-estimates the observed relationship in a separate holdout sample to remove bias from overfitting",
        "C": "Correction for attenuation, which uses the degree of measurement consistency in both instruments to estimate the relationship between the constructs they measure when error is removed",
        "D": "Inter-rater reliability adjustment, which standardizes the performance scores across evaluators to remove systematic rater bias before recomputing the validity coefficient"
      },
      "correct_answer": "C",
      "option_explanations": {
        "C": "Correct. The scenario describes high retest consistency in the questionnaire (high predictor reliability) and low consistency in the performance task due to single-occasion, different-evaluator ratings (low criterion reliability). The statistician uses the reliability estimates of both measures in the correction for attenuation formula to estimate the disattenuated correlation of .54, reflecting the true relationship between constructs.",
        "A": "Incorrect. Although the vignette deliberately includes a statement about the sample being similar to the broader applicant population — a red herring suggesting restriction of range is not a concern — the team members' own observation rules this out. The restriction of range correction addresses variance reduction in the sample, not inconsistency in measurement, and would not be the appropriate choice here.",
        "B": "Incorrect. Cross-validation is used to evaluate how well a regression model or prediction equation generalizes to a new sample, correcting for overfitting or capitalization on chance. It does not use reliability coefficients to estimate disattenuated correlations and does not address the inconsistency in the criterion ratings described.",
        "D": "Incorrect. Standardizing performance scores across raters or adjusting for rater bias is a data transformation strategy that might reduce systematic error but is not the formula-based psychometric procedure described. The correction for attenuation operates on known reliability estimates mathematically rather than transforming raw scores, making this option a plausible but incorrect choice."
      },
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    }
  ]
}