{
  "domain_code": "CASS",
  "domain_name": "Clinical Assessment & Interpretation",
  "question_type": "contrast",
  "total": 15,
  "questions": [
    {
      "id": "TES_CONT_006",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Test Score Interpretation",
      "item_x": "Norm-referenced interpretation",
      "item_y": "Criterion-referenced interpretation",
      "question": "What is the difference between norm-referenced and criterion-referenced test interpretation?",
      "answer": "Norm-referenced interpretation compares an individual's test performance to the performance of a normative reference group (standardization sample). Scores are expressed as percentile ranks, z-scores, T-scores, or IQ scores indicating where the person stands relative to others. Most clinical and intelligence tests use norm-referenced interpretation (e.g., 'IQ of 115 is in the 84th percentile'). Criterion-referenced interpretation compares an individual's performance to a predetermined standard or criterion rather than to other test-takers (e.g., 'passed/failed,' '80% mastery level'). Licensure exams, academic competency tests, and mastery tests use criterion-referenced interpretation.",
      "key_distinction": "Norm-referenced = compared to other people; Criterion-referenced = compared to a predetermined standard.",
      "commonly_confused_because": "Students may not recognize which type of interpretation is most appropriate for a given testing purpose (clinical assessment vs. licensure/mastery).",
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "TES_CONT_008",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Test Score Interpretation",
      "item_x": "Percentile rank",
      "item_y": "Standard score",
      "question": "What is the difference between a percentile rank and a standard score in test score interpretation?",
      "answer": "A percentile rank indicates the percentage of individuals in the normative group who scored at or below a given raw score. Percentile ranks range from 1 to 99 and form an ordinal, non-equal-interval scale — the distance between the 50th and 60th percentile represents a much smaller raw-score gap than the distance between the 90th and 99th percentile. A standard score expresses a raw score in terms of standard deviation units from the normative mean (e.g., z-scores: M=0, SD=1; T-scores: M=50, SD=10; IQ scores: M=100, SD=15). Standard scores form an equal-interval scale that permits arithmetic comparison and averaging across tests.",
      "key_distinction": "Percentile rank = ordinal position in the norming group (non-equal-interval); Standard score = equal-interval deviation score referenced to the normative mean and SD.",
      "commonly_confused_because": "Both express where an individual stands relative to the normative sample, but percentile ranks compress differences near the extremes while standard scores preserve equal intervals throughout the distribution.",
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "TES_CONT_010",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Test Score Interpretation",
      "item_x": "Age equivalent score",
      "item_y": "Grade equivalent score",
      "question": "What is the difference between an age equivalent score and a grade equivalent score?",
      "answer": "An age equivalent score expresses a child's test performance as the chronological age (in years and months) of children in the normative sample for whom the obtained raw score is average. For example, a 9-year-old who earns an age equivalent of 11-3 performed as well as the average 11-year-3-month-old. A grade equivalent score expresses performance as the grade and month of schooling for which the raw score is average (e.g., 5.4 = average performance in the fourth month of fifth grade). Both score types are intuitive for lay audiences but share important limitations: they assume linear developmental growth, cannot be averaged across subtests or compared arithmetically, and are easily misinterpreted as implying that a child should be placed in a higher grade or age group.",
      "key_distinction": "Age equivalent = performance referenced to average chronological age; Grade equivalent = performance referenced to average grade and month of schooling.",
      "commonly_confused_because": "Both express a child's score as a developmental reference point that is easy for parents and teachers to understand, and both carry the same misinterpretation risks; the distinction is whether the reference norm is age-based or grade-based.",
      "legacy_domain_code": "TES",
      "legacy_domain_name": "Testing and Measurement"
    },
    {
      "id": "PAS_CONT_001",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Stanford-Binet and Wechsler Tests",
      "item_x": "Fluid intelligence (Gf)",
      "item_y": "Crystallized intelligence (Gc)",
      "question": "What is the difference between fluid intelligence and crystallized intelligence?",
      "answer": "Fluid intelligence (Gf) refers to the capacity for novel problem-solving, abstract reasoning, pattern recognition, and information processing — independent of accumulated knowledge. It is associated with processing speed and working memory. Gf peaks in early adulthood and declines with age, as it is sensitive to neurological changes. Crystallized intelligence (Gc) refers to accumulated knowledge, verbal ability, vocabulary, and cultural learning acquired through experience and education. Gc is relatively stable across adulthood and may even increase into old age. Age-related changes in fluid intelligence are specifically linked to declines in both processing speed and working memory.",
      "key_distinction": "Fluid = novel reasoning, declines with age; Crystallized = accumulated knowledge, maintained with age.",
      "commonly_confused_because": "Students may assume all intelligence declines uniformly with age; the differential aging of Gf vs. Gc is a key EPPP distinction.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_002",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Stanford-Binet and Wechsler Tests",
      "item_x": "Verbal Comprehension Index (VCI)",
      "item_y": "Perceptual Reasoning Index (PRI)",
      "question": "What is the difference between the Verbal Comprehension Index (VCI) and the Perceptual Reasoning Index (PRI) on the WAIS-IV?",
      "answer": "The Verbal Comprehension Index (VCI) measures verbal abilities including vocabulary knowledge, verbal concept formation, abstract verbal reasoning, and general acquired knowledge (e.g., Similarities, Vocabulary, Information subtests). It primarily draws on crystallized intelligence (Gc). The Perceptual Reasoning Index (PRI) measures visual-spatial reasoning, non-verbal reasoning, and fluid intelligence applied to visual tasks (e.g., Block Design, Matrix Reasoning, Visual Puzzles). It draws more on fluid intelligence (Gf). Older adults with Alzheimer's disease typically score highest on VCI (crystallized knowledge preserved) and lowest on PSI (processing speed declines first).",
      "key_distinction": "VCI = verbal/crystallized intelligence; PRI = visual-spatial/fluid reasoning.",
      "commonly_confused_because": "Both are WAIS-IV composite scores contributing to FSIQ; students may conflate verbal ability with overall intelligence.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_003",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "MMPI-2",
      "item_x": "L scale (Lie)",
      "item_y": "K scale (Correction)",
      "question": "What is the difference between the L scale and K scale on the MMPI-2?",
      "answer": "The L (Lie) scale is composed of items that most people endorse occasionally but that overly virtuous or naive respondents deny. High L scores suggest an unsophisticated attempt to present oneself in an unrealistically positive light — denying common human faults. It is an obvious, transparent measure of faking good. The K (Correction) scale measures more subtle psychological defensiveness. High K scores indicate a more sophisticated tendency to minimize psychological problems and present oneself as well-adjusted without endorsing obviously virtuous items. K scores are sometimes used as a correction factor added to several clinical scales to account for this defensiveness. Both indicate faking good; L is transparent, K is subtle.",
      "key_distinction": "L = obvious/naive defensiveness (denying minor faults); K = subtle/sophisticated defensiveness (minimizing psychological problems).",
      "commonly_confused_because": "Both are validity scales suggesting favorable self-presentation; students confuse the sophistication level and mechanism of each.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_004",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "MMPI-2",
      "item_x": "Faking good MMPI-2 profile",
      "item_y": "Faking bad (malingering) MMPI-2 profile",
      "question": "What is the difference between faking good and faking bad patterns on the MMPI-2?",
      "answer": "A faking good (defensive) profile on the MMPI-2 is characterized by a V-shaped validity scale pattern: high L and K scores (denying faults, appearing very well-adjusted) with a low F score (not endorsing unusual or pathological items). Clinical scales are often suppressed. This pattern suggests the person is minimizing or denying psychological difficulties. A faking bad (malingering) profile shows the opposite: an elevated F scale (endorsing many infrequent, pathological items, presenting more problems than are likely real) with low L and K scales. Clinical scales are often highly elevated. This pattern suggests exaggeration of pathology, seen in forensic contexts.",
      "key_distinction": "Faking good = high L + high K + low F (defensive, denying problems); Faking bad = high F + low L + low K (exaggerating pathology).",
      "commonly_confused_because": "Students confuse which validity scale pattern corresponds to which type of dissimulation; the V-shape (good) vs. inverted-V or flat (bad) patterns are testable.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_005",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Other Measures of Personality",
      "item_x": "Projective personality tests",
      "item_y": "Objective (self-report) personality tests",
      "question": "What is the difference between projective and objective personality assessment approaches?",
      "answer": "Projective tests (e.g., Rorschach, TAT) present ambiguous stimuli (inkblots, pictures) and ask clients to respond freely. The assumption is that individuals project their unconscious conflicts, needs, and personality onto ambiguous material. They are theoretically rich but have lower reliability and validity than objective tests, and are more subject to examiner subjectivity and interpretation. Objective tests (e.g., MMPI-2, NEO-PI-3) use structured, standardized items with fixed response formats (true/false, Likert). They have strong normative data, known reliability and validity coefficients, and less examiner subjectivity. They are susceptible to response sets (social desirability, malingering), hence validity scales.",
      "key_distinction": "Projective = ambiguous stimuli, free response, unconscious projection; Objective = structured items, standardized scoring, quantifiable.",
      "commonly_confused_because": "Students may assume projective tests are more accurate because they are less controllable; objective tests generally have stronger psychometric properties.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_006",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Other Measures of Personality",
      "item_x": "Rorschach Inkblot Test",
      "item_y": "Thematic Apperception Test (TAT)",
      "question": "What is the difference between the Rorschach and the TAT as projective assessment tools?",
      "answer": "The Rorschach consists of 10 inkblot cards; clients describe what they see in each image. It is scored using Exner's Comprehensive System (or newer RO-PAX), examining location, determinants (color, shading, movement), form quality, content, and popular responses. It provides information about perceptual and cognitive processing, as well as personality variables. The TAT (Thematic Apperception Test, Murray) presents 30+ ambiguous pictures of interpersonal situations and asks the client to tell a story about each. It is interpreted through the lens of Murray's need-press theory, examining needs (achievement, power, affiliation) and environmental press. TAT is more focused on interpersonal themes and narrative content.",
      "key_distinction": "Rorschach = ambiguous inkblots, structured scoring system, perceptual/cognitive focus; TAT = interpersonal pictures, narrative stories, need-press theory.",
      "commonly_confused_because": "Both are projective tests relying on ambiguous stimuli; their theoretical basis, stimuli, and scoring approaches differ substantially.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_007",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Other Measures of Cognitive Ability",
      "item_x": "Culture-reduced tests",
      "item_y": "Culture-free tests",
      "question": "What is the difference between a culture-reduced and a culture-free test of intelligence?",
      "answer": "A culture-free test would be entirely unaffected by cultural experience, language, or values — theoretically eliminating all cultural bias. In practice, no test can be truly culture-free because even visual pattern recognition and spatial reasoning are shaped by cultural and educational experience. A culture-reduced test is designed to minimize (but not eliminate) cultural loading by using non-verbal, visual tasks that are less dependent on specific language or academic knowledge. Examples: Raven's Progressive Matrices, Leiter-3. These tests reduce — but cannot eliminate — cultural influences, particularly for speed, testtaking behavior, and motivation.",
      "key_distinction": "Culture-free = theoretically zero cultural influence (does not exist in practice); Culture-reduced = minimized cultural influence through non-verbal tasks.",
      "commonly_confused_because": "Students assume non-verbal IQ tests are culture-free; the distinction between reducing vs. eliminating cultural influence is important.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_008",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Interest Inventories",
      "item_x": "Empirical criterion keying",
      "item_y": "Rational/content-based test construction",
      "question": "What is the difference between empirical criterion keying and rational (content) methods of test development?",
      "answer": "Empirical criterion keying selects items based on their statistical ability to differentiate between criterion groups, regardless of whether the items appear theoretically relevant to the construct. Items are included because they WORK, not because they make conceptual sense. Examples: MMPI-2 clinical scales (items that distinguished psychiatric from normal groups), Strong Interest Inventory Occupational Scales. Rational (content) construction selects items based on theory, expert judgment, and face/content relevance to the construct. Items are chosen because they logically represent the construct. Example: Early versions of intelligence tests, many self-report personality inventories. Content method results in tests with higher face validity but not always better predictive validity.",
      "key_distinction": "Empirical keying = items chosen by statistical group discrimination (may lack face validity); Rational = items chosen by theory/content relevance (higher face validity).",
      "commonly_confused_because": "Students assume that tests need to look like what they measure; empirical keying shows that face validity and actual validity can diverge.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_009",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Clinical Tests",
      "item_x": "Structured clinical interview",
      "item_y": "Unstructured clinical interview",
      "question": "What is the difference between a structured and an unstructured clinical interview?",
      "answer": "A structured clinical interview consists of predetermined, standardized questions asked in a fixed order, with specific probes and scoring criteria for each response. Examples include the SCID-5 (Structured Clinical Interview for DSM-5 Disorders) and the MINI International Neuropsychiatric Interview. Structured interviews maximize interrater reliability and diagnostic consistency because every examiner follows the same protocol, making them the gold standard for research and epidemiological studies. An unstructured clinical interview has no fixed format — the clinician follows the client's lead, explores areas of clinical interest, and adapts questions to the individual. It allows for flexibility and rapport building but produces lower interrater reliability because findings depend heavily on the interviewer's skill, training, and judgment. Semi-structured interviews (e.g., CAPS-5) fall between these extremes, providing a guiding framework with room for clinical follow-up.",
      "key_distinction": "Structured = standardized questions, fixed order, high interrater reliability, preferred for research; Unstructured = flexible, clinician-guided, higher rapport but lower reliability.",
      "commonly_confused_because": "Students may assume all clinical interviews are equivalent in reliability; the structured format is specifically designed to improve diagnostic consistency across examiners.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_010",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Clinical Tests",
      "item_x": "Performance Validity Test (PVT)",
      "item_y": "Symptom Validity Test (SVT)",
      "question": "What is the difference between a Performance Validity Test (PVT) and a Symptom Validity Test (SVT)?",
      "answer": "A Performance Validity Test (PVT) is a measure administered alongside cognitive tests to detect invalid effort or feigned cognitive impairment. PVTs typically use forced-choice formats where chance performance is 50%; scores at or below chance indicate deliberate failure rather than genuine impairment. Examples include the Test of Memory Malingering (TOMM) and the Word Memory Test (WMT). A Symptom Validity Test (SVT) is a self-report measure that evaluates whether a client's reported psychiatric or somatic symptoms are credible and consistent with known clinical presentations. SVTs detect over-reporting or exaggeration of symptoms rather than cognitive effort. Examples include the SIMS (Structured Inventory of Malingered Symptomatology) and select MMPI-2 validity scales (e.g., F, Fp, FBS-r). PVTs assess the validity of cognitive test performance; SVTs assess the validity of reported symptom complaints.",
      "key_distinction": "PVT = detects feigned cognitive impairment through effort testing; SVT = detects exaggerated or fabricated psychiatric symptom reporting.",
      "commonly_confused_because": "Both are used in forensic and neuropsychological contexts to assess response validity and detect malingering; students conflate them because both serve a similar purpose but target different domains (cognitive performance vs. symptom reporting).",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_011",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Other Measures of Cognitive Ability",
      "item_x": "Cattell-Horn-Carroll (CHC) model",
      "item_y": "Spearman's g (general factor theory)",
      "question": "What is the difference between the Cattell-Horn-Carroll (CHC) model of intelligence and Spearman's g?",
      "answer": "Spearman's g (general factor) theory proposes that a single underlying general intelligence factor — g — explains the positive correlations observed among all cognitive ability tests. Spearman derived g using factor analysis and argued it represents a general mental capacity that pervades all intellectual performance. The model is hierarchical in the simplest sense: one factor at the apex. The Cattell-Horn-Carroll (CHC) model is a comprehensive three-stratum hierarchical model that integrates Spearman's g at Stratum III, Horn and Cattell's fluid-crystallized theory at Stratum II (including Gf, Gc, Gv, Gsm, Gs, and approximately eight other broad abilities), and Carroll's factor-analytic work identifying narrow cognitive abilities at Stratum I. CHC retains g but adds substantial structure below it. Most modern cognitive batteries — including the SB5, WAIS-IV, WJ-IV, and KABC-II — are explicitly grounded in CHC theory rather than the simpler g model.",
      "key_distinction": "Spearman's g = single general factor explaining all cognitive variance; CHC = three-stratum hierarchy with g at the top, multiple broad abilities in the middle, and many narrow abilities at the base.",
      "commonly_confused_because": "CHC incorporates g as its apex, so students assume the models are interchangeable; CHC adds the crucial middle stratum of broad abilities (Gf, Gc, Gs, Gv, etc.) that Spearman's model does not differentiate.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    },
    {
      "id": "PAS_CONT_012",
      "domain_code": "CASS",
      "domain_name": "Clinical Assessment & Interpretation",
      "subdomain": "Interest Inventories",
      "item_x": "Strong Interest Inventory (SII)",
      "item_y": "Holland's Self-Directed Search (SDS)",
      "question": "What is the difference between the Strong Interest Inventory (SII) and Holland's Self-Directed Search (SDS)?",
      "answer": "The Strong Interest Inventory (SII) is a comprehensive, professionally administered interest inventory that compares an individual's interests against those of people in specific occupations using empirically derived Occupational Scales developed through empirical criterion keying. It also produces scores on Holland's six RIASEC General Occupational Themes and Basic Interest Scales. The SII requires professional administration and interpretation and generates detailed normative comparison reports. Holland's Self-Directed Search (SDS) is a brief, self-administered and self-scored inventory built directly around Holland's RIASEC hexagonal model (Realistic, Investigative, Artistic, Social, Enterprising, Conventional). It yields a three-letter Holland code that the test-taker uses independently to identify compatible occupational environments from an accompanying Occupations Finder. The SDS is designed for use without a professional interpreter, making it more accessible for large-scale career counseling and educational settings. Both instruments are grounded in Holland's RIASEC theory, but the SII provides far richer normative and occupational comparison data.",
      "key_distinction": "SII = professionally administered, empirically derived occupational scales, detailed normative comparisons; SDS = self-administered and self-scored, directly derives Holland RIASEC code for independent career exploration.",
      "commonly_confused_because": "Both instruments use Holland's RIASEC framework and produce Holland codes; students conflate them because they share the same theoretical model, overlooking the differences in administration, scoring, and interpretive depth.",
      "legacy_domain_code": "PAS",
      "legacy_domain_name": "Psychological Assessment"
    }
  ]
}