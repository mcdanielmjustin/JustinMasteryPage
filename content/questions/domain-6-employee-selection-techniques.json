{
  "domain": "Domain 6: Workforce Development & Leadership",
  "subdomain": "Employee Selection - Techniques",
  "totalQuestions": 26,
  "questionTypes": {
    "singleChoice": 5,
    "multipleChoice": 5,
    "matrixSingle": 2,
    "matrixMultiple": 2,
    "clozeDropdown": 2,
    "bowtie": 2,
    "highlight": 2,
    "dragDropOrdered": 2,
    "trendAnalysis": 2,
    "dragDropCategorize": 2
  },
  "questions": [
    {
      "type": "single-choice",
      "id": "d6-st-sc-001",
      "stem": "In a structured behavioral interview, candidates are asked to describe specific past situations, actions taken, and outcomes. This approach is based on the premise that:",
      "options": [
        {"id": "A", "text": "Future behavior is best predicted by intentions and goals"},
        {"id": "B", "text": "Past behavior in similar situations is the best predictor of future behavior"},
        {"id": "C", "text": "Hypothetical responses reveal unconscious motivations"},
        {"id": "D", "text": "Self-report of personality traits is more valid than behavioral evidence"}
      ],
      "correctAnswer": "B",
      "explanation": "Behavioral (or behavioral description) interviews are based on the behavioral consistency principle—past behavior is the best predictor of future behavior. Candidates describe specific situations they encountered (Situation), actions they took (Action), and results they achieved (Result)—often called the STAR method. This contrasts with situational interviews, which present hypothetical scenarios and ask what the candidate would do. Both structured formats show higher validity than unstructured interviews, but the behavioral approach draws directly on actual experience rather than hypothetical intent.",
      "difficulty": "hard",
      "tags": ["behavioral interview", "structured interview", "STAR method"]
    },
    {
      "type": "single-choice",
      "id": "d6-st-sc-002",
      "stem": "An assessment center typically includes multiple exercises to evaluate candidates. Which characteristic is MOST essential to the construct validity of an assessment center?",
      "options": [
        {"id": "A", "text": "Use of a single exercise to measure all dimensions"},
        {"id": "B", "text": "Multiple assessors rating candidates across multiple exercises measuring multiple dimensions"},
        {"id": "C", "text": "Exclusive reliance on paper-and-pencil tests"},
        {"id": "D", "text": "Use of only one trained assessor to ensure rating consistency"}
      ],
      "correctAnswer": "B",
      "explanation": "The hallmark of assessment centers is the multiple assessment principle: multiple exercises (e.g., in-basket, leaderless group discussion, role play), multiple assessors, and multiple dimensions. This triangulation approach provides convergent evidence. However, research on assessment center construct validity has revealed a 'construct validity paradox'—dimension ratings tend to correlate more highly within exercises (exercise effects) than across exercises measuring the same dimension (convergent validity). Despite this, assessment centers show good criterion-related validity for predicting managerial success.",
      "difficulty": "hard",
      "tags": ["assessment center", "construct validity", "multiple assessment"]
    },
    {
      "type": "single-choice",
      "id": "d6-st-sc-003",
      "stem": "The in-basket exercise, commonly used in assessment centers and selection, evaluates a candidate's ability to:",
      "options": [
        {"id": "A", "text": "Perform physical tasks under timed conditions"},
        {"id": "B", "text": "Prioritize, delegate, organize, and make decisions about accumulated correspondence and memos"},
        {"id": "C", "text": "Lead a group discussion without an assigned leader"},
        {"id": "D", "text": "Role-play a customer service interaction"}
      ],
      "correctAnswer": "B",
      "explanation": "The in-basket exercise simulates the accumulated work that a manager might find in their inbox. Candidates must prioritize items, delegate tasks, make decisions, and organize responses within a time limit. It assesses administrative skills, planning, problem-solving, delegation, and judgment. Leaderless group discussions (C) assess leadership and interpersonal skills. Role plays (D) assess interpersonal and communication skills. Physical tasks (A) are work sample tests for physical jobs, not typical assessment center exercises.",
      "difficulty": "hard",
      "tags": ["in-basket", "assessment center", "administrative skills"]
    },
    {
      "type": "single-choice",
      "id": "d6-st-sc-004",
      "stem": "Biodata (biographical data) instruments used in selection are characterized by:",
      "options": [
        {"id": "A", "text": "Items that assess only demographic information such as age and race"},
        {"id": "B", "text": "Empirically keyed items reflecting past behaviors, experiences, and interests that predict job criteria"},
        {"id": "C", "text": "Projective questions that assess unconscious motivations"},
        {"id": "D", "text": "Open-ended essay questions about life history"}
      ],
      "correctAnswer": "B",
      "explanation": "Biodata instruments use empirically keyed items about past behaviors, experiences, interests, and achievements. Items are selected based on their empirical relationship to job criteria (criterion keying) rather than face validity or theoretical rationale. Effective biodata items are verifiable, non-invasive, and job-related. They do not focus on protected demographic characteristics (A), which would be illegal in selection. Meta-analyses show biodata has good validity (r ~ .35) and can add incremental validity beyond cognitive ability. The concern is that empirical keying without theoretical rationale can lead to items that may not generalize or may inadvertently discriminate.",
      "difficulty": "hard",
      "tags": ["biodata", "empirical keying", "selection technique"]
    },
    {
      "type": "single-choice",
      "id": "d6-st-sc-005",
      "stem": "Situational judgment tests (SJTs) present candidates with hypothetical work scenarios and ask them to select the best course of action. Research has shown that SJTs:",
      "options": [
        {"id": "A", "text": "Have zero validity for predicting job performance"},
        {"id": "B", "text": "Show less adverse impact than cognitive ability tests and add incremental validity beyond cognitive ability and personality"},
        {"id": "C", "text": "Are identical to cognitive ability tests in terms of what they measure"},
        {"id": "D", "text": "Have high validity but produce more adverse impact than any other selection method"}
      ],
      "correctAnswer": "B",
      "explanation": "Research on SJTs shows: moderate validity (r ~ .26-.34), less adverse impact against minorities than cognitive ability tests, and incremental validity beyond both cognitive ability and personality measures. SJTs appear to measure a blend of constructs including practical judgment, job knowledge, and interpersonal skills. The knowledge-based response format ('what should you do?') tends to correlate more with cognitive ability, while the behavioral tendency format ('what would you do?') correlates more with personality measures. SJTs are versatile and increasingly used in high-stakes selection.",
      "difficulty": "hard",
      "tags": ["SJT", "situational judgment test", "adverse impact", "incremental validity"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-st-mc-001",
      "stem": "Which of the following are recognized advantages of structured interviews over unstructured interviews for employee selection? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Higher criterion-related validity"},
        {"id": "B", "text": "Better legal defensibility due to standardization"},
        {"id": "C", "text": "Reduced vulnerability to interviewer biases (e.g., first impression, halo)"},
        {"id": "D", "text": "Higher inter-rater reliability due to anchored rating scales"},
        {"id": "E", "text": "Greater ability to explore candidates' unconscious motivations"}
      ],
      "correctAnswers": ["A", "B", "C", "D"],
      "explanation": "Structured interviews outperform unstructured interviews in several ways: higher validity (~.51 vs. .38 in meta-analyses) (A); better legal defensibility because standardized questions based on job analysis are more defensible than idiosyncratic questioning (B); reduced biases because consistent questions limit irrelevant factors like first impressions (C); higher inter-rater reliability because anchored rating scales (BARS or BOS) guide ratings (D). Exploring unconscious motivations (E) is not a goal of structured selection interviews—this relates more to psychodynamic clinical interviews.",
      "difficulty": "hard",
      "tags": ["structured interview", "unstructured interview", "interview validity", "bias reduction"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-st-mc-002",
      "stem": "Assessment center exercises commonly include which of the following? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "In-basket exercise"},
        {"id": "B", "text": "Leaderless group discussion"},
        {"id": "C", "text": "Oral presentation or case analysis"},
        {"id": "D", "text": "Role play (e.g., coaching a subordinate)"},
        {"id": "E", "text": "Polygraph examination"}
      ],
      "correctAnswers": ["A", "B", "C", "D"],
      "explanation": "Standard assessment center exercises include: In-basket (administrative decision-making and prioritization), leaderless group discussion (leadership, teamwork, communication), oral presentations/case analyses (analytical and communication skills), and role plays (interpersonal skills, coaching ability). The Employee Polygraph Protection Act (1988) prohibits most private employers from using polygraph examinations (E) for pre-employment screening, and polygraphs are not a recognized assessment center exercise. Assessment centers may also include fact-finding exercises and scheduling/planning exercises.",
      "difficulty": "hard",
      "tags": ["assessment center", "exercises", "in-basket", "leaderless group"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-st-mc-003",
      "stem": "Integrity tests used in employee selection can be categorized as overt or personality-based. Which statements about integrity tests are supported by research? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Overt integrity tests directly ask about attitudes toward dishonesty and past dishonest behavior"},
        {"id": "B", "text": "Personality-based integrity tests measure constructs related to conscientiousness, agreeableness, and emotional stability"},
        {"id": "C", "text": "Meta-analyses show integrity tests have moderate validity for predicting counterproductive work behaviors"},
        {"id": "D", "text": "Integrity tests violate the Employee Polygraph Protection Act"},
        {"id": "E", "text": "Integrity tests tend to produce less adverse impact than cognitive ability tests"}
      ],
      "correctAnswers": ["A", "B", "C", "E"],
      "explanation": "Overt integrity tests (e.g., Reid Report) directly assess attitudes toward theft and dishonesty and admissions of past behavior (A). Personality-based tests (e.g., Hogan Reliability Scale) measure broader personality constructs like conscientiousness, agreeableness, and emotional stability (B). Meta-analyses by Ones et al. found moderate validity (~.34-.41) for predicting CWBs and broader job performance (C). Integrity tests produce less adverse impact than cognitive tests (E) because they do not show large racial group mean differences. They do NOT violate EPPA (D)—EPPA specifically exempts paper-and-pencil honesty tests from its restrictions.",
      "difficulty": "hard",
      "tags": ["integrity tests", "overt", "personality-based", "CWB prediction"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-st-mc-004",
      "stem": "Work sample tests are considered high-fidelity selection methods. Which of the following are characteristics of work sample tests? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "They require candidates to perform tasks that closely simulate actual job duties"},
        {"id": "B", "text": "They typically show high criterion-related validity (r ~ .54 in meta-analyses)"},
        {"id": "C", "text": "They tend to show less adverse impact than cognitive ability tests"},
        {"id": "D", "text": "They are inexpensive to develop and administer across all job types"},
        {"id": "E", "text": "They have high face validity and are generally perceived as fair by candidates"}
      ],
      "correctAnswers": ["A", "B", "C", "E"],
      "explanation": "Work sample tests have candidates perform actual job tasks under standardized conditions (A). Schmidt and Hunter found them to be among the most valid predictors (r ~ .54) (B). They show less adverse impact because performance is job-specific rather than measuring abstract ability (C). They are perceived as fair because the connection to the job is obvious (high face validity) (E). However, they are NOT inexpensive (D)—they require significant resources to develop, may need specialized equipment, and are most practical for jobs with observable, standardized tasks. They are less feasible for complex knowledge work or managerial positions.",
      "difficulty": "hard",
      "tags": ["work sample tests", "high fidelity", "selection validity", "face validity"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-st-mc-005",
      "stem": "Which of the following are recognized concerns about using personality tests (e.g., Big Five measures) in employee selection? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Susceptibility to faking or socially desirable responding"},
        {"id": "B", "text": "Modest criterion-related validity for individual traits when used alone"},
        {"id": "C", "text": "Questions about whether broad trait measures are optimal versus narrower facet-level measures"},
        {"id": "D", "text": "Complete inability to predict any work-related outcomes"},
        {"id": "E", "text": "Legal concerns when personality measures assess constructs related to mental health conditions"}
      ],
      "correctAnswers": ["A", "B", "C", "E"],
      "explanation": "Personality test concerns include: faking vulnerability, especially in high-stakes selection (A); modest validity for individual traits (conscientiousness r ~ .22-.31 is the strongest predictor) (B); the bandwidth-fidelity debate about whether broad traits or narrow facets are better predictors for specific criteria (C); and ADA concerns if personality items assess psychopathology-related constructs—tests designed to diagnose mental conditions may qualify as medical examinations under the ADA (E). Personality tests do NOT have zero validity (D)—they predict meaningful outcomes and add incremental validity beyond cognitive ability.",
      "difficulty": "hard",
      "tags": ["personality testing", "Big Five", "faking", "bandwidth-fidelity"]
    },
    {
      "type": "matrix-single",
      "id": "d6-st-ms-001",
      "stem": "Match each selection technique to its primary construct or skill being assessed:",
      "rows": [
        {"id": "row1", "text": "Cognitive ability test"},
        {"id": "row2", "text": "In-basket exercise"},
        {"id": "row3", "text": "Leaderless group discussion"},
        {"id": "row4", "text": "Situational judgment test"}
      ],
      "columns": [
        {"id": "col1", "text": "General mental ability and reasoning"},
        {"id": "col2", "text": "Administrative decision-making and prioritization"},
        {"id": "col3", "text": "Leadership emergence and interpersonal effectiveness"},
        {"id": "col4", "text": "Practical judgment and procedural knowledge"}
      ],
      "correctAnswers": {
        "row1": "col1",
        "row2": "col2",
        "row3": "col3",
        "row4": "col4"
      },
      "explanation": "Cognitive ability tests primarily assess g (general mental ability), including reasoning, verbal comprehension, and quantitative ability. In-basket exercises assess managerial competencies like prioritizing, delegating, organizing, and making decisions about administrative matters. Leaderless group discussions reveal leadership emergence, persuasion, collaboration, and communication skills in a group setting. SJTs assess practical judgment—knowledge of appropriate responses to workplace situations—which is a blend of cognitive ability, personality, and job knowledge.",
      "difficulty": "hard",
      "tags": ["selection techniques", "constructs assessed", "assessment methods"]
    },
    {
      "type": "matrix-single",
      "id": "d6-st-ms-002",
      "stem": "Match each interview format to its defining characteristic:",
      "rows": [
        {"id": "row1", "text": "Behavioral interview"},
        {"id": "row2", "text": "Situational interview"},
        {"id": "row3", "text": "Unstructured interview"},
        {"id": "row4", "text": "Panel interview"}
      ],
      "columns": [
        {"id": "col1", "text": "Asks candidates to describe past behavior in specific situations"},
        {"id": "col2", "text": "Presents hypothetical scenarios and asks what the candidate would do"},
        {"id": "col3", "text": "No predetermined questions; interviewer follows up on candidate responses freely"},
        {"id": "col4", "text": "Multiple interviewers simultaneously evaluate the candidate"}
      ],
      "correctAnswers": {
        "row1": "col1",
        "row2": "col2",
        "row3": "col3",
        "row4": "col4"
      },
      "explanation": "Behavioral interviews (Janz, 1982) ask candidates to describe specific past examples (STAR format: Situation, Task, Action, Result). Situational interviews (Latham et al., 1980) use critical incident-based hypothetical scenarios with scoring guides. Unstructured interviews have no standardized questions—the interviewer improvises, which reduces reliability and validity. Panel interviews involve multiple interviewers (often 3-5) who independently rate the candidate, increasing reliability through multiple perspectives and reducing individual biases.",
      "difficulty": "hard",
      "tags": ["interview formats", "behavioral", "situational", "panel"]
    },
    {
      "type": "matrix-multiple",
      "id": "d6-st-mm-001",
      "stem": "For each selection method, select ALL psychometric properties that research has consistently demonstrated:",
      "rows": [
        {"id": "row1", "text": "Cognitive ability tests"},
        {"id": "row2", "text": "Work sample tests"},
        {"id": "row3", "text": "Personality tests (conscientiousness)"}
      ],
      "columns": [
        {"id": "col1", "text": "High criterion validity (r > .40)"},
        {"id": "col2", "text": "Low adverse impact"},
        {"id": "col3", "text": "High face validity"},
        {"id": "col4", "text": "Good validity generalization"},
        {"id": "col5", "text": "Vulnerable to faking"}
      ],
      "correctAnswers": {
        "row1": ["col1", "col4"],
        "row2": ["col1", "col2", "col3"],
        "row3": ["col2", "col5"]
      },
      "explanation": "Cognitive ability tests: high validity (~.51) and excellent validity generalization, but produce adverse impact and have mixed face validity. Work sample tests: high validity (~.54), less adverse impact than cognitive tests, and high face validity because they mirror job tasks. Personality (conscientiousness): lower validity (~.22-.31, so not >. 40), low adverse impact (small group differences), and vulnerable to faking in high-stakes settings. Note: Cognitive tests show good validity generalization (col4); work samples are more job-specific and less generalizable.",
      "difficulty": "hard",
      "tags": ["selection methods comparison", "psychometric properties", "validity", "adverse impact"]
    },
    {
      "type": "matrix-multiple",
      "id": "d6-st-mm-002",
      "stem": "For each type of interview bias, select ALL strategies that effectively mitigate it:",
      "rows": [
        {"id": "row1", "text": "First impression bias (primacy)"},
        {"id": "row2", "text": "Halo effect"},
        {"id": "row3", "text": "Similar-to-me bias"}
      ],
      "columns": [
        {"id": "col1", "text": "Use standardized questions for all candidates"},
        {"id": "col2", "text": "Train interviewers to rate dimensions independently"},
        {"id": "col3", "text": "Use behaviorally anchored rating scales"},
        {"id": "col4", "text": "Delay global evaluations until end of interview"},
        {"id": "col5", "text": "Use diverse interview panels"}
      ],
      "correctAnswers": {
        "row1": ["col1", "col4"],
        "row2": ["col2", "col3"],
        "row3": ["col1", "col5"]
      },
      "explanation": "First impression bias is mitigated by standardized questions (preventing initial impressions from guiding follow-ups) and delaying judgments until all information is gathered. Halo effect (one dimension coloring all ratings) is mitigated by training raters to evaluate dimensions independently and using BARS to anchor each dimension separately. Similar-to-me bias (favoring candidates who resemble the interviewer) is mitigated by standardized questions (reducing informal bonding) and diverse panels (ensuring no single demographic perspective dominates). Multiple strategies can address multiple biases.",
      "difficulty": "hard",
      "tags": ["interview bias", "primacy", "halo", "similar-to-me", "mitigation"]
    },
    {
      "type": "cloze-dropdown",
      "id": "d6-st-cd-001",
      "stem": "Complete the passage about assessment center methodology:\n\nAssessment centers were originally developed by the [BLANK1] during World War II for selecting intelligence operatives. The landmark [BLANK2] study at AT&T demonstrated their long-term predictive validity for managerial advancement. A key finding from construct validity research is the [BLANK3], which refers to the finding that dimension ratings correlate more highly within exercises than across exercises measuring the same dimension. Assessment center ratings are typically integrated through a [BLANK4] process where assessors discuss and reconcile their ratings. The International Task Force on Assessment Center Guidelines requires a minimum of [BLANK5] simulation exercises.",
      "blanks": {
        "BLANK1": {
          "options": ["U.S. Army", "OSS (Office of Strategic Services)", "British Navy", "FBI"],
          "correct": "OSS (Office of Strategic Services)"
        },
        "BLANK2": {
          "options": ["Hawthorne", "Management Progress", "Western Electric", "Framingham"],
          "correct": "Management Progress"
        },
        "BLANK3": {
          "options": ["criterion problem", "exercise effect (construct validity paradox)", "halo effect", "range restriction problem"],
          "correct": "exercise effect (construct validity paradox)"
        },
        "BLANK4": {
          "options": ["statistical averaging", "consensus discussion (integration)", "individual ranking", "random assignment"],
          "correct": "consensus discussion (integration)"
        },
        "BLANK5": {
          "options": ["one", "two", "three", "five"],
          "correct": "two"
        }
      },
      "explanation": "Assessment centers originated with the OSS in WWII. Bray's Management Progress Study at AT&T (1956-1966) provided landmark longitudinal validation. The exercise effect (construct validity paradox) is a major challenge—assessors' ratings are more consistent within an exercise than within a dimension across exercises, suggesting exercise-specific variance dominates. Integration typically involves consensus discussion among assessors. The International Guidelines require at least two job-related simulation exercises, multiple assessors, and multiple dimensions.",
      "difficulty": "hard",
      "tags": ["assessment center", "history", "exercise effect", "construct validity paradox"]
    },
    {
      "type": "cloze-dropdown",
      "id": "d6-st-cd-002",
      "stem": "Complete the passage about cognitive ability testing in selection:\n\nCognitive ability tests measure [BLANK1], which Spearman termed the 'g factor.' Meta-analyses by Schmidt and Hunter found that GMA tests have a mean validity of approximately [BLANK2] for predicting job performance. The validity of GMA tests tends to be [BLANK3] for more complex jobs. A major concern is that GMA tests produce approximately [BLANK4] standard deviation difference in mean scores between Black and White test-takers. The [BLANK5] refers to the tension between maximizing selection validity and minimizing adverse impact.",
      "blanks": {
        "BLANK1": {
          "options": ["physical strength", "general mental ability", "personality traits", "emotional intelligence"],
          "correct": "general mental ability"
        },
        "BLANK2": {
          "options": [".10", ".25", ".51", ".85"],
          "correct": ".51"
        },
        "BLANK3": {
          "options": ["lower", "higher", "identical", "irrelevant"],
          "correct": "higher"
        },
        "BLANK4": {
          "options": ["0.1", "0.5", "1.0", "2.0"],
          "correct": "1.0"
        },
        "BLANK5": {
          "options": ["validity-diversity dilemma", "criterion problem", "bandwidth-fidelity dilemma", "base rate paradox"],
          "correct": "validity-diversity dilemma"
        }
      },
      "explanation": "GMA tests measure Spearman's g factor—general cognitive ability. Meta-analytic validity is approximately .51. Validity increases with job complexity because more complex jobs have greater cognitive demands (Hunter & Hunter, 1984). The approximately 1.0 SD Black-White mean difference is well-documented and produces adverse impact when used in top-down selection. The validity-diversity dilemma (or diversity-validity tradeoff) describes the tension between using the most valid predictors (which tend to produce adverse impact) and achieving a diverse workforce.",
      "difficulty": "hard",
      "tags": ["cognitive ability", "GMA", "g factor", "adverse impact", "diversity-validity dilemma"]
    },
    {
      "type": "bowtie",
      "id": "d6-st-bt-001",
      "stem": "An organization is selecting candidates for a senior project manager role. They use a panel interview where three interviewers independently ask different unrelated questions, form their own impressions, and then average their ratings.",
      "centerQuestion": "What is the primary methodological concern with this approach?",
      "leftColumn": {
        "label": "Problems with This Approach",
        "options": [
          {"id": "L1", "text": "Each interviewer assesses different content, reducing standardization"},
          {"id": "L2", "text": "Independent question sets prevent meaningful comparison across candidates"},
          {"id": "L3", "text": "Averaging non-standardized ratings does not correct for the lack of structure"},
          {"id": "L4", "text": "Using three interviewers automatically eliminates all bias"}
        ],
        "correctAnswers": ["L1", "L2", "L3"]
      },
      "rightColumn": {
        "label": "Recommended Improvements",
        "options": [
          {"id": "R1", "text": "Develop standardized questions from job analysis for all interviewers"},
          {"id": "R2", "text": "Create behaviorally anchored rating scales for each competency"},
          {"id": "R3", "text": "Have all candidates answer the same questions to enable comparison"},
          {"id": "R4", "text": "Reduce to one interviewer to eliminate disagreement"}
        ],
        "correctAnswers": ["R1", "R2", "R3"]
      },
      "centerOptions": [
        {"id": "C1", "text": "Lack of standardization despite multiple interviewers (structured content is missing)"},
        {"id": "C2", "text": "Too many interviewers creating cognitive overload"},
        {"id": "C3", "text": "The panel format inherently reduces validity"},
        {"id": "C4", "text": "Averaging is statistically inappropriate for ratings"}
      ],
      "correctCenter": "C1",
      "explanation": "Having multiple interviewers (a panel) is beneficial, but without standardized questions and rating scales, the approach remains essentially unstructured. Structure has two components: (1) standardized content (same job-related questions for all candidates) and (2) standardized evaluation (anchored rating scales). This panel has neither—each interviewer asks different questions and forms idiosyncratic impressions. Simply averaging unstructured ratings does not overcome the fundamental lack of standardization. Improvements should focus on structured content (job-analysis-based questions) and structured evaluation (BARS).",
      "difficulty": "hard",
      "tags": ["structured interview", "panel interview", "standardization", "interview design"]
    },
    {
      "type": "bowtie",
      "id": "d6-st-bt-002",
      "stem": "A technology company uses a take-home coding project as its primary selection method. Candidates are given 48 hours to complete a project at home and submit their code for evaluation by a hiring committee.",
      "centerQuestion": "What type of selection method does this represent, and what is a key limitation?",
      "leftColumn": {
        "label": "Advantages of This Approach",
        "options": [
          {"id": "L1", "text": "High face validity—directly samples job-relevant skills"},
          {"id": "L2", "text": "Candidates can demonstrate technical competence in a realistic context"},
          {"id": "L3", "text": "Reduced test anxiety compared to timed in-person tests"},
          {"id": "L4", "text": "Guaranteed elimination of all group differences in performance"}
        ],
        "correctAnswers": ["L1", "L2", "L3"]
      },
      "rightColumn": {
        "label": "Key Limitations",
        "options": [
          {"id": "R1", "text": "Lack of proctoring means the candidate's identity and independent work cannot be verified"},
          {"id": "R2", "text": "May disadvantage candidates with caregiving responsibilities or other time constraints"},
          {"id": "R3", "text": "Scoring criteria must be standardized across evaluators to ensure inter-rater reliability"},
          {"id": "R4", "text": "The test has zero content validity for technical roles"}
        ],
        "correctAnswers": ["R1", "R2", "R3"]
      },
      "centerOptions": [
        {"id": "C1", "text": "Work sample test with concerns about standardization and security"},
        {"id": "C2", "text": "Cognitive ability test with high adverse impact"},
        {"id": "C3", "text": "Personality assessment with faking concerns"},
        {"id": "C4", "text": "Biodata instrument with empirical keying"}
      ],
      "correctCenter": "C1",
      "explanation": "This is a work sample test (candidates perform actual job tasks). Advantages include high face validity, realistic assessment of technical skills, and reduced test anxiety. Key limitations: the take-home format lacks proctoring (candidates could receive help or use unauthorized resources), creating a test security concern; the 48-hour commitment may create adverse impact against candidates with caregiving or other responsibilities; and inter-rater reliability must be ensured through standardized scoring rubrics. These limitations can be partially addressed through follow-up verification interviews and calibrated scoring.",
      "difficulty": "hard",
      "tags": ["work sample test", "test security", "standardization", "selection design"]
    },
    {
      "type": "highlight",
      "id": "d6-st-hl-001",
      "stem": "A selection specialist is reviewing best practices for interview design. Highlight the THREE accurate statements:",
      "passage": "(A) Structured interviews based on job analysis are more legally defensible than unstructured interviews. (B) Behavioral interviews and situational interviews both qualify as structured formats when properly designed. (C) The best way to improve interview validity is to allow each interviewer to use their preferred questioning style. (D) Anchored rating scales (BARS) improve inter-rater reliability in interview evaluations. (E) Interview validity cannot exceed r = .20 regardless of structure. (F) Unstructured interviews are preferred for senior executive selection because they allow rapport building.",
      "correctHighlights": ["A", "B", "D"],
      "explanation": "Accurate: (A) Structured interviews grounded in job analysis are more defensible in legal challenges. (B) Both behavioral and situational interviews are structured formats with demonstrated validity. (D) BARS provide concrete behavioral examples for each rating level, improving rater agreement. Incorrect: (C) Allowing idiosyncratic questioning reduces standardization and validity. (E) Structured interview validity reaches ~.51 in meta-analyses. (F) While rapport is important, unstructured formats for executives still suffer from the same validity limitations—structured approaches can be adapted for senior roles.",
      "difficulty": "hard",
      "tags": ["interview best practices", "structured interview", "BARS", "legal defensibility"]
    },
    {
      "type": "highlight",
      "id": "d6-st-hl-002",
      "stem": "An HR director is reviewing selection method options for a manufacturing supervisor position. Highlight the FOUR accurate statements:",
      "passage": "(A) Assessment centers are among the most comprehensive selection methods for managerial positions. (B) The construct validity paradox in assessment centers refers to higher cross-exercise convergence than within-exercise consistency. (C) Work sample tests typically show less adverse impact than cognitive ability tests. (D) Integrity tests can legally be used in pre-employment screening in the United States. (E) The Employee Polygraph Protection Act permits routine polygraph screening for all private sector employees. (F) Biographical data instruments can show good validity when items are empirically keyed against job criteria. (G) Reference checks are the single most valid predictor of job performance.",
      "correctHighlights": ["A", "C", "D", "F"],
      "explanation": "Accurate: (A) Assessment centers provide comprehensive managerial assessment. (C) Work samples show less adverse impact than cognitive tests. (D) Integrity tests are legal; EPPA specifically exempts written honesty tests. (F) Biodata can be valid when properly developed with empirical keying. Incorrect: (B) The paradox is actually higher within-exercise consistency than cross-exercise convergence (the opposite). (E) EPPA prohibits routine polygraph screening for most private employers, with exceptions for security and pharmaceutical industries. (G) Reference checks have modest validity (~.26); GMA tests, structured interviews, and work samples are substantially more valid.",
      "difficulty": "hard",
      "tags": ["selection methods", "assessment center", "integrity tests", "biodata"]
    },
    {
      "type": "drag-drop-ordered",
      "id": "d6-st-do-001",
      "stem": "Arrange the steps for developing a structured interview in correct sequence:",
      "options": [
        {"id": "opt1", "text": "Develop behaviorally anchored rating scales (BARS) for evaluating responses"},
        {"id": "opt2", "text": "Conduct job analysis to identify critical competencies"},
        {"id": "opt3", "text": "Train interviewers on administering questions and using rating scales"},
        {"id": "opt4", "text": "Develop standardized questions based on critical incidents or competencies"},
        {"id": "opt5", "text": "Pilot test the interview and refine questions and scales"}
      ],
      "correctOrder": ["opt2", "opt4", "opt1", "opt5", "opt3"],
      "explanation": "Structured interview development follows this sequence: (1) Job analysis—identifies the competencies, KSAOs, and critical incidents that form the basis for questions; (2) Question development—behavioral or situational questions are written to assess identified competencies; (3) Rating scale development—BARS are created with example responses at each scale point; (4) Pilot testing—the interview is tried out and refined based on clarity, differentiation, and rater agreement; (5) Interviewer training—ensures consistent administration, probing, and rating. Each step builds on the previous one.",
      "difficulty": "hard",
      "tags": ["structured interview development", "BARS", "job analysis"]
    },
    {
      "type": "drag-drop-ordered",
      "id": "d6-st-do-002",
      "stem": "Arrange these selection methods from HIGHEST to LOWEST mean criterion-related validity according to meta-analytic research (Schmidt & Hunter, 1998):",
      "options": [
        {"id": "opt1", "text": "Unstructured interviews (r ~ .38)"},
        {"id": "opt2", "text": "Work sample tests (r ~ .54)"},
        {"id": "opt3", "text": "Conscientiousness measures (r ~ .31)"},
        {"id": "opt4", "text": "Reference checks (r ~ .26)"},
        {"id": "opt5", "text": "Structured interviews (r ~ .51)"}
      ],
      "correctOrder": ["opt2", "opt5", "opt1", "opt3", "opt4"],
      "explanation": "Meta-analytic validity rankings: Work sample tests (.54) highest—directly sampling job performance. Structured interviews (.51)—standardized questions and ratings improve prediction. Unstructured interviews (.38)—less valid than structured due to bias and inconsistency. Conscientiousness (.31)—the most valid Big Five personality trait for job performance. Reference checks (.26)—modest validity due to range restriction (referees are typically positive) and unreliability. Note that GMA tests (.51) are comparable to structured interviews but were not included as an option.",
      "difficulty": "hard",
      "tags": ["selection validity ranking", "meta-analysis", "Schmidt Hunter"]
    },
    {
      "type": "trend-analysis",
      "id": "d6-st-ta-001",
      "stem": "Review data comparing adverse impact ratios (selection rate minority / selection rate majority) across different selection methods:",
      "data": {
        "labels": ["Cognitive ability tests", "Structured interviews", "SJTs", "Personality tests", "Work sample tests", "Biodata"],
        "datasets": [
          {
            "name": "Adverse impact ratio (Black/White) - higher is less adverse impact",
            "values": [0.42, 0.68, 0.72, 0.88, 0.75, 0.65]
          },
          {
            "name": "Mean criterion validity (r)",
            "values": [0.51, 0.51, 0.34, 0.22, 0.54, 0.35]
          }
        ]
      },
      "questions": [
        {
          "id": "q1",
          "question": "Which selection method(s) offer the best combination of high validity and low adverse impact?",
          "options": [
            {"id": "a", "text": "Cognitive ability tests alone"},
            {"id": "b", "text": "Work sample tests and structured interviews"},
            {"id": "c", "text": "Personality tests alone"},
            {"id": "d", "text": "All methods show identical validity-adverse impact profiles"}
          ],
          "correctAnswer": "b"
        },
        {
          "id": "q2",
          "question": "What strategy do these data support for addressing the diversity-validity dilemma?",
          "options": [
            {"id": "a", "text": "Use only cognitive ability tests because they are most valid"},
            {"id": "b", "text": "Use composite selection batteries that combine high-validity methods with methods showing less adverse impact"},
            {"id": "c", "text": "Eliminate all selection testing"},
            {"id": "d", "text": "Use only personality tests because they show the least adverse impact"}
          ],
          "correctAnswer": "b"
        }
      ],
      "explanation": "The data illustrate the diversity-validity dilemma: cognitive tests are highly valid but produce the most adverse impact (.42 ratio, well below .80 threshold). Work samples (.54 validity, .75 ratio) and structured interviews (.51 validity, .68 ratio) offer the best balance. The optimal strategy is a composite battery—combining cognitive measures with less impactful methods (SJTs, personality, structured interviews) can maintain overall prediction while reducing adverse impact. This is consistent with the recommendation to use multiple valid predictors rather than relying on a single method.",
      "difficulty": "hard",
      "tags": ["adverse impact", "diversity-validity dilemma", "composite selection", "selection methods"]
    },
    {
      "type": "trend-analysis",
      "id": "d6-st-ta-002",
      "stem": "Examine data on applicant reactions (perceived fairness ratings, 1-5 scale) across different selection methods:",
      "data": {
        "labels": ["Work samples", "Structured interviews", "Cognitive tests", "Personality tests", "Assessment centers", "Graphology", "Honesty tests"],
        "datasets": [
          {
            "name": "Perceived fairness (1-5 scale)",
            "values": [4.5, 4.2, 3.5, 3.1, 4.0, 1.5, 2.5]
          },
          {
            "name": "Perceived job-relatedness (1-5 scale)",
            "values": [4.7, 4.3, 3.2, 2.8, 4.1, 1.2, 2.1]
          }
        ]
      },
      "questions": [
        {
          "id": "q1",
          "question": "What is the strongest predictor of perceived fairness in these data?",
          "options": [
            {"id": "a", "text": "Test cost to the organization"},
            {"id": "b", "text": "Perceived job-relatedness of the selection method"},
            {"id": "c", "text": "Whether the test is computerized"},
            {"id": "d", "text": "The number of items on the test"}
          ],
          "correctAnswer": "b"
        },
        {
          "id": "q2",
          "question": "According to Gilliland's organizational justice framework, why do applicant reactions matter for selection?",
          "options": [
            {"id": "a", "text": "They are irrelevant because only validity matters"},
            {"id": "b", "text": "Negative reactions can lead to legal challenges, lower offer acceptance, and reduced organizational attractiveness"},
            {"id": "c", "text": "Positive reactions guarantee the test is valid"},
            {"id": "d", "text": "Applicant reactions determine test reliability"}
          ],
          "correctAnswer": "b"
        }
      ],
      "explanation": "The data show that perceived fairness closely tracks perceived job-relatedness—methods that clearly relate to the job (work samples, interviews, assessment centers) are perceived as fairer than methods with less obvious job connections (personality tests, honesty tests, graphology). Gilliland's (1993) model of applicant reactions, based on organizational justice theory, predicts that perceived fairness affects litigation intentions, offer acceptance, organizational attractiveness, and recommendation behavior. While fairness perceptions don't determine validity, they affect the practical effectiveness of selection systems.",
      "difficulty": "hard",
      "tags": ["applicant reactions", "Gilliland", "organizational justice", "perceived fairness"]
    },
    {
      "type": "drag-drop-categorize",
      "id": "d6-st-dc-001",
      "stem": "Categorize each selection method according to its primary classification:",
      "categories": [
        {"id": "cat1", "label": "Cognitive/Ability-Based Methods"},
        {"id": "cat2", "label": "Non-Cognitive/Personality-Based Methods"},
        {"id": "cat3", "label": "Simulation/Performance-Based Methods"},
        {"id": "cat4", "label": "Background/History-Based Methods"}
      ],
      "items": [
        {"id": "item1", "text": "General mental ability test"},
        {"id": "item2", "text": "Big Five personality inventory"},
        {"id": "item3", "text": "In-basket exercise"},
        {"id": "item4", "text": "Biographical data questionnaire"},
        {"id": "item5", "text": "Work sample test"},
        {"id": "item6", "text": "Integrity test"},
        {"id": "item7", "text": "Reference check"},
        {"id": "item8", "text": "Specific aptitude battery"}
      ],
      "correctCategorization": {
        "cat1": ["item1", "item8"],
        "cat2": ["item2", "item6"],
        "cat3": ["item3", "item5"],
        "cat4": ["item4", "item7"]
      },
      "explanation": "Cognitive methods: GMA tests and specific aptitude batteries assess mental abilities. Non-cognitive/personality: Big Five inventories and integrity tests (especially personality-based integrity tests) assess dispositions. Simulation/performance: In-baskets and work samples require candidates to perform job-related tasks. Background/history: Biodata questionnaires and reference checks assess past experiences and behaviors. This categorization is important because methods from different categories often show less overlap and greater incremental validity when combined.",
      "difficulty": "hard",
      "tags": ["selection methods classification", "cognitive", "personality", "simulation", "background"]
    },
    {
      "type": "drag-drop-categorize",
      "id": "d6-st-dc-002",
      "stem": "Categorize each assessment center component according to the dimension it is primarily designed to measure:",
      "categories": [
        {"id": "cat1", "label": "Administrative/Organizational Skills"},
        {"id": "cat2", "label": "Interpersonal/Leadership Skills"},
        {"id": "cat3", "label": "Analytical/Problem-Solving Skills"},
        {"id": "cat4", "label": "Communication Skills"}
      ],
      "items": [
        {"id": "item1", "text": "In-basket exercise with memos and scheduling conflicts"},
        {"id": "item2", "text": "Leaderless group discussion on resource allocation"},
        {"id": "item3", "text": "Case analysis requiring data interpretation"},
        {"id": "item4", "text": "Oral presentation to simulated board of directors"},
        {"id": "item5", "text": "One-on-one role play coaching an underperforming employee"},
        {"id": "item6", "text": "Fact-finding exercise requiring probing for missing information"},
        {"id": "item7", "text": "Scheduling exercise with competing priorities"},
        {"id": "item8", "text": "Written report summarizing strategic recommendations"}
      ],
      "correctCategorization": {
        "cat1": ["item1", "item7"],
        "cat2": ["item2", "item5"],
        "cat3": ["item3", "item6"],
        "cat4": ["item4", "item8"]
      },
      "explanation": "Administrative/organizational: In-basket and scheduling exercises assess prioritization, delegation, and time management. Interpersonal/leadership: Leaderless group discussions reveal leadership emergence and influence; coaching role plays assess interpersonal sensitivity and supervisory skills. Analytical/problem-solving: Case analyses test data interpretation; fact-finding exercises assess information-gathering and analytical reasoning. Communication: Oral presentations assess verbal persuasion; written reports assess clarity and organization of written communication. Assessment centers triangulate by measuring dimensions across multiple exercise types.",
      "difficulty": "hard",
      "tags": ["assessment center exercises", "dimensions", "competency assessment"]
    }
  ]
}