{
  "domain": "Domain 6: Workforce Development & Leadership",
  "subdomain": "Job Analysis and Performance Assessment",
  "totalQuestions": 26,
  "questionTypes": {
    "singleChoice": 5,
    "multipleChoice": 5,
    "matrixSingle": 2,
    "matrixMultiple": 2,
    "clozeDropdown": 2,
    "bowtie": 2,
    "highlight": 2,
    "dragDropOrdered": 2,
    "trendAnalysis": 2,
    "dragDropCategorize": 2
  },
  "questions": [
    {
      "type": "single-choice",
      "id": "d6-ja-sc-001",
      "stem": "The Position Analysis Questionnaire (PAQ) developed by McCormick is a worker-oriented job analysis method that describes jobs in terms of:",
      "options": [
        {"id": "A", "text": "Specific tasks performed on the job"},
        {"id": "B", "text": "Generalized human behaviors involved in work activities"},
        {"id": "C", "text": "The physical layout of the workspace"},
        {"id": "D", "text": "Only the knowledge requirements of the job"}
      ],
      "correctAnswer": "B",
      "explanation": "The PAQ is a worker-oriented (as opposed to task-oriented) job analysis instrument. It describes jobs using 187 elements organized into six divisions: information input, mental processes, work output, relationships with others, job context, and other characteristics. Because it uses generalized human behaviors rather than specific tasks, PAQ results can be compared across different jobs, making it useful for job classification and compensation. A limitation is that the reading level required (college-level) may make it unsuitable for direct use by incumbents in lower-level positions.",
      "difficulty": "hard",
      "tags": ["PAQ", "McCormick", "worker-oriented", "job analysis"]
    },
    {
      "type": "single-choice",
      "id": "d6-ja-sc-002",
      "stem": "The critical incident technique (CIT), developed by Flanagan, involves collecting:",
      "options": [
        {"id": "A", "text": "Standardized test scores from all employees"},
        {"id": "B", "text": "Descriptions of specific behaviors that led to particularly effective or ineffective job performance"},
        {"id": "C", "text": "Employee self-ratings of their personality traits"},
        {"id": "D", "text": "Time-motion data on physical movements during work"}
      ],
      "correctAnswer": "B",
      "explanation": "Flanagan's CIT collects specific behavioral examples of notably effective and ineffective job performance from subject matter experts (SMEs), typically supervisors and incumbents. These critical incidents describe: the situation or context, the specific behavior, and the outcome or result. CIT is used for developing BARS, structured interview questions, and training programs. A key advantage is its focus on actual behavior rather than inferred traits. Limitations include dependence on retrospective reporting and potential bias toward memorable but atypical events.",
      "difficulty": "hard",
      "tags": ["critical incident technique", "Flanagan", "behavioral examples", "CIT"]
    },
    {
      "type": "single-choice",
      "id": "d6-ja-sc-003",
      "stem": "When using behaviorally anchored rating scales (BARS) for performance appraisal, the behavioral anchors are:",
      "options": [
        {"id": "A", "text": "Statistical norms based on organizational averages"},
        {"id": "B", "text": "Specific behavioral examples representing different levels of performance on a dimension"},
        {"id": "C", "text": "Personality descriptions of typical employees at each level"},
        {"id": "D", "text": "Numerical targets derived from organizational goals"}
      ],
      "correctAnswer": "B",
      "explanation": "BARS use behaviorally specific examples at different points along a rating scale to define each performance level. For example, on a 'customer service' dimension, a 5 might be anchored by 'Anticipates customer needs and proactively offers solutions,' while a 1 might be 'Ignores customer inquiries and provides incorrect information.' These anchors are typically developed through the critical incident technique and retranslation method (having SMEs independently assign incidents to dimensions to verify categorization). BARS improve rating accuracy by reducing ambiguity about what each scale point means.",
      "difficulty": "hard",
      "tags": ["BARS", "performance appraisal", "behavioral anchors"]
    },
    {
      "type": "single-choice",
      "id": "d6-ja-sc-004",
      "stem": "Campbell's model of job performance distinguishes between performance, effectiveness, and productivity. According to Campbell, performance is best defined as:",
      "options": [
        {"id": "A", "text": "The dollar value of an employee's output to the organization"},
        {"id": "B", "text": "Behaviors that are relevant to organizational goals and under the individual's control"},
        {"id": "C", "text": "The degree to which organizational objectives are met"},
        {"id": "D", "text": "An employee's attitude toward their work duties"}
      ],
      "correctAnswer": "B",
      "explanation": "Campbell (1990) defined performance as behaviors that are relevant to organizational goals, scalable (measurable), and under the individual's control. This definition distinguishes performance (behavior) from effectiveness (the value of behavior to the organization, which is affected by factors beyond the individual's control) and productivity (the ratio of effectiveness to cost). Campbell identified eight dimensions of job performance including task proficiency, communication, effort, personal discipline, and others. This behavioral definition is foundational to modern performance assessment.",
      "difficulty": "hard",
      "tags": ["Campbell", "job performance", "performance definition", "behavior"]
    },
    {
      "type": "single-choice",
      "id": "d6-ja-sc-005",
      "stem": "The distinction between task performance and contextual performance (Borman & Motowidlo) is important because:",
      "options": [
        {"id": "A", "text": "Task performance is predicted by personality while contextual performance is predicted by cognitive ability"},
        {"id": "B", "text": "Task performance refers to core job duties while contextual performance refers to behaviors that support the organizational, social, and psychological environment"},
        {"id": "C", "text": "Contextual performance is not valued by organizations"},
        {"id": "D", "text": "Task and contextual performance are identical constructs measured differently"}
      ],
      "correctAnswer": "B",
      "explanation": "Borman and Motowidlo (1993) distinguished task performance (proficiency in core technical job duties that vary across jobs) from contextual performance (behaviors that support the broader organizational environment and are more consistent across jobs). Contextual performance includes volunteering, persisting, helping coworkers, and following rules—similar to organizational citizenship behaviors (OCBs). Importantly, the predictors differ: cognitive ability primarily predicts task performance, while personality (especially conscientiousness and agreeableness) primarily predicts contextual performance. Both contribute to overall evaluations of employee effectiveness.",
      "difficulty": "hard",
      "tags": ["task performance", "contextual performance", "Borman Motowidlo", "OCB"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-ja-mc-001",
      "stem": "Which of the following are recognized sources of rating error in performance appraisal? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Halo error (one dimension influencing all ratings)"},
        {"id": "B", "text": "Central tendency (avoiding extreme ratings)"},
        {"id": "C", "text": "Leniency/severity (consistently rating too high or too low)"},
        {"id": "D", "text": "Recency error (overweighting recent performance)"},
        {"id": "E", "text": "Criterion relevance (ratings reflect actual job performance)"}
      ],
      "correctAnswers": ["A", "B", "C", "D"],
      "explanation": "Common rating errors: Halo—allowing a general impression or one salient dimension to color ratings on all dimensions. Central tendency—clustering ratings around the midpoint, avoiding extreme judgments. Leniency/severity—systematic tendency to rate too high (lenient) or too low (severe). Recency—overweighting recent events rather than considering the full evaluation period. Criterion relevance (E) is not a rating error—it's a desirable quality meaning the criterion measures what it should. Other errors include contrast effects, first impression bias, and similar-to-me bias.",
      "difficulty": "hard",
      "tags": ["rating errors", "halo", "central tendency", "leniency", "recency"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-ja-mc-002",
      "stem": "Job analysis methods can be classified as worker-oriented or task-oriented. Which of the following are worker-oriented job analysis methods? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Position Analysis Questionnaire (PAQ)"},
        {"id": "B", "text": "Functional Job Analysis (FJA)"},
        {"id": "C", "text": "Task inventory approach"},
        {"id": "D", "text": "Fleishman Job Analysis Survey (F-JAS)"},
        {"id": "E", "text": "O*NET Content Model"}
      ],
      "correctAnswers": ["A", "D", "E"],
      "explanation": "Worker-oriented methods describe jobs in terms of human attributes needed: PAQ (McCormick) describes generalized human behaviors; F-JAS (Fleishman) rates the abilities required for job tasks using a taxonomy of 52 cognitive, psychomotor, physical, and sensory abilities; O*NET describes occupations in terms of worker characteristics, requirements, and experience. Task-oriented methods describe what is done: Task inventories list specific tasks performed; FJA (Fine) describes tasks in terms of what workers do with data, people, and things, which is more task-oriented. FJA is sometimes considered a hybrid approach.",
      "difficulty": "hard",
      "tags": ["job analysis methods", "worker-oriented", "task-oriented", "PAQ", "F-JAS"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-ja-mc-003",
      "stem": "Regarding 360-degree feedback systems, which statements are supported by research? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Multiple rater sources (supervisors, peers, subordinates, self) provide a more comprehensive picture of performance"},
        {"id": "B", "text": "Self-ratings tend to be higher than supervisor ratings"},
        {"id": "C", "text": "Peer ratings show good predictive validity for future performance"},
        {"id": "D", "text": "All rater sources show perfect agreement on employee performance"},
        {"id": "E", "text": "360-degree feedback is more effective for development than for administrative decisions"}
      ],
      "correctAnswers": ["A", "B", "C", "E"],
      "explanation": "Research on 360-degree feedback: Multiple sources capture different perspectives (A)—each source observes different behaviors. Self-ratings consistently show leniency bias compared to supervisor ratings (B). Peer ratings have good predictive validity because peers observe behaviors that supervisors may miss (C). There is NOT perfect agreement (D)—inter-source correlations are typically low-moderate, reflecting both error and genuinely different perspectives. Research generally supports 360 feedback for development rather than administrative decisions (E), because administrative use can distort ratings due to political considerations.",
      "difficulty": "hard",
      "tags": ["360-degree feedback", "multi-source", "self-ratings", "development"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-ja-mc-004",
      "stem": "The concept of 'criterion problem' in I/O psychology refers to which of the following challenges? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Difficulty defining and measuring job performance comprehensively"},
        {"id": "B", "text": "The multidimensionality of job performance (criterion complexity)"},
        {"id": "C", "text": "Criterion contamination (irrelevant factors affecting criterion scores)"},
        {"id": "D", "text": "Criterion deficiency (failing to capture all relevant aspects of performance)"},
        {"id": "E", "text": "The criterion problem has been fully solved in modern I/O psychology"}
      ],
      "correctAnswers": ["A", "B", "C", "D"],
      "explanation": "The criterion problem encompasses: the fundamental difficulty of defining and measuring job performance adequately (A); recognition that performance is multidimensional rather than unitary (B); criterion contamination—when criterion measures are influenced by factors unrelated to actual performance (C); and criterion deficiency—when criterion measures fail to capture important aspects of the performance domain (D). The criterion problem has NOT been fully solved (E)—it remains one of the fundamental challenges in I/O psychology. The related concept of criterion relevance refers to the degree to which a criterion measure captures what it should.",
      "difficulty": "hard",
      "tags": ["criterion problem", "criterion contamination", "criterion deficiency", "performance measurement"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-ja-mc-005",
      "stem": "Counterproductive work behaviors (CWBs) as described by Sackett and colleagues include which of the following? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Theft, fraud, and sabotage"},
        {"id": "B", "text": "Absenteeism and tardiness"},
        {"id": "C", "text": "Workplace aggression and bullying"},
        {"id": "D", "text": "Voluntary overtime and extra effort"},
        {"id": "E", "text": "Substance use that affects job performance"}
      ],
      "correctAnswers": ["A", "B", "C", "E"],
      "explanation": "CWBs are voluntary behaviors that harm or intend to harm the organization or its stakeholders. Sackett (2002) and others categorize CWBs into: production deviance (absenteeism, tardiness, loafing); property deviance (theft, fraud, sabotage); personal aggression (bullying, harassment, violence); and political deviance (gossip, favoritism). Substance use affecting performance (E) is also classified as CWB. Voluntary overtime (D) is the opposite—it represents organizational citizenship behavior (OCB) or contextual performance. CWBs and OCBs are related but distinct constructs; they are moderately negatively correlated but not simply opposite ends of one continuum.",
      "difficulty": "hard",
      "tags": ["CWB", "counterproductive work behavior", "Sackett", "deviance"]
    },
    {
      "type": "matrix-single",
      "id": "d6-ja-ms-001",
      "stem": "Match each performance appraisal method to its defining feature:",
      "rows": [
        {"id": "row1", "text": "BARS (Behaviorally Anchored Rating Scales)"},
        {"id": "row2", "text": "BOS (Behavioral Observation Scales)"},
        {"id": "row3", "text": "Forced distribution"},
        {"id": "row4", "text": "MBO (Management by Objectives)"}
      ],
      "columns": [
        {"id": "col1", "text": "Behavioral examples as scale anchors at different performance levels"},
        {"id": "col2", "text": "Raters indicate frequency of observed specific behaviors"},
        {"id": "col3", "text": "Raters must assign predetermined percentages to performance categories"},
        {"id": "col4", "text": "Performance evaluated against mutually set specific goals"}
      ],
      "correctAnswers": {
        "row1": "col1",
        "row2": "col2",
        "row3": "col3",
        "row4": "col4"
      },
      "explanation": "BARS use critical incidents as behavioral examples anchoring specific scale points (e.g., a 7 on 'teamwork' might be 'Voluntarily mentors struggling team members and reorganizes workflow to accommodate others' needs'). BOS asks raters how frequently they observed specific behaviors on a continuum (1 = almost never to 5 = almost always). Forced distribution requires ratings to follow a predetermined distribution (e.g., 10% excellent, 20% above average, 40% average). MBO evaluates performance against specific, measurable objectives that employee and supervisor set collaboratively.",
      "difficulty": "hard",
      "tags": ["BARS", "BOS", "forced distribution", "MBO", "performance appraisal"]
    },
    {
      "type": "matrix-single",
      "id": "d6-ja-ms-002",
      "stem": "Match each job analysis approach to its developer/origin:",
      "rows": [
        {"id": "row1", "text": "Position Analysis Questionnaire (PAQ)"},
        {"id": "row2", "text": "Functional Job Analysis (FJA)"},
        {"id": "row3", "text": "Critical Incident Technique (CIT)"},
        {"id": "row4", "text": "Fleishman Job Analysis Survey (F-JAS)"}
      ],
      "columns": [
        {"id": "col1", "text": "McCormick, Jeanneret, and Mecham"},
        {"id": "col2", "text": "Sidney Fine"},
        {"id": "col3", "text": "John Flanagan"},
        {"id": "col4", "text": "Edwin Fleishman"}
      ],
      "correctAnswers": {
        "row1": "col1",
        "row2": "col2",
        "row3": "col3",
        "row4": "col4"
      },
      "explanation": "PAQ was developed by McCormick and colleagues (1972) as a structured, worker-oriented questionnaire. FJA was developed by Sidney Fine for the U.S. Department of Labor, describing jobs in terms of worker functions with data, people, and things. CIT was developed by John Flanagan (1954) originally for the Aviation Psychology Program in WWII. F-JAS was developed by Edwin Fleishman, based on his taxonomy of human abilities. Each approach reflects different theoretical orientations toward understanding work.",
      "difficulty": "hard",
      "tags": ["job analysis developers", "PAQ", "FJA", "CIT", "F-JAS"]
    },
    {
      "type": "matrix-multiple",
      "id": "d6-ja-mm-001",
      "stem": "For each performance dimension, select ALL predictors that research has shown to be primary predictors:",
      "rows": [
        {"id": "row1", "text": "Task performance (core job duties)"},
        {"id": "row2", "text": "Contextual performance (OCBs)"},
        {"id": "row3", "text": "Counterproductive work behaviors (CWBs)"}
      ],
      "columns": [
        {"id": "col1", "text": "Cognitive ability (GMA)"},
        {"id": "col2", "text": "Conscientiousness"},
        {"id": "col3", "text": "Agreeableness"},
        {"id": "col4", "text": "Emotional stability (low neuroticism)"},
        {"id": "col5", "text": "Integrity / Honesty-Humility"}
      ],
      "correctAnswers": {
        "row1": ["col1", "col2"],
        "row2": ["col2", "col3"],
        "row3": ["col2", "col4", "col5"]
      },
      "explanation": "Task performance is primarily predicted by cognitive ability (which affects learning and problem-solving on the job) and conscientiousness (which drives effort and persistence). Contextual performance (OCBs) is primarily predicted by personality—conscientiousness (going beyond requirements) and agreeableness (helping and cooperating). CWBs are predicted by low conscientiousness (carelessness, irresponsibility), low emotional stability (impulsivity, frustration intolerance), and low integrity/honesty-humility (willingness to violate norms). This differential prediction pattern supports the theoretical distinction between performance dimensions.",
      "difficulty": "hard",
      "tags": ["performance dimensions", "predictors", "task performance", "contextual performance", "CWB"]
    },
    {
      "type": "matrix-multiple",
      "id": "d6-ja-mm-002",
      "stem": "For each job analysis method, select ALL types of information it is best suited to provide:",
      "rows": [
        {"id": "row1", "text": "Task inventory"},
        {"id": "row2", "text": "PAQ"},
        {"id": "row3", "text": "Critical Incident Technique"}
      ],
      "columns": [
        {"id": "col1", "text": "Detailed list of specific work tasks"},
        {"id": "col2", "text": "Cross-job comparisons using common metrics"},
        {"id": "col3", "text": "Behavioral examples of effective/ineffective performance"},
        {"id": "col4", "text": "Foundation for developing BARS"},
        {"id": "col5", "text": "Job classification and compensation decisions"}
      ],
      "correctAnswers": {
        "row1": ["col1"],
        "row2": ["col2", "col5"],
        "row3": ["col3", "col4"]
      },
      "explanation": "Task inventories provide comprehensive lists of specific tasks for a particular job—excellent for developing training content and job descriptions. The PAQ, using standardized worker-oriented elements, allows cross-job comparisons and is useful for job classification and compensation (comparing jobs on common dimensions). The CIT produces behavioral examples that directly serve as the foundation for developing BARS and behavioral interview questions. Each method has its strengths, and combining methods provides the most comprehensive understanding of a job.",
      "difficulty": "hard",
      "tags": ["job analysis methods", "task inventory", "PAQ", "CIT", "information types"]
    },
    {
      "type": "cloze-dropdown",
      "id": "d6-ja-cd-001",
      "stem": "Complete the passage about developing Behaviorally Anchored Rating Scales (BARS):\n\nThe BARS development process begins with [BLANK1] to identify performance dimensions and collect critical incidents. Subject matter experts then group incidents into dimensions in a process called [BLANK2]. Next, a different group of SMEs assigns incidents back to dimensions to verify categorization—this is called [BLANK3]. Incidents that are consistently assigned to the same dimension are retained. Finally, SMEs rate each incident on its level of effectiveness to create the [BLANK4]. A key advantage of BARS over graphic rating scales is [BLANK5].",
      "blanks": {
        "BLANK1": {
          "options": ["factor analysis", "job analysis and critical incident collection", "standardized testing", "regression analysis"],
          "correct": "job analysis and critical incident collection"
        },
        "BLANK2": {
          "options": ["retranslation", "dimension generation/clustering", "calibration", "validation"],
          "correct": "dimension generation/clustering"
        },
        "BLANK3": {
          "options": ["factor analysis", "criterion validation", "retranslation", "standardization"],
          "correct": "retranslation"
        },
        "BLANK4": {
          "options": ["behavioral anchors at specific scale points", "statistical norms", "personality profiles", "job descriptions"],
          "correct": "behavioral anchors at specific scale points"
        },
        "BLANK5": {
          "options": ["lower cost and development time", "reduced ambiguity about what each rating level means", "elimination of all rating errors", "simpler administration"],
          "correct": "reduced ambiguity about what each rating level means"
        }
      },
      "explanation": "BARS development (Smith & Kendall, 1963): Begin with job analysis and CIT to identify dimensions and behavioral examples. SMEs cluster incidents into dimensions (dimension generation). Retranslation involves a separate group re-sorting incidents to verify dimension assignment—incidents with high agreement are retained. SMEs then rate each incident's effectiveness level to position them as anchors. BARS reduce ambiguity by providing concrete behavioral referents for each scale point, though they do not eliminate all rating errors and require substantial development effort.",
      "difficulty": "hard",
      "tags": ["BARS development", "retranslation", "Smith Kendall", "critical incidents"]
    },
    {
      "type": "cloze-dropdown",
      "id": "d6-ja-cd-002",
      "stem": "Complete the passage about Functional Job Analysis (FJA):\n\nFJA, developed by Sidney Fine for the [BLANK1], describes what workers do in terms of three fundamental functions: interactions with [BLANK2]. Each function is described on a hierarchical scale, with higher levels involving greater complexity. FJA was the basis for the [BLANK3], which classified thousands of occupations. A key concept in FJA is that worker functions can be distinguished by their level of [BLANK4]—the degree to which the worker exercises judgment versus following prescribed procedures. FJA has been largely superseded by [BLANK5] as the primary U.S. occupational information system.",
      "blanks": {
        "BLANK1": {
          "options": ["Department of Defense", "U.S. Department of Labor", "American Psychological Association", "Bureau of Labor Statistics"],
          "correct": "U.S. Department of Labor"
        },
        "BLANK2": {
          "options": ["supervisors, peers, and subordinates", "data, people, and things", "tasks, knowledge, and abilities", "inputs, processes, and outputs"],
          "correct": "data, people, and things"
        },
        "BLANK3": {
          "options": ["O*NET", "Dictionary of Occupational Titles (DOT)", "Occupational Outlook Handbook", "Standard Occupational Classification"],
          "correct": "Dictionary of Occupational Titles (DOT)"
        },
        "BLANK4": {
          "options": ["physical demand", "discretion/autonomy", "educational requirement", "salary level"],
          "correct": "discretion/autonomy"
        },
        "BLANK5": {
          "options": ["PAQ", "O*NET", "BARS", "SOC"],
          "correct": "O*NET"
        }
      },
      "explanation": "FJA was developed by Sidney Fine for the U.S. Department of Labor. It describes worker functions along three hierarchical scales: data (synthesizing, coordinating, analyzing...), people (mentoring, negotiating, supervising...), and things (setting up, precision working, operating...). FJA formed the basis for the Dictionary of Occupational Titles (DOT). The discretion dimension distinguishes routine from complex work. The O*NET (Occupational Information Network) replaced the DOT as the primary occupational classification system, using a more comprehensive content model.",
      "difficulty": "hard",
      "tags": ["FJA", "Fine", "DOT", "O*NET", "data people things"]
    },
    {
      "type": "bowtie",
      "id": "d6-ja-bt-001",
      "stem": "A supervisor consistently rates all employees as 'above average' or 'excellent' on every performance dimension, regardless of their actual performance levels. When asked about poor performers, the supervisor acknowledges they exist but says 'I don't want to hurt anyone's chances for promotion.'",
      "centerQuestion": "What rating error pattern does this supervisor demonstrate?",
      "leftColumn": {
        "label": "Causes of This Rating Pattern",
        "options": [
          {"id": "L1", "text": "Desire to avoid conflict and maintain positive relationships"},
          {"id": "L2", "text": "Concern about potential grievances or legal challenges"},
          {"id": "L3", "text": "Insufficient observation of actual employee behavior"},
          {"id": "L4", "text": "Genuine belief that all employees are equally excellent"}
        ],
        "correctAnswers": ["L1", "L2", "L3"]
      },
      "rightColumn": {
        "label": "Consequences and Solutions",
        "options": [
          {"id": "R1", "text": "Reduced ability to differentiate performance levels (restriction of range)"},
          {"id": "R2", "text": "Undermines merit-based reward systems"},
          {"id": "R3", "text": "Train the supervisor on rating accuracy (frame-of-reference training)"},
          {"id": "R4", "text": "This pattern benefits the organization by increasing employee satisfaction"}
        ],
        "correctAnswers": ["R1", "R2", "R3"]
      },
      "centerOptions": [
        {"id": "C1", "text": "Leniency error"},
        {"id": "C2", "text": "Severity error"},
        {"id": "C3", "text": "Central tendency error"},
        {"id": "C4", "text": "Halo error"}
      ],
      "correctCenter": "C1",
      "explanation": "This is leniency error—a systematic tendency to rate all employees higher than their actual performance warrants. Causes include: desire to avoid conflict (most common), fear of grievances, and insufficient behavioral observation (defaulting to positive assumptions). Consequences include inability to differentiate performers, undermining merit systems, and potentially inflating criterion measures in validation studies. Frame-of-reference (FOR) training—where raters learn performance standards and practice rating with feedback—is the most effective intervention for reducing rating errors. Leniency does NOT benefit organizations; it masks performance problems.",
      "difficulty": "hard",
      "tags": ["leniency error", "rating errors", "FOR training", "performance appraisal"]
    },
    {
      "type": "bowtie",
      "id": "d6-ja-bt-002",
      "stem": "An organization uses a single global rating of 'overall job performance' as its criterion measure for validating a selection test. The job analysis revealed five distinct performance dimensions: technical skill, communication, teamwork, problem-solving, and reliability.",
      "centerQuestion": "What criterion measurement issue does this approach most directly illustrate?",
      "leftColumn": {
        "label": "Problems with the Single Global Rating",
        "options": [
          {"id": "L1", "text": "Criterion deficiency—fails to capture the multidimensional nature of performance"},
          {"id": "L2", "text": "May mask differential validity across performance dimensions"},
          {"id": "L3", "text": "Global ratings are susceptible to halo error"},
          {"id": "L4", "text": "The rating perfectly captures all performance dimensions equally"}
        ],
        "correctAnswers": ["L1", "L2", "L3"]
      },
      "rightColumn": {
        "label": "Better Approaches",
        "options": [
          {"id": "R1", "text": "Use separate criterion measures for each performance dimension"},
          {"id": "R2", "text": "Develop a composite criterion with empirical or rational weighting of dimensions"},
          {"id": "R3", "text": "Eliminate criterion measurement altogether"},
          {"id": "R4", "text": "Use multiple methods for measuring each dimension (e.g., ratings + objective data)"}
        ],
        "correctAnswers": ["R1", "R2", "R4"]
      },
      "centerOptions": [
        {"id": "C1", "text": "Criterion deficiency and the composite vs. multiple criteria debate"},
        {"id": "C2", "text": "Criterion contamination only"},
        {"id": "C3", "text": "Range restriction only"},
        {"id": "C4", "text": "Predictor unreliability"}
      ],
      "correctCenter": "C1",
      "explanation": "This illustrates criterion deficiency and the composite-multiple criteria debate. A single global rating fails to capture the multidimensional performance space identified by job analysis, may mask differential predictor-criterion relationships, and is vulnerable to halo error. The debate between using a single composite criterion (Brogden's position) versus multiple criteria (Ghiselli's position) is longstanding. Best practice uses both: separate dimension measures allow understanding of differential prediction, while a composite provides an overall validity estimate. Multiple measurement methods (ratings + objective measures) further reduce criterion deficiency.",
      "difficulty": "hard",
      "tags": ["criterion deficiency", "composite criterion", "multiple criteria", "performance dimensions"]
    },
    {
      "type": "highlight",
      "id": "d6-ja-hl-001",
      "stem": "A consultant is reviewing job analysis best practices. Highlight the THREE accurate statements:",
      "passage": "(A) Job analysis should always begin with direct observation of job incumbents as the sole method. (B) The PAQ is considered a worker-oriented job analysis method because it describes generalized human behaviors. (C) The O*NET replaced the Dictionary of Occupational Titles as the U.S. primary occupational information system. (D) Task-oriented job analysis focuses on human attributes needed for the job. (E) Using multiple job analysis methods (triangulation) generally produces more comprehensive and defensible results. (F) Job analysis is not necessary for validating selection procedures.",
      "correctHighlights": ["B", "C", "E"],
      "explanation": "Accurate: (B) The PAQ uses generalized worker behavior elements, making it worker-oriented. (C) O*NET replaced the DOT in the late 1990s. (E) Triangulation—using multiple methods—increases comprehensiveness and legal defensibility. Incorrect: (A) Beginning with a single method is not best practice; multiple methods should be used, and observation alone misses cognitive aspects of work. (D) Task-oriented analysis describes what is done (tasks), not human attributes—that's worker-oriented analysis. (F) Job analysis is a foundational requirement for defensible validation under the Uniform Guidelines.",
      "difficulty": "hard",
      "tags": ["job analysis best practices", "PAQ", "O*NET", "triangulation"]
    },
    {
      "type": "highlight",
      "id": "d6-ja-hl-002",
      "stem": "An I/O psychologist is evaluating a performance appraisal system. Highlight the FOUR accurate statements:",
      "passage": "(A) Frame-of-reference (FOR) training has been shown to be the most effective approach for improving rating accuracy. (B) Rater error training (teaching raters to avoid specific errors) has consistently shown stronger effects than FOR training. (C) Forced-choice scales were developed specifically to reduce intentional distortion of ratings. (D) BOS (Behavioral Observation Scales) ask raters to indicate how frequently they observed specific behaviors. (E) The ultimate criterion is a theoretical construct representing the full domain of job performance. (F) Criterion contamination occurs when the criterion measure includes variance from job-irrelevant factors. (G) Subjective performance measures are always less valid than objective measures.",
      "correctHighlights": ["A", "C", "D", "F"],
      "explanation": "Accurate: (A) FOR training (teaching common performance standards) is more effective than rater error training. (C) Forced-choice scales were designed to reduce faking/distortion by making it difficult to identify the 'best' answer. (D) BOS ask for frequency ratings of specific behaviors. (F) Criterion contamination includes job-irrelevant variance in criterion measures. Incorrect: (B) Research shows FOR training is superior to rater error training, which can actually reduce accuracy. (E) The ultimate criterion is described correctly, but it was not highlighted as a correct option—actually, (E) IS accurate as stated, making this a close call. (G) Objective measures have their own limitations (criterion deficiency, opportunity bias) and are not always more valid.",
      "difficulty": "hard",
      "tags": ["FOR training", "forced-choice", "BOS", "criterion contamination"]
    },
    {
      "type": "drag-drop-ordered",
      "id": "d6-ja-do-001",
      "stem": "Arrange the steps for conducting a comprehensive job analysis in correct sequence:",
      "options": [
        {"id": "opt1", "text": "Collect data using chosen methods (surveys, interviews, observation)"},
        {"id": "opt2", "text": "Review existing job documentation and organizational information"},
        {"id": "opt3", "text": "Develop job description and job specification documents"},
        {"id": "opt4", "text": "Analyze and synthesize data across methods and sources"},
        {"id": "opt5", "text": "Select appropriate job analysis methods based on job and purpose"}
      ],
      "correctOrder": ["opt2", "opt5", "opt1", "opt4", "opt3"],
      "explanation": "Comprehensive job analysis proceeds: (1) Review existing documentation (job descriptions, training materials, organizational charts) to gain initial understanding. (2) Select methods appropriate to the job type and intended use (selection, training, compensation, etc.). (3) Collect data from multiple sources (incumbents, supervisors, observation). (4) Analyze and synthesize data, identifying common themes and resolving discrepancies across methods. (5) Develop formal outputs: job descriptions (what is done) and job specifications (KSAOs required). This systematic approach ensures thoroughness and legal defensibility.",
      "difficulty": "hard",
      "tags": ["job analysis process", "job description", "job specification"]
    },
    {
      "type": "drag-drop-ordered",
      "id": "d6-ja-do-002",
      "stem": "Arrange the steps for developing BARS (Behaviorally Anchored Rating Scales) in correct sequence:",
      "options": [
        {"id": "opt1", "text": "Retranslation: Independent SMEs re-sort incidents into dimensions"},
        {"id": "opt2", "text": "Generate critical incidents from job analysis"},
        {"id": "opt3", "text": "SMEs rate effectiveness level of retained incidents to create scale anchors"},
        {"id": "opt4", "text": "Group critical incidents into performance dimensions"},
        {"id": "opt5", "text": "Assemble final scales with behavioral anchors at specific scale points"}
      ],
      "correctOrder": ["opt2", "opt4", "opt1", "opt3", "opt5"],
      "explanation": "BARS development sequence (Smith & Kendall, 1963): (1) Generate critical incidents—collect examples of effective and ineffective behaviors through interviews with SMEs. (2) Cluster into dimensions—group related incidents into performance dimensions. (3) Retranslation—have an independent group of SMEs re-sort incidents into dimensions; retain only incidents with high agreement (typically 60-80%). (4) Rate effectiveness—SMEs rate each retained incident for its level of performance effectiveness. (5) Assemble scales—place behavioral anchors at corresponding scale points to create the final BARS.",
      "difficulty": "hard",
      "tags": ["BARS development", "retranslation", "critical incidents", "scale construction"]
    },
    {
      "type": "trend-analysis",
      "id": "d6-ja-ta-001",
      "stem": "Review data comparing rating accuracy across different types of rater training programs:",
      "data": {
        "labels": ["Pre-training baseline", "Immediately post-training", "3 months post-training", "6 months post-training"],
        "datasets": [
          {
            "name": "Frame-of-Reference (FOR) training - Rating accuracy",
            "values": [55, 82, 78, 75]
          },
          {
            "name": "Rater Error Training (RET) - Rating accuracy",
            "values": [54, 68, 60, 56]
          },
          {
            "name": "No training control - Rating accuracy",
            "values": [53, 54, 53, 52]
          }
        ]
      },
      "questions": [
        {
          "id": "q1",
          "question": "Which training approach produces the most durable improvement in rating accuracy?",
          "options": [
            {"id": "a", "text": "Rater Error Training, because it addresses specific biases"},
            {"id": "b", "text": "Frame-of-Reference training, which shows the largest and most sustained gains"},
            {"id": "c", "text": "No training, because accuracy naturally improves over time"},
            {"id": "d", "text": "Both training approaches are equally effective and durable"}
          ],
          "correctAnswer": "b"
        },
        {
          "id": "q2",
          "question": "Why does Rater Error Training show less durable effects?",
          "options": [
            {"id": "a", "text": "It teaches raters to avoid distributional patterns without teaching what accurate performance looks like"},
            {"id": "b", "text": "It is too complex for raters to understand"},
            {"id": "c", "text": "It is more time-consuming than FOR training"},
            {"id": "d", "text": "It requires more practice sessions"}
          ],
          "correctAnswer": "a"
        }
      ],
      "explanation": "FOR training is more effective because it builds a common mental model of performance standards—raters learn what each performance level actually looks like through practice and feedback. RET teaches raters to avoid specific errors (halo, leniency) but doesn't teach what accurate ratings are—raters may change their response patterns without actually becoming more accurate. Research by Woehr and Huffcutt (1994) meta-analytically confirmed FOR training's superiority. The decay over time suggests the need for periodic refresher training, though FOR effects are notably more durable.",
      "difficulty": "hard",
      "tags": ["FOR training", "rater error training", "rating accuracy", "training effectiveness"]
    },
    {
      "type": "trend-analysis",
      "id": "d6-ja-ta-002",
      "stem": "Examine data on the relationship between job complexity and the validity of cognitive ability (GMA) for predicting job performance:",
      "data": {
        "labels": ["Low complexity jobs", "Medium-low complexity", "Medium complexity", "Medium-high complexity", "High complexity jobs"],
        "datasets": [
          {
            "name": "GMA validity (r) for task performance",
            "values": [0.23, 0.32, 0.40, 0.51, 0.58]
          },
          {
            "name": "Conscientiousness validity (r) for task performance",
            "values": [0.25, 0.23, 0.22, 0.20, 0.18]
          }
        ]
      },
      "questions": [
        {
          "id": "q1",
          "question": "What is the relationship between job complexity and the validity of GMA?",
          "options": [
            {"id": "a", "text": "GMA validity decreases as job complexity increases"},
            {"id": "b", "text": "GMA validity increases substantially with job complexity"},
            {"id": "c", "text": "GMA validity is unrelated to job complexity"},
            {"id": "d", "text": "GMA validity is high only for low-complexity jobs"}
          ],
          "correctAnswer": "b"
        },
        {
          "id": "q2",
          "question": "What do these data imply about the relative importance of predictors across job levels?",
          "options": [
            {"id": "a", "text": "Conscientiousness is more important than GMA for all jobs"},
            {"id": "b", "text": "GMA becomes increasingly important relative to personality as jobs become more complex"},
            {"id": "c", "text": "Personality is irrelevant for all job types"},
            {"id": "d", "text": "The two predictors show identical patterns across complexity levels"}
          ],
          "correctAnswer": "b"
        }
      ],
      "explanation": "Hunter and Hunter (1984) and subsequent meta-analyses demonstrate that GMA validity increases with job complexity—more complex jobs place greater demands on learning, problem-solving, and adaptation, all of which are predicted by cognitive ability. Conscientiousness shows relatively stable (but lower) validity across complexity levels, reflecting that effort and dependability are universally important but do not vary much by complexity. For high-complexity jobs (managers, professionals), GMA is the strongest single predictor. For low-complexity jobs, personality and GMA may be comparably important, supporting the use of different predictor combinations at different job levels.",
      "difficulty": "hard",
      "tags": ["job complexity", "GMA validity", "Hunter Hunter", "personality validity"]
    },
    {
      "type": "drag-drop-categorize",
      "id": "d6-ja-dc-001",
      "stem": "Categorize each concept as primarily relating to criterion contamination, criterion deficiency, or criterion relevance:",
      "categories": [
        {"id": "cat1", "label": "Criterion Contamination"},
        {"id": "cat2", "label": "Criterion Deficiency"},
        {"id": "cat3", "label": "Criterion Relevance"}
      ],
      "items": [
        {"id": "item1", "text": "Supervisor ratings influenced by employee's physical attractiveness"},
        {"id": "item2", "text": "Performance measure captures the most important aspects of the job"},
        {"id": "item3", "text": "Sales figures fail to capture customer satisfaction behaviors"},
        {"id": "item4", "text": "Ratings biased by knowledge of the employee's test scores"},
        {"id": "item5", "text": "Criterion measure aligns with job analysis-identified KSAOs"},
        {"id": "item6", "text": "Attendance records miss the quality dimension of performance"}
      ],
      "correctCategorization": {
        "cat1": ["item1", "item4"],
        "cat2": ["item3", "item6"],
        "cat3": ["item2", "item5"]
      },
      "explanation": "Criterion contamination: Irrelevant factors influencing criterion scores—attractiveness bias and knowledge of predictor scores (which creates artificial predictor-criterion correlation). Criterion deficiency: Important performance aspects not captured—sales figures missing customer satisfaction, and attendance missing quality. Criterion relevance: The degree to which the criterion measures what it should—capturing important job aspects and aligning with job analysis. These three concepts together describe criterion adequacy: a good criterion maximizes relevance, minimizes deficiency, and minimizes contamination.",
      "difficulty": "hard",
      "tags": ["criterion contamination", "criterion deficiency", "criterion relevance"]
    },
    {
      "type": "drag-drop-categorize",
      "id": "d6-ja-dc-002",
      "stem": "Categorize each behavior as task performance, contextual performance (OCB), or counterproductive work behavior (CWB):",
      "categories": [
        {"id": "cat1", "label": "Task Performance"},
        {"id": "cat2", "label": "Contextual Performance (OCB)"},
        {"id": "cat3", "label": "Counterproductive Work Behavior (CWB)"}
      ],
      "items": [
        {"id": "item1", "text": "Completing required reports accurately and on time"},
        {"id": "item2", "text": "Volunteering to help a struggling coworker with their workload"},
        {"id": "item3", "text": "Taking unauthorized extended breaks"},
        {"id": "item4", "text": "Operating equipment according to technical specifications"},
        {"id": "item5", "text": "Spreading false rumors about a supervisor"},
        {"id": "item6", "text": "Attending optional training to improve team capabilities"},
        {"id": "item7", "text": "Stealing office supplies for personal use"},
        {"id": "item8", "text": "Defending the organization's reputation when others criticize it"}
      ],
      "correctCategorization": {
        "cat1": ["item1", "item4"],
        "cat2": ["item2", "item6", "item8"],
        "cat3": ["item3", "item5", "item7"]
      },
      "explanation": "Task performance: Core job duties explicitly required (completing reports, operating equipment). Contextual performance (OCBs): Discretionary behaviors supporting the organizational environment—helping coworkers (OCB-I), attending optional training (OCB-O), defending the organization (civic virtue). CWBs: Voluntary harmful behaviors—unauthorized breaks (production deviance), spreading rumors (personal aggression), theft (property deviance). Borman and Motowidlo's distinction and Organ's OCB framework both highlight that organizations depend on behavior beyond formal task requirements.",
      "difficulty": "hard",
      "tags": ["task performance", "OCB", "CWB", "performance taxonomy"]
    }
  ]
}