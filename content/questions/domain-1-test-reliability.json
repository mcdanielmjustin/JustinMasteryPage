{
  "domain": "Domain 1: Psychometrics & Research Methods",
  "domainId": 1,
  "subdomain": "Consistency of Measurement \u2014 Test Reliability",
  "asppbDomain": 5,
  "questionCount": 26,
  "questions": [
    {
      "type": "single_choice",
      "id": "d1-tr-sc-001",
      "stem": "A psychologist administers the same depression inventory to a group of outpatients on two occasions separated by two weeks. The correlation between scores is r = 0.89. This coefficient represents:",
      "options": [
        {"id": "a", "text": "Internal consistency reliability", "isCorrect": false},
        {"id": "b", "text": "Test-retest reliability", "isCorrect": true},
        {"id": "c", "text": "Inter-rater reliability", "isCorrect": false},
        {"id": "d", "text": "Alternate forms reliability", "isCorrect": false}
      ],
      "explanation": "Test-retest reliability (temporal stability) measures consistency of scores across time by correlating scores from the same test administered on two occasions. r = 0.89 indicates good stability. This method assumes the construct being measured is stable over the testing interval.",
      "difficulty": "hard",
      "tags": ["test-retest", "temporal stability", "reliability types"]
    },
    {
      "type": "single_choice",
      "id": "d1-tr-sc-002",
      "stem": "A test developer calculates Cronbach's alpha for a 20-item anxiety scale and obtains α = 0.65. According to widely accepted standards, this indicates:",
      "options": [
        {"id": "a", "text": "Excellent internal consistency", "isCorrect": false},
        {"id": "b", "text": "Good internal consistency", "isCorrect": false},
        {"id": "c", "text": "Questionable/marginal internal consistency", "isCorrect": true},
        {"id": "d", "text": "Completely unacceptable reliability", "isCorrect": false}
      ],
      "explanation": "Common alpha guidelines: ≥.90 excellent, .80-.89 good, .70-.79 acceptable, .60-.69 questionable, <.60 poor. Alpha of .65 is marginal—may be acceptable for research but problematic for clinical decisions. Consider item revision or longer test.",
      "difficulty": "hard",
      "tags": ["Cronbach's alpha", "internal consistency", "interpretation"]
    },
    {
      "type": "single_choice",
      "id": "d1-tr-sc-003",
      "stem": "An item on an anxiety test has a discrimination index of -0.15. This indicates that:",
      "options": [
        {"id": "a", "text": "The item discriminates well between high and low anxiety individuals", "isCorrect": false},
        {"id": "b", "text": "High scorers are LESS likely to endorse this item than low scorers", "isCorrect": true},
        {"id": "c", "text": "The item has moderate difficulty", "isCorrect": false},
        {"id": "d", "text": "The item should be retained without modification", "isCorrect": false}
      ],
      "explanation": "Negative discrimination index means high total scorers are less likely to endorse the item than low scorers—the item is working backwards. This suggests a flawed item (possibly reverse-scored incorrectly, ambiguous wording, or measuring a different construct). Should be revised or removed.",
      "difficulty": "hard",
      "tags": ["item discrimination", "item analysis", "negative discrimination"]
    },
    {
      "type": "single_choice",
      "id": "d1-tr-sc-004",
      "stem": "For a test used to make high-stakes clinical decisions (e.g., determining competency), the MINIMUM acceptable reliability coefficient should be:",
      "options": [
        {"id": "a", "text": "0.70", "isCorrect": false},
        {"id": "b", "text": "0.80", "isCorrect": false},
        {"id": "c", "text": "0.90 or higher", "isCorrect": true},
        {"id": "d", "text": "0.60 is sufficient", "isCorrect": false}
      ],
      "explanation": "For high-stakes individual decisions (clinical diagnosis, placement, competency), reliability of 0.90 or higher is recommended. Lower reliability means greater measurement error, which is unacceptable when decisions significantly affect individuals. Research use can accept lower reliability.",
      "difficulty": "hard",
      "tags": ["reliability standards", "high-stakes testing", "clinical decisions"]
    },
    {
      "type": "single_choice",
      "id": "d1-tr-sc-005",
      "stem": "The standard error of measurement (SEM) is MOST directly used to:",
      "options": [
        {"id": "a", "text": "Determine if a test is valid", "isCorrect": false},
        {"id": "b", "text": "Construct confidence intervals around observed scores", "isCorrect": true},
        {"id": "c", "text": "Calculate the difficulty of test items", "isCorrect": false},
        {"id": "d", "text": "Assess inter-rater agreement", "isCorrect": false}
      ],
      "explanation": "SEM quantifies measurement error in score units, allowing construction of confidence intervals around observed scores. An observed score of 100 with SEM of 5 gives 95% CI of approximately 100 ± 10 (±2 SEM). SEM = SD × √(1 - reliability).",
      "difficulty": "hard",
      "tags": ["SEM", "confidence intervals", "measurement error"]
    },
    {
      "type": "multiple_choice",
      "id": "d1-tr-mc-001",
      "stem": "Which factors typically INCREASE a test's reliability coefficient? Select all that apply.",
      "options": [
        {"id": "a", "text": "Increasing the number of items on the test", "isCorrect": true},
        {"id": "b", "text": "Using a more heterogeneous (diverse) sample", "isCorrect": true},
        {"id": "c", "text": "Using items of moderate difficulty", "isCorrect": true},
        {"id": "d", "text": "Reducing the time allowed for completion", "isCorrect": false},
        {"id": "e", "text": "Increasing item discrimination values", "isCorrect": true}
      ],
      "explanation": "Reliability increases with: more items (Spearman-Brown prophecy), heterogeneous samples (more variance), moderate difficulty items (maximize variance), and high discrimination items. Reducing time increases error/guessing, typically reducing reliability.",
      "difficulty": "hard",
      "tags": ["factors affecting reliability", "test length", "sample heterogeneity"]
    },
    {
      "type": "multiple_choice",
      "id": "d1-tr-mc-002",
      "stem": "Which statements about Cronbach's alpha are TRUE? Select all that apply.",
      "options": [
        {"id": "a", "text": "It measures internal consistency", "isCorrect": true},
        {"id": "b", "text": "It represents the mean of all possible split-half coefficients", "isCorrect": true},
        {"id": "c", "text": "Higher alpha always indicates a better test", "isCorrect": false},
        {"id": "d", "text": "It can be artificially inflated by redundant items", "isCorrect": true},
        {"id": "e", "text": "It assumes items are essentially tau-equivalent", "isCorrect": true}
      ],
      "explanation": "Alpha measures internal consistency and equals mean of all split-half reliabilities. BUT: very high alpha (>.95) may indicate redundant items. Alpha assumes tau-equivalence (equal item-total covariances)—if violated, it underestimates reliability. Consider omega coefficients for multidimensional scales.",
      "difficulty": "hard",
      "tags": ["Cronbach's alpha", "limitations", "assumptions"]
    },
    {
      "type": "multiple_choice",
      "id": "d1-tr-mc-003",
      "stem": "Which types of reliability evidence would be appropriate for an observational behavioral coding system? Select all that apply.",
      "options": [
        {"id": "a", "text": "Inter-rater reliability (Cohen's kappa or ICC)", "isCorrect": true},
        {"id": "b", "text": "Internal consistency (Cronbach's alpha)", "isCorrect": false},
        {"id": "c", "text": "Percent agreement between raters", "isCorrect": true},
        {"id": "d", "text": "Intra-class correlation coefficient", "isCorrect": true},
        {"id": "e", "text": "Test-retest reliability", "isCorrect": false}
      ],
      "explanation": "Behavioral coding reliability concerns agreement between observers. Kappa and ICC account for chance agreement (preferred). Percent agreement is simple but inflated by chance. Alpha isn't applicable (items don't exist). Test-retest isn't relevant for observational coding.",
      "difficulty": "hard",
      "tags": ["inter-rater reliability", "Cohen's kappa", "ICC", "behavioral observation"]
    },
    {
      "type": "multiple_choice",
      "id": "d1-tr-mc-004",
      "stem": "Which item analysis statistics indicate a GOOD item? Select all that apply.",
      "options": [
        {"id": "a", "text": "Discrimination index between 0.30 and 0.70", "isCorrect": true},
        {"id": "b", "text": "Item difficulty (p-value) near 0.50 for maximum discrimination", "isCorrect": true},
        {"id": "c", "text": "Positive, moderate corrected item-total correlation", "isCorrect": true},
        {"id": "d", "text": "All distractors attracting equal numbers of respondents", "isCorrect": false},
        {"id": "e", "text": "Removal of item would substantially lower Cronbach's alpha", "isCorrect": true}
      ],
      "explanation": "Good items: moderate discrimination (.30-.70), moderate difficulty (p≈.50), positive item-total correlation, and contribute to alpha. Distractor analysis: incorrect options should primarily attract low scorers—equal attraction suggests random guessing or flawed distractors.",
      "difficulty": "hard",
      "tags": ["item analysis criteria", "good items"]
    },
    {
      "type": "multiple_choice",
      "id": "d1-tr-mc-005",
      "stem": "The Spearman-Brown prophecy formula can be used to estimate which of the following? Select all that apply.",
      "options": [
        {"id": "a", "text": "Reliability if the test were lengthened", "isCorrect": true},
        {"id": "b", "text": "Reliability if the test were shortened", "isCorrect": true},
        {"id": "c", "text": "How many items needed to achieve a desired reliability", "isCorrect": true},
        {"id": "d", "text": "Validity of the test", "isCorrect": false},
        {"id": "e", "text": "Reliability of a full test from split-half correlation", "isCorrect": true}
      ],
      "explanation": "Spearman-Brown estimates reliability change with length change. Uses: estimate full test reliability from split-half, project reliability for longer/shorter tests, calculate items needed for target reliability. It doesn't address validity—a long unreliable test remains invalid.",
      "difficulty": "hard",
      "tags": ["Spearman-Brown", "test length", "reliability estimation"]
    },
    {
      "type": "matrix_single",
      "id": "d1-tr-ms-001",
      "stem": "Match each reliability type with its appropriate use.",
      "rows": [
        {"id": "r1", "text": "Test-retest reliability"},
        {"id": "r2", "text": "Internal consistency (alpha)"},
        {"id": "r3", "text": "Inter-rater reliability"},
        {"id": "r4", "text": "Alternate forms reliability"}
      ],
      "columns": [
        {"id": "c1", "text": "Assess stability of trait over time"},
        {"id": "c2", "text": "Assess homogeneity of item content"},
        {"id": "c3", "text": "Assess consistency between observers/scorers"},
        {"id": "c4", "text": "Assess equivalence of parallel versions"}
      ],
      "correctAnswers": [
        {"rowId": "r1", "columnId": "c1"},
        {"rowId": "r2", "columnId": "c2"},
        {"rowId": "r3", "columnId": "c3"},
        {"rowId": "r4", "columnId": "c4"}
      ],
      "explanation": "Test-retest: temporal stability. Alpha: internal consistency/homogeneity. Inter-rater: observer agreement. Alternate forms: equivalence of different versions. Choose reliability type based on what consistency is most important for your measure's intended use.",
      "difficulty": "hard",
      "tags": ["reliability types", "appropriate use"]
    },
    {
      "type": "matrix_single",
      "id": "d1-tr-ms-002",
      "stem": "Match each item statistic with its correct interpretation.",
      "rows": [
        {"id": "r1", "text": "Item difficulty (p) = 0.85"},
        {"id": "r2", "text": "Item difficulty (p) = 0.20"},
        {"id": "r3", "text": "Discrimination index = 0.45"},
        {"id": "r4", "text": "Discrimination index = -0.10"}
      ],
      "columns": [
        {"id": "c1", "text": "Easy item—most people answer correctly"},
        {"id": "c2", "text": "Difficult item—few people answer correctly"},
        {"id": "c3", "text": "Good discrimination—high scorers more likely to endorse"},
        {"id": "c4", "text": "Problematic item—working backwards"}
      ],
      "correctAnswers": [
        {"rowId": "r1", "columnId": "c1"},
        {"rowId": "r2", "columnId": "c2"},
        {"rowId": "r3", "columnId": "c3"},
        {"rowId": "r4", "columnId": "c4"}
      ],
      "explanation": "Difficulty (p) = proportion correct: high p = easy, low p = hard. Discrimination: positive = high scorers endorse more (good); negative = low scorers endorse more (problematic—item works against the scale). Ideal items have moderate difficulty and positive discrimination.",
      "difficulty": "hard",
      "tags": ["item statistics interpretation"]
    },
    {
      "type": "matrix_multiple",
      "id": "d1-tr-mm-001",
      "stem": "Select ALL sources of error that each reliability type is designed to capture.",
      "rows": [
        {"id": "r1", "text": "Test-retest reliability"},
        {"id": "r2", "text": "Internal consistency"},
        {"id": "r3", "text": "Inter-rater reliability"},
        {"id": "r4", "text": "Alternate forms reliability"}
      ],
      "columns": [
        {"id": "c1", "text": "Time/occasion sampling error"},
        {"id": "c2", "text": "Item/content sampling error"},
        {"id": "c3", "text": "Scorer/observer error"},
        {"id": "c4", "text": "Form/version differences"}
      ],
      "correctAnswers": [
        {"rowId": "r1", "columnId": "c1"},
        {"rowId": "r2", "columnId": "c2"},
        {"rowId": "r3", "columnId": "c3"},
        {"rowId": "r4", "columnId": "c2"},
        {"rowId": "r4", "columnId": "c4"}
      ],
      "explanation": "Different reliability types address different error sources: Test-retest = time fluctuations. Internal consistency = item sampling. Inter-rater = scorer variability. Alternate forms = both item sampling and form differences. Comprehensive reliability assessment examines multiple sources.",
      "difficulty": "hard",
      "tags": ["error sources", "reliability types"]
    },
    {
      "type": "matrix_multiple",
      "id": "d1-tr-mm-002",
      "stem": "For each scenario, select ALL appropriate methods for assessing reliability.",
      "rows": [
        {"id": "r1", "text": "A 50-item personality questionnaire"},
        {"id": "r2", "text": "A projective test scored by clinicians"},
        {"id": "r3", "text": "A speeded clerical ability test"},
        {"id": "r4", "text": "A test meant to measure stable IQ"}
      ],
      "columns": [
        {"id": "c1", "text": "Internal consistency (alpha)"},
        {"id": "c2", "text": "Test-retest"},
        {"id": "c3", "text": "Inter-rater reliability"},
        {"id": "c4", "text": "Split-half (odd-even)"}
      ],
      "correctAnswers": [
        {"rowId": "r1", "columnId": "c1"},
        {"rowId": "r1", "columnId": "c2"},
        {"rowId": "r2", "columnId": "c3"},
        {"rowId": "r3", "columnId": "c2"},
        {"rowId": "r4", "columnId": "c1"},
        {"rowId": "r4", "columnId": "c2"}
      ],
      "explanation": "Personality questionnaire: alpha and test-retest appropriate. Projective (subjective scoring): inter-rater essential. Speeded test: alpha inappropriate (speed, not item content, determines responses)—use test-retest or parallel forms. Stable IQ: test-retest confirms stability, alpha confirms homogeneity.",
      "difficulty": "hard",
      "tags": ["reliability method selection", "speeded tests", "projective tests"]
    },
    {
      "type": "cloze_dropdown",
      "id": "d1-tr-cd-001",
      "stem": "Classical Test Theory states that an observed score (X) equals the {{blank1}} plus {{blank2}}. Reliability is the proportion of {{blank3}} in observed scores. A test with reliability of 0.80 means that {{blank4}}% of score variance is due to measurement error.",
      "blanks": [
        {
          "id": "blank1",
          "options": [
            {"id": "a", "text": "true score (T)", "isCorrect": true},
            {"id": "b", "text": "error score (E)", "isCorrect": false},
            {"id": "c", "text": "standard error", "isCorrect": false},
            {"id": "d", "text": "validity coefficient", "isCorrect": false}
          ]
        },
        {
          "id": "blank2",
          "options": [
            {"id": "a", "text": "true score (T)", "isCorrect": false},
            {"id": "b", "text": "error (E)", "isCorrect": true},
            {"id": "c", "text": "validity", "isCorrect": false},
            {"id": "d", "text": "standard deviation", "isCorrect": false}
          ]
        },
        {
          "id": "blank3",
          "options": [
            {"id": "a", "text": "error variance", "isCorrect": false},
            {"id": "b", "text": "true score variance", "isCorrect": true},
            {"id": "c", "text": "total variance", "isCorrect": false},
            {"id": "d", "text": "covariance", "isCorrect": false}
          ]
        },
        {
          "id": "blank4",
          "options": [
            {"id": "a", "text": "80", "isCorrect": false},
            {"id": "b", "text": "20", "isCorrect": true},
            {"id": "c", "text": "64", "isCorrect": false},
            {"id": "d", "text": "36", "isCorrect": false}
          ]
        }
      ],
      "explanation": "CTT: X = T + E (observed = true + error). Reliability = true score variance / observed variance. If reliability = .80, true score variance = 80%, error variance = 20% (1 - reliability). This is why higher reliability means less measurement error.",
      "difficulty": "hard",
      "tags": ["Classical Test Theory", "CTT formula", "error variance"]
    },
    {
      "type": "cloze_dropdown",
      "id": "d1-tr-cd-002",
      "stem": "The Standard Error of Measurement (SEM) is calculated as SEM = {{blank1}} × √(1 - {{blank2}}). If a test has SD = 15 and reliability = 0.91, the SEM equals {{blank3}}. A 95% confidence interval around an observed score uses approximately {{blank4}} times the SEM.",
      "blanks": [
        {
          "id": "blank1",
          "options": [
            {"id": "a", "text": "SD", "isCorrect": true},
            {"id": "b", "text": "mean", "isCorrect": false},
            {"id": "c", "text": "reliability", "isCorrect": false},
            {"id": "d", "text": "variance", "isCorrect": false}
          ]
        },
        {
          "id": "blank2",
          "options": [
            {"id": "a", "text": "SD", "isCorrect": false},
            {"id": "b", "text": "reliability", "isCorrect": true},
            {"id": "c", "text": "mean", "isCorrect": false},
            {"id": "d", "text": "variance", "isCorrect": false}
          ]
        },
        {
          "id": "blank3",
          "options": [
            {"id": "a", "text": "4.5", "isCorrect": true},
            {"id": "b", "text": "1.5", "isCorrect": false},
            {"id": "c", "text": "15", "isCorrect": false},
            {"id": "d", "text": "0.91", "isCorrect": false}
          ]
        },
        {
          "id": "blank4",
          "options": [
            {"id": "a", "text": "1", "isCorrect": false},
            {"id": "b", "text": "2 (±1.96)", "isCorrect": true},
            {"id": "c", "text": "3", "isCorrect": false},
            {"id": "d", "text": "0.5", "isCorrect": false}
          ]
        }
      ],
      "explanation": "SEM = SD × √(1 - rxx). With SD=15, reliability=.91: SEM = 15 × √(0.09) = 15 × 0.30 = 4.5. For 95% CI: ±1.96 SEM (≈2 SEM). So observed score of 100 has 95% CI of approximately 91-109.",
      "difficulty": "hard",
      "tags": ["SEM formula", "confidence intervals", "calculation"]
    },
    {
      "type": "bowtie",
      "id": "d1-tr-bt-001",
      "stem": "A researcher develops a 15-item scale measuring social anxiety and obtains Cronbach's alpha = 0.68. The scale is intended for clinical screening decisions.",
      "config": {
        "actionsLabel": "Problems Identified",
        "conditionLabel": "Core Issue",
        "parametersLabel": "Solutions"
      },
      "options": [
        {"id": "a1", "text": "Alpha is below acceptable threshold for clinical use (.90)", "category": "action", "isCorrect": true},
        {"id": "a2", "text": "Measurement error will be relatively high", "category": "action", "isCorrect": true},
        {"id": "a3", "text": "Some items may have low discrimination", "category": "action", "isCorrect": true},
        {"id": "a4", "text": "The scale has too many items", "category": "action", "isCorrect": false},
        {"id": "c1", "text": "Inadequate internal consistency for high-stakes decisions", "category": "condition", "isCorrect": true},
        {"id": "c2", "text": "The scale lacks construct validity", "category": "condition", "isCorrect": false},
        {"id": "c3", "text": "Inter-rater reliability is poor", "category": "condition", "isCorrect": false},
        {"id": "p1", "text": "Conduct item analysis and remove/revise poor items", "category": "parameter", "isCorrect": true},
        {"id": "p2", "text": "Add more high-quality items (Spearman-Brown)", "category": "parameter", "isCorrect": true},
        {"id": "p3", "text": "Use for research screening only, not clinical decisions", "category": "parameter", "isCorrect": true},
        {"id": "p4", "text": "Remove all items and start over", "category": "parameter", "isCorrect": false}
      ],
      "explanation": "Alpha .68 is marginal for research, inadequate for clinical screening. Solutions: item analysis to identify weak items, add quality items (reliability increases with length per Spearman-Brown), or restrict use to lower-stakes applications until improved.",
      "difficulty": "hard",
      "tags": ["low reliability", "clinical standards", "improvement strategies"]
    },
    {
      "type": "bowtie",
      "id": "d1-tr-bt-002",
      "stem": "A researcher examines item analysis for a 30-item achievement test. Item 17 has: difficulty (p) = 0.95, discrimination index = 0.08, and corrected item-total correlation = 0.05.",
      "config": {
        "actionsLabel": "Item Problems",
        "conditionLabel": "Interpretation",
        "parametersLabel": "Recommended Actions"
      },
      "options": [
        {"id": "a1", "text": "Item is too easy (95% correct)", "category": "action", "isCorrect": true},
        {"id": "a2", "text": "Item fails to discriminate between high and low performers", "category": "action", "isCorrect": true},
        {"id": "a3", "text": "Item contributes minimally to overall reliability", "category": "action", "isCorrect": true},
        {"id": "a4", "text": "Item is too difficult", "category": "action", "isCorrect": false},
        {"id": "c1", "text": "Ceiling effect—almost everyone gets it right", "category": "condition", "isCorrect": true},
        {"id": "c2", "text": "Item measures a different construct", "category": "condition", "isCorrect": false},
        {"id": "c3", "text": "Item is perfectly calibrated", "category": "condition", "isCorrect": false},
        {"id": "p1", "text": "Revise item to be more challenging", "category": "parameter", "isCorrect": true},
        {"id": "p2", "text": "Consider removing from scored test", "category": "parameter", "isCorrect": true},
        {"id": "p3", "text": "Keep as a warm-up/confidence item if needed", "category": "parameter", "isCorrect": true},
        {"id": "p4", "text": "Weight this item more heavily", "category": "parameter", "isCorrect": false}
      ],
      "explanation": "Item 17 shows classic 'too easy' pattern: high p (0.95), near-zero discrimination (0.08), minimal item-total correlation. It provides no information about ability differences. Options: make harder, remove from scoring, or keep only for engagement/rapport purposes.",
      "difficulty": "hard",
      "tags": ["item analysis", "ceiling effect", "item revision"]
    },
    {
      "type": "highlight",
      "id": "d1-tr-hl-001",
      "stem": "Highlight ALL statements that are TRUE about reliability.",
      "passage": "Reliability is fundamental to psychological measurement. [Reliability refers to the consistency or stability of measurement]. [A test can be valid without being reliable]. [Reliability sets an upper limit on validity—a test cannot be more valid than it is reliable]. [High reliability guarantees high validity]. [Reliability coefficients range from 0 to 1, with higher values indicating less measurement error]. [The same reliability coefficient is appropriate for all testing purposes].",
      "highlightableSegments": [
        {"id": "h1", "text": "Reliability refers to the consistency or stability of measurement", "isCorrect": true},
        {"id": "h2", "text": "A test can be valid without being reliable", "isCorrect": false},
        {"id": "h3", "text": "Reliability sets an upper limit on validity—a test cannot be more valid than it is reliable", "isCorrect": true},
        {"id": "h4", "text": "High reliability guarantees high validity", "isCorrect": false},
        {"id": "h5", "text": "Reliability coefficients range from 0 to 1, with higher values indicating less measurement error", "isCorrect": true},
        {"id": "h6", "text": "The same reliability coefficient is appropriate for all testing purposes", "isCorrect": false}
      ],
      "explanation": "True: reliability is consistency; reliability limits validity (validity ≤ √reliability); coefficients 0-1. False: reliability is necessary but not sufficient for validity; a test cannot be valid without reliability; different purposes require different reliability levels (.70 for research, .90 for clinical).",
      "difficulty": "hard",
      "tags": ["reliability principles", "reliability-validity relationship"]
    },
    {
      "type": "highlight",
      "id": "d1-tr-hl-002",
      "stem": "Highlight ALL correct statements about item difficulty and discrimination.",
      "passage": "Item analysis provides crucial information for test development. [Item difficulty (p) is the proportion of examinees who answer correctly—higher p means easier items]. [Items with difficulty near 0.50 provide maximum discrimination for norm-referenced tests]. [Discrimination index compares performance between high and low scorers on the total test]. [Negative discrimination values indicate items where low scorers outperform high scorers]. [All items should have the same difficulty level for a good test]. [Items with very high or very low difficulty contribute less to test reliability].",
      "highlightableSegments": [
        {"id": "h1", "text": "Item difficulty (p) is the proportion of examinees who answer correctly—higher p means easier items", "isCorrect": true},
        {"id": "h2", "text": "Items with difficulty near 0.50 provide maximum discrimination for norm-referenced tests", "isCorrect": true},
        {"id": "h3", "text": "Discrimination index compares performance between high and low scorers on the total test", "isCorrect": true},
        {"id": "h4", "text": "Negative discrimination values indicate items where low scorers outperform high scorers", "isCorrect": true},
        {"id": "h5", "text": "All items should have the same difficulty level for a good test", "isCorrect": false},
        {"id": "h6", "text": "Items with very high or very low difficulty contribute less to test reliability", "isCorrect": true}
      ],
      "explanation": "All statements true except: items should vary in difficulty to assess different ability levels and prevent floor/ceiling effects. The principle is moderate average difficulty with range. Very easy/hard items provide little variance, reducing reliability.",
      "difficulty": "hard",
      "tags": ["item difficulty", "item discrimination", "item analysis"]
    },
    {
      "type": "drag_drop_ordered",
      "id": "d1-tr-do-001",
      "stem": "Arrange the steps of item analysis for test development in the correct sequence.",
      "items": [
        {"id": "i1", "text": "Administer test to appropriate sample", "correctPosition": 1},
        {"id": "i2", "text": "Calculate item difficulty (p-values) for each item", "correctPosition": 2},
        {"id": "i3", "text": "Calculate item discrimination indices", "correctPosition": 3},
        {"id": "i4", "text": "Examine distractor analysis for multiple-choice items", "correctPosition": 4},
        {"id": "i5", "text": "Identify items for revision or removal", "correctPosition": 5},
        {"id": "i6", "text": "Recalculate reliability after item changes", "correctPosition": 6}
      ],
      "explanation": "Item analysis sequence: administer test → calculate difficulty → calculate discrimination → analyze distractors → identify problem items → iterate. After removing/revising items, recalculate reliability to confirm improvement. This is an iterative process.",
      "difficulty": "hard",
      "tags": ["item analysis procedure", "test development"]
    },
    {
      "type": "drag_drop_ordered",
      "id": "d1-tr-do-002",
      "stem": "Arrange reliability coefficient values from LOWEST to HIGHEST acceptable standard.",
      "items": [
        {"id": "i1", "text": "Minimum for early research/exploration: ≥ 0.60", "correctPosition": 1},
        {"id": "i2", "text": "Acceptable for research purposes: ≥ 0.70", "correctPosition": 2},
        {"id": "i3", "text": "Good for group-level decisions: ≥ 0.80", "correctPosition": 3},
        {"id": "i4", "text": "Required for individual clinical decisions: ≥ 0.90", "correctPosition": 4},
        {"id": "i5", "text": "Ideal for high-stakes individual testing: ≥ 0.95", "correctPosition": 5}
      ],
      "explanation": "Reliability standards increase with stakes: .60 exploratory, .70 research, .80 group decisions, .90 individual clinical, .95 high-stakes individual. Higher stakes demand more precision because errors have greater consequences.",
      "difficulty": "hard",
      "tags": ["reliability standards", "acceptable thresholds"]
    },
    {
      "type": "trend",
      "id": "d1-tr-tr-001",
      "stem": "A test developer examines how reliability changes as items are added to a scale (using Spearman-Brown projections).",
      "data": {
        "timePoints": ["10 items", "20 items", "30 items", "40 items", "50 items", "100 items"],
        "measurements": [
          {"label": "Projected Reliability", "values": [0.70, 0.82, 0.88, 0.90, 0.92, 0.96]}
        ]
      },
      "question": "What does this pattern illustrate about the relationship between test length and reliability?",
      "options": [
        {"id": "a", "text": "Reliability increases linearly with test length", "isCorrect": false},
        {"id": "b", "text": "Reliability shows diminishing returns as items are added", "isCorrect": true},
        {"id": "c", "text": "Reliability plateaus at exactly 50 items", "isCorrect": false},
        {"id": "d", "text": "Adding items always improves reliability proportionally", "isCorrect": false}
      ],
      "explanation": "Diminishing returns: early items dramatically improve reliability (10→20 items: .70→.82), but gains decrease as the test lengthens (50→100 items: .92→.96). Eventually, practical constraints (time, fatigue) outweigh marginal reliability gains. Spearman-Brown helps find optimal length.",
      "difficulty": "hard",
      "tags": ["Spearman-Brown", "test length", "diminishing returns"]
    },
    {
      "type": "trend",
      "id": "d1-tr-tr-002",
      "stem": "A researcher examines test-retest reliability across different time intervals for a measure of state anxiety.",
      "data": {
        "timePoints": ["1 day", "1 week", "2 weeks", "1 month", "3 months", "6 months"],
        "measurements": [
          {"label": "Test-Retest Correlation", "values": [0.92, 0.85, 0.78, 0.65, 0.52, 0.41]}
        ]
      },
      "question": "This pattern suggests the measure is assessing:",
      "options": [
        {"id": "a", "text": "A stable trait that doesn't change over time", "isCorrect": false},
        {"id": "b", "text": "A state characteristic that fluctuates over time", "isCorrect": true},
        {"id": "c", "text": "An unreliable construct", "isCorrect": false},
        {"id": "d", "text": "A construct with increasing measurement error", "isCorrect": false}
      ],
      "explanation": "Declining test-retest correlation over time indicates the measure captures something that genuinely changes (state), not measurement error. State anxiety fluctuates; trait anxiety would show stable correlations. High short-term stability (r=.92) suggests adequate reliability; the decline reflects real change.",
      "difficulty": "hard",
      "tags": ["state vs trait", "test-retest intervals", "construct nature"]
    },
    {
      "type": "drag_drop_categorize",
      "id": "d1-tr-dc-001",
      "stem": "Categorize each scenario by the type of reliability evidence needed.",
      "categories": [
        {"id": "cat1", "name": "Internal Consistency"},
        {"id": "cat2", "name": "Test-Retest"},
        {"id": "cat3", "name": "Inter-Rater"},
        {"id": "cat4", "name": "Alternate Forms"}
      ],
      "items": [
        {"id": "i1", "text": "Determining if items on a depression scale measure the same construct", "correctCategory": "cat1"},
        {"id": "i2", "text": "Assessing if personality scores are stable over 6 months", "correctCategory": "cat2"},
        {"id": "i3", "text": "Checking if two clinicians assign the same diagnosis", "correctCategory": "cat3"},
        {"id": "i4", "text": "Ensuring Form A and Form B of a test are equivalent", "correctCategory": "cat4"},
        {"id": "i5", "text": "Determining if Rorschach responses are scored consistently", "correctCategory": "cat3"},
        {"id": "i6", "text": "Assessing if GRE verbal sections are interchangeable", "correctCategory": "cat4"}
      ],
      "explanation": "Internal consistency: item homogeneity. Test-retest: temporal stability. Inter-rater: scorer agreement (diagnoses, projective scoring). Alternate forms: parallel version equivalence (standardized test forms).",
      "difficulty": "hard",
      "tags": ["reliability type selection"]
    },
    {
      "type": "drag_drop_categorize",
      "id": "d1-tr-dc-002",
      "stem": "Categorize each item analysis result as indicating a GOOD ITEM, PROBLEMATIC ITEM, or NEEDS REVISION.",
      "categories": [
        {"id": "cat1", "name": "Good Item—Retain"},
        {"id": "cat2", "name": "Problematic—Consider Removing"},
        {"id": "cat3", "name": "Needs Revision"}
      ],
      "items": [
        {"id": "i1", "text": "p = 0.55, discrimination = 0.42", "correctCategory": "cat1"},
        {"id": "i2", "text": "p = 0.98, discrimination = 0.05", "correctCategory": "cat2"},
        {"id": "i3", "text": "p = 0.45, discrimination = -0.18", "correctCategory": "cat2"},
        {"id": "i4", "text": "p = 0.65, discrimination = 0.25", "correctCategory": "cat3"},
        {"id": "i5", "text": "p = 0.50, discrimination = 0.55, all distractors function well", "correctCategory": "cat1"},
        {"id": "i6", "text": "p = 0.40, good discrimination, but one distractor never chosen", "correctCategory": "cat3"}
      ],
      "explanation": "Good: moderate difficulty, positive discrimination (≥.30), functional distractors. Problematic: too easy/hard, negative/zero discrimination. Needs revision: acceptable but improvable (low discrimination, non-functional distractors).",
      "difficulty": "hard",
      "tags": ["item analysis decisions", "item quality"]
    }
  ]
}
