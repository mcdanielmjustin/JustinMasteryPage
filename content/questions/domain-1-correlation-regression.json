{
  "domain": "Domain 1: Psychometrics & Research Methods",
  "domainId": 1,
  "subdomain": "Relationships in Data \u2014 Correlation & Regression",
  "asppbDomain": 7,
  "questionCount": 26,
  "questions": [
    {
      "type": "single_choice",
      "id": "d1-cr-sc-001",
      "stem": "A researcher finds a correlation of r = -0.85 between hours of sleep deprivation and cognitive test performance. Which interpretation is MOST accurate?",
      "options": [
        {"id": "a", "text": "Sleep deprivation causes poor cognitive performance", "isCorrect": false},
        {"id": "b", "text": "There is a strong negative linear relationship; more deprivation associates with lower scores", "isCorrect": true},
        {"id": "c", "text": "85% of variance in test scores is explained by sleep deprivation", "isCorrect": false},
        {"id": "d", "text": "The relationship is weak because the correlation is negative", "isCorrect": false}
      ],
      "explanation": "r = -0.85 indicates a strong negative linear relationship—as one variable increases, the other decreases. Correlation never implies causation. The coefficient of determination (r² = 0.72) would indicate explained variance, not r itself. Negative correlations can be just as strong as positive ones.",
      "difficulty": "hard",
      "tags": ["correlation interpretation", "negative correlation", "causation"]
    },
    {
      "type": "single_choice",
      "id": "d1-cr-sc-002",
      "stem": "In a multiple regression analysis predicting job performance from three predictors, the researcher obtains R² = 0.45 and adjusted R² = 0.42. The difference between these values indicates:",
      "options": [
        {"id": "a", "text": "The model is overfitting due to too many predictors relative to sample size", "isCorrect": true},
        {"id": "b", "text": "Multicollinearity among predictors", "isCorrect": false},
        {"id": "c", "text": "Heteroscedasticity in the residuals", "isCorrect": false},
        {"id": "d", "text": "Non-linear relationships between predictors and criterion", "isCorrect": false}
      ],
      "explanation": "Adjusted R² penalizes for the number of predictors relative to sample size. The gap between R² and adjusted R² indicates some inflation from adding predictors that don't substantially improve prediction—suggesting potential overfitting. Larger gaps indicate more concern.",
      "difficulty": "hard",
      "tags": ["multiple regression", "R-squared", "adjusted R-squared", "overfitting"]
    },
    {
      "type": "single_choice",
      "id": "d1-cr-sc-003",
      "stem": "A psychologist correlates a new anxiety measure with an established anxiety scale and obtains r = 0.78. When the new measure is correlated with a depression scale, r = 0.75. These results suggest:",
      "options": [
        {"id": "a", "text": "Excellent convergent validity with minimal discriminant validity concerns", "isCorrect": false},
        {"id": "b", "text": "The new measure may lack discriminant validity", "isCorrect": true},
        {"id": "c", "text": "The new measure has excellent concurrent validity", "isCorrect": false},
        {"id": "d", "text": "Both correlations indicate strong predictive validity", "isCorrect": false}
      ],
      "explanation": "Convergent validity requires high correlation with similar constructs (anxiety-anxiety = 0.78, good). Discriminant validity requires lower correlation with dissimilar constructs—but anxiety-depression correlation (0.75) is nearly as high, suggesting the measure may not discriminate between anxiety and depression.",
      "difficulty": "hard",
      "tags": ["convergent validity", "discriminant validity", "construct validity"]
    },
    {
      "type": "single_choice",
      "id": "d1-cr-sc-004",
      "stem": "A researcher uses Spearman's rho instead of Pearson's r to correlate two variables. This choice is MOST appropriate when:",
      "options": [
        {"id": "a", "text": "The sample size is very large", "isCorrect": false},
        {"id": "b", "text": "One or both variables are ordinal or the relationship is monotonic but nonlinear", "isCorrect": true},
        {"id": "c", "text": "Both variables are normally distributed", "isCorrect": false},
        {"id": "d", "text": "The researcher wants to establish causation", "isCorrect": false}
      ],
      "explanation": "Spearman's rho is a rank-order correlation appropriate for ordinal data or when the relationship is monotonic but not strictly linear. It's more robust to outliers and non-normality than Pearson's r, which assumes interval/ratio data and linear relationships.",
      "difficulty": "hard",
      "tags": ["Spearman", "Pearson", "correlation types", "ordinal data"]
    },
    {
      "type": "single_choice",
      "id": "d1-cr-sc-005",
      "stem": "In a regression analysis, the standardized regression coefficient (beta) for predictor X is 0.45. This means that:",
      "options": [
        {"id": "a", "text": "For every 1-unit increase in X, Y increases by 0.45 units", "isCorrect": false},
        {"id": "b", "text": "For every 1-SD increase in X, Y increases by 0.45 SD, controlling for other predictors", "isCorrect": true},
        {"id": "c", "text": "X explains 45% of the variance in Y", "isCorrect": false},
        {"id": "d", "text": "The correlation between X and Y is 0.45", "isCorrect": false}
      ],
      "explanation": "Standardized beta coefficients express the relationship in standard deviation units, controlling for other predictors. A beta of 0.45 means a 1-SD increase in X predicts a 0.45-SD increase in Y. Unstandardized coefficients (B) show raw unit changes.",
      "difficulty": "hard",
      "tags": ["standardized coefficients", "beta weights", "regression interpretation"]
    },
    {
      "type": "multiple_choice",
      "id": "d1-cr-mc-001",
      "stem": "Which of the following would cause Pearson's r to UNDERESTIMATE the true strength of a relationship? Select all that apply.",
      "options": [
        {"id": "a", "text": "Restriction of range in one or both variables", "isCorrect": true},
        {"id": "b", "text": "A curvilinear (nonlinear) relationship between variables", "isCorrect": true},
        {"id": "c", "text": "Unreliability in measurement of one or both variables", "isCorrect": true},
        {"id": "d", "text": "A very large sample size", "isCorrect": false},
        {"id": "e", "text": "Extreme outliers in the data", "isCorrect": false}
      ],
      "explanation": "Range restriction, unreliability, and curvilinearity all attenuate (reduce) correlation coefficients. Large samples don't affect r magnitude (they affect statistical significance). Outliers can either inflate or deflate correlations depending on their position.",
      "difficulty": "hard",
      "tags": ["attenuation", "range restriction", "correlation assumptions"]
    },
    {
      "type": "multiple_choice",
      "id": "d1-cr-mc-002",
      "stem": "A researcher is checking assumptions before conducting multiple regression. Which assumptions must be evaluated? Select all that apply.",
      "options": [
        {"id": "a", "text": "Linearity of relationships between predictors and criterion", "isCorrect": true},
        {"id": "b", "text": "Homoscedasticity (constant variance of residuals)", "isCorrect": true},
        {"id": "c", "text": "Independence of residuals", "isCorrect": true},
        {"id": "d", "text": "Multicollinearity among predictors is not excessive", "isCorrect": true},
        {"id": "e", "text": "All predictors must be normally distributed", "isCorrect": false}
      ],
      "explanation": "Key regression assumptions: linearity, homoscedasticity, independence of errors, normality of residuals (not predictors), and absence of severe multicollinearity. Predictors don't need to be normally distributed—it's the residuals that should be approximately normal.",
      "difficulty": "hard",
      "tags": ["regression assumptions", "homoscedasticity", "multicollinearity"]
    },
    {
      "type": "multiple_choice",
      "id": "d1-cr-mc-003",
      "stem": "Which statements about partial and semi-partial (part) correlations are TRUE? Select all that apply.",
      "options": [
        {"id": "a", "text": "Partial correlation removes the effect of a third variable from both X and Y", "isCorrect": true},
        {"id": "b", "text": "Semi-partial correlation removes the effect of a third variable from X only", "isCorrect": true},
        {"id": "c", "text": "Squared semi-partial correlations indicate unique variance explained", "isCorrect": true},
        {"id": "d", "text": "Partial correlations are always larger than zero-order correlations", "isCorrect": false},
        {"id": "e", "text": "Both control for confounding variables", "isCorrect": true}
      ],
      "explanation": "Partial r controls for Z in both X and Y; semi-partial (part) r controls for Z in X only. Squared semi-partial r shows unique variance contribution. Both help control confounds. Partial correlations can be larger, smaller, or even opposite sign from zero-order r.",
      "difficulty": "hard",
      "tags": ["partial correlation", "semi-partial correlation", "variance partitioning"]
    },
    {
      "type": "multiple_choice",
      "id": "d1-cr-mc-004",
      "stem": "In hierarchical multiple regression, a researcher enters demographic variables in Block 1, then cognitive variables in Block 2. Which statistics should be examined? Select all that apply.",
      "options": [
        {"id": "a", "text": "R² change (ΔR²) for Block 2", "isCorrect": true},
        {"id": "b", "text": "F change significance for Block 2", "isCorrect": true},
        {"id": "c", "text": "Individual beta weights in the final model", "isCorrect": true},
        {"id": "d", "text": "Zero-order correlations between all predictors", "isCorrect": false},
        {"id": "e", "text": "Total R² of the final model", "isCorrect": true}
      ],
      "explanation": "Hierarchical regression examines incremental validity—whether later blocks add significantly beyond earlier ones. Key statistics: ΔR² shows additional variance explained, F change tests significance of increase, final betas show relative contributions, total R² shows overall model fit.",
      "difficulty": "hard",
      "tags": ["hierarchical regression", "incremental validity", "R-squared change"]
    },
    {
      "type": "multiple_choice",
      "id": "d1-cr-mc-005",
      "stem": "A correlation matrix shows that two predictor variables have r = 0.92. What concerns does this raise for regression analysis? Select all that apply.",
      "options": [
        {"id": "a", "text": "Multicollinearity inflating standard errors of regression coefficients", "isCorrect": true},
        {"id": "b", "text": "Difficulty interpreting unique contributions of each predictor", "isCorrect": true},
        {"id": "c", "text": "Unstable beta weights that may change dramatically with small sample changes", "isCorrect": true},
        {"id": "d", "text": "The overall R² will be artificially deflated", "isCorrect": false},
        {"id": "e", "text": "VIF values likely exceeding acceptable thresholds", "isCorrect": true}
      ],
      "explanation": "High intercorrelation (multicollinearity) causes inflated standard errors, unstable coefficients, and interpretation problems. VIF (variance inflation factor) will be high. However, R² is not deflated—the model can still predict well, but attributing variance to specific predictors becomes unreliable.",
      "difficulty": "hard",
      "tags": ["multicollinearity", "VIF", "regression problems"]
    },
    {
      "type": "matrix_single",
      "id": "d1-cr-ms-001",
      "stem": "Match each correlation coefficient with the appropriate type of data or research situation.",
      "rows": [
        {"id": "r1", "text": "Pearson's r"},
        {"id": "r2", "text": "Spearman's rho"},
        {"id": "r3", "text": "Point-biserial"},
        {"id": "r4", "text": "Phi coefficient"}
      ],
      "columns": [
        {"id": "c1", "text": "Both variables continuous, linear relationship"},
        {"id": "c2", "text": "Both variables dichotomous"},
        {"id": "c3", "text": "One dichotomous, one continuous"},
        {"id": "c4", "text": "Both variables ordinal or ranked"}
      ],
      "correctAnswers": [
        {"rowId": "r1", "columnId": "c1"},
        {"rowId": "r2", "columnId": "c4"},
        {"rowId": "r3", "columnId": "c3"},
        {"rowId": "r4", "columnId": "c2"}
      ],
      "explanation": "Pearson: continuous, linear. Spearman: ordinal/ranked data or nonlinear monotonic. Point-biserial: one truly dichotomous (e.g., gender), one continuous. Phi: both dichotomous (2×2 situation). Choosing the right coefficient depends on measurement scales.",
      "difficulty": "hard",
      "tags": ["correlation types", "measurement scales"]
    },
    {
      "type": "matrix_single",
      "id": "d1-cr-ms-002",
      "stem": "Match each regression concept with its correct interpretation.",
      "rows": [
        {"id": "r1", "text": "B (unstandardized coefficient)"},
        {"id": "r2", "text": "β (standardized coefficient)"},
        {"id": "r3", "text": "R²"},
        {"id": "r4", "text": "Standard error of estimate (SEE)"}
      ],
      "columns": [
        {"id": "c1", "text": "Change in Y for 1-unit change in X (raw units)"},
        {"id": "c2", "text": "Proportion of Y variance explained by model"},
        {"id": "c3", "text": "Change in Y (in SDs) for 1-SD change in X"},
        {"id": "c4", "text": "Typical prediction error in Y units"}
      ],
      "correctAnswers": [
        {"rowId": "r1", "columnId": "c1"},
        {"rowId": "r2", "columnId": "c3"},
        {"rowId": "r3", "columnId": "c2"},
        {"rowId": "r4", "columnId": "c4"}
      ],
      "explanation": "B is in raw units for interpretation with specific measures. Beta allows comparing predictors on common scale. R² shows model explanatory power. SEE indicates prediction accuracy—smaller values mean more precise predictions.",
      "difficulty": "hard",
      "tags": ["regression coefficients", "interpretation", "SEE"]
    },
    {
      "type": "matrix_multiple",
      "id": "d1-cr-mm-001",
      "stem": "For each research question, select ALL appropriate analytical approaches.",
      "rows": [
        {"id": "r1", "text": "Does therapy attendance predict symptom reduction?"},
        {"id": "r2", "text": "Do IQ, motivation, and study time jointly predict GPA?"},
        {"id": "r3", "text": "Does adding personality measures improve prediction beyond cognitive tests?"},
        {"id": "r4", "text": "What is the relationship between ordinal rankings of job applicants by two raters?"}
      ],
      "columns": [
        {"id": "c1", "text": "Simple regression"},
        {"id": "c2", "text": "Multiple regression"},
        {"id": "c3", "text": "Hierarchical regression"},
        {"id": "c4", "text": "Spearman correlation"}
      ],
      "correctAnswers": [
        {"rowId": "r1", "columnId": "c1"},
        {"rowId": "r2", "columnId": "c2"},
        {"rowId": "r3", "columnId": "c2"},
        {"rowId": "r3", "columnId": "c3"},
        {"rowId": "r4", "columnId": "c4"}
      ],
      "explanation": "Single predictor = simple regression. Multiple predictors combined = multiple regression. Testing incremental validity (adding predictors) = hierarchical regression. Ranked/ordinal data = Spearman. Question 3 specifically tests whether Block 2 adds beyond Block 1.",
      "difficulty": "hard",
      "tags": ["analysis selection", "research questions", "regression types"]
    },
    {
      "type": "matrix_multiple",
      "id": "d1-cr-mm-002",
      "stem": "Select ALL factors that would affect the magnitude or interpretation of a correlation coefficient.",
      "rows": [
        {"id": "r1", "text": "Sample is restricted on one variable (e.g., only high scorers)"},
        {"id": "r2", "text": "Relationship between variables is curvilinear"},
        {"id": "r3", "text": "Sample size is increased from 50 to 500"},
        {"id": "r4", "text": "Measurement reliability of both variables is low"}
      ],
      "columns": [
        {"id": "c1", "text": "Reduces magnitude of r"},
        {"id": "c2", "text": "Increases magnitude of r"},
        {"id": "c3", "text": "Affects statistical significance"},
        {"id": "c4", "text": "No effect on r magnitude"}
      ],
      "correctAnswers": [
        {"rowId": "r1", "columnId": "c1"},
        {"rowId": "r2", "columnId": "c1"},
        {"rowId": "r3", "columnId": "c3"},
        {"rowId": "r3", "columnId": "c4"},
        {"rowId": "r4", "columnId": "c1"}
      ],
      "explanation": "Range restriction attenuates r. Curvilinearity reduces r (linear measure misses curve). Sample size affects significance (power) but not r magnitude. Low reliability attenuates r (measurement error obscures true relationship). Correction formulas exist for range restriction and attenuation.",
      "difficulty": "hard",
      "tags": ["attenuation", "range restriction", "sample size effects"]
    },
    {
      "type": "cloze_dropdown",
      "id": "d1-cr-cd-001",
      "stem": "In regression, the coefficient of determination (R²) equals {{blank1}}. If R² = 0.64, this means that {{blank2}}% of variance in the criterion is explained by the predictor(s). The remaining {{blank3}}% represents {{blank4}}.",
      "blanks": [
        {
          "id": "blank1",
          "options": [
            {"id": "a", "text": "the square of the correlation coefficient", "isCorrect": true},
            {"id": "b", "text": "the correlation coefficient divided by 2", "isCorrect": false},
            {"id": "c", "text": "the standard error of estimate", "isCorrect": false},
            {"id": "d", "text": "the sum of squared residuals", "isCorrect": false}
          ]
        },
        {
          "id": "blank2",
          "options": [
            {"id": "a", "text": "36", "isCorrect": false},
            {"id": "b", "text": "64", "isCorrect": true},
            {"id": "c", "text": "80", "isCorrect": false},
            {"id": "d", "text": "8", "isCorrect": false}
          ]
        },
        {
          "id": "blank3",
          "options": [
            {"id": "a", "text": "64", "isCorrect": false},
            {"id": "b", "text": "36", "isCorrect": true},
            {"id": "c", "text": "20", "isCorrect": false},
            {"id": "d", "text": "46", "isCorrect": false}
          ]
        },
        {
          "id": "blank4",
          "options": [
            {"id": "a", "text": "unexplained or residual variance", "isCorrect": true},
            {"id": "b", "text": "the standard error", "isCorrect": false},
            {"id": "c", "text": "sampling error only", "isCorrect": false},
            {"id": "d", "text": "Type I error", "isCorrect": false}
          ]
        }
      ],
      "explanation": "R² = r² (squared correlation). R² represents proportion of criterion variance explained by predictors. If R² = 0.64, 64% is explained variance; 36% (1 - R²) is unexplained/residual variance due to other factors, measurement error, or random variation.",
      "difficulty": "hard",
      "tags": ["coefficient of determination", "explained variance", "residual variance"]
    },
    {
      "type": "cloze_dropdown",
      "id": "d1-cr-cd-002",
      "stem": "The regression equation Y' = a + bX uses {{blank1}} as the Y-intercept and {{blank2}} as the slope. The slope indicates that for each 1-unit increase in X, Y is predicted to change by {{blank3}}. When X = 0, the predicted Y equals {{blank4}}.",
      "blanks": [
        {
          "id": "blank1",
          "options": [
            {"id": "a", "text": "a (the constant)", "isCorrect": true},
            {"id": "b", "text": "b (the coefficient)", "isCorrect": false},
            {"id": "c", "text": "Y' (the predicted score)", "isCorrect": false},
            {"id": "d", "text": "r (the correlation)", "isCorrect": false}
          ]
        },
        {
          "id": "blank2",
          "options": [
            {"id": "a", "text": "a (the constant)", "isCorrect": false},
            {"id": "b", "text": "b (the regression coefficient)", "isCorrect": true},
            {"id": "c", "text": "X (the predictor)", "isCorrect": false},
            {"id": "d", "text": "r² (the coefficient of determination)", "isCorrect": false}
          ]
        },
        {
          "id": "blank3",
          "options": [
            {"id": "a", "text": "a units", "isCorrect": false},
            {"id": "b", "text": "b units", "isCorrect": true},
            {"id": "c", "text": "1 unit", "isCorrect": false},
            {"id": "d", "text": "r units", "isCorrect": false}
          ]
        },
        {
          "id": "blank4",
          "options": [
            {"id": "a", "text": "b", "isCorrect": false},
            {"id": "b", "text": "a (the intercept)", "isCorrect": true},
            {"id": "c", "text": "zero", "isCorrect": false},
            {"id": "d", "text": "the mean of Y", "isCorrect": false}
          ]
        }
      ],
      "explanation": "In Y' = a + bX: 'a' is the Y-intercept (value of Y when X = 0), 'b' is the slope (change in Y for each 1-unit change in X). The equation defines a line where predictions are made by multiplying X by the slope and adding the intercept.",
      "difficulty": "hard",
      "tags": ["regression equation", "slope", "intercept"]
    },
    {
      "type": "bowtie",
      "id": "d1-cr-bt-001",
      "stem": "A researcher finds r = 0.75 between depression and anxiety scores in a clinical sample. However, when examined in a general population sample, r = 0.45. Both correlations are statistically significant.",
      "config": {
        "actionsLabel": "Explanatory Factors",
        "conditionLabel": "Statistical Phenomenon",
        "parametersLabel": "Implications"
      },
      "options": [
        {"id": "a1", "text": "Clinical sample has restricted range on both variables", "category": "action", "isCorrect": false},
        {"id": "a2", "text": "General population has greater range/variability", "category": "action", "isCorrect": true},
        {"id": "a3", "text": "Different measurement instruments were used", "category": "action", "isCorrect": false},
        {"id": "a4", "text": "Clinical sample has greater range due to pathology", "category": "action", "isCorrect": true},
        {"id": "c1", "text": "Range restriction affects correlation magnitude", "category": "condition", "isCorrect": false},
        {"id": "c2", "text": "Greater variability produces larger correlations", "category": "condition", "isCorrect": true},
        {"id": "c3", "text": "Clinical populations show different relationships", "category": "condition", "isCorrect": false},
        {"id": "p1", "text": "Correlations should be interpreted in context of sample variability", "category": "parameter", "isCorrect": true},
        {"id": "p2", "text": "The relationship is fundamentally different across populations", "category": "parameter", "isCorrect": false},
        {"id": "p3", "text": "Clinical sample correlation may overestimate general population relationship", "category": "parameter", "isCorrect": true},
        {"id": "p4", "text": "Both correlations are equally valid for clinical prediction", "category": "parameter", "isCorrect": false}
      ],
      "explanation": "Clinical samples typically have greater variability on psychopathology measures (extreme scores), which can inflate correlations. General population samples have restricted range on clinical variables. Correlations must be interpreted considering sample characteristics; the clinical correlation overestimates the relationship in less severe populations.",
      "difficulty": "hard",
      "tags": ["range effects", "sample characteristics", "correlation interpretation"]
    },
    {
      "type": "bowtie",
      "id": "d1-cr-bt-002",
      "stem": "A selection researcher develops a regression equation to predict job performance (Y) from cognitive ability (X₁) and conscientiousness (X₂). The equation works well in the development sample but shows much poorer prediction in a new sample.",
      "config": {
        "actionsLabel": "Potential Causes",
        "conditionLabel": "Statistical Problem",
        "parametersLabel": "Solutions"
      },
      "options": [
        {"id": "a1", "text": "Regression weights were optimized for the specific sample", "category": "action", "isCorrect": true},
        {"id": "a2", "text": "Development sample was too small for stable estimates", "category": "action", "isCorrect": true},
        {"id": "a3", "text": "Multicollinearity between predictors", "category": "action", "isCorrect": false},
        {"id": "a4", "text": "New sample has similar characteristics to development sample", "category": "action", "isCorrect": false},
        {"id": "c1", "text": "Shrinkage/cross-validation failure", "category": "condition", "isCorrect": true},
        {"id": "c2", "text": "Heteroscedasticity", "category": "condition", "isCorrect": false},
        {"id": "c3", "text": "Restriction of range", "category": "condition", "isCorrect": false},
        {"id": "p1", "text": "Use cross-validation during model development", "category": "parameter", "isCorrect": true},
        {"id": "p2", "text": "Use larger development samples", "category": "parameter", "isCorrect": true},
        {"id": "p3", "text": "Report adjusted R² rather than R²", "category": "parameter", "isCorrect": true},
        {"id": "p4", "text": "Add more predictors to improve fit", "category": "parameter", "isCorrect": false}
      ],
      "explanation": "Shrinkage occurs when regression equations optimized for one sample perform worse in new samples—weights capitalize on sample-specific variance. Solutions: larger samples, cross-validation, reporting adjusted R², and avoiding overfitting with too many predictors. Adding more predictors worsens shrinkage.",
      "difficulty": "hard",
      "tags": ["shrinkage", "cross-validation", "prediction", "generalizability"]
    },
    {
      "type": "highlight",
      "id": "d1-cr-hl-001",
      "stem": "Highlight ALL statements that correctly describe limitations or assumptions of Pearson's r.",
      "passage": "Pearson's correlation coefficient is widely used in psychological research. [It assumes a linear relationship between variables, so curvilinear relationships will yield misleadingly low values]. The coefficient ranges from -1 to +1. [Correlation does not imply causation—a third variable may explain the relationship]. [High correlations indicate that one variable causes changes in the other]. [The correlation can be attenuated (reduced) by unreliable measurement of either variable]. Both variables must be measured on interval or ratio scales. [Outliers can dramatically affect the correlation, either inflating or deflating it]. [A correlation of zero definitively proves no relationship exists between variables].",
      "highlightableSegments": [
        {"id": "h1", "text": "It assumes a linear relationship between variables, so curvilinear relationships will yield misleadingly low values", "isCorrect": true},
        {"id": "h2", "text": "Correlation does not imply causation—a third variable may explain the relationship", "isCorrect": true},
        {"id": "h3", "text": "High correlations indicate that one variable causes changes in the other", "isCorrect": false},
        {"id": "h4", "text": "The correlation can be attenuated (reduced) by unreliable measurement of either variable", "isCorrect": true},
        {"id": "h5", "text": "Outliers can dramatically affect the correlation, either inflating or deflating it", "isCorrect": true},
        {"id": "h6", "text": "A correlation of zero definitively proves no relationship exists between variables", "isCorrect": false}
      ],
      "explanation": "Correct limitations: linearity assumption, correlation ≠ causation, attenuation from unreliability, outlier sensitivity. Incorrect: correlation never implies causation regardless of magnitude; r = 0 only means no LINEAR relationship—curvilinear relationships may exist.",
      "difficulty": "hard",
      "tags": ["correlation limitations", "assumptions", "interpretation errors"]
    },
    {
      "type": "highlight",
      "id": "d1-cr-hl-002",
      "stem": "Highlight ALL statements that correctly describe multicollinearity in multiple regression.",
      "passage": "Multicollinearity occurs when predictor variables are highly intercorrelated. [It makes it difficult to determine the unique contribution of each predictor]. [It causes the overall R² to be artificially reduced]. [It inflates the standard errors of regression coefficients, making them less stable]. Multicollinearity can be detected using VIF values. [VIF values above 10 are generally considered problematic]. [Multicollinearity can be addressed by removing redundant predictors or combining them]. [It only occurs when predictors correlate perfectly at r = 1.00].",
      "highlightableSegments": [
        {"id": "h1", "text": "It makes it difficult to determine the unique contribution of each predictor", "isCorrect": true},
        {"id": "h2", "text": "It causes the overall R² to be artificially reduced", "isCorrect": false},
        {"id": "h3", "text": "It inflates the standard errors of regression coefficients, making them less stable", "isCorrect": true},
        {"id": "h4", "text": "VIF values above 10 are generally considered problematic", "isCorrect": true},
        {"id": "h5", "text": "Multicollinearity can be addressed by removing redundant predictors or combining them", "isCorrect": true},
        {"id": "h6", "text": "It only occurs when predictors correlate perfectly at r = 1.00", "isCorrect": false}
      ],
      "explanation": "Multicollinearity: impedes unique variance attribution, inflates standard errors (coefficients unstable), VIF > 10 problematic, addressed by removing/combining predictors. R² is NOT reduced—prediction can be fine, interpretation is the problem. Occurs at high (not perfect) correlations.",
      "difficulty": "hard",
      "tags": ["multicollinearity", "regression diagnostics", "VIF"]
    },
    {
      "type": "drag_drop_ordered",
      "id": "d1-cr-do-001",
      "stem": "Arrange the steps for conducting and interpreting a multiple regression analysis in the correct sequence.",
      "items": [
        {"id": "i1", "text": "Specify research question and identify predictor and criterion variables", "correctPosition": 1},
        {"id": "i2", "text": "Check assumptions (linearity, normality of residuals, homoscedasticity, multicollinearity)", "correctPosition": 2},
        {"id": "i3", "text": "Run regression analysis and obtain output", "correctPosition": 3},
        {"id": "i4", "text": "Evaluate overall model fit (R², F-test significance)", "correctPosition": 4},
        {"id": "i5", "text": "Examine individual predictor contributions (beta weights, t-tests)", "correctPosition": 5},
        {"id": "i6", "text": "Check residual plots for assumption violations", "correctPosition": 6}
      ],
      "explanation": "Proper sequence: define variables → check assumptions BEFORE analysis → run analysis → evaluate overall model → examine predictors → post-hoc assumption checks (residuals). Assumption checking should occur both before and after to ensure valid interpretation.",
      "difficulty": "hard",
      "tags": ["regression procedure", "analysis steps", "assumption checking"]
    },
    {
      "type": "drag_drop_ordered",
      "id": "d1-cr-do-002",
      "stem": "Arrange the correlation coefficients from WEAKEST to STRONGEST relationship.",
      "items": [
        {"id": "i1", "text": "r = 0.15", "correctPosition": 1},
        {"id": "i2", "text": "r = -0.35", "correctPosition": 2},
        {"id": "i3", "text": "r = 0.42", "correctPosition": 3},
        {"id": "i4", "text": "r = -0.68", "correctPosition": 4},
        {"id": "i5", "text": "r = 0.71", "correctPosition": 5},
        {"id": "i6", "text": "r = -0.89", "correctPosition": 6}
      ],
      "explanation": "Correlation strength is determined by absolute value, not sign. The sign indicates direction (positive = same direction, negative = opposite), but magnitude (0.15 < 0.35 < 0.42 < 0.68 < 0.71 < 0.89) determines strength. r = -0.89 is a stronger relationship than r = 0.71.",
      "difficulty": "hard",
      "tags": ["correlation magnitude", "positive vs negative", "interpretation"]
    },
    {
      "type": "trend",
      "id": "d1-cr-tr-001",
      "stem": "A researcher examines how correlation between job satisfaction and productivity changes across different sample sizes in a meta-analysis.",
      "data": {
        "timePoints": ["n=30", "n=75", "n=150", "n=300", "n=500"],
        "measurements": [
          {"label": "Correlation (r)", "values": [0.52, 0.38, 0.35, 0.33, 0.32]},
          {"label": "95% CI Width", "values": [0.58, 0.32, 0.22, 0.15, 0.12]}
        ]
      },
      "question": "What do these data primarily demonstrate about correlation estimates?",
      "options": [
        {"id": "a", "text": "The true correlation is approximately 0.52", "isCorrect": false},
        {"id": "b", "text": "Larger samples provide more stable, precise estimates with narrower confidence intervals", "isCorrect": true},
        {"id": "c", "text": "The relationship weakens as more participants are studied", "isCorrect": false},
        {"id": "d", "text": "Sample size has no effect on correlation magnitude", "isCorrect": false}
      ],
      "explanation": "Small samples produce unstable correlation estimates—the n=30 value (0.52) is likely inflated by sampling error. As n increases, estimates stabilize around the population value (~0.32), and confidence intervals narrow. This demonstrates why small-sample correlations should be interpreted cautiously.",
      "difficulty": "hard",
      "tags": ["sample size", "sampling error", "confidence intervals", "meta-analysis"]
    },
    {
      "type": "trend",
      "id": "d1-cr-tr-002",
      "stem": "A researcher adds predictors sequentially in hierarchical regression and records R² at each step.",
      "data": {
        "timePoints": ["Step 1 (Age)", "Step 2 (+Education)", "Step 3 (+IQ)", "Step 4 (+Experience)", "Step 5 (+Personality)"],
        "measurements": [
          {"label": "R²", "values": [0.08, 0.22, 0.41, 0.43, 0.44]},
          {"label": "ΔR²", "values": [0.08, 0.14, 0.19, 0.02, 0.01]}
        ]
      },
      "question": "Based on the ΔR² values, which predictor(s) add the most incremental validity?",
      "options": [
        {"id": "a", "text": "Age and Personality together", "isCorrect": false},
        {"id": "b", "text": "IQ and Education", "isCorrect": true},
        {"id": "c", "text": "Experience and Personality", "isCorrect": false},
        {"id": "d", "text": "All predictors contribute equally", "isCorrect": false}
      ],
      "explanation": "ΔR² shows unique variance added at each step. IQ adds the most (0.19), followed by Education (0.14). Experience (0.02) and Personality (0.01) add minimal incremental validity beyond the earlier predictors—their variance is likely shared with IQ/Education.",
      "difficulty": "hard",
      "tags": ["hierarchical regression", "incremental validity", "R-squared change"]
    },
    {
      "type": "drag_drop_categorize",
      "id": "d1-cr-dc-001",
      "stem": "Categorize each scenario by the type of correlation problem present.",
      "categories": [
        {"id": "cat1", "name": "Range Restriction"},
        {"id": "cat2", "name": "Curvilinearity"},
        {"id": "cat3", "name": "Third Variable/Confound"}
      ],
      "items": [
        {"id": "i1", "text": "Studying GRE-performance correlation only in students who scored >160 on GRE", "correctCategory": "cat1"},
        {"id": "i2", "text": "Ice cream sales and drowning deaths both increase together in summer", "correctCategory": "cat3"},
        {"id": "i3", "text": "Moderate anxiety improves performance but high anxiety impairs it", "correctCategory": "cat2"},
        {"id": "i4", "text": "Correlating anxiety and depression only in psychiatric inpatients", "correctCategory": "cat1"},
        {"id": "i5", "text": "Relationship between arousal and task performance follows inverted-U shape", "correctCategory": "cat2"},
        {"id": "i6", "text": "Shoe size correlates with reading ability in children (age is confound)", "correctCategory": "cat3"}
      ],
      "explanation": "Range restriction: studying only part of the range (high GRE scorers, psychiatric patients). Curvilinearity: relationship changes direction (anxiety-performance, Yerkes-Dodson arousal curve). Third variable: confound explains relationship (summer causes both ice cream/drowning; age causes both shoe size/reading).",
      "difficulty": "hard",
      "tags": ["correlation problems", "confounds", "curvilinearity", "range restriction"]
    },
    {
      "type": "drag_drop_categorize",
      "id": "d1-cr-dc-002",
      "stem": "Categorize each statistic as belonging to CORRELATION analysis or REGRESSION analysis (or both).",
      "categories": [
        {"id": "cat1", "name": "Correlation Only"},
        {"id": "cat2", "name": "Regression Only"},
        {"id": "cat3", "name": "Both/Related"}
      ],
      "items": [
        {"id": "i1", "text": "Pearson's r", "correctCategory": "cat1"},
        {"id": "i2", "text": "Unstandardized coefficient (B)", "correctCategory": "cat2"},
        {"id": "i3", "text": "R² (coefficient of determination)", "correctCategory": "cat3"},
        {"id": "i4", "text": "Standard error of estimate (SEE)", "correctCategory": "cat2"},
        {"id": "i5", "text": "Y-intercept (a)", "correctCategory": "cat2"},
        {"id": "i6", "text": "Standardized beta (β)", "correctCategory": "cat2"},
        {"id": "i7", "text": "Spearman's rho", "correctCategory": "cat1"},
        {"id": "i8", "text": "VIF (variance inflation factor)", "correctCategory": "cat2"}
      ],
      "explanation": "Correlation-specific: Pearson's r, Spearman's rho. Regression-specific: B, SEE, intercept, beta, VIF. R² applies to both (in simple regression, R² = r²). Understanding which statistics belong to which analysis is essential for proper interpretation.",
      "difficulty": "hard",
      "tags": ["correlation vs regression", "statistics terminology"]
    }
  ]
}
