{
  "domain": "Domain 6: Workforce Development & Leadership",
  "subdomain": "Employee Selection - Evaluation of Techniques",
  "totalQuestions": 26,
  "questionTypes": {
    "singleChoice": 5,
    "multipleChoice": 5,
    "matrixSingle": 2,
    "matrixMultiple": 2,
    "clozeDropdown": 2,
    "bowtie": 2,
    "highlight": 2,
    "dragDropOrdered": 2,
    "trendAnalysis": 2,
    "dragDropCategorize": 2
  },
  "questions": [
    {
      "type": "single-choice",
      "id": "d6-se-sc-001",
      "stem": "A selection test has a validity coefficient of r = .35 against job performance. Using the coefficient of determination, approximately what percentage of variance in job performance is accounted for by the test?",
      "options": [
        {"id": "A", "text": "35%"},
        {"id": "B", "text": "12%"},
        {"id": "C", "text": "65%"},
        {"id": "D", "text": "70%"}
      ],
      "correctAnswer": "B",
      "explanation": "The coefficient of determination (r-squared) indicates the proportion of variance in one variable accounted for by another. With r = .35, r-squared = .1225, or approximately 12%. While this may seem small, validity coefficients in this range are typical for single selection measures and can have substantial practical utility. The Taylor-Russell tables demonstrate that even modest validity coefficients can substantially improve selection outcomes when the selection ratio is favorable and the base rate is moderate.",
      "difficulty": "hard",
      "tags": ["coefficient of determination", "validity", "selection utility"]
    },
    {
      "type": "single-choice",
      "id": "d6-se-sc-002",
      "stem": "According to Schmidt and Hunter's (1998) meta-analysis of selection methods, which combination of predictors provides the highest validity for predicting overall job performance?",
      "options": [
        {"id": "A", "text": "Unstructured interview + years of education"},
        {"id": "B", "text": "General mental ability (GMA) + structured interview"},
        {"id": "C", "text": "Personality test (conscientiousness) + reference check"},
        {"id": "D", "text": "Job experience + graphology"}
      ],
      "correctAnswer": "B",
      "explanation": "Schmidt and Hunter's (1998) landmark meta-analysis found that general mental ability (GMA) tests have the highest single-predictor validity (r = .51) for job performance. When combined with a structured interview (which adds incremental validity), the multiple correlation reaches approximately .63. GMA + work sample tests and GMA + integrity tests were also highly valid combinations. Unstructured interviews have lower validity than structured ones; years of education add little beyond GMA; graphology has zero validity.",
      "difficulty": "hard",
      "tags": ["Schmidt Hunter", "meta-analysis", "GMA", "structured interview", "validity"]
    },
    {
      "type": "single-choice",
      "id": "d6-se-sc-003",
      "stem": "When evaluating adverse impact in selection, the 'four-fifths rule' (80% rule) states that adverse impact exists when:",
      "options": [
        {"id": "A", "text": "The selection rate for any group is less than 80% overall"},
        {"id": "B", "text": "The selection rate for a protected group is less than four-fifths of the rate for the group with the highest selection rate"},
        {"id": "C", "text": "Fewer than 80% of applicants from a protected group apply"},
        {"id": "D", "text": "The validity coefficient is less than .80"}
      ],
      "correctAnswer": "B",
      "explanation": "The four-fifths rule from the Uniform Guidelines on Employee Selection Procedures (1978) states that adverse impact is indicated when the selection rate for a protected group is less than 80% (four-fifths) of the selection rate for the group with the highest rate. For example, if 60% of White applicants are hired and only 30% of Black applicants, the ratio is 30/60 = .50, which is below .80, indicating adverse impact. This is a practical guideline, not a legal standard—statistical significance and practical significance must also be considered.",
      "difficulty": "hard",
      "tags": ["adverse impact", "four-fifths rule", "Uniform Guidelines", "selection"]
    },
    {
      "type": "single-choice",
      "id": "d6-se-sc-004",
      "stem": "A psychologist conducts a criterion-related validity study and obtains a validity coefficient of r = .45 in a sample with range restriction on the predictor. After applying a correction for range restriction, the estimated population validity is most likely to:",
      "options": [
        {"id": "A", "text": "Decrease, because range restriction inflates validity"},
        {"id": "B", "text": "Increase, because range restriction attenuates validity"},
        {"id": "C", "text": "Remain unchanged, because range restriction does not affect validity"},
        {"id": "D", "text": "Become negative due to the mathematical correction"}
      ],
      "correctAnswer": "B",
      "explanation": "Range restriction occurs when validation is conducted on a restricted sample (e.g., only those already hired). This reduces the variability of scores on the predictor, which attenuates (reduces) the observed validity coefficient. Correcting for range restriction using Thorndike's or Lawley's formula typically produces a higher estimated population validity. The uncorrected coefficient of .45 would increase to reflect what validity would be if the full range of applicants were included. This correction is commonly applied in validity generalization studies.",
      "difficulty": "hard",
      "tags": ["range restriction", "validity correction", "Thorndike", "criterion-related validity"]
    },
    {
      "type": "single-choice",
      "id": "d6-se-sc-005",
      "stem": "In the context of selection test validation, content validity is MOST appropriate when:",
      "options": [
        {"id": "A", "text": "The test measures a construct like general intelligence"},
        {"id": "B", "text": "The test directly samples the knowledge or skills required by the job"},
        {"id": "C", "text": "There is a large sample available for statistical analysis"},
        {"id": "D", "text": "The organization wants to predict future potential rather than current competence"}
      ],
      "correctAnswer": "B",
      "explanation": "Content validity is most appropriate when the selection instrument directly samples the job domain—the test content represents the knowledge, skills, or behaviors actually required for job performance. This approach is especially useful when: sample sizes are too small for criterion-related validation, the test closely mirrors job tasks (e.g., work samples, job knowledge tests), and the focus is on current competence rather than future potential. For constructs like general intelligence (A) or future potential (D), construct validity or criterion-related validity would be more appropriate.",
      "difficulty": "hard",
      "tags": ["content validity", "validation strategies", "work samples"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-se-mc-001",
      "stem": "Which of the following are recognized threats to the criterion-related validity of a selection test? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Range restriction on the predictor variable"},
        {"id": "B", "text": "Criterion contamination (criterion scores influenced by knowledge of predictor scores)"},
        {"id": "C", "text": "Criterion deficiency (criterion measure fails to capture important aspects of performance)"},
        {"id": "D", "text": "Large sample size"},
        {"id": "E", "text": "Unreliability of the criterion measure"}
      ],
      "correctAnswers": ["A", "B", "C", "E"],
      "explanation": "Threats to criterion-related validity include: Range restriction (A) attenuates observed validity by reducing predictor variance. Criterion contamination (B) can inflate or distort validity when criterion ratings are biased by knowledge of predictor scores. Criterion deficiency (C) reduces validity by not fully capturing the performance domain. Unreliable criterion measures (E) attenuate validity through measurement error. Large sample size (D) is not a threat—it actually increases statistical power to detect true validity and provides more precise estimates.",
      "difficulty": "hard",
      "tags": ["criterion-related validity", "threats", "range restriction", "criterion contamination"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-se-mc-002",
      "stem": "The concept of validity generalization (VG) as developed by Schmidt and Hunter is based on which of the following assumptions? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Much of the variability in validity coefficients across studies is due to statistical artifacts"},
        {"id": "B", "text": "Cognitive ability tests have validity that generalizes across jobs and settings"},
        {"id": "C", "text": "Every selection situation requires a new local validation study"},
        {"id": "D", "text": "Corrections for range restriction and criterion unreliability reveal true validity"},
        {"id": "E", "text": "Situational specificity of validity is largely an illusion caused by sampling error"}
      ],
      "correctAnswers": ["A", "B", "D", "E"],
      "explanation": "Schmidt and Hunter's validity generalization (VG) theory argues that: variability in observed validity coefficients is primarily due to artifacts such as sampling error, range restriction, and criterion unreliability (A); cognitive ability tests show generalizable validity (B); correcting for these artifacts reveals higher and more consistent true validity (D); and the 'situational specificity hypothesis'—that validity varies across settings—is largely an artifact of these statistical problems (E). VG explicitly challenges the assumption that local validation is always necessary (C is the opposite of VG's position).",
      "difficulty": "hard",
      "tags": ["validity generalization", "Schmidt Hunter", "statistical artifacts", "situational specificity"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-se-mc-003",
      "stem": "Which of the following utility analysis methods can be used to estimate the dollar value of a selection procedure? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Taylor-Russell model (proportion of successful employees)"},
        {"id": "B", "text": "Brogden-Cronbach-Gleser model (SDy estimation)"},
        {"id": "C", "text": "Naylor-Shine model (improvement in mean criterion score)"},
        {"id": "D", "text": "Factor analysis"},
        {"id": "E", "text": "Raju, Burke, and Normand's model (incorporating economic variables)"}
      ],
      "correctAnswers": ["A", "B", "C", "E"],
      "explanation": "Utility analysis models include: Taylor-Russell (1939)—estimates improvement in success rate given validity, selection ratio, and base rate; Brogden-Cronbach-Gleser (BCG)—translates validity into dollar terms using standard deviation of job performance in dollars (SDy); Naylor-Shine (1965)—estimates improvement in mean criterion score for selected applicants; Raju et al.'s model—extends BCG by incorporating taxes, discounting, and variable costs. Factor analysis (D) is a psychometric method for identifying latent constructs, not a utility analysis technique.",
      "difficulty": "hard",
      "tags": ["utility analysis", "Taylor-Russell", "Brogden-Cronbach-Gleser", "Naylor-Shine"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-se-mc-004",
      "stem": "Regarding test fairness in selection, which of the following statements are accurate according to the Cleary model and related research? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "A test is unfair if it systematically over- or under-predicts criterion performance for a subgroup"},
        {"id": "B", "text": "Research generally shows that cognitive ability tests over-predict job performance for minority groups"},
        {"id": "C", "text": "Differential prediction requires comparing regression lines across groups"},
        {"id": "D", "text": "A test showing group mean differences is automatically unfair"},
        {"id": "E", "text": "Predictive bias is assessed through moderated regression analysis"}
      ],
      "correctAnswers": ["A", "B", "C", "E"],
      "explanation": "The Cleary (1968) model defines test fairness in terms of predictive bias: a test is unfair if it systematically overpredicts or underpredicts criterion performance for a subgroup relative to a common regression line (A). Research consistently shows that cognitive ability tests slightly overpredict job performance for Black applicants (B)—meaning the tests are not biased against them in a predictive sense. Differential prediction is assessed by comparing regression intercepts and slopes across groups (C) using moderated regression (E). Group mean differences alone (D) do not constitute unfairness—only differential prediction does.",
      "difficulty": "hard",
      "tags": ["Cleary model", "test fairness", "predictive bias", "differential prediction"]
    },
    {
      "type": "multiple-choice",
      "id": "d6-se-mc-005",
      "stem": "Which of the following strategies can be used to reduce adverse impact in selection while maintaining validity? (Select ALL that apply)",
      "options": [
        {"id": "A", "text": "Supplementing cognitive ability tests with non-cognitive predictors (e.g., structured interviews, personality)"},
        {"id": "B", "text": "Using banding to treat statistically equivalent scores as interchangeable"},
        {"id": "C", "text": "Simply lowering the cutoff score for minority applicants"},
        {"id": "D", "text": "Designing biodata instruments with attention to subgroup differences"},
        {"id": "E", "text": "Using situational judgment tests as supplements to cognitive tests"}
      ],
      "correctAnswers": ["A", "B", "D", "E"],
      "explanation": "Strategies to reduce adverse impact: Using composite predictors that include non-cognitive measures (A) can reduce group differences while maintaining validity. Banding (B) treats score differences within a band as measurement error, allowing diversity considerations among equivalent scores. Developing fair biodata instruments (D) with item-level DIF analysis can reduce adverse impact. SJTs (E) typically show smaller group differences than cognitive tests. Simply lowering cutoffs for minority groups (C) constitutes race-norming, which was prohibited by the Civil Rights Act of 1991.",
      "difficulty": "hard",
      "tags": ["adverse impact reduction", "banding", "selection fairness", "Civil Rights Act"]
    },
    {
      "type": "matrix-single",
      "id": "d6-se-ms-001",
      "stem": "Match each type of validity evidence to its primary method of evaluation:",
      "rows": [
        {"id": "row1", "text": "Criterion-related (predictive)"},
        {"id": "row2", "text": "Criterion-related (concurrent)"},
        {"id": "row3", "text": "Content validity"},
        {"id": "row4", "text": "Construct validity"}
      ],
      "columns": [
        {"id": "col1", "text": "Expert judgment of test-job content overlap"},
        {"id": "col2", "text": "Correlating test scores with later performance of hired applicants"},
        {"id": "col3", "text": "Correlating test scores with current performance of existing employees"},
        {"id": "col4", "text": "Convergent/discriminant evidence, factor analysis, nomological network"}
      ],
      "correctAnswers": {
        "row1": "col2",
        "row2": "col3",
        "row3": "col1",
        "row4": "col4"
      },
      "explanation": "Predictive criterion-related validity: Administer test to applicants, hire without using test scores, later correlate with job performance. Concurrent: Administer test to current employees and correlate with their current performance (both measured at the same time). Content validity: Subject matter experts judge whether test content adequately samples the job domain. Construct validity: Uses multiple methods including convergent/discriminant evidence (MTMM), factor analysis, and examination of the nomological network of relationships.",
      "difficulty": "hard",
      "tags": ["validity types", "validation strategies", "construct validity"]
    },
    {
      "type": "matrix-single",
      "id": "d6-se-ms-002",
      "stem": "Match each decision outcome in selection to its correct definition:",
      "rows": [
        {"id": "row1", "text": "True positive (hit)"},
        {"id": "row2", "text": "False positive"},
        {"id": "row3", "text": "True negative (correct rejection)"},
        {"id": "row4", "text": "False negative (miss)"}
      ],
      "columns": [
        {"id": "col1", "text": "Selected and succeeds on the job"},
        {"id": "col2", "text": "Rejected and would have failed"},
        {"id": "col3", "text": "Selected but fails on the job"},
        {"id": "col4", "text": "Rejected but would have succeeded"}
      ],
      "correctAnswers": {
        "row1": "col1",
        "row2": "col3",
        "row3": "col2",
        "row4": "col4"
      },
      "explanation": "In selection decision theory: True positive—correctly selected (would succeed and was hired). False positive—incorrectly selected (fails despite being hired; costly in training, turnover). True negative—correctly rejected (would have failed and was not hired). False negative—incorrectly rejected (would have succeeded but was not hired; costly in lost talent, potential legal claims if systematic). Maximizing true positives and true negatives is the goal, and the ratio depends on the base rate, selection ratio, and validity of the predictor.",
      "difficulty": "hard",
      "tags": ["decision theory", "selection outcomes", "hits", "misses"]
    },
    {
      "type": "matrix-multiple",
      "id": "d6-se-mm-001",
      "stem": "For each type of reliability evidence, select ALL situations where it is most relevant to selection testing:",
      "rows": [
        {"id": "row1", "text": "Test-retest reliability"},
        {"id": "row2", "text": "Internal consistency (Cronbach's alpha)"},
        {"id": "row3", "text": "Inter-rater reliability"}
      ],
      "columns": [
        {"id": "col1", "text": "Evaluating stability of personality test scores over time"},
        {"id": "col2", "text": "Assessing whether items on a cognitive test measure the same construct"},
        {"id": "col3", "text": "Evaluating consistency of interview ratings across different interviewers"},
        {"id": "col4", "text": "Assessing consistency of assessment center ratings across assessors"},
        {"id": "col5", "text": "Evaluating scoring agreement for work sample tests"}
      ],
      "correctAnswers": {
        "row1": ["col1"],
        "row2": ["col2"],
        "row3": ["col3", "col4", "col5"]
      },
      "explanation": "Test-retest reliability is most relevant when temporal stability is important (personality traits should be stable over time). Internal consistency (alpha) is relevant when evaluating whether items on a scale measure the same underlying construct. Inter-rater reliability is critical whenever human judgment is involved: interview ratings, assessment center evaluations, and scored work samples all require agreement among raters to be meaningful. Low inter-rater reliability caps the validity of any selection method involving subjective judgment.",
      "difficulty": "hard",
      "tags": ["reliability", "test-retest", "internal consistency", "inter-rater"]
    },
    {
      "type": "matrix-multiple",
      "id": "d6-se-mm-002",
      "stem": "For each selection method, select ALL characteristics that research has consistently demonstrated:",
      "rows": [
        {"id": "row1", "text": "General mental ability (GMA) tests"},
        {"id": "row2", "text": "Structured interviews"},
        {"id": "row3", "text": "Unstructured interviews"}
      ],
      "columns": [
        {"id": "col1", "text": "High criterion-related validity (r > .40)"},
        {"id": "col2", "text": "Low adverse impact against minorities"},
        {"id": "col3", "text": "Good validity generalization across jobs"},
        {"id": "col4", "text": "Susceptible to interviewer biases"},
        {"id": "col5", "text": "High incremental validity over GMA"}
      ],
      "correctAnswers": {
        "row1": ["col1", "col3"],
        "row2": ["col1", "col5"],
        "row3": ["col4"]
      },
      "explanation": "GMA tests: Consistently high validity (~.51) and excellent validity generalization across jobs and settings, but produce adverse impact. Structured interviews: High validity (~.51) and add incremental validity over GMA because they capture constructs beyond cognitive ability (interpersonal skills, job knowledge). Unstructured interviews: Lower validity (~.38), susceptible to interviewer biases (first impressions, halo, contrast effects), and less defensible legally. Neither GMA tests nor structured interviews have low adverse impact against minorities (col2), though structured interviews show less adverse impact than GMA tests.",
      "difficulty": "hard",
      "tags": ["selection methods", "GMA", "structured interview", "validity comparison"]
    },
    {
      "type": "cloze-dropdown",
      "id": "d6-se-cd-001",
      "stem": "Complete the passage about utility analysis in selection:\n\nThe Taylor-Russell model estimates the improvement in [BLANK1] that results from using a valid selection test. The model requires three inputs: test validity, [BLANK2], and base rate. As the selection ratio decreases (more selective hiring), utility [BLANK3]. The Brogden-Cronbach-Gleser model extends utility analysis by expressing outcomes in [BLANK4] terms, using the standard deviation of job performance in dollars (SDy). A major challenge in utility analysis is that decision-makers often find utility estimates [BLANK5], possibly due to their large magnitude.",
      "blanks": {
        "BLANK1": {
          "options": ["selection ratio", "success rate among those hired", "number of applicants", "test reliability"],
          "correct": "success rate among those hired"
        },
        "BLANK2": {
          "options": ["selection ratio", "test cost", "number of items", "organizational size"],
          "correct": "selection ratio"
        },
        "BLANK3": {
          "options": ["decreases", "increases", "remains constant", "becomes irrelevant"],
          "correct": "increases"
        },
        "BLANK4": {
          "options": ["psychological", "statistical", "dollar/monetary", "percentage"],
          "correct": "dollar/monetary"
        },
        "BLANK5": {
          "options": ["compelling and actionable", "not credible or difficult to accept", "irrelevant to decision-making", "perfectly accurate"],
          "correct": "not credible or difficult to accept"
        }
      },
      "explanation": "The Taylor-Russell model shows how validity improves the proportion of successful hires (success rate). It uses three inputs: validity, selection ratio, and base rate. More selective hiring (lower selection ratio) increases utility because only the highest-scoring applicants are selected. The BCG model expresses utility in dollar terms. Research by Latham and Whyte showed that even when presented with utility data, managers often find the large dollar estimates not credible, undermining the practical impact of utility analysis on organizational decision-making.",
      "difficulty": "hard",
      "tags": ["utility analysis", "Taylor-Russell", "BCG model", "decision-making"]
    },
    {
      "type": "cloze-dropdown",
      "id": "d6-se-cd-002",
      "stem": "Complete the passage about legal and professional standards for selection:\n\nThe [BLANK1] (1978) established guidelines for validating employee selection procedures in the United States. Under Title VII of the Civil Rights Act, employment tests that have [BLANK2] against a protected group must be shown to be job-related and consistent with business necessity. The [BLANK3] specifically prohibited the practice of race-norming (adjusting scores based on group membership). The SIOP Principles for the Validation and Use of Personnel Selection Procedures recommends that organizations document [BLANK4] evidence when conducting validation research. Employers bear the burden of demonstrating validity once [BLANK5] has been established by the plaintiff.",
      "blanks": {
        "BLANK1": {
          "options": ["APA Ethical Principles", "Uniform Guidelines on Employee Selection Procedures", "Americans with Disabilities Act", "SIOP Principles"],
          "correct": "Uniform Guidelines on Employee Selection Procedures"
        },
        "BLANK2": {
          "options": ["adverse impact", "poor reliability", "high cost", "low face validity"],
          "correct": "adverse impact"
        },
        "BLANK3": {
          "options": ["ADA of 1990", "Civil Rights Act of 1991", "Equal Pay Act of 1963", "Age Discrimination Act of 1967"],
          "correct": "Civil Rights Act of 1991"
        },
        "BLANK4": {
          "options": ["only face validity", "multiple types of validity", "only content validity", "only concurrent validity"],
          "correct": "multiple types of validity"
        },
        "BLANK5": {
          "options": ["test unfairness", "adverse impact / prima facie case", "disparate treatment", "low reliability"],
          "correct": "adverse impact / prima facie case"
        }
      },
      "explanation": "The Uniform Guidelines (1978) govern selection validation standards. Under Title VII's disparate impact framework, tests causing adverse impact must be validated. The Civil Rights Act of 1991 explicitly banned race-norming (section 106). SIOP Principles recommend comprehensive validity evidence. The legal framework involves: (1) plaintiff establishes prima facie adverse impact, (2) employer must demonstrate job-relatedness and business necessity, (3) plaintiff can show less discriminatory alternative exists. This burden-shifting framework (from Griggs v. Duke Power, 1971) remains fundamental to employment discrimination law.",
      "difficulty": "hard",
      "tags": ["Uniform Guidelines", "Title VII", "CRA 1991", "legal standards"]
    },
    {
      "type": "bowtie",
      "id": "d6-se-bt-001",
      "stem": "An organization uses a cognitive ability test for hiring that has a validity coefficient of .50 against job performance. However, data show that the selection rate for Black applicants is 25% while the rate for White applicants is 60%.",
      "centerQuestion": "What primary legal/psychometric concern does this situation present?",
      "leftColumn": {
        "label": "Contributing Factors",
        "options": [
          {"id": "L1", "text": "Approximately 1 standard deviation mean difference in cognitive test scores between racial groups"},
          {"id": "L2", "text": "High validity does not eliminate adverse impact when group mean differences exist"},
          {"id": "L3", "text": "Top-down selection amplifies the effect of group mean differences"},
          {"id": "L4", "text": "The test has zero validity for minority applicants"}
        ],
        "correctAnswers": ["L1", "L2", "L3"]
      },
      "rightColumn": {
        "label": "Possible Organizational Responses",
        "options": [
          {"id": "R1", "text": "Explore supplementing with valid predictors that show smaller group differences"},
          {"id": "R2", "text": "Consider banding procedures to allow diversity within score bands"},
          {"id": "R3", "text": "Simply eliminate the test regardless of its validity"},
          {"id": "R4", "text": "Document validity evidence and explore less adverse alternatives as required by law"}
        ],
        "correctAnswers": ["R1", "R2", "R4"]
      },
      "centerOptions": [
        {"id": "C1", "text": "Adverse impact (selection rate ratio of .42, violating four-fifths rule)"},
        {"id": "C2", "text": "Predictive bias against Black applicants"},
        {"id": "C3", "text": "Low test validity"},
        {"id": "C4", "text": "Content validity deficiency"}
      ],
      "correctCenter": "C1",
      "explanation": "The selection rate ratio is 25/60 = .42, well below the .80 threshold of the four-fifths rule, indicating adverse impact. This creates a legal obligation to demonstrate job-relatedness. Contributing factors include the well-documented ~1 SD group mean difference on cognitive tests, which produces adverse impact even with valid tests. High validity does not immunize against adverse impact claims. Note: This is NOT predictive bias—research shows cognitive tests typically overpredict (not underpredict) minority performance. Responses should include exploring valid alternatives with less adverse impact, as required under the CRA framework.",
      "difficulty": "hard",
      "tags": ["adverse impact", "four-fifths rule", "cognitive ability", "diversity-validity dilemma"]
    },
    {
      "type": "bowtie",
      "id": "d6-se-bt-002",
      "stem": "A small organization (N = 30 employees) wants to validate a new selection test. They administer the test to current employees and correlate scores with supervisor ratings, obtaining r = .28 (not statistically significant at p < .05).",
      "centerQuestion": "What is the primary methodological limitation of this validation study?",
      "leftColumn": {
        "label": "Threats to Validity in This Design",
        "options": [
          {"id": "L1", "text": "Low statistical power due to small sample size"},
          {"id": "L2", "text": "Range restriction on the predictor (current employees are a selected group)"},
          {"id": "L3", "text": "Concurrent rather than predictive design may not reflect applicant conditions"},
          {"id": "L4", "text": "Supervisor ratings may be contaminated by knowledge of test scores"}
        ],
        "correctAnswers": ["L1", "L2", "L3"]
      },
      "rightColumn": {
        "label": "Appropriate Alternatives for This Organization",
        "options": [
          {"id": "R1", "text": "Use validity generalization evidence from meta-analyses for similar tests and jobs"},
          {"id": "R2", "text": "Conduct content validation using job analysis and expert judgment"},
          {"id": "R3", "text": "Conclude the test is invalid and abandon it"},
          {"id": "R4", "text": "Form a consortium with similar organizations to increase sample size"}
        ],
        "correctAnswers": ["R1", "R2", "R4"]
      },
      "centerOptions": [
        {"id": "C1", "text": "Inadequate statistical power to detect a meaningful validity coefficient"},
        {"id": "C2", "text": "The test is definitively invalid"},
        {"id": "C3", "text": "The criterion measure is unreliable"},
        {"id": "C4", "text": "Adverse impact against protected groups"}
      ],
      "correctCenter": "C1",
      "explanation": "With N = 30, statistical power to detect a true validity of .30 is approximately .30-.40—meaning the study would fail to find significance more than half the time even if the test is truly valid. The non-significant result should not be interpreted as evidence of invalidity. Additionally, range restriction (current employees are a selected subsample) and concurrent design limitations further complicate interpretation. For small organizations, validity generalization evidence, content validation, and consortium studies are more appropriate strategies than underpowered local criterion-related studies.",
      "difficulty": "hard",
      "tags": ["statistical power", "small sample", "validation alternatives", "validity generalization"]
    },
    {
      "type": "highlight",
      "id": "d6-se-hl-001",
      "stem": "An I/O psychologist is reviewing principles of selection test evaluation. Highlight the THREE accurate statements:",
      "passage": "(A) Validity generalization research has shown that cognitive ability test validity is highly situationally specific, varying dramatically across jobs. (B) The correction for attenuation due to criterion unreliability provides an estimate of what validity would be with a perfectly reliable criterion. (C) Incremental validity refers to the increase in prediction obtained by adding a new predictor to an existing selection battery. (D) A selection test can be valid even if it is not reliable. (E) Content validity evidence is established through expert judgment rather than correlation coefficients. (F) Construct validity is relevant only for academic research, not practical selection.",
      "correctHighlights": ["B", "C", "E"],
      "explanation": "Accurate: (B) The correction for attenuation estimates validity with a perfect criterion—useful for understanding theoretical limits. (C) Incremental validity is the additional predictive power of a new predictor beyond existing ones. (E) Content validity relies on expert judgment of test-job content overlap rather than statistical analysis. Incorrect: (A) VG research shows the opposite—cognitive test validity generalizes across settings. (D) Reliability is a necessary (but not sufficient) condition for validity—an unreliable test cannot be valid. (F) Construct validity is relevant in all testing contexts, including selection, as recognized by the unitarian view of validity.",
      "difficulty": "hard",
      "tags": ["validity principles", "incremental validity", "content validity", "reliability"]
    },
    {
      "type": "highlight",
      "id": "d6-se-hl-002",
      "stem": "A selection consultant is advising on test evaluation practices. Highlight the FOUR accurate statements:",
      "passage": "(A) The Taylor-Russell tables show that utility increases as the selection ratio decreases. (B) Synthetic validity involves inferring test validity for a job by combining validity evidence for job components. (C) Differential item functioning (DIF) is irrelevant to selection test evaluation. (D) Meta-analysis combines results across studies to estimate true population validity. (E) Face validity is the most important technical consideration in test evaluation. (F) Predictive validity designs avoid the range restriction problem inherent in concurrent designs. (G) The standard error of measurement indicates the band of error around an individual's observed score.",
      "correctHighlights": ["A", "B", "D", "G"],
      "explanation": "Accurate: (A) Taylor-Russell tables show more selective hiring increases the proportion of successful hires. (B) Synthetic (or job component) validity combines validity evidence for individual job elements. (D) Meta-analysis aggregates findings to estimate population parameters corrected for artifacts. (G) SEM defines the confidence interval around an observed score. Incorrect: (C) DIF analysis is important for detecting item bias across groups. (E) Face validity is not a technical validity standard—it relates to examinee perceptions. (F) Predictive designs can also have range restriction if applicants self-select out of the process.",
      "difficulty": "hard",
      "tags": ["utility", "synthetic validity", "meta-analysis", "SEM"]
    },
    {
      "type": "drag-drop-ordered",
      "id": "d6-se-do-001",
      "stem": "Arrange the steps of a predictive criterion-related validation study in correct sequence:",
      "options": [
        {"id": "opt1", "text": "Correlate predictor scores with criterion scores to determine validity"},
        {"id": "opt2", "text": "Conduct job analysis to identify important KSAOs"},
        {"id": "opt3", "text": "Collect criterion data (e.g., performance ratings) after an appropriate time period"},
        {"id": "opt4", "text": "Select or develop a test that measures identified KSAOs"},
        {"id": "opt5", "text": "Administer the test to all applicants and hire without using test scores"}
      ],
      "correctOrder": ["opt2", "opt4", "opt5", "opt3", "opt1"],
      "explanation": "A predictive validity study follows this sequence: (1) Job analysis—identify critical KSAOs for the position; (2) Test selection/development—choose or create measures of those KSAOs; (3) Administration—test all applicants and hire based on other criteria (not the test); (4) Criterion collection—gather job performance data after employees have had sufficient time to demonstrate performance; (5) Correlation—analyze the relationship between predictor and criterion scores. Hiring without using test scores avoids range restriction on the predictor, which is the key advantage of predictive over concurrent designs.",
      "difficulty": "hard",
      "tags": ["predictive validity", "validation study", "job analysis"]
    },
    {
      "type": "drag-drop-ordered",
      "id": "d6-se-do-002",
      "stem": "Arrange the legal burden-shifting framework for disparate impact cases (Griggs framework) in correct order:",
      "options": [
        {"id": "opt1", "text": "Employer demonstrates the test is job-related and consistent with business necessity"},
        {"id": "opt2", "text": "Plaintiff establishes prima facie case of adverse impact (e.g., four-fifths rule violation)"},
        {"id": "opt3", "text": "Plaintiff shows an alternative selection method with less adverse impact exists"},
        {"id": "opt4", "text": "Court determines whether employer's justification is sufficient"}
      ],
      "correctOrder": ["opt2", "opt1", "opt4", "opt3"],
      "explanation": "The disparate impact framework (Griggs v. Duke Power, 1971; codified by CRA 1991): (1) Plaintiff establishes prima facie adverse impact through statistical evidence (four-fifths rule, statistical significance); (2) Burden shifts to employer to demonstrate job-relatedness and business necessity through validity evidence; (3) Court evaluates the employer's justification; (4) Even if employer demonstrates validity, plaintiff can prevail by showing an equally valid alternative procedure with less adverse impact exists and employer refused to adopt it. This framework balances anti-discrimination goals with employers' legitimate needs for valid selection.",
      "difficulty": "hard",
      "tags": ["Griggs", "disparate impact", "burden-shifting", "legal framework"]
    },
    {
      "type": "trend-analysis",
      "id": "d6-se-ta-001",
      "stem": "Review data from a meta-analysis comparing the criterion-related validity of different selection methods:",
      "data": {
        "labels": ["GMA tests", "Structured interviews", "Work sample tests", "Conscientiousness", "Unstructured interviews", "Reference checks", "Years of education", "Graphology"],
        "datasets": [
          {
            "name": "Mean corrected validity coefficient (r)",
            "values": [0.51, 0.51, 0.54, 0.31, 0.38, 0.26, 0.10, 0.02]
          }
        ]
      },
      "questions": [
        {
          "id": "q1",
          "question": "Which selection methods show the highest validity for predicting job performance?",
          "options": [
            {"id": "a", "text": "Reference checks and years of education"},
            {"id": "b", "text": "Work sample tests, GMA tests, and structured interviews"},
            {"id": "c", "text": "Graphology and unstructured interviews"},
            {"id": "d", "text": "Conscientiousness alone outperforms all other methods"}
          ],
          "correctAnswer": "b"
        },
        {
          "id": "q2",
          "question": "What do these data suggest about the relative value of structuring interviews?",
          "options": [
            {"id": "a", "text": "Structure does not improve interview validity"},
            {"id": "b", "text": "Structured interviews (.51) are substantially more valid than unstructured interviews (.38)"},
            {"id": "c", "text": "Unstructured interviews are more valid because they allow flexibility"},
            {"id": "d", "text": "Both interview types have identical validity"}
          ],
          "correctAnswer": "b"
        }
      ],
      "explanation": "Schmidt and Hunter's meta-analysis remains influential: Work samples (.54), GMA (.51), and structured interviews (.51) are the most valid single predictors. Structuring interviews (standardized questions, anchored rating scales, trained interviewers) dramatically improves validity from .38 to .51. Conscientiousness (.31) adds incremental validity beyond GMA. Years of education (.10) and reference checks (.26) have modest validity. Graphology (.02) has essentially zero validity—it persists in some countries despite evidence. These findings support using multiple valid methods in combination.",
      "difficulty": "hard",
      "tags": ["meta-analysis", "selection validity", "Schmidt Hunter", "structured interviews"]
    },
    {
      "type": "trend-analysis",
      "id": "d6-se-ta-002",
      "stem": "Examine data showing the relationship between selection ratio, test validity, and success rate (from Taylor-Russell tables, base rate = .50):",
      "data": {
        "labels": ["SR = .90", "SR = .70", "SR = .50", "SR = .30", "SR = .10"],
        "datasets": [
          {
            "name": "Success rate with validity r = .20",
            "values": [0.53, 0.56, 0.60, 0.65, 0.74]
          },
          {
            "name": "Success rate with validity r = .50",
            "values": [0.56, 0.63, 0.70, 0.78, 0.90]
          }
        ]
      },
      "questions": [
        {
          "id": "q1",
          "question": "What happens to the success rate as the selection ratio becomes more favorable (lower)?",
          "options": [
            {"id": "a", "text": "Success rate decreases"},
            {"id": "b", "text": "Success rate increases substantially"},
            {"id": "c", "text": "Success rate remains unchanged"},
            {"id": "d", "text": "Success rate first increases then decreases"}
          ],
          "correctAnswer": "b"
        },
        {
          "id": "q2",
          "question": "What practical conclusion can be drawn from comparing the two validity levels?",
          "options": [
            {"id": "a", "text": "Higher validity provides no additional benefit when selection is stringent"},
            {"id": "b", "text": "Even a modest validity of .20 can meaningfully improve selection when the ratio is low"},
            {"id": "c", "text": "Validity is irrelevant when many applicants are available"},
            {"id": "d", "text": "Both validity levels produce identical success rates"}
          ],
          "correctAnswer": "b"
        }
      ],
      "explanation": "Taylor-Russell tables demonstrate key selection principles: (1) Lower selection ratios (more applicants per position) increase the success rate because only top scorers are selected. (2) Higher validity always produces better outcomes. (3) Even modest validity (.20) can be practically useful when the selection ratio is favorable—at SR = .10, a validity of .20 raises the success rate from 50% to 74%. (4) The interaction between validity and selection ratio is multiplicative—high validity with a low selection ratio produces the greatest gains (90% success rate with r = .50, SR = .10).",
      "difficulty": "hard",
      "tags": ["Taylor-Russell", "selection ratio", "utility", "base rate"]
    },
    {
      "type": "drag-drop-categorize",
      "id": "d6-se-dc-001",
      "stem": "Categorize each concept according to the type of validity evidence it represents:",
      "categories": [
        {"id": "cat1", "label": "Criterion-Related Validity Evidence"},
        {"id": "cat2", "label": "Content Validity Evidence"},
        {"id": "cat3", "label": "Construct Validity Evidence"}
      ],
      "items": [
        {"id": "item1", "text": "Correlation between test scores and supervisor performance ratings"},
        {"id": "item2", "text": "SME ratings of test-job content overlap"},
        {"id": "item3", "text": "Convergent and discriminant validity via MTMM matrix"},
        {"id": "item4", "text": "Factor analysis confirming hypothesized test structure"},
        {"id": "item5", "text": "Predictive study correlating applicant scores with later job success"},
        {"id": "item6", "text": "Job analysis demonstrating that test tasks mirror job tasks"}
      ],
      "correctCategorization": {
        "cat1": ["item1", "item5"],
        "cat2": ["item2", "item6"],
        "cat3": ["item3", "item4"]
      },
      "explanation": "Criterion-related: Correlating predictor scores with criterion measures (concurrent or predictive designs). Content: Expert judgment of test-job overlap, job analysis demonstrating representativeness of test content. Construct: Examining the internal structure (factor analysis) and pattern of relationships with other measures (convergent/discriminant via MTMM). Modern validity theory (Messick) views these as facets of a unitary construct validity concept, but the practical distinctions remain useful for planning validation strategies.",
      "difficulty": "hard",
      "tags": ["validity evidence types", "criterion", "content", "construct"]
    },
    {
      "type": "drag-drop-categorize",
      "id": "d6-se-dc-002",
      "stem": "Categorize each statistical artifact according to its effect on observed validity coefficients:",
      "categories": [
        {"id": "cat1", "label": "Attenuates (Reduces) Observed Validity"},
        {"id": "cat2", "label": "May Inflate Observed Validity"},
        {"id": "cat3", "label": "Does Not Systematically Affect Validity"}
      ],
      "items": [
        {"id": "item1", "text": "Range restriction on the predictor"},
        {"id": "item2", "text": "Criterion unreliability"},
        {"id": "item3", "text": "Criterion contamination (raters know predictor scores)"},
        {"id": "item4", "text": "Sampling error in small studies"},
        {"id": "item5", "text": "Predictor unreliability"},
        {"id": "item6", "text": "Using a dichotomized criterion instead of continuous"}
      ],
      "correctCategorization": {
        "cat1": ["item1", "item2", "item5", "item6"],
        "cat2": ["item3"],
        "cat3": ["item4"]
      },
      "explanation": "Attenuating artifacts: Range restriction reduces predictor variance, lowering correlations. Criterion and predictor unreliability introduce measurement error, attenuating observed correlations. Dichotomizing a continuous criterion reduces information and attenuates validity. Inflating artifact: Criterion contamination (when raters know predictor scores and this influences ratings) can artificially inflate validity. Sampling error: In small samples, observed validity may be higher or lower than true validity—the effect is random, not systematic, so it does not consistently inflate or attenuate. These corrections are central to validity generalization methodology.",
      "difficulty": "hard",
      "tags": ["statistical artifacts", "attenuation", "range restriction", "criterion contamination"]
    }
  ]
}